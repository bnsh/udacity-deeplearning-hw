{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    words = set(text)\n",
    "    vocab_to_int = {word:idx for idx, word in enumerate(sorted(words))}\n",
    "    int_to_vocab = {idx:word for idx, word in enumerate(sorted(words))}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return {\n",
    "        \".\": \"||Period||\",\n",
    "        \",\": \"||Comma||\",\n",
    "        \"\\\"\": \"||Quotation_mark||\",\n",
    "        \";\": \"||Semicolon||\",\n",
    "        \"!\": \"||Exclamation_mark||\",\n",
    "        \"?\": \"||Question_mark||\",\n",
    "        \"(\": \"||Left_Parentheses||\",\n",
    "        \")\": \"||Right_Parentheses||\",\n",
    "        \"--\": \"||Dash||\",\n",
    "        \"\\n\": \"||Return||\"\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_word_indices = tf.placeholder(name=\"input\", shape=(None, None), dtype=tf.int32)\n",
    "    target_word_indices = tf.placeholder(name=\"target\", shape=(None, None), dtype=tf.int32)\n",
    "    learning_rate = tf.placeholder(name=\"learning_rate\", dtype=tf.float32)\n",
    "    return input_word_indices, target_word_indices, learning_rate\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    layers = 1\n",
    "    single_cell = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "    multi_cell = tf.contrib.rnn.MultiRNNCell([single_cell] * layers)\n",
    "    initial_state = tf.identity(multi_cell.zero_state(batch_size=batch_size, dtype=tf.float32), name=\"initial_state\")\n",
    "    return multi_cell, initial_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lookup_table = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    return tf.nn.embedding_lookup(lookup_table, input_data)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    return outputs, tf.identity(final_state, name=\"final_state\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    embedding = get_embed(input_data, vocab_size, embed_dim)\n",
    "    rnn_output, final_state = build_rnn(cell, embedding)\n",
    "    dense_output = tf.layers.dense(rnn_output, vocab_size)\n",
    "    return dense_output, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # So. batch_size refers to what?? I don't know why they\n",
    "    # made it so that two things in the example are _both_ 3 (the output data above\n",
    "    # is 3x2x3x2) Ugh. OK, at least I get the first 2 and the second 2.\n",
    "    # is 3x(i/t)x3xsequence (i/t) is basically input vs target\n",
    "    # I suppose the easiest way to replicate the behaviour I see above is this:\n",
    "    # We get passed in 5000, 128, 5 and are expected to produce 7x2x128x5. Fine.\n",
    "    num_batches = len(int_text) // (batch_size * seq_length)\n",
    "    \n",
    "    realsz = num_batches * (batch_size * seq_length)\n",
    "    input_data = int_text[0:realsz]\n",
    "    target_data = input_data[1:] + int_text[0:1]\n",
    "    \n",
    "    def sequence(data):\n",
    "        return [data[x:x+seq_length] for x in range(0, realsz, seq_length)]\n",
    "\n",
    "    input_sequences = sequence(input_data)\n",
    "    target_sequences = sequence(target_data)\n",
    "\n",
    "    # So, we need batches of these sequences, but sort of transposed. So,\n",
    "    # instead, let's make it by num_batches, then we'll zip.\n",
    "    def batch(data):\n",
    "        return [_ for _ in zip(*[data[x:x+num_batches] for x in range(0, len(data), num_batches)])]\n",
    "\n",
    "    input_batch = batch(input_sequences)\n",
    "    target_batch = batch(target_sequences)\n",
    "\n",
    "    stitched = [_ for _ in zip(input_batch, target_batch)]\n",
    "    \n",
    "    return np.array(stitched)\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 100\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "# RNN Size\n",
    "rnn_size = 16\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 8\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 1\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/269   train_loss = 8.823\n",
      "Epoch   0 Batch    1/269   train_loss = 8.797\n",
      "Epoch   0 Batch    2/269   train_loss = 8.768\n",
      "Epoch   0 Batch    3/269   train_loss = 8.714\n",
      "Epoch   0 Batch    4/269   train_loss = 8.679\n",
      "Epoch   0 Batch    5/269   train_loss = 8.605\n",
      "Epoch   0 Batch    6/269   train_loss = 8.500\n",
      "Epoch   0 Batch    7/269   train_loss = 8.391\n",
      "Epoch   0 Batch    8/269   train_loss = 8.294\n",
      "Epoch   0 Batch    9/269   train_loss = 8.100\n",
      "Epoch   0 Batch   10/269   train_loss = 8.032\n",
      "Epoch   0 Batch   11/269   train_loss = 7.868\n",
      "Epoch   0 Batch   12/269   train_loss = 7.699\n",
      "Epoch   0 Batch   13/269   train_loss = 7.583\n",
      "Epoch   0 Batch   14/269   train_loss = 7.284\n",
      "Epoch   0 Batch   15/269   train_loss = 7.243\n",
      "Epoch   0 Batch   16/269   train_loss = 7.126\n",
      "Epoch   0 Batch   17/269   train_loss = 6.993\n",
      "Epoch   0 Batch   18/269   train_loss = 6.797\n",
      "Epoch   0 Batch   19/269   train_loss = 6.706\n",
      "Epoch   0 Batch   20/269   train_loss = 6.711\n",
      "Epoch   0 Batch   21/269   train_loss = 6.568\n",
      "Epoch   0 Batch   22/269   train_loss = 6.250\n",
      "Epoch   0 Batch   23/269   train_loss = 6.382\n",
      "Epoch   0 Batch   24/269   train_loss = 6.406\n",
      "Epoch   0 Batch   25/269   train_loss = 6.267\n",
      "Epoch   0 Batch   26/269   train_loss = 6.226\n",
      "Epoch   0 Batch   27/269   train_loss = 6.328\n",
      "Epoch   0 Batch   28/269   train_loss = 6.217\n",
      "Epoch   0 Batch   29/269   train_loss = 6.417\n",
      "Epoch   0 Batch   30/269   train_loss = 6.141\n",
      "Epoch   0 Batch   31/269   train_loss = 6.161\n",
      "Epoch   0 Batch   32/269   train_loss = 5.877\n",
      "Epoch   0 Batch   33/269   train_loss = 6.052\n",
      "Epoch   0 Batch   34/269   train_loss = 6.179\n",
      "Epoch   0 Batch   35/269   train_loss = 6.210\n",
      "Epoch   0 Batch   36/269   train_loss = 6.352\n",
      "Epoch   0 Batch   37/269   train_loss = 6.050\n",
      "Epoch   0 Batch   38/269   train_loss = 6.283\n",
      "Epoch   0 Batch   39/269   train_loss = 6.389\n",
      "Epoch   0 Batch   40/269   train_loss = 6.211\n",
      "Epoch   0 Batch   41/269   train_loss = 6.093\n",
      "Epoch   0 Batch   42/269   train_loss = 6.829\n",
      "Epoch   0 Batch   43/269   train_loss = 6.489\n",
      "Epoch   0 Batch   44/269   train_loss = 6.177\n",
      "Epoch   0 Batch   45/269   train_loss = 6.123\n",
      "Epoch   0 Batch   46/269   train_loss = 6.545\n",
      "Epoch   0 Batch   47/269   train_loss = 6.055\n",
      "Epoch   0 Batch   48/269   train_loss = 6.456\n",
      "Epoch   0 Batch   49/269   train_loss = 6.184\n",
      "Epoch   0 Batch   50/269   train_loss = 6.529\n",
      "Epoch   0 Batch   51/269   train_loss = 6.132\n",
      "Epoch   0 Batch   52/269   train_loss = 6.380\n",
      "Epoch   0 Batch   53/269   train_loss = 6.168\n",
      "Epoch   0 Batch   54/269   train_loss = 6.207\n",
      "Epoch   0 Batch   55/269   train_loss = 6.345\n",
      "Epoch   0 Batch   56/269   train_loss = 6.059\n",
      "Epoch   0 Batch   57/269   train_loss = 6.117\n",
      "Epoch   0 Batch   58/269   train_loss = 6.579\n",
      "Epoch   0 Batch   59/269   train_loss = 6.074\n",
      "Epoch   0 Batch   60/269   train_loss = 6.299\n",
      "Epoch   0 Batch   61/269   train_loss = 6.350\n",
      "Epoch   0 Batch   62/269   train_loss = 6.245\n",
      "Epoch   0 Batch   63/269   train_loss = 6.390\n",
      "Epoch   0 Batch   64/269   train_loss = 6.087\n",
      "Epoch   0 Batch   65/269   train_loss = 6.164\n",
      "Epoch   0 Batch   66/269   train_loss = 6.368\n",
      "Epoch   0 Batch   67/269   train_loss = 6.031\n",
      "Epoch   0 Batch   68/269   train_loss = 6.424\n",
      "Epoch   0 Batch   69/269   train_loss = 6.340\n",
      "Epoch   0 Batch   70/269   train_loss = 6.003\n",
      "Epoch   0 Batch   71/269   train_loss = 6.174\n",
      "Epoch   0 Batch   72/269   train_loss = 6.081\n",
      "Epoch   0 Batch   73/269   train_loss = 6.123\n",
      "Epoch   0 Batch   74/269   train_loss = 5.999\n",
      "Epoch   0 Batch   75/269   train_loss = 6.387\n",
      "Epoch   0 Batch   76/269   train_loss = 6.259\n",
      "Epoch   0 Batch   77/269   train_loss = 5.866\n",
      "Epoch   0 Batch   78/269   train_loss = 6.286\n",
      "Epoch   0 Batch   79/269   train_loss = 5.977\n",
      "Epoch   0 Batch   80/269   train_loss = 5.861\n",
      "Epoch   0 Batch   81/269   train_loss = 5.717\n",
      "Epoch   0 Batch   82/269   train_loss = 6.029\n",
      "Epoch   0 Batch   83/269   train_loss = 6.158\n",
      "Epoch   0 Batch   84/269   train_loss = 6.199\n",
      "Epoch   0 Batch   85/269   train_loss = 6.269\n",
      "Epoch   0 Batch   86/269   train_loss = 6.122\n",
      "Epoch   0 Batch   87/269   train_loss = 5.987\n",
      "Epoch   0 Batch   88/269   train_loss = 5.934\n",
      "Epoch   0 Batch   89/269   train_loss = 6.004\n",
      "Epoch   0 Batch   90/269   train_loss = 6.241\n",
      "Epoch   0 Batch   91/269   train_loss = 6.109\n",
      "Epoch   0 Batch   92/269   train_loss = 6.189\n",
      "Epoch   0 Batch   93/269   train_loss = 5.514\n",
      "Epoch   0 Batch   94/269   train_loss = 6.107\n",
      "Epoch   0 Batch   95/269   train_loss = 5.960\n",
      "Epoch   0 Batch   96/269   train_loss = 5.884\n",
      "Epoch   0 Batch   97/269   train_loss = 5.998\n",
      "Epoch   0 Batch   98/269   train_loss = 5.901\n",
      "Epoch   0 Batch   99/269   train_loss = 6.181\n",
      "Epoch   0 Batch  100/269   train_loss = 5.784\n",
      "Epoch   0 Batch  101/269   train_loss = 5.987\n",
      "Epoch   0 Batch  102/269   train_loss = 6.053\n",
      "Epoch   0 Batch  103/269   train_loss = 5.999\n",
      "Epoch   0 Batch  104/269   train_loss = 6.064\n",
      "Epoch   0 Batch  105/269   train_loss = 6.014\n",
      "Epoch   0 Batch  106/269   train_loss = 5.898\n",
      "Epoch   0 Batch  107/269   train_loss = 6.225\n",
      "Epoch   0 Batch  108/269   train_loss = 5.977\n",
      "Epoch   0 Batch  109/269   train_loss = 5.959\n",
      "Epoch   0 Batch  110/269   train_loss = 6.137\n",
      "Epoch   0 Batch  111/269   train_loss = 5.928\n",
      "Epoch   0 Batch  112/269   train_loss = 6.067\n",
      "Epoch   0 Batch  113/269   train_loss = 5.713\n",
      "Epoch   0 Batch  114/269   train_loss = 5.876\n",
      "Epoch   0 Batch  115/269   train_loss = 5.872\n",
      "Epoch   0 Batch  116/269   train_loss = 5.743\n",
      "Epoch   0 Batch  117/269   train_loss = 5.759\n",
      "Epoch   0 Batch  118/269   train_loss = 5.934\n",
      "Epoch   0 Batch  119/269   train_loss = 6.035\n",
      "Epoch   0 Batch  120/269   train_loss = 6.012\n",
      "Epoch   0 Batch  121/269   train_loss = 5.880\n",
      "Epoch   0 Batch  122/269   train_loss = 5.837\n",
      "Epoch   0 Batch  123/269   train_loss = 5.834\n",
      "Epoch   0 Batch  124/269   train_loss = 5.960\n",
      "Epoch   0 Batch  125/269   train_loss = 5.682\n",
      "Epoch   0 Batch  126/269   train_loss = 5.369\n",
      "Epoch   0 Batch  127/269   train_loss = 5.850\n",
      "Epoch   0 Batch  128/269   train_loss = 5.883\n",
      "Epoch   0 Batch  129/269   train_loss = 6.080\n",
      "Epoch   0 Batch  130/269   train_loss = 6.043\n",
      "Epoch   0 Batch  131/269   train_loss = 5.957\n",
      "Epoch   0 Batch  132/269   train_loss = 5.830\n",
      "Epoch   0 Batch  133/269   train_loss = 5.555\n",
      "Epoch   0 Batch  134/269   train_loss = 5.846\n",
      "Epoch   0 Batch  135/269   train_loss = 5.487\n",
      "Epoch   0 Batch  136/269   train_loss = 5.645\n",
      "Epoch   0 Batch  137/269   train_loss = 5.876\n",
      "Epoch   0 Batch  138/269   train_loss = 5.818\n",
      "Epoch   0 Batch  139/269   train_loss = 5.859\n",
      "Epoch   0 Batch  140/269   train_loss = 5.878\n",
      "Epoch   0 Batch  141/269   train_loss = 5.796\n",
      "Epoch   0 Batch  142/269   train_loss = 5.647\n",
      "Epoch   0 Batch  143/269   train_loss = 5.859\n",
      "Epoch   0 Batch  144/269   train_loss = 5.887\n",
      "Epoch   0 Batch  145/269   train_loss = 5.528\n",
      "Epoch   0 Batch  146/269   train_loss = 5.851\n",
      "Epoch   0 Batch  147/269   train_loss = 5.571\n",
      "Epoch   0 Batch  148/269   train_loss = 6.008\n",
      "Epoch   0 Batch  149/269   train_loss = 5.688\n",
      "Epoch   0 Batch  150/269   train_loss = 5.609\n",
      "Epoch   0 Batch  151/269   train_loss = 5.865\n",
      "Epoch   0 Batch  152/269   train_loss = 5.519\n",
      "Epoch   0 Batch  153/269   train_loss = 6.027\n",
      "Epoch   0 Batch  154/269   train_loss = 5.986\n",
      "Epoch   0 Batch  155/269   train_loss = 5.684\n",
      "Epoch   0 Batch  156/269   train_loss = 6.058\n",
      "Epoch   0 Batch  157/269   train_loss = 6.393\n",
      "Epoch   0 Batch  158/269   train_loss = 5.780\n",
      "Epoch   0 Batch  159/269   train_loss = 5.991\n",
      "Epoch   0 Batch  160/269   train_loss = 5.672\n",
      "Epoch   0 Batch  161/269   train_loss = 5.782\n",
      "Epoch   0 Batch  162/269   train_loss = 5.520\n",
      "Epoch   0 Batch  163/269   train_loss = 5.904\n",
      "Epoch   0 Batch  164/269   train_loss = 6.053\n",
      "Epoch   0 Batch  165/269   train_loss = 6.057\n",
      "Epoch   0 Batch  166/269   train_loss = 5.869\n",
      "Epoch   0 Batch  167/269   train_loss = 5.790\n",
      "Epoch   0 Batch  168/269   train_loss = 5.492\n",
      "Epoch   0 Batch  169/269   train_loss = 6.035\n",
      "Epoch   0 Batch  170/269   train_loss = 5.771\n",
      "Epoch   0 Batch  171/269   train_loss = 6.401\n",
      "Epoch   0 Batch  172/269   train_loss = 5.801\n",
      "Epoch   0 Batch  173/269   train_loss = 6.032\n",
      "Epoch   0 Batch  174/269   train_loss = 5.981\n",
      "Epoch   0 Batch  175/269   train_loss = 5.551\n",
      "Epoch   0 Batch  176/269   train_loss = 5.725\n",
      "Epoch   0 Batch  177/269   train_loss = 5.635\n",
      "Epoch   0 Batch  178/269   train_loss = 6.027\n",
      "Epoch   0 Batch  179/269   train_loss = 5.721\n",
      "Epoch   0 Batch  180/269   train_loss = 5.456\n",
      "Epoch   0 Batch  181/269   train_loss = 5.666\n",
      "Epoch   0 Batch  182/269   train_loss = 5.331\n",
      "Epoch   0 Batch  183/269   train_loss = 5.746\n",
      "Epoch   0 Batch  184/269   train_loss = 5.814\n",
      "Epoch   0 Batch  185/269   train_loss = 5.645\n",
      "Epoch   0 Batch  186/269   train_loss = 5.511\n",
      "Epoch   0 Batch  187/269   train_loss = 5.821\n",
      "Epoch   0 Batch  188/269   train_loss = 5.639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  189/269   train_loss = 5.615\n",
      "Epoch   0 Batch  190/269   train_loss = 5.754\n",
      "Epoch   0 Batch  191/269   train_loss = 5.561\n",
      "Epoch   0 Batch  192/269   train_loss = 5.496\n",
      "Epoch   0 Batch  193/269   train_loss = 5.454\n",
      "Epoch   0 Batch  194/269   train_loss = 5.804\n",
      "Epoch   0 Batch  195/269   train_loss = 5.578\n",
      "Epoch   0 Batch  196/269   train_loss = 5.890\n",
      "Epoch   0 Batch  197/269   train_loss = 5.913\n",
      "Epoch   0 Batch  198/269   train_loss = 6.017\n",
      "Epoch   0 Batch  199/269   train_loss = 6.267\n",
      "Epoch   0 Batch  200/269   train_loss = 6.240\n",
      "Epoch   0 Batch  201/269   train_loss = 5.445\n",
      "Epoch   0 Batch  202/269   train_loss = 5.470\n",
      "Epoch   0 Batch  203/269   train_loss = 5.654\n",
      "Epoch   0 Batch  204/269   train_loss = 5.565\n",
      "Epoch   0 Batch  205/269   train_loss = 5.871\n",
      "Epoch   0 Batch  206/269   train_loss = 5.678\n",
      "Epoch   0 Batch  207/269   train_loss = 5.540\n",
      "Epoch   0 Batch  208/269   train_loss = 5.667\n",
      "Epoch   0 Batch  209/269   train_loss = 5.678\n",
      "Epoch   0 Batch  210/269   train_loss = 5.608\n",
      "Epoch   0 Batch  211/269   train_loss = 5.383\n",
      "Epoch   0 Batch  212/269   train_loss = 6.108\n",
      "Epoch   0 Batch  213/269   train_loss = 5.720\n",
      "Epoch   0 Batch  214/269   train_loss = 5.942\n",
      "Epoch   0 Batch  215/269   train_loss = 5.869\n",
      "Epoch   0 Batch  216/269   train_loss = 5.956\n",
      "Epoch   0 Batch  217/269   train_loss = 5.627\n",
      "Epoch   0 Batch  218/269   train_loss = 5.590\n",
      "Epoch   0 Batch  219/269   train_loss = 5.264\n",
      "Epoch   0 Batch  220/269   train_loss = 5.582\n",
      "Epoch   0 Batch  221/269   train_loss = 5.561\n",
      "Epoch   0 Batch  222/269   train_loss = 5.607\n",
      "Epoch   0 Batch  223/269   train_loss = 5.770\n",
      "Epoch   0 Batch  224/269   train_loss = 5.487\n",
      "Epoch   0 Batch  225/269   train_loss = 6.027\n",
      "Epoch   0 Batch  226/269   train_loss = 5.854\n",
      "Epoch   0 Batch  227/269   train_loss = 5.549\n",
      "Epoch   0 Batch  228/269   train_loss = 5.537\n",
      "Epoch   0 Batch  229/269   train_loss = 5.665\n",
      "Epoch   0 Batch  230/269   train_loss = 5.961\n",
      "Epoch   0 Batch  231/269   train_loss = 5.583\n",
      "Epoch   0 Batch  232/269   train_loss = 5.393\n",
      "Epoch   0 Batch  233/269   train_loss = 5.444\n",
      "Epoch   0 Batch  234/269   train_loss = 5.406\n",
      "Epoch   0 Batch  235/269   train_loss = 5.682\n",
      "Epoch   0 Batch  236/269   train_loss = 5.462\n",
      "Epoch   0 Batch  237/269   train_loss = 5.336\n",
      "Epoch   0 Batch  238/269   train_loss = 5.578\n",
      "Epoch   0 Batch  239/269   train_loss = 6.009\n",
      "Epoch   0 Batch  240/269   train_loss = 5.631\n",
      "Epoch   0 Batch  241/269   train_loss = 5.768\n",
      "Epoch   0 Batch  242/269   train_loss = 5.168\n",
      "Epoch   0 Batch  243/269   train_loss = 5.554\n",
      "Epoch   0 Batch  244/269   train_loss = 5.252\n",
      "Epoch   0 Batch  245/269   train_loss = 5.384\n",
      "Epoch   0 Batch  246/269   train_loss = 5.157\n",
      "Epoch   0 Batch  247/269   train_loss = 5.714\n",
      "Epoch   0 Batch  248/269   train_loss = 5.704\n",
      "Epoch   0 Batch  249/269   train_loss = 5.581\n",
      "Epoch   0 Batch  250/269   train_loss = 4.834\n",
      "Epoch   0 Batch  251/269   train_loss = 5.337\n",
      "Epoch   0 Batch  252/269   train_loss = 5.635\n",
      "Epoch   0 Batch  253/269   train_loss = 5.671\n",
      "Epoch   0 Batch  254/269   train_loss = 5.193\n",
      "Epoch   0 Batch  255/269   train_loss = 5.736\n",
      "Epoch   0 Batch  256/269   train_loss = 5.514\n",
      "Epoch   0 Batch  257/269   train_loss = 5.024\n",
      "Epoch   0 Batch  258/269   train_loss = 5.384\n",
      "Epoch   0 Batch  259/269   train_loss = 5.291\n",
      "Epoch   0 Batch  260/269   train_loss = 5.476\n",
      "Epoch   0 Batch  261/269   train_loss = 5.628\n",
      "Epoch   0 Batch  262/269   train_loss = 5.345\n",
      "Epoch   0 Batch  263/269   train_loss = 5.172\n",
      "Epoch   0 Batch  264/269   train_loss = 5.865\n",
      "Epoch   0 Batch  265/269   train_loss = 5.563\n",
      "Epoch   0 Batch  266/269   train_loss = 5.319\n",
      "Epoch   0 Batch  267/269   train_loss = 5.655\n",
      "Epoch   0 Batch  268/269   train_loss = 5.861\n",
      "Epoch   1 Batch    0/269   train_loss = 5.324\n",
      "Epoch   1 Batch    1/269   train_loss = 4.920\n",
      "Epoch   1 Batch    2/269   train_loss = 5.105\n",
      "Epoch   1 Batch    3/269   train_loss = 5.130\n",
      "Epoch   1 Batch    4/269   train_loss = 5.590\n",
      "Epoch   1 Batch    5/269   train_loss = 5.461\n",
      "Epoch   1 Batch    6/269   train_loss = 5.446\n",
      "Epoch   1 Batch    7/269   train_loss = 5.286\n",
      "Epoch   1 Batch    8/269   train_loss = 5.318\n",
      "Epoch   1 Batch    9/269   train_loss = 5.163\n",
      "Epoch   1 Batch   10/269   train_loss = 5.440\n",
      "Epoch   1 Batch   11/269   train_loss = 5.297\n",
      "Epoch   1 Batch   12/269   train_loss = 5.304\n",
      "Epoch   1 Batch   13/269   train_loss = 5.376\n",
      "Epoch   1 Batch   14/269   train_loss = 5.092\n",
      "Epoch   1 Batch   15/269   train_loss = 5.444\n",
      "Epoch   1 Batch   16/269   train_loss = 5.338\n",
      "Epoch   1 Batch   17/269   train_loss = 5.401\n",
      "Epoch   1 Batch   18/269   train_loss = 5.324\n",
      "Epoch   1 Batch   19/269   train_loss = 5.344\n",
      "Epoch   1 Batch   20/269   train_loss = 5.560\n",
      "Epoch   1 Batch   21/269   train_loss = 5.397\n",
      "Epoch   1 Batch   22/269   train_loss = 5.050\n",
      "Epoch   1 Batch   23/269   train_loss = 5.195\n",
      "Epoch   1 Batch   24/269   train_loss = 5.308\n",
      "Epoch   1 Batch   25/269   train_loss = 5.341\n",
      "Epoch   1 Batch   26/269   train_loss = 5.266\n",
      "Epoch   1 Batch   27/269   train_loss = 5.258\n",
      "Epoch   1 Batch   28/269   train_loss = 5.270\n",
      "Epoch   1 Batch   29/269   train_loss = 5.440\n",
      "Epoch   1 Batch   30/269   train_loss = 5.213\n",
      "Epoch   1 Batch   31/269   train_loss = 5.143\n",
      "Epoch   1 Batch   32/269   train_loss = 4.907\n",
      "Epoch   1 Batch   33/269   train_loss = 5.008\n",
      "Epoch   1 Batch   34/269   train_loss = 5.284\n",
      "Epoch   1 Batch   35/269   train_loss = 5.243\n",
      "Epoch   1 Batch   36/269   train_loss = 5.146\n",
      "Epoch   1 Batch   37/269   train_loss = 5.074\n",
      "Epoch   1 Batch   38/269   train_loss = 5.174\n",
      "Epoch   1 Batch   39/269   train_loss = 5.323\n",
      "Epoch   1 Batch   40/269   train_loss = 4.987\n",
      "Epoch   1 Batch   41/269   train_loss = 5.051\n",
      "Epoch   1 Batch   42/269   train_loss = 5.799\n",
      "Epoch   1 Batch   43/269   train_loss = 5.385\n",
      "Epoch   1 Batch   44/269   train_loss = 5.163\n",
      "Epoch   1 Batch   45/269   train_loss = 4.998\n",
      "Epoch   1 Batch   46/269   train_loss = 5.569\n",
      "Epoch   1 Batch   47/269   train_loss = 4.965\n",
      "Epoch   1 Batch   48/269   train_loss = 5.506\n",
      "Epoch   1 Batch   49/269   train_loss = 5.149\n",
      "Epoch   1 Batch   50/269   train_loss = 5.557\n",
      "Epoch   1 Batch   51/269   train_loss = 5.194\n",
      "Epoch   1 Batch   52/269   train_loss = 5.399\n",
      "Epoch   1 Batch   53/269   train_loss = 5.303\n",
      "Epoch   1 Batch   54/269   train_loss = 5.202\n",
      "Epoch   1 Batch   55/269   train_loss = 5.406\n",
      "Epoch   1 Batch   56/269   train_loss = 5.081\n",
      "Epoch   1 Batch   57/269   train_loss = 5.174\n",
      "Epoch   1 Batch   58/269   train_loss = 5.562\n",
      "Epoch   1 Batch   59/269   train_loss = 5.016\n",
      "Epoch   1 Batch   60/269   train_loss = 5.457\n",
      "Epoch   1 Batch   61/269   train_loss = 5.459\n",
      "Epoch   1 Batch   62/269   train_loss = 5.239\n",
      "Epoch   1 Batch   63/269   train_loss = 5.521\n",
      "Epoch   1 Batch   64/269   train_loss = 5.104\n",
      "Epoch   1 Batch   65/269   train_loss = 5.225\n",
      "Epoch   1 Batch   66/269   train_loss = 5.418\n",
      "Epoch   1 Batch   67/269   train_loss = 5.178\n",
      "Epoch   1 Batch   68/269   train_loss = 5.452\n",
      "Epoch   1 Batch   69/269   train_loss = 5.498\n",
      "Epoch   1 Batch   70/269   train_loss = 4.983\n",
      "Epoch   1 Batch   71/269   train_loss = 5.369\n",
      "Epoch   1 Batch   72/269   train_loss = 5.088\n",
      "Epoch   1 Batch   73/269   train_loss = 5.218\n",
      "Epoch   1 Batch   74/269   train_loss = 5.133\n",
      "Epoch   1 Batch   75/269   train_loss = 5.485\n",
      "Epoch   1 Batch   76/269   train_loss = 5.345\n",
      "Epoch   1 Batch   77/269   train_loss = 4.892\n",
      "Epoch   1 Batch   78/269   train_loss = 5.411\n",
      "Epoch   1 Batch   79/269   train_loss = 5.169\n",
      "Epoch   1 Batch   80/269   train_loss = 4.938\n",
      "Epoch   1 Batch   81/269   train_loss = 4.912\n",
      "Epoch   1 Batch   82/269   train_loss = 5.250\n",
      "Epoch   1 Batch   83/269   train_loss = 5.329\n",
      "Epoch   1 Batch   84/269   train_loss = 5.413\n",
      "Epoch   1 Batch   85/269   train_loss = 5.548\n",
      "Epoch   1 Batch   86/269   train_loss = 5.299\n",
      "Epoch   1 Batch   87/269   train_loss = 5.180\n",
      "Epoch   1 Batch   88/269   train_loss = 5.037\n",
      "Epoch   1 Batch   89/269   train_loss = 5.182\n",
      "Epoch   1 Batch   90/269   train_loss = 5.370\n",
      "Epoch   1 Batch   91/269   train_loss = 5.383\n",
      "Epoch   1 Batch   92/269   train_loss = 5.428\n",
      "Epoch   1 Batch   93/269   train_loss = 4.635\n",
      "Epoch   1 Batch   94/269   train_loss = 5.363\n",
      "Epoch   1 Batch   95/269   train_loss = 5.074\n",
      "Epoch   1 Batch   96/269   train_loss = 5.154\n",
      "Epoch   1 Batch   97/269   train_loss = 5.142\n",
      "Epoch   1 Batch   98/269   train_loss = 5.125\n",
      "Epoch   1 Batch   99/269   train_loss = 5.363\n",
      "Epoch   1 Batch  100/269   train_loss = 5.044\n",
      "Epoch   1 Batch  101/269   train_loss = 5.214\n",
      "Epoch   1 Batch  102/269   train_loss = 5.184\n",
      "Epoch   1 Batch  103/269   train_loss = 5.282\n",
      "Epoch   1 Batch  104/269   train_loss = 5.315\n",
      "Epoch   1 Batch  105/269   train_loss = 5.280\n",
      "Epoch   1 Batch  106/269   train_loss = 5.293\n",
      "Epoch   1 Batch  107/269   train_loss = 5.455\n",
      "Epoch   1 Batch  108/269   train_loss = 5.244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch  109/269   train_loss = 5.191\n",
      "Epoch   1 Batch  110/269   train_loss = 5.268\n",
      "Epoch   1 Batch  111/269   train_loss = 5.285\n",
      "Epoch   1 Batch  112/269   train_loss = 5.381\n",
      "Epoch   1 Batch  113/269   train_loss = 4.924\n",
      "Epoch   1 Batch  114/269   train_loss = 5.074\n",
      "Epoch   1 Batch  115/269   train_loss = 5.143\n",
      "Epoch   1 Batch  116/269   train_loss = 5.002\n",
      "Epoch   1 Batch  117/269   train_loss = 4.953\n",
      "Epoch   1 Batch  118/269   train_loss = 5.246\n",
      "Epoch   1 Batch  119/269   train_loss = 5.143\n",
      "Epoch   1 Batch  120/269   train_loss = 5.215\n",
      "Epoch   1 Batch  121/269   train_loss = 5.129\n",
      "Epoch   1 Batch  122/269   train_loss = 5.183\n",
      "Epoch   1 Batch  123/269   train_loss = 5.007\n",
      "Epoch   1 Batch  124/269   train_loss = 5.229\n",
      "Epoch   1 Batch  125/269   train_loss = 4.857\n",
      "Epoch   1 Batch  126/269   train_loss = 4.663\n",
      "Epoch   1 Batch  127/269   train_loss = 5.068\n",
      "Epoch   1 Batch  128/269   train_loss = 5.090\n",
      "Epoch   1 Batch  129/269   train_loss = 5.253\n",
      "Epoch   1 Batch  130/269   train_loss = 5.405\n",
      "Epoch   1 Batch  131/269   train_loss = 5.160\n",
      "Epoch   1 Batch  132/269   train_loss = 5.166\n",
      "Epoch   1 Batch  133/269   train_loss = 4.848\n",
      "Epoch   1 Batch  134/269   train_loss = 5.221\n",
      "Epoch   1 Batch  135/269   train_loss = 4.797\n",
      "Epoch   1 Batch  136/269   train_loss = 4.833\n",
      "Epoch   1 Batch  137/269   train_loss = 5.257\n",
      "Epoch   1 Batch  138/269   train_loss = 5.068\n",
      "Epoch   1 Batch  139/269   train_loss = 5.140\n",
      "Epoch   1 Batch  140/269   train_loss = 5.227\n",
      "Epoch   1 Batch  141/269   train_loss = 5.093\n",
      "Epoch   1 Batch  142/269   train_loss = 4.921\n",
      "Epoch   1 Batch  143/269   train_loss = 5.140\n",
      "Epoch   1 Batch  144/269   train_loss = 5.214\n",
      "Epoch   1 Batch  145/269   train_loss = 4.834\n",
      "Epoch   1 Batch  146/269   train_loss = 5.210\n",
      "Epoch   1 Batch  147/269   train_loss = 4.842\n",
      "Epoch   1 Batch  148/269   train_loss = 5.347\n",
      "Epoch   1 Batch  149/269   train_loss = 5.047\n",
      "Epoch   1 Batch  150/269   train_loss = 4.970\n",
      "Epoch   1 Batch  151/269   train_loss = 5.160\n",
      "Epoch   1 Batch  152/269   train_loss = 4.881\n",
      "Epoch   1 Batch  153/269   train_loss = 5.358\n",
      "Epoch   1 Batch  154/269   train_loss = 5.204\n",
      "Epoch   1 Batch  155/269   train_loss = 5.037\n",
      "Epoch   1 Batch  156/269   train_loss = 5.483\n",
      "Epoch   1 Batch  157/269   train_loss = 5.657\n",
      "Epoch   1 Batch  158/269   train_loss = 5.149\n",
      "Epoch   1 Batch  159/269   train_loss = 5.338\n",
      "Epoch   1 Batch  160/269   train_loss = 5.050\n",
      "Epoch   1 Batch  161/269   train_loss = 5.145\n",
      "Epoch   1 Batch  162/269   train_loss = 4.942\n",
      "Epoch   1 Batch  163/269   train_loss = 5.266\n",
      "Epoch   1 Batch  164/269   train_loss = 5.420\n",
      "Epoch   1 Batch  165/269   train_loss = 5.417\n",
      "Epoch   1 Batch  166/269   train_loss = 5.208\n",
      "Epoch   1 Batch  167/269   train_loss = 5.101\n",
      "Epoch   1 Batch  168/269   train_loss = 4.856\n",
      "Epoch   1 Batch  169/269   train_loss = 5.364\n",
      "Epoch   1 Batch  170/269   train_loss = 5.034\n",
      "Epoch   1 Batch  171/269   train_loss = 5.743\n",
      "Epoch   1 Batch  172/269   train_loss = 5.116\n",
      "Epoch   1 Batch  173/269   train_loss = 5.343\n",
      "Epoch   1 Batch  174/269   train_loss = 5.337\n",
      "Epoch   1 Batch  175/269   train_loss = 4.970\n",
      "Epoch   1 Batch  176/269   train_loss = 5.075\n",
      "Epoch   1 Batch  177/269   train_loss = 5.031\n",
      "Epoch   1 Batch  178/269   train_loss = 5.410\n",
      "Epoch   1 Batch  179/269   train_loss = 5.121\n",
      "Epoch   1 Batch  180/269   train_loss = 4.851\n",
      "Epoch   1 Batch  181/269   train_loss = 5.033\n",
      "Epoch   1 Batch  182/269   train_loss = 4.735\n",
      "Epoch   1 Batch  183/269   train_loss = 5.138\n",
      "Epoch   1 Batch  184/269   train_loss = 5.192\n",
      "Epoch   1 Batch  185/269   train_loss = 5.036\n",
      "Epoch   1 Batch  186/269   train_loss = 4.882\n",
      "Epoch   1 Batch  187/269   train_loss = 5.315\n",
      "Epoch   1 Batch  188/269   train_loss = 4.963\n",
      "Epoch   1 Batch  189/269   train_loss = 5.061\n",
      "Epoch   1 Batch  190/269   train_loss = 5.265\n",
      "Epoch   1 Batch  191/269   train_loss = 4.994\n",
      "Epoch   1 Batch  192/269   train_loss = 4.931\n",
      "Epoch   1 Batch  193/269   train_loss = 4.902\n",
      "Epoch   1 Batch  194/269   train_loss = 5.164\n",
      "Epoch   1 Batch  195/269   train_loss = 4.992\n",
      "Epoch   1 Batch  196/269   train_loss = 5.318\n",
      "Epoch   1 Batch  197/269   train_loss = 5.324\n",
      "Epoch   1 Batch  198/269   train_loss = 5.349\n",
      "Epoch   1 Batch  199/269   train_loss = 5.588\n",
      "Epoch   1 Batch  200/269   train_loss = 5.547\n",
      "Epoch   1 Batch  201/269   train_loss = 4.846\n",
      "Epoch   1 Batch  202/269   train_loss = 4.895\n",
      "Epoch   1 Batch  203/269   train_loss = 5.052\n",
      "Epoch   1 Batch  204/269   train_loss = 4.963\n",
      "Epoch   1 Batch  205/269   train_loss = 5.279\n",
      "Epoch   1 Batch  206/269   train_loss = 5.111\n",
      "Epoch   1 Batch  207/269   train_loss = 4.987\n",
      "Epoch   1 Batch  208/269   train_loss = 5.058\n",
      "Epoch   1 Batch  209/269   train_loss = 5.180\n",
      "Epoch   1 Batch  210/269   train_loss = 5.015\n",
      "Epoch   1 Batch  211/269   train_loss = 4.876\n",
      "Epoch   1 Batch  212/269   train_loss = 5.505\n",
      "Epoch   1 Batch  213/269   train_loss = 5.158\n",
      "Epoch   1 Batch  214/269   train_loss = 5.242\n",
      "Epoch   1 Batch  215/269   train_loss = 5.288\n",
      "Epoch   1 Batch  216/269   train_loss = 5.332\n",
      "Epoch   1 Batch  217/269   train_loss = 5.094\n",
      "Epoch   1 Batch  218/269   train_loss = 5.033\n",
      "Epoch   1 Batch  219/269   train_loss = 4.749\n",
      "Epoch   1 Batch  220/269   train_loss = 5.024\n",
      "Epoch   1 Batch  221/269   train_loss = 4.982\n",
      "Epoch   1 Batch  222/269   train_loss = 5.053\n",
      "Epoch   1 Batch  223/269   train_loss = 5.228\n",
      "Epoch   1 Batch  224/269   train_loss = 4.988\n",
      "Epoch   1 Batch  225/269   train_loss = 5.475\n",
      "Epoch   1 Batch  226/269   train_loss = 5.350\n",
      "Epoch   1 Batch  227/269   train_loss = 4.918\n",
      "Epoch   1 Batch  228/269   train_loss = 4.988\n",
      "Epoch   1 Batch  229/269   train_loss = 5.112\n",
      "Epoch   1 Batch  230/269   train_loss = 5.378\n",
      "Epoch   1 Batch  231/269   train_loss = 5.071\n",
      "Epoch   1 Batch  232/269   train_loss = 4.854\n",
      "Epoch   1 Batch  233/269   train_loss = 4.996\n",
      "Epoch   1 Batch  234/269   train_loss = 4.905\n",
      "Epoch   1 Batch  235/269   train_loss = 5.264\n",
      "Epoch   1 Batch  236/269   train_loss = 4.978\n",
      "Epoch   1 Batch  237/269   train_loss = 4.756\n",
      "Epoch   1 Batch  238/269   train_loss = 5.043\n",
      "Epoch   1 Batch  239/269   train_loss = 5.472\n",
      "Epoch   1 Batch  240/269   train_loss = 5.036\n",
      "Epoch   1 Batch  241/269   train_loss = 5.280\n",
      "Epoch   1 Batch  242/269   train_loss = 4.676\n",
      "Epoch   1 Batch  243/269   train_loss = 5.059\n",
      "Epoch   1 Batch  244/269   train_loss = 4.787\n",
      "Epoch   1 Batch  245/269   train_loss = 4.855\n",
      "Epoch   1 Batch  246/269   train_loss = 4.671\n",
      "Epoch   1 Batch  247/269   train_loss = 5.116\n",
      "Epoch   1 Batch  248/269   train_loss = 5.209\n",
      "Epoch   1 Batch  249/269   train_loss = 5.053\n",
      "Epoch   1 Batch  250/269   train_loss = 4.415\n",
      "Epoch   1 Batch  251/269   train_loss = 4.915\n",
      "Epoch   1 Batch  252/269   train_loss = 5.152\n",
      "Epoch   1 Batch  253/269   train_loss = 5.130\n",
      "Epoch   1 Batch  254/269   train_loss = 4.719\n",
      "Epoch   1 Batch  255/269   train_loss = 5.189\n",
      "Epoch   1 Batch  256/269   train_loss = 5.002\n",
      "Epoch   1 Batch  257/269   train_loss = 4.602\n",
      "Epoch   1 Batch  258/269   train_loss = 4.822\n",
      "Epoch   1 Batch  259/269   train_loss = 4.768\n",
      "Epoch   1 Batch  260/269   train_loss = 4.997\n",
      "Epoch   1 Batch  261/269   train_loss = 5.113\n",
      "Epoch   1 Batch  262/269   train_loss = 4.843\n",
      "Epoch   1 Batch  263/269   train_loss = 4.698\n",
      "Epoch   1 Batch  264/269   train_loss = 5.414\n",
      "Epoch   1 Batch  265/269   train_loss = 5.061\n",
      "Epoch   1 Batch  266/269   train_loss = 4.963\n",
      "Epoch   1 Batch  267/269   train_loss = 5.107\n",
      "Epoch   1 Batch  268/269   train_loss = 5.371\n",
      "Epoch   2 Batch    0/269   train_loss = 4.945\n",
      "Epoch   2 Batch    1/269   train_loss = 4.536\n",
      "Epoch   2 Batch    2/269   train_loss = 4.713\n",
      "Epoch   2 Batch    3/269   train_loss = 4.798\n",
      "Epoch   2 Batch    4/269   train_loss = 5.288\n",
      "Epoch   2 Batch    5/269   train_loss = 5.027\n",
      "Epoch   2 Batch    6/269   train_loss = 5.048\n",
      "Epoch   2 Batch    7/269   train_loss = 4.891\n",
      "Epoch   2 Batch    8/269   train_loss = 5.015\n",
      "Epoch   2 Batch    9/269   train_loss = 4.809\n",
      "Epoch   2 Batch   10/269   train_loss = 5.016\n",
      "Epoch   2 Batch   11/269   train_loss = 4.914\n",
      "Epoch   2 Batch   12/269   train_loss = 4.958\n",
      "Epoch   2 Batch   13/269   train_loss = 5.055\n",
      "Epoch   2 Batch   14/269   train_loss = 4.762\n",
      "Epoch   2 Batch   15/269   train_loss = 5.084\n",
      "Epoch   2 Batch   16/269   train_loss = 5.002\n",
      "Epoch   2 Batch   17/269   train_loss = 5.067\n",
      "Epoch   2 Batch   18/269   train_loss = 4.984\n",
      "Epoch   2 Batch   19/269   train_loss = 4.953\n",
      "Epoch   2 Batch   20/269   train_loss = 5.266\n",
      "Epoch   2 Batch   21/269   train_loss = 5.054\n",
      "Epoch   2 Batch   22/269   train_loss = 4.711\n",
      "Epoch   2 Batch   23/269   train_loss = 4.910\n",
      "Epoch   2 Batch   24/269   train_loss = 4.942\n",
      "Epoch   2 Batch   25/269   train_loss = 5.032\n",
      "Epoch   2 Batch   26/269   train_loss = 4.946\n",
      "Epoch   2 Batch   27/269   train_loss = 4.899\n",
      "Epoch   2 Batch   28/269   train_loss = 4.961\n",
      "Epoch   2 Batch   29/269   train_loss = 5.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch   30/269   train_loss = 4.944\n",
      "Epoch   2 Batch   31/269   train_loss = 4.840\n",
      "Epoch   2 Batch   32/269   train_loss = 4.629\n",
      "Epoch   2 Batch   33/269   train_loss = 4.689\n",
      "Epoch   2 Batch   34/269   train_loss = 4.945\n",
      "Epoch   2 Batch   35/269   train_loss = 4.931\n",
      "Epoch   2 Batch   36/269   train_loss = 4.755\n",
      "Epoch   2 Batch   37/269   train_loss = 4.795\n",
      "Epoch   2 Batch   38/269   train_loss = 4.852\n",
      "Epoch   2 Batch   39/269   train_loss = 4.986\n",
      "Epoch   2 Batch   40/269   train_loss = 4.672\n",
      "Epoch   2 Batch   41/269   train_loss = 4.744\n",
      "Epoch   2 Batch   42/269   train_loss = 5.423\n",
      "Epoch   2 Batch   43/269   train_loss = 5.084\n",
      "Epoch   2 Batch   44/269   train_loss = 4.846\n",
      "Epoch   2 Batch   45/269   train_loss = 4.693\n",
      "Epoch   2 Batch   46/269   train_loss = 5.229\n",
      "Epoch   2 Batch   47/269   train_loss = 4.626\n",
      "Epoch   2 Batch   48/269   train_loss = 5.203\n",
      "Epoch   2 Batch   49/269   train_loss = 4.889\n",
      "Epoch   2 Batch   50/269   train_loss = 5.200\n",
      "Epoch   2 Batch   51/269   train_loss = 4.849\n",
      "Epoch   2 Batch   52/269   train_loss = 5.096\n",
      "Epoch   2 Batch   53/269   train_loss = 5.032\n",
      "Epoch   2 Batch   54/269   train_loss = 4.907\n",
      "Epoch   2 Batch   55/269   train_loss = 5.062\n",
      "Epoch   2 Batch   56/269   train_loss = 4.873\n",
      "Epoch   2 Batch   57/269   train_loss = 4.867\n",
      "Epoch   2 Batch   58/269   train_loss = 5.229\n",
      "Epoch   2 Batch   59/269   train_loss = 4.663\n",
      "Epoch   2 Batch   60/269   train_loss = 5.106\n",
      "Epoch   2 Batch   61/269   train_loss = 5.186\n",
      "Epoch   2 Batch   62/269   train_loss = 4.911\n",
      "Epoch   2 Batch   63/269   train_loss = 5.179\n",
      "Epoch   2 Batch   64/269   train_loss = 4.800\n",
      "Epoch   2 Batch   65/269   train_loss = 4.911\n",
      "Epoch   2 Batch   66/269   train_loss = 5.085\n",
      "Epoch   2 Batch   67/269   train_loss = 4.870\n",
      "Epoch   2 Batch   68/269   train_loss = 5.108\n",
      "Epoch   2 Batch   69/269   train_loss = 5.161\n",
      "Epoch   2 Batch   70/269   train_loss = 4.666\n",
      "Epoch   2 Batch   71/269   train_loss = 5.039\n",
      "Epoch   2 Batch   72/269   train_loss = 4.774\n",
      "Epoch   2 Batch   73/269   train_loss = 4.912\n",
      "Epoch   2 Batch   74/269   train_loss = 4.842\n",
      "Epoch   2 Batch   75/269   train_loss = 5.200\n",
      "Epoch   2 Batch   76/269   train_loss = 5.045\n",
      "Epoch   2 Batch   77/269   train_loss = 4.585\n",
      "Epoch   2 Batch   78/269   train_loss = 5.094\n",
      "Epoch   2 Batch   79/269   train_loss = 4.906\n",
      "Epoch   2 Batch   80/269   train_loss = 4.650\n",
      "Epoch   2 Batch   81/269   train_loss = 4.611\n",
      "Epoch   2 Batch   82/269   train_loss = 4.994\n",
      "Epoch   2 Batch   83/269   train_loss = 5.035\n",
      "Epoch   2 Batch   84/269   train_loss = 5.094\n",
      "Epoch   2 Batch   85/269   train_loss = 5.218\n",
      "Epoch   2 Batch   86/269   train_loss = 5.018\n",
      "Epoch   2 Batch   87/269   train_loss = 4.899\n",
      "Epoch   2 Batch   88/269   train_loss = 4.711\n",
      "Epoch   2 Batch   89/269   train_loss = 4.893\n",
      "Epoch   2 Batch   90/269   train_loss = 5.054\n",
      "Epoch   2 Batch   91/269   train_loss = 5.139\n",
      "Epoch   2 Batch   92/269   train_loss = 5.148\n",
      "Epoch   2 Batch   93/269   train_loss = 4.381\n",
      "Epoch   2 Batch   94/269   train_loss = 5.073\n",
      "Epoch   2 Batch   95/269   train_loss = 4.764\n",
      "Epoch   2 Batch   96/269   train_loss = 4.851\n",
      "Epoch   2 Batch   97/269   train_loss = 4.885\n",
      "Epoch   2 Batch   98/269   train_loss = 4.815\n",
      "Epoch   2 Batch   99/269   train_loss = 5.023\n",
      "Epoch   2 Batch  100/269   train_loss = 4.790\n",
      "Epoch   2 Batch  101/269   train_loss = 4.919\n",
      "Epoch   2 Batch  102/269   train_loss = 4.821\n",
      "Epoch   2 Batch  103/269   train_loss = 4.943\n",
      "Epoch   2 Batch  104/269   train_loss = 5.005\n",
      "Epoch   2 Batch  105/269   train_loss = 4.944\n",
      "Epoch   2 Batch  106/269   train_loss = 4.979\n",
      "Epoch   2 Batch  107/269   train_loss = 5.105\n",
      "Epoch   2 Batch  108/269   train_loss = 4.950\n",
      "Epoch   2 Batch  109/269   train_loss = 4.900\n",
      "Epoch   2 Batch  110/269   train_loss = 4.979\n",
      "Epoch   2 Batch  111/269   train_loss = 4.968\n",
      "Epoch   2 Batch  112/269   train_loss = 5.014\n",
      "Epoch   2 Batch  113/269   train_loss = 4.658\n",
      "Epoch   2 Batch  114/269   train_loss = 4.778\n",
      "Epoch   2 Batch  115/269   train_loss = 4.820\n",
      "Epoch   2 Batch  116/269   train_loss = 4.712\n",
      "Epoch   2 Batch  117/269   train_loss = 4.653\n",
      "Epoch   2 Batch  118/269   train_loss = 4.949\n",
      "Epoch   2 Batch  119/269   train_loss = 4.865\n",
      "Epoch   2 Batch  120/269   train_loss = 4.930\n",
      "Epoch   2 Batch  121/269   train_loss = 4.824\n",
      "Epoch   2 Batch  122/269   train_loss = 4.904\n",
      "Epoch   2 Batch  123/269   train_loss = 4.666\n",
      "Epoch   2 Batch  124/269   train_loss = 4.948\n",
      "Epoch   2 Batch  125/269   train_loss = 4.529\n",
      "Epoch   2 Batch  126/269   train_loss = 4.383\n",
      "Epoch   2 Batch  127/269   train_loss = 4.785\n",
      "Epoch   2 Batch  128/269   train_loss = 4.789\n",
      "Epoch   2 Batch  129/269   train_loss = 4.922\n",
      "Epoch   2 Batch  130/269   train_loss = 5.112\n",
      "Epoch   2 Batch  131/269   train_loss = 4.832\n",
      "Epoch   2 Batch  132/269   train_loss = 4.871\n",
      "Epoch   2 Batch  133/269   train_loss = 4.570\n",
      "Epoch   2 Batch  134/269   train_loss = 4.963\n",
      "Epoch   2 Batch  135/269   train_loss = 4.548\n",
      "Epoch   2 Batch  136/269   train_loss = 4.557\n",
      "Epoch   2 Batch  137/269   train_loss = 4.922\n",
      "Epoch   2 Batch  138/269   train_loss = 4.744\n",
      "Epoch   2 Batch  139/269   train_loss = 4.814\n",
      "Epoch   2 Batch  140/269   train_loss = 4.934\n",
      "Epoch   2 Batch  141/269   train_loss = 4.870\n",
      "Epoch   2 Batch  142/269   train_loss = 4.640\n",
      "Epoch   2 Batch  143/269   train_loss = 4.838\n",
      "Epoch   2 Batch  144/269   train_loss = 4.983\n",
      "Epoch   2 Batch  145/269   train_loss = 4.574\n",
      "Epoch   2 Batch  146/269   train_loss = 4.934\n",
      "Epoch   2 Batch  147/269   train_loss = 4.541\n",
      "Epoch   2 Batch  148/269   train_loss = 5.039\n",
      "Epoch   2 Batch  149/269   train_loss = 4.762\n",
      "Epoch   2 Batch  150/269   train_loss = 4.737\n",
      "Epoch   2 Batch  151/269   train_loss = 4.852\n",
      "Epoch   2 Batch  152/269   train_loss = 4.639\n",
      "Epoch   2 Batch  153/269   train_loss = 5.052\n",
      "Epoch   2 Batch  154/269   train_loss = 4.883\n",
      "Epoch   2 Batch  155/269   train_loss = 4.763\n",
      "Epoch   2 Batch  156/269   train_loss = 5.294\n",
      "Epoch   2 Batch  157/269   train_loss = 5.373\n",
      "Epoch   2 Batch  158/269   train_loss = 4.851\n",
      "Epoch   2 Batch  159/269   train_loss = 5.099\n",
      "Epoch   2 Batch  160/269   train_loss = 4.766\n",
      "Epoch   2 Batch  161/269   train_loss = 4.879\n",
      "Epoch   2 Batch  162/269   train_loss = 4.669\n",
      "Epoch   2 Batch  163/269   train_loss = 5.003\n",
      "Epoch   2 Batch  164/269   train_loss = 5.138\n",
      "Epoch   2 Batch  165/269   train_loss = 5.171\n",
      "Epoch   2 Batch  166/269   train_loss = 4.904\n",
      "Epoch   2 Batch  167/269   train_loss = 4.873\n",
      "Epoch   2 Batch  168/269   train_loss = 4.623\n",
      "Epoch   2 Batch  169/269   train_loss = 5.096\n",
      "Epoch   2 Batch  170/269   train_loss = 4.744\n",
      "Epoch   2 Batch  171/269   train_loss = 5.422\n",
      "Epoch   2 Batch  172/269   train_loss = 4.813\n",
      "Epoch   2 Batch  173/269   train_loss = 4.973\n",
      "Epoch   2 Batch  174/269   train_loss = 5.076\n",
      "Epoch   2 Batch  175/269   train_loss = 4.708\n",
      "Epoch   2 Batch  176/269   train_loss = 4.807\n",
      "Epoch   2 Batch  177/269   train_loss = 4.774\n",
      "Epoch   2 Batch  178/269   train_loss = 5.103\n",
      "Epoch   2 Batch  179/269   train_loss = 4.891\n",
      "Epoch   2 Batch  180/269   train_loss = 4.588\n",
      "Epoch   2 Batch  181/269   train_loss = 4.788\n",
      "Epoch   2 Batch  182/269   train_loss = 4.479\n",
      "Epoch   2 Batch  183/269   train_loss = 4.902\n",
      "Epoch   2 Batch  184/269   train_loss = 4.939\n",
      "Epoch   2 Batch  185/269   train_loss = 4.770\n",
      "Epoch   2 Batch  186/269   train_loss = 4.612\n",
      "Epoch   2 Batch  187/269   train_loss = 5.015\n",
      "Epoch   2 Batch  188/269   train_loss = 4.709\n",
      "Epoch   2 Batch  189/269   train_loss = 4.802\n",
      "Epoch   2 Batch  190/269   train_loss = 4.995\n",
      "Epoch   2 Batch  191/269   train_loss = 4.733\n",
      "Epoch   2 Batch  192/269   train_loss = 4.642\n",
      "Epoch   2 Batch  193/269   train_loss = 4.629\n",
      "Epoch   2 Batch  194/269   train_loss = 4.885\n",
      "Epoch   2 Batch  195/269   train_loss = 4.692\n",
      "Epoch   2 Batch  196/269   train_loss = 5.008\n",
      "Epoch   2 Batch  197/269   train_loss = 5.056\n",
      "Epoch   2 Batch  198/269   train_loss = 5.046\n",
      "Epoch   2 Batch  199/269   train_loss = 5.296\n",
      "Epoch   2 Batch  200/269   train_loss = 5.221\n",
      "Epoch   2 Batch  201/269   train_loss = 4.635\n",
      "Epoch   2 Batch  202/269   train_loss = 4.636\n",
      "Epoch   2 Batch  203/269   train_loss = 4.779\n",
      "Epoch   2 Batch  204/269   train_loss = 4.686\n",
      "Epoch   2 Batch  205/269   train_loss = 4.968\n",
      "Epoch   2 Batch  206/269   train_loss = 4.866\n",
      "Epoch   2 Batch  207/269   train_loss = 4.762\n",
      "Epoch   2 Batch  208/269   train_loss = 4.819\n",
      "Epoch   2 Batch  209/269   train_loss = 4.936\n",
      "Epoch   2 Batch  210/269   train_loss = 4.721\n",
      "Epoch   2 Batch  211/269   train_loss = 4.664\n",
      "Epoch   2 Batch  212/269   train_loss = 5.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch  213/269   train_loss = 4.837\n",
      "Epoch   2 Batch  214/269   train_loss = 4.954\n",
      "Epoch   2 Batch  215/269   train_loss = 5.063\n",
      "Epoch   2 Batch  216/269   train_loss = 5.047\n",
      "Epoch   2 Batch  217/269   train_loss = 4.837\n",
      "Epoch   2 Batch  218/269   train_loss = 4.737\n",
      "Epoch   2 Batch  219/269   train_loss = 4.484\n",
      "Epoch   2 Batch  220/269   train_loss = 4.734\n",
      "Epoch   2 Batch  221/269   train_loss = 4.735\n",
      "Epoch   2 Batch  222/269   train_loss = 4.776\n",
      "Epoch   2 Batch  223/269   train_loss = 4.883\n",
      "Epoch   2 Batch  224/269   train_loss = 4.784\n",
      "Epoch   2 Batch  225/269   train_loss = 5.188\n",
      "Epoch   2 Batch  226/269   train_loss = 5.053\n",
      "Epoch   2 Batch  227/269   train_loss = 4.600\n",
      "Epoch   2 Batch  228/269   train_loss = 4.731\n",
      "Epoch   2 Batch  229/269   train_loss = 4.882\n",
      "Epoch   2 Batch  230/269   train_loss = 5.073\n",
      "Epoch   2 Batch  231/269   train_loss = 4.842\n",
      "Epoch   2 Batch  232/269   train_loss = 4.637\n",
      "Epoch   2 Batch  233/269   train_loss = 4.792\n",
      "Epoch   2 Batch  234/269   train_loss = 4.708\n",
      "Epoch   2 Batch  235/269   train_loss = 5.032\n",
      "Epoch   2 Batch  236/269   train_loss = 4.724\n",
      "Epoch   2 Batch  237/269   train_loss = 4.497\n",
      "Epoch   2 Batch  238/269   train_loss = 4.770\n",
      "Epoch   2 Batch  239/269   train_loss = 5.169\n",
      "Epoch   2 Batch  240/269   train_loss = 4.782\n",
      "Epoch   2 Batch  241/269   train_loss = 5.006\n",
      "Epoch   2 Batch  242/269   train_loss = 4.431\n",
      "Epoch   2 Batch  243/269   train_loss = 4.832\n",
      "Epoch   2 Batch  244/269   train_loss = 4.542\n",
      "Epoch   2 Batch  245/269   train_loss = 4.606\n",
      "Epoch   2 Batch  246/269   train_loss = 4.483\n",
      "Epoch   2 Batch  247/269   train_loss = 4.877\n",
      "Epoch   2 Batch  248/269   train_loss = 4.962\n",
      "Epoch   2 Batch  249/269   train_loss = 4.777\n",
      "Epoch   2 Batch  250/269   train_loss = 4.211\n",
      "Epoch   2 Batch  251/269   train_loss = 4.689\n",
      "Epoch   2 Batch  252/269   train_loss = 4.891\n",
      "Epoch   2 Batch  253/269   train_loss = 4.884\n",
      "Epoch   2 Batch  254/269   train_loss = 4.463\n",
      "Epoch   2 Batch  255/269   train_loss = 4.950\n",
      "Epoch   2 Batch  256/269   train_loss = 4.713\n",
      "Epoch   2 Batch  257/269   train_loss = 4.394\n",
      "Epoch   2 Batch  258/269   train_loss = 4.544\n",
      "Epoch   2 Batch  259/269   train_loss = 4.494\n",
      "Epoch   2 Batch  260/269   train_loss = 4.757\n",
      "Epoch   2 Batch  261/269   train_loss = 4.854\n",
      "Epoch   2 Batch  262/269   train_loss = 4.577\n",
      "Epoch   2 Batch  263/269   train_loss = 4.463\n",
      "Epoch   2 Batch  264/269   train_loss = 5.188\n",
      "Epoch   2 Batch  265/269   train_loss = 4.813\n",
      "Epoch   2 Batch  266/269   train_loss = 4.735\n",
      "Epoch   2 Batch  267/269   train_loss = 4.854\n",
      "Epoch   2 Batch  268/269   train_loss = 5.090\n",
      "Epoch   3 Batch    0/269   train_loss = 4.723\n",
      "Epoch   3 Batch    1/269   train_loss = 4.322\n",
      "Epoch   3 Batch    2/269   train_loss = 4.489\n",
      "Epoch   3 Batch    3/269   train_loss = 4.607\n",
      "Epoch   3 Batch    4/269   train_loss = 5.053\n",
      "Epoch   3 Batch    5/269   train_loss = 4.829\n",
      "Epoch   3 Batch    6/269   train_loss = 4.828\n",
      "Epoch   3 Batch    7/269   train_loss = 4.692\n",
      "Epoch   3 Batch    8/269   train_loss = 4.833\n",
      "Epoch   3 Batch    9/269   train_loss = 4.582\n",
      "Epoch   3 Batch   10/269   train_loss = 4.774\n",
      "Epoch   3 Batch   11/269   train_loss = 4.732\n",
      "Epoch   3 Batch   12/269   train_loss = 4.736\n",
      "Epoch   3 Batch   13/269   train_loss = 4.859\n",
      "Epoch   3 Batch   14/269   train_loss = 4.587\n",
      "Epoch   3 Batch   15/269   train_loss = 4.912\n",
      "Epoch   3 Batch   16/269   train_loss = 4.796\n",
      "Epoch   3 Batch   17/269   train_loss = 4.884\n",
      "Epoch   3 Batch   18/269   train_loss = 4.765\n",
      "Epoch   3 Batch   19/269   train_loss = 4.727\n",
      "Epoch   3 Batch   20/269   train_loss = 5.093\n",
      "Epoch   3 Batch   21/269   train_loss = 4.878\n",
      "Epoch   3 Batch   22/269   train_loss = 4.482\n",
      "Epoch   3 Batch   23/269   train_loss = 4.732\n",
      "Epoch   3 Batch   24/269   train_loss = 4.700\n",
      "Epoch   3 Batch   25/269   train_loss = 4.845\n",
      "Epoch   3 Batch   26/269   train_loss = 4.819\n",
      "Epoch   3 Batch   27/269   train_loss = 4.702\n",
      "Epoch   3 Batch   28/269   train_loss = 4.772\n",
      "Epoch   3 Batch   29/269   train_loss = 4.935\n",
      "Epoch   3 Batch   30/269   train_loss = 4.775\n",
      "Epoch   3 Batch   31/269   train_loss = 4.658\n",
      "Epoch   3 Batch   32/269   train_loss = 4.487\n",
      "Epoch   3 Batch   33/269   train_loss = 4.512\n",
      "Epoch   3 Batch   34/269   train_loss = 4.759\n",
      "Epoch   3 Batch   35/269   train_loss = 4.700\n",
      "Epoch   3 Batch   36/269   train_loss = 4.557\n",
      "Epoch   3 Batch   37/269   train_loss = 4.595\n",
      "Epoch   3 Batch   38/269   train_loss = 4.658\n",
      "Epoch   3 Batch   39/269   train_loss = 4.851\n",
      "Epoch   3 Batch   40/269   train_loss = 4.517\n",
      "Epoch   3 Batch   41/269   train_loss = 4.550\n",
      "Epoch   3 Batch   42/269   train_loss = 5.225\n",
      "Epoch   3 Batch   43/269   train_loss = 4.870\n",
      "Epoch   3 Batch   44/269   train_loss = 4.653\n",
      "Epoch   3 Batch   45/269   train_loss = 4.476\n",
      "Epoch   3 Batch   46/269   train_loss = 4.967\n",
      "Epoch   3 Batch   47/269   train_loss = 4.409\n",
      "Epoch   3 Batch   48/269   train_loss = 5.021\n",
      "Epoch   3 Batch   49/269   train_loss = 4.726\n",
      "Epoch   3 Batch   50/269   train_loss = 4.984\n",
      "Epoch   3 Batch   51/269   train_loss = 4.650\n",
      "Epoch   3 Batch   52/269   train_loss = 4.909\n",
      "Epoch   3 Batch   53/269   train_loss = 4.840\n",
      "Epoch   3 Batch   54/269   train_loss = 4.744\n",
      "Epoch   3 Batch   55/269   train_loss = 4.868\n",
      "Epoch   3 Batch   56/269   train_loss = 4.663\n",
      "Epoch   3 Batch   57/269   train_loss = 4.695\n",
      "Epoch   3 Batch   58/269   train_loss = 4.994\n",
      "Epoch   3 Batch   59/269   train_loss = 4.489\n",
      "Epoch   3 Batch   60/269   train_loss = 4.884\n",
      "Epoch   3 Batch   61/269   train_loss = 5.007\n",
      "Epoch   3 Batch   62/269   train_loss = 4.702\n",
      "Epoch   3 Batch   63/269   train_loss = 4.983\n",
      "Epoch   3 Batch   64/269   train_loss = 4.619\n",
      "Epoch   3 Batch   65/269   train_loss = 4.726\n",
      "Epoch   3 Batch   66/269   train_loss = 4.871\n",
      "Epoch   3 Batch   67/269   train_loss = 4.689\n",
      "Epoch   3 Batch   68/269   train_loss = 4.907\n",
      "Epoch   3 Batch   69/269   train_loss = 4.948\n",
      "Epoch   3 Batch   70/269   train_loss = 4.474\n",
      "Epoch   3 Batch   71/269   train_loss = 4.839\n",
      "Epoch   3 Batch   72/269   train_loss = 4.578\n",
      "Epoch   3 Batch   73/269   train_loss = 4.712\n",
      "Epoch   3 Batch   74/269   train_loss = 4.670\n",
      "Epoch   3 Batch   75/269   train_loss = 5.017\n",
      "Epoch   3 Batch   76/269   train_loss = 4.837\n",
      "Epoch   3 Batch   77/269   train_loss = 4.370\n",
      "Epoch   3 Batch   78/269   train_loss = 4.859\n",
      "Epoch   3 Batch   79/269   train_loss = 4.738\n",
      "Epoch   3 Batch   80/269   train_loss = 4.472\n",
      "Epoch   3 Batch   81/269   train_loss = 4.446\n",
      "Epoch   3 Batch   82/269   train_loss = 4.812\n",
      "Epoch   3 Batch   83/269   train_loss = 4.850\n",
      "Epoch   3 Batch   84/269   train_loss = 4.906\n",
      "Epoch   3 Batch   85/269   train_loss = 5.035\n",
      "Epoch   3 Batch   86/269   train_loss = 4.855\n",
      "Epoch   3 Batch   87/269   train_loss = 4.701\n",
      "Epoch   3 Batch   88/269   train_loss = 4.568\n",
      "Epoch   3 Batch   89/269   train_loss = 4.718\n",
      "Epoch   3 Batch   90/269   train_loss = 4.823\n",
      "Epoch   3 Batch   91/269   train_loss = 4.961\n",
      "Epoch   3 Batch   92/269   train_loss = 4.936\n",
      "Epoch   3 Batch   93/269   train_loss = 4.232\n",
      "Epoch   3 Batch   94/269   train_loss = 4.927\n",
      "Epoch   3 Batch   95/269   train_loss = 4.552\n",
      "Epoch   3 Batch   96/269   train_loss = 4.651\n",
      "Epoch   3 Batch   97/269   train_loss = 4.768\n",
      "Epoch   3 Batch   98/269   train_loss = 4.642\n",
      "Epoch   3 Batch   99/269   train_loss = 4.842\n",
      "Epoch   3 Batch  100/269   train_loss = 4.609\n",
      "Epoch   3 Batch  101/269   train_loss = 4.764\n",
      "Epoch   3 Batch  102/269   train_loss = 4.677\n",
      "Epoch   3 Batch  103/269   train_loss = 4.719\n",
      "Epoch   3 Batch  104/269   train_loss = 4.769\n",
      "Epoch   3 Batch  105/269   train_loss = 4.715\n",
      "Epoch   3 Batch  106/269   train_loss = 4.825\n",
      "Epoch   3 Batch  107/269   train_loss = 4.884\n",
      "Epoch   3 Batch  108/269   train_loss = 4.739\n",
      "Epoch   3 Batch  109/269   train_loss = 4.719\n",
      "Epoch   3 Batch  110/269   train_loss = 4.766\n",
      "Epoch   3 Batch  111/269   train_loss = 4.795\n",
      "Epoch   3 Batch  112/269   train_loss = 4.803\n",
      "Epoch   3 Batch  113/269   train_loss = 4.481\n",
      "Epoch   3 Batch  114/269   train_loss = 4.578\n",
      "Epoch   3 Batch  115/269   train_loss = 4.601\n",
      "Epoch   3 Batch  116/269   train_loss = 4.522\n",
      "Epoch   3 Batch  117/269   train_loss = 4.472\n",
      "Epoch   3 Batch  118/269   train_loss = 4.776\n",
      "Epoch   3 Batch  119/269   train_loss = 4.644\n",
      "Epoch   3 Batch  120/269   train_loss = 4.743\n",
      "Epoch   3 Batch  121/269   train_loss = 4.637\n",
      "Epoch   3 Batch  122/269   train_loss = 4.691\n",
      "Epoch   3 Batch  123/269   train_loss = 4.482\n",
      "Epoch   3 Batch  124/269   train_loss = 4.782\n",
      "Epoch   3 Batch  125/269   train_loss = 4.354\n",
      "Epoch   3 Batch  126/269   train_loss = 4.227\n",
      "Epoch   3 Batch  127/269   train_loss = 4.598\n",
      "Epoch   3 Batch  128/269   train_loss = 4.620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 Batch  129/269   train_loss = 4.739\n",
      "Epoch   3 Batch  130/269   train_loss = 4.891\n",
      "Epoch   3 Batch  131/269   train_loss = 4.630\n",
      "Epoch   3 Batch  132/269   train_loss = 4.676\n",
      "Epoch   3 Batch  133/269   train_loss = 4.463\n",
      "Epoch   3 Batch  134/269   train_loss = 4.832\n",
      "Epoch   3 Batch  135/269   train_loss = 4.380\n",
      "Epoch   3 Batch  136/269   train_loss = 4.383\n",
      "Epoch   3 Batch  137/269   train_loss = 4.728\n",
      "Epoch   3 Batch  138/269   train_loss = 4.589\n",
      "Epoch   3 Batch  139/269   train_loss = 4.606\n",
      "Epoch   3 Batch  140/269   train_loss = 4.747\n",
      "Epoch   3 Batch  141/269   train_loss = 4.658\n",
      "Epoch   3 Batch  142/269   train_loss = 4.434\n",
      "Epoch   3 Batch  143/269   train_loss = 4.640\n",
      "Epoch   3 Batch  144/269   train_loss = 4.817\n",
      "Epoch   3 Batch  145/269   train_loss = 4.435\n",
      "Epoch   3 Batch  146/269   train_loss = 4.754\n",
      "Epoch   3 Batch  147/269   train_loss = 4.373\n",
      "Epoch   3 Batch  148/269   train_loss = 4.788\n",
      "Epoch   3 Batch  149/269   train_loss = 4.585\n",
      "Epoch   3 Batch  150/269   train_loss = 4.579\n",
      "Epoch   3 Batch  151/269   train_loss = 4.655\n",
      "Epoch   3 Batch  152/269   train_loss = 4.497\n",
      "Epoch   3 Batch  153/269   train_loss = 4.845\n",
      "Epoch   3 Batch  154/269   train_loss = 4.654\n",
      "Epoch   3 Batch  155/269   train_loss = 4.559\n",
      "Epoch   3 Batch  156/269   train_loss = 5.106\n",
      "Epoch   3 Batch  157/269   train_loss = 5.169\n",
      "Epoch   3 Batch  158/269   train_loss = 4.664\n",
      "Epoch   3 Batch  159/269   train_loss = 4.865\n",
      "Epoch   3 Batch  160/269   train_loss = 4.588\n",
      "Epoch   3 Batch  161/269   train_loss = 4.707\n",
      "Epoch   3 Batch  162/269   train_loss = 4.530\n",
      "Epoch   3 Batch  163/269   train_loss = 4.837\n",
      "Epoch   3 Batch  164/269   train_loss = 4.920\n",
      "Epoch   3 Batch  165/269   train_loss = 4.957\n",
      "Epoch   3 Batch  166/269   train_loss = 4.702\n",
      "Epoch   3 Batch  167/269   train_loss = 4.671\n",
      "Epoch   3 Batch  168/269   train_loss = 4.492\n",
      "Epoch   3 Batch  169/269   train_loss = 4.894\n",
      "Epoch   3 Batch  170/269   train_loss = 4.519\n",
      "Epoch   3 Batch  171/269   train_loss = 5.175\n",
      "Epoch   3 Batch  172/269   train_loss = 4.603\n",
      "Epoch   3 Batch  173/269   train_loss = 4.763\n",
      "Epoch   3 Batch  174/269   train_loss = 4.867\n",
      "Epoch   3 Batch  175/269   train_loss = 4.562\n",
      "Epoch   3 Batch  176/269   train_loss = 4.622\n",
      "Epoch   3 Batch  177/269   train_loss = 4.596\n",
      "Epoch   3 Batch  178/269   train_loss = 4.920\n",
      "Epoch   3 Batch  179/269   train_loss = 4.693\n",
      "Epoch   3 Batch  180/269   train_loss = 4.418\n",
      "Epoch   3 Batch  181/269   train_loss = 4.597\n",
      "Epoch   3 Batch  182/269   train_loss = 4.308\n",
      "Epoch   3 Batch  183/269   train_loss = 4.727\n",
      "Epoch   3 Batch  184/269   train_loss = 4.737\n",
      "Epoch   3 Batch  185/269   train_loss = 4.572\n",
      "Epoch   3 Batch  186/269   train_loss = 4.438\n",
      "Epoch   3 Batch  187/269   train_loss = 4.826\n",
      "Epoch   3 Batch  188/269   train_loss = 4.513\n",
      "Epoch   3 Batch  189/269   train_loss = 4.654\n",
      "Epoch   3 Batch  190/269   train_loss = 4.815\n",
      "Epoch   3 Batch  191/269   train_loss = 4.581\n",
      "Epoch   3 Batch  192/269   train_loss = 4.483\n",
      "Epoch   3 Batch  193/269   train_loss = 4.452\n",
      "Epoch   3 Batch  194/269   train_loss = 4.683\n",
      "Epoch   3 Batch  195/269   train_loss = 4.494\n",
      "Epoch   3 Batch  196/269   train_loss = 4.807\n",
      "Epoch   3 Batch  197/269   train_loss = 4.876\n",
      "Epoch   3 Batch  198/269   train_loss = 4.863\n",
      "Epoch   3 Batch  199/269   train_loss = 5.057\n",
      "Epoch   3 Batch  200/269   train_loss = 4.975\n",
      "Epoch   3 Batch  201/269   train_loss = 4.454\n",
      "Epoch   3 Batch  202/269   train_loss = 4.417\n",
      "Epoch   3 Batch  203/269   train_loss = 4.578\n",
      "Epoch   3 Batch  204/269   train_loss = 4.511\n",
      "Epoch   3 Batch  205/269   train_loss = 4.752\n",
      "Epoch   3 Batch  206/269   train_loss = 4.695\n",
      "Epoch   3 Batch  207/269   train_loss = 4.563\n",
      "Epoch   3 Batch  208/269   train_loss = 4.651\n",
      "Epoch   3 Batch  209/269   train_loss = 4.743\n",
      "Epoch   3 Batch  210/269   train_loss = 4.550\n",
      "Epoch   3 Batch  211/269   train_loss = 4.484\n",
      "Epoch   3 Batch  212/269   train_loss = 5.093\n",
      "Epoch   3 Batch  213/269   train_loss = 4.670\n",
      "Epoch   3 Batch  214/269   train_loss = 4.752\n",
      "Epoch   3 Batch  215/269   train_loss = 4.867\n",
      "Epoch   3 Batch  216/269   train_loss = 4.837\n",
      "Epoch   3 Batch  217/269   train_loss = 4.633\n",
      "Epoch   3 Batch  218/269   train_loss = 4.584\n",
      "Epoch   3 Batch  219/269   train_loss = 4.318\n",
      "Epoch   3 Batch  220/269   train_loss = 4.596\n",
      "Epoch   3 Batch  221/269   train_loss = 4.549\n",
      "Epoch   3 Batch  222/269   train_loss = 4.634\n",
      "Epoch   3 Batch  223/269   train_loss = 4.667\n",
      "Epoch   3 Batch  224/269   train_loss = 4.698\n",
      "Epoch   3 Batch  225/269   train_loss = 4.976\n",
      "Epoch   3 Batch  226/269   train_loss = 4.838\n",
      "Epoch   3 Batch  227/269   train_loss = 4.430\n",
      "Epoch   3 Batch  228/269   train_loss = 4.573\n",
      "Epoch   3 Batch  229/269   train_loss = 4.721\n",
      "Epoch   3 Batch  230/269   train_loss = 4.902\n",
      "Epoch   3 Batch  231/269   train_loss = 4.668\n",
      "Epoch   3 Batch  232/269   train_loss = 4.440\n",
      "Epoch   3 Batch  233/269   train_loss = 4.633\n",
      "Epoch   3 Batch  234/269   train_loss = 4.565\n",
      "Epoch   3 Batch  235/269   train_loss = 4.882\n",
      "Epoch   3 Batch  236/269   train_loss = 4.530\n",
      "Epoch   3 Batch  237/269   train_loss = 4.297\n",
      "Epoch   3 Batch  238/269   train_loss = 4.606\n",
      "Epoch   3 Batch  239/269   train_loss = 4.985\n",
      "Epoch   3 Batch  240/269   train_loss = 4.642\n",
      "Epoch   3 Batch  241/269   train_loss = 4.830\n",
      "Epoch   3 Batch  242/269   train_loss = 4.293\n",
      "Epoch   3 Batch  243/269   train_loss = 4.642\n",
      "Epoch   3 Batch  244/269   train_loss = 4.356\n",
      "Epoch   3 Batch  245/269   train_loss = 4.431\n",
      "Epoch   3 Batch  246/269   train_loss = 4.284\n",
      "Epoch   3 Batch  247/269   train_loss = 4.701\n",
      "Epoch   3 Batch  248/269   train_loss = 4.750\n",
      "Epoch   3 Batch  249/269   train_loss = 4.608\n",
      "Epoch   3 Batch  250/269   train_loss = 4.081\n",
      "Epoch   3 Batch  251/269   train_loss = 4.535\n",
      "Epoch   3 Batch  252/269   train_loss = 4.701\n",
      "Epoch   3 Batch  253/269   train_loss = 4.694\n",
      "Epoch   3 Batch  254/269   train_loss = 4.303\n",
      "Epoch   3 Batch  255/269   train_loss = 4.740\n",
      "Epoch   3 Batch  256/269   train_loss = 4.533\n",
      "Epoch   3 Batch  257/269   train_loss = 4.262\n",
      "Epoch   3 Batch  258/269   train_loss = 4.377\n",
      "Epoch   3 Batch  259/269   train_loss = 4.318\n",
      "Epoch   3 Batch  260/269   train_loss = 4.588\n",
      "Epoch   3 Batch  261/269   train_loss = 4.636\n",
      "Epoch   3 Batch  262/269   train_loss = 4.428\n",
      "Epoch   3 Batch  263/269   train_loss = 4.330\n",
      "Epoch   3 Batch  264/269   train_loss = 4.983\n",
      "Epoch   3 Batch  265/269   train_loss = 4.644\n",
      "Epoch   3 Batch  266/269   train_loss = 4.540\n",
      "Epoch   3 Batch  267/269   train_loss = 4.633\n",
      "Epoch   3 Batch  268/269   train_loss = 4.896\n",
      "Epoch   4 Batch    0/269   train_loss = 4.536\n",
      "Epoch   4 Batch    1/269   train_loss = 4.185\n",
      "Epoch   4 Batch    2/269   train_loss = 4.329\n",
      "Epoch   4 Batch    3/269   train_loss = 4.458\n",
      "Epoch   4 Batch    4/269   train_loss = 4.908\n",
      "Epoch   4 Batch    5/269   train_loss = 4.657\n",
      "Epoch   4 Batch    6/269   train_loss = 4.637\n",
      "Epoch   4 Batch    7/269   train_loss = 4.531\n",
      "Epoch   4 Batch    8/269   train_loss = 4.682\n",
      "Epoch   4 Batch    9/269   train_loss = 4.423\n",
      "Epoch   4 Batch   10/269   train_loss = 4.568\n",
      "Epoch   4 Batch   11/269   train_loss = 4.601\n",
      "Epoch   4 Batch   12/269   train_loss = 4.577\n",
      "Epoch   4 Batch   13/269   train_loss = 4.691\n",
      "Epoch   4 Batch   14/269   train_loss = 4.431\n",
      "Epoch   4 Batch   15/269   train_loss = 4.752\n",
      "Epoch   4 Batch   16/269   train_loss = 4.659\n",
      "Epoch   4 Batch   17/269   train_loss = 4.732\n",
      "Epoch   4 Batch   18/269   train_loss = 4.602\n",
      "Epoch   4 Batch   19/269   train_loss = 4.545\n",
      "Epoch   4 Batch   20/269   train_loss = 4.932\n",
      "Epoch   4 Batch   21/269   train_loss = 4.701\n",
      "Epoch   4 Batch   22/269   train_loss = 4.351\n",
      "Epoch   4 Batch   23/269   train_loss = 4.616\n",
      "Epoch   4 Batch   24/269   train_loss = 4.509\n",
      "Epoch   4 Batch   25/269   train_loss = 4.738\n",
      "Epoch   4 Batch   26/269   train_loss = 4.652\n",
      "Epoch   4 Batch   27/269   train_loss = 4.538\n",
      "Epoch   4 Batch   28/269   train_loss = 4.641\n",
      "Epoch   4 Batch   29/269   train_loss = 4.791\n",
      "Epoch   4 Batch   30/269   train_loss = 4.669\n",
      "Epoch   4 Batch   31/269   train_loss = 4.529\n",
      "Epoch   4 Batch   32/269   train_loss = 4.378\n",
      "Epoch   4 Batch   33/269   train_loss = 4.376\n",
      "Epoch   4 Batch   34/269   train_loss = 4.634\n",
      "Epoch   4 Batch   35/269   train_loss = 4.538\n",
      "Epoch   4 Batch   36/269   train_loss = 4.428\n",
      "Epoch   4 Batch   37/269   train_loss = 4.463\n",
      "Epoch   4 Batch   38/269   train_loss = 4.496\n",
      "Epoch   4 Batch   39/269   train_loss = 4.708\n",
      "Epoch   4 Batch   40/269   train_loss = 4.382\n",
      "Epoch   4 Batch   41/269   train_loss = 4.427\n",
      "Epoch   4 Batch   42/269   train_loss = 5.035\n",
      "Epoch   4 Batch   43/269   train_loss = 4.726\n",
      "Epoch   4 Batch   44/269   train_loss = 4.520\n",
      "Epoch   4 Batch   45/269   train_loss = 4.361\n",
      "Epoch   4 Batch   46/269   train_loss = 4.826\n",
      "Epoch   4 Batch   47/269   train_loss = 4.257\n",
      "Epoch   4 Batch   48/269   train_loss = 4.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch   49/269   train_loss = 4.630\n",
      "Epoch   4 Batch   50/269   train_loss = 4.852\n",
      "Epoch   4 Batch   51/269   train_loss = 4.529\n",
      "Epoch   4 Batch   52/269   train_loss = 4.739\n",
      "Epoch   4 Batch   53/269   train_loss = 4.693\n",
      "Epoch   4 Batch   54/269   train_loss = 4.631\n",
      "Epoch   4 Batch   55/269   train_loss = 4.728\n",
      "Epoch   4 Batch   56/269   train_loss = 4.513\n",
      "Epoch   4 Batch   57/269   train_loss = 4.546\n",
      "Epoch   4 Batch   58/269   train_loss = 4.767\n",
      "Epoch   4 Batch   59/269   train_loss = 4.327\n",
      "Epoch   4 Batch   60/269   train_loss = 4.703\n",
      "Epoch   4 Batch   61/269   train_loss = 4.889\n",
      "Epoch   4 Batch   62/269   train_loss = 4.567\n",
      "Epoch   4 Batch   63/269   train_loss = 4.833\n",
      "Epoch   4 Batch   64/269   train_loss = 4.490\n",
      "Epoch   4 Batch   65/269   train_loss = 4.597\n",
      "Epoch   4 Batch   66/269   train_loss = 4.702\n",
      "Epoch   4 Batch   67/269   train_loss = 4.536\n",
      "Epoch   4 Batch   68/269   train_loss = 4.740\n",
      "Epoch   4 Batch   69/269   train_loss = 4.761\n",
      "Epoch   4 Batch   70/269   train_loss = 4.331\n",
      "Epoch   4 Batch   71/269   train_loss = 4.716\n",
      "Epoch   4 Batch   72/269   train_loss = 4.398\n",
      "Epoch   4 Batch   73/269   train_loss = 4.518\n",
      "Epoch   4 Batch   74/269   train_loss = 4.527\n",
      "Epoch   4 Batch   75/269   train_loss = 4.834\n",
      "Epoch   4 Batch   76/269   train_loss = 4.658\n",
      "Epoch   4 Batch   77/269   train_loss = 4.223\n",
      "Epoch   4 Batch   78/269   train_loss = 4.680\n",
      "Epoch   4 Batch   79/269   train_loss = 4.582\n",
      "Epoch   4 Batch   80/269   train_loss = 4.355\n",
      "Epoch   4 Batch   81/269   train_loss = 4.286\n",
      "Epoch   4 Batch   82/269   train_loss = 4.647\n",
      "Epoch   4 Batch   83/269   train_loss = 4.647\n",
      "Epoch   4 Batch   84/269   train_loss = 4.753\n",
      "Epoch   4 Batch   85/269   train_loss = 4.894\n",
      "Epoch   4 Batch   86/269   train_loss = 4.719\n",
      "Epoch   4 Batch   87/269   train_loss = 4.561\n",
      "Epoch   4 Batch   88/269   train_loss = 4.415\n",
      "Epoch   4 Batch   89/269   train_loss = 4.558\n",
      "Epoch   4 Batch   90/269   train_loss = 4.672\n",
      "Epoch   4 Batch   91/269   train_loss = 4.801\n",
      "Epoch   4 Batch   92/269   train_loss = 4.751\n",
      "Epoch   4 Batch   93/269   train_loss = 4.123\n",
      "Epoch   4 Batch   94/269   train_loss = 4.776\n",
      "Epoch   4 Batch   95/269   train_loss = 4.390\n",
      "Epoch   4 Batch   96/269   train_loss = 4.482\n",
      "Epoch   4 Batch   97/269   train_loss = 4.565\n",
      "Epoch   4 Batch   98/269   train_loss = 4.481\n",
      "Epoch   4 Batch   99/269   train_loss = 4.706\n",
      "Epoch   4 Batch  100/269   train_loss = 4.484\n",
      "Epoch   4 Batch  101/269   train_loss = 4.582\n",
      "Epoch   4 Batch  102/269   train_loss = 4.508\n",
      "Epoch   4 Batch  103/269   train_loss = 4.565\n",
      "Epoch   4 Batch  104/269   train_loss = 4.577\n",
      "Epoch   4 Batch  105/269   train_loss = 4.600\n",
      "Epoch   4 Batch  106/269   train_loss = 4.665\n",
      "Epoch   4 Batch  107/269   train_loss = 4.766\n",
      "Epoch   4 Batch  108/269   train_loss = 4.620\n",
      "Epoch   4 Batch  109/269   train_loss = 4.613\n",
      "Epoch   4 Batch  110/269   train_loss = 4.574\n",
      "Epoch   4 Batch  111/269   train_loss = 4.634\n",
      "Epoch   4 Batch  112/269   train_loss = 4.610\n",
      "Epoch   4 Batch  113/269   train_loss = 4.360\n",
      "Epoch   4 Batch  114/269   train_loss = 4.416\n",
      "Epoch   4 Batch  115/269   train_loss = 4.424\n",
      "Epoch   4 Batch  116/269   train_loss = 4.369\n",
      "Epoch   4 Batch  117/269   train_loss = 4.300\n",
      "Epoch   4 Batch  118/269   train_loss = 4.637\n",
      "Epoch   4 Batch  119/269   train_loss = 4.473\n",
      "Epoch   4 Batch  120/269   train_loss = 4.603\n",
      "Epoch   4 Batch  121/269   train_loss = 4.505\n",
      "Epoch   4 Batch  122/269   train_loss = 4.551\n",
      "Epoch   4 Batch  123/269   train_loss = 4.327\n",
      "Epoch   4 Batch  124/269   train_loss = 4.678\n",
      "Epoch   4 Batch  125/269   train_loss = 4.206\n",
      "Epoch   4 Batch  126/269   train_loss = 4.116\n",
      "Epoch   4 Batch  127/269   train_loss = 4.441\n",
      "Epoch   4 Batch  128/269   train_loss = 4.423\n",
      "Epoch   4 Batch  129/269   train_loss = 4.573\n",
      "Epoch   4 Batch  130/269   train_loss = 4.740\n",
      "Epoch   4 Batch  131/269   train_loss = 4.463\n",
      "Epoch   4 Batch  132/269   train_loss = 4.551\n",
      "Epoch   4 Batch  133/269   train_loss = 4.302\n",
      "Epoch   4 Batch  134/269   train_loss = 4.650\n",
      "Epoch   4 Batch  135/269   train_loss = 4.264\n",
      "Epoch   4 Batch  136/269   train_loss = 4.230\n",
      "Epoch   4 Batch  137/269   train_loss = 4.551\n",
      "Epoch   4 Batch  138/269   train_loss = 4.416\n",
      "Epoch   4 Batch  139/269   train_loss = 4.496\n",
      "Epoch   4 Batch  140/269   train_loss = 4.654\n",
      "Epoch   4 Batch  141/269   train_loss = 4.560\n",
      "Epoch   4 Batch  142/269   train_loss = 4.271\n",
      "Epoch   4 Batch  143/269   train_loss = 4.492\n",
      "Epoch   4 Batch  144/269   train_loss = 4.655\n",
      "Epoch   4 Batch  145/269   train_loss = 4.310\n",
      "Epoch   4 Batch  146/269   train_loss = 4.623\n",
      "Epoch   4 Batch  147/269   train_loss = 4.213\n",
      "Epoch   4 Batch  148/269   train_loss = 4.619\n",
      "Epoch   4 Batch  149/269   train_loss = 4.471\n",
      "Epoch   4 Batch  150/269   train_loss = 4.429\n",
      "Epoch   4 Batch  151/269   train_loss = 4.508\n",
      "Epoch   4 Batch  152/269   train_loss = 4.353\n",
      "Epoch   4 Batch  153/269   train_loss = 4.658\n",
      "Epoch   4 Batch  154/269   train_loss = 4.510\n",
      "Epoch   4 Batch  155/269   train_loss = 4.416\n",
      "Epoch   4 Batch  156/269   train_loss = 4.951\n",
      "Epoch   4 Batch  157/269   train_loss = 4.977\n",
      "Epoch   4 Batch  158/269   train_loss = 4.540\n",
      "Epoch   4 Batch  159/269   train_loss = 4.728\n",
      "Epoch   4 Batch  160/269   train_loss = 4.450\n",
      "Epoch   4 Batch  161/269   train_loss = 4.567\n",
      "Epoch   4 Batch  162/269   train_loss = 4.423\n",
      "Epoch   4 Batch  163/269   train_loss = 4.699\n",
      "Epoch   4 Batch  164/269   train_loss = 4.765\n",
      "Epoch   4 Batch  165/269   train_loss = 4.781\n",
      "Epoch   4 Batch  166/269   train_loss = 4.547\n",
      "Epoch   4 Batch  167/269   train_loss = 4.543\n",
      "Epoch   4 Batch  168/269   train_loss = 4.393\n",
      "Epoch   4 Batch  169/269   train_loss = 4.802\n",
      "Epoch   4 Batch  170/269   train_loss = 4.372\n",
      "Epoch   4 Batch  171/269   train_loss = 4.969\n",
      "Epoch   4 Batch  172/269   train_loss = 4.438\n",
      "Epoch   4 Batch  173/269   train_loss = 4.628\n",
      "Epoch   4 Batch  174/269   train_loss = 4.722\n",
      "Epoch   4 Batch  175/269   train_loss = 4.431\n",
      "Epoch   4 Batch  176/269   train_loss = 4.476\n",
      "Epoch   4 Batch  177/269   train_loss = 4.483\n",
      "Epoch   4 Batch  178/269   train_loss = 4.757\n",
      "Epoch   4 Batch  179/269   train_loss = 4.519\n",
      "Epoch   4 Batch  180/269   train_loss = 4.289\n",
      "Epoch   4 Batch  181/269   train_loss = 4.465\n",
      "Epoch   4 Batch  182/269   train_loss = 4.222\n",
      "Epoch   4 Batch  183/269   train_loss = 4.576\n",
      "Epoch   4 Batch  184/269   train_loss = 4.616\n",
      "Epoch   4 Batch  185/269   train_loss = 4.447\n",
      "Epoch   4 Batch  186/269   train_loss = 4.328\n",
      "Epoch   4 Batch  187/269   train_loss = 4.719\n",
      "Epoch   4 Batch  188/269   train_loss = 4.430\n",
      "Epoch   4 Batch  189/269   train_loss = 4.496\n",
      "Epoch   4 Batch  190/269   train_loss = 4.734\n",
      "Epoch   4 Batch  191/269   train_loss = 4.448\n",
      "Epoch   4 Batch  192/269   train_loss = 4.392\n",
      "Epoch   4 Batch  193/269   train_loss = 4.347\n",
      "Epoch   4 Batch  194/269   train_loss = 4.556\n",
      "Epoch   4 Batch  195/269   train_loss = 4.355\n",
      "Epoch   4 Batch  196/269   train_loss = 4.665\n",
      "Epoch   4 Batch  197/269   train_loss = 4.699\n",
      "Epoch   4 Batch  198/269   train_loss = 4.714\n",
      "Epoch   4 Batch  199/269   train_loss = 4.877\n",
      "Epoch   4 Batch  200/269   train_loss = 4.811\n",
      "Epoch   4 Batch  201/269   train_loss = 4.336\n",
      "Epoch   4 Batch  202/269   train_loss = 4.262\n",
      "Epoch   4 Batch  203/269   train_loss = 4.420\n",
      "Epoch   4 Batch  204/269   train_loss = 4.376\n",
      "Epoch   4 Batch  205/269   train_loss = 4.603\n",
      "Epoch   4 Batch  206/269   train_loss = 4.536\n",
      "Epoch   4 Batch  207/269   train_loss = 4.414\n",
      "Epoch   4 Batch  208/269   train_loss = 4.488\n",
      "Epoch   4 Batch  209/269   train_loss = 4.604\n",
      "Epoch   4 Batch  210/269   train_loss = 4.414\n",
      "Epoch   4 Batch  211/269   train_loss = 4.366\n",
      "Epoch   4 Batch  212/269   train_loss = 4.939\n",
      "Epoch   4 Batch  213/269   train_loss = 4.533\n",
      "Epoch   4 Batch  214/269   train_loss = 4.621\n",
      "Epoch   4 Batch  215/269   train_loss = 4.761\n",
      "Epoch   4 Batch  216/269   train_loss = 4.667\n",
      "Epoch   4 Batch  217/269   train_loss = 4.465\n",
      "Epoch   4 Batch  218/269   train_loss = 4.440\n",
      "Epoch   4 Batch  219/269   train_loss = 4.189\n",
      "Epoch   4 Batch  220/269   train_loss = 4.486\n",
      "Epoch   4 Batch  221/269   train_loss = 4.388\n",
      "Epoch   4 Batch  222/269   train_loss = 4.479\n",
      "Epoch   4 Batch  223/269   train_loss = 4.503\n",
      "Epoch   4 Batch  224/269   train_loss = 4.573\n",
      "Epoch   4 Batch  225/269   train_loss = 4.786\n",
      "Epoch   4 Batch  226/269   train_loss = 4.691\n",
      "Epoch   4 Batch  227/269   train_loss = 4.272\n",
      "Epoch   4 Batch  228/269   train_loss = 4.421\n",
      "Epoch   4 Batch  229/269   train_loss = 4.606\n",
      "Epoch   4 Batch  230/269   train_loss = 4.732\n",
      "Epoch   4 Batch  231/269   train_loss = 4.488\n",
      "Epoch   4 Batch  232/269   train_loss = 4.326\n",
      "Epoch   4 Batch  233/269   train_loss = 4.499\n",
      "Epoch   4 Batch  234/269   train_loss = 4.470\n",
      "Epoch   4 Batch  235/269   train_loss = 4.725\n",
      "Epoch   4 Batch  236/269   train_loss = 4.390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch  237/269   train_loss = 4.167\n",
      "Epoch   4 Batch  238/269   train_loss = 4.428\n",
      "Epoch   4 Batch  239/269   train_loss = 4.839\n",
      "Epoch   4 Batch  240/269   train_loss = 4.512\n",
      "Epoch   4 Batch  241/269   train_loss = 4.687\n",
      "Epoch   4 Batch  242/269   train_loss = 4.176\n",
      "Epoch   4 Batch  243/269   train_loss = 4.480\n",
      "Epoch   4 Batch  244/269   train_loss = 4.226\n",
      "Epoch   4 Batch  245/269   train_loss = 4.348\n",
      "Epoch   4 Batch  246/269   train_loss = 4.150\n",
      "Epoch   4 Batch  247/269   train_loss = 4.570\n",
      "Epoch   4 Batch  248/269   train_loss = 4.621\n",
      "Epoch   4 Batch  249/269   train_loss = 4.491\n",
      "Epoch   4 Batch  250/269   train_loss = 3.986\n",
      "Epoch   4 Batch  251/269   train_loss = 4.411\n",
      "Epoch   4 Batch  252/269   train_loss = 4.606\n",
      "Epoch   4 Batch  253/269   train_loss = 4.536\n",
      "Epoch   4 Batch  254/269   train_loss = 4.232\n",
      "Epoch   4 Batch  255/269   train_loss = 4.596\n",
      "Epoch   4 Batch  256/269   train_loss = 4.424\n",
      "Epoch   4 Batch  257/269   train_loss = 4.135\n",
      "Epoch   4 Batch  258/269   train_loss = 4.206\n",
      "Epoch   4 Batch  259/269   train_loss = 4.185\n",
      "Epoch   4 Batch  260/269   train_loss = 4.453\n",
      "Epoch   4 Batch  261/269   train_loss = 4.500\n",
      "Epoch   4 Batch  262/269   train_loss = 4.316\n",
      "Epoch   4 Batch  263/269   train_loss = 4.215\n",
      "Epoch   4 Batch  264/269   train_loss = 4.856\n",
      "Epoch   4 Batch  265/269   train_loss = 4.513\n",
      "Epoch   4 Batch  266/269   train_loss = 4.397\n",
      "Epoch   4 Batch  267/269   train_loss = 4.469\n",
      "Epoch   4 Batch  268/269   train_loss = 4.749\n",
      "Epoch   5 Batch    0/269   train_loss = 4.403\n",
      "Epoch   5 Batch    1/269   train_loss = 4.085\n",
      "Epoch   5 Batch    2/269   train_loss = 4.227\n",
      "Epoch   5 Batch    3/269   train_loss = 4.349\n",
      "Epoch   5 Batch    4/269   train_loss = 4.815\n",
      "Epoch   5 Batch    5/269   train_loss = 4.527\n",
      "Epoch   5 Batch    6/269   train_loss = 4.462\n",
      "Epoch   5 Batch    7/269   train_loss = 4.389\n",
      "Epoch   5 Batch    8/269   train_loss = 4.559\n",
      "Epoch   5 Batch    9/269   train_loss = 4.284\n",
      "Epoch   5 Batch   10/269   train_loss = 4.408\n",
      "Epoch   5 Batch   11/269   train_loss = 4.447\n",
      "Epoch   5 Batch   12/269   train_loss = 4.417\n",
      "Epoch   5 Batch   13/269   train_loss = 4.551\n",
      "Epoch   5 Batch   14/269   train_loss = 4.337\n",
      "Epoch   5 Batch   15/269   train_loss = 4.635\n",
      "Epoch   5 Batch   16/269   train_loss = 4.504\n",
      "Epoch   5 Batch   17/269   train_loss = 4.626\n",
      "Epoch   5 Batch   18/269   train_loss = 4.466\n",
      "Epoch   5 Batch   19/269   train_loss = 4.427\n",
      "Epoch   5 Batch   20/269   train_loss = 4.776\n",
      "Epoch   5 Batch   21/269   train_loss = 4.577\n",
      "Epoch   5 Batch   22/269   train_loss = 4.232\n",
      "Epoch   5 Batch   23/269   train_loss = 4.490\n",
      "Epoch   5 Batch   24/269   train_loss = 4.400\n",
      "Epoch   5 Batch   25/269   train_loss = 4.631\n",
      "Epoch   5 Batch   26/269   train_loss = 4.546\n",
      "Epoch   5 Batch   27/269   train_loss = 4.418\n",
      "Epoch   5 Batch   28/269   train_loss = 4.532\n",
      "Epoch   5 Batch   29/269   train_loss = 4.669\n",
      "Epoch   5 Batch   30/269   train_loss = 4.566\n",
      "Epoch   5 Batch   31/269   train_loss = 4.429\n",
      "Epoch   5 Batch   32/269   train_loss = 4.271\n",
      "Epoch   5 Batch   33/269   train_loss = 4.251\n",
      "Epoch   5 Batch   34/269   train_loss = 4.506\n",
      "Epoch   5 Batch   35/269   train_loss = 4.381\n",
      "Epoch   5 Batch   36/269   train_loss = 4.315\n",
      "Epoch   5 Batch   37/269   train_loss = 4.338\n",
      "Epoch   5 Batch   38/269   train_loss = 4.363\n",
      "Epoch   5 Batch   39/269   train_loss = 4.623\n",
      "Epoch   5 Batch   40/269   train_loss = 4.287\n",
      "Epoch   5 Batch   41/269   train_loss = 4.320\n",
      "Epoch   5 Batch   42/269   train_loss = 4.915\n",
      "Epoch   5 Batch   43/269   train_loss = 4.585\n",
      "Epoch   5 Batch   44/269   train_loss = 4.410\n",
      "Epoch   5 Batch   45/269   train_loss = 4.232\n",
      "Epoch   5 Batch   46/269   train_loss = 4.706\n",
      "Epoch   5 Batch   47/269   train_loss = 4.164\n",
      "Epoch   5 Batch   48/269   train_loss = 4.787\n",
      "Epoch   5 Batch   49/269   train_loss = 4.558\n",
      "Epoch   5 Batch   50/269   train_loss = 4.714\n",
      "Epoch   5 Batch   51/269   train_loss = 4.418\n",
      "Epoch   5 Batch   52/269   train_loss = 4.613\n",
      "Epoch   5 Batch   53/269   train_loss = 4.613\n",
      "Epoch   5 Batch   54/269   train_loss = 4.513\n",
      "Epoch   5 Batch   55/269   train_loss = 4.598\n",
      "Epoch   5 Batch   56/269   train_loss = 4.441\n",
      "Epoch   5 Batch   57/269   train_loss = 4.459\n",
      "Epoch   5 Batch   58/269   train_loss = 4.613\n",
      "Epoch   5 Batch   59/269   train_loss = 4.250\n",
      "Epoch   5 Batch   60/269   train_loss = 4.559\n",
      "Epoch   5 Batch   61/269   train_loss = 4.765\n",
      "Epoch   5 Batch   62/269   train_loss = 4.432\n",
      "Epoch   5 Batch   63/269   train_loss = 4.727\n",
      "Epoch   5 Batch   64/269   train_loss = 4.370\n",
      "Epoch   5 Batch   65/269   train_loss = 4.483\n",
      "Epoch   5 Batch   66/269   train_loss = 4.568\n",
      "Epoch   5 Batch   67/269   train_loss = 4.417\n",
      "Epoch   5 Batch   68/269   train_loss = 4.604\n",
      "Epoch   5 Batch   69/269   train_loss = 4.643\n",
      "Epoch   5 Batch   70/269   train_loss = 4.219\n",
      "Epoch   5 Batch   71/269   train_loss = 4.562\n",
      "Epoch   5 Batch   72/269   train_loss = 4.260\n",
      "Epoch   5 Batch   73/269   train_loss = 4.443\n",
      "Epoch   5 Batch   74/269   train_loss = 4.422\n",
      "Epoch   5 Batch   75/269   train_loss = 4.674\n",
      "Epoch   5 Batch   76/269   train_loss = 4.551\n",
      "Epoch   5 Batch   77/269   train_loss = 4.097\n",
      "Epoch   5 Batch   78/269   train_loss = 4.521\n",
      "Epoch   5 Batch   79/269   train_loss = 4.489\n",
      "Epoch   5 Batch   80/269   train_loss = 4.265\n",
      "Epoch   5 Batch   81/269   train_loss = 4.179\n",
      "Epoch   5 Batch   82/269   train_loss = 4.542\n",
      "Epoch   5 Batch   83/269   train_loss = 4.514\n",
      "Epoch   5 Batch   84/269   train_loss = 4.622\n",
      "Epoch   5 Batch   85/269   train_loss = 4.768\n",
      "Epoch   5 Batch   86/269   train_loss = 4.609\n",
      "Epoch   5 Batch   87/269   train_loss = 4.460\n",
      "Epoch   5 Batch   88/269   train_loss = 4.319\n",
      "Epoch   5 Batch   89/269   train_loss = 4.443\n",
      "Epoch   5 Batch   90/269   train_loss = 4.521\n",
      "Epoch   5 Batch   91/269   train_loss = 4.649\n",
      "Epoch   5 Batch   92/269   train_loss = 4.608\n",
      "Epoch   5 Batch   93/269   train_loss = 4.071\n",
      "Epoch   5 Batch   94/269   train_loss = 4.661\n",
      "Epoch   5 Batch   95/269   train_loss = 4.298\n",
      "Epoch   5 Batch   96/269   train_loss = 4.353\n",
      "Epoch   5 Batch   97/269   train_loss = 4.470\n",
      "Epoch   5 Batch   98/269   train_loss = 4.370\n",
      "Epoch   5 Batch   99/269   train_loss = 4.581\n",
      "Epoch   5 Batch  100/269   train_loss = 4.422\n",
      "Epoch   5 Batch  101/269   train_loss = 4.484\n",
      "Epoch   5 Batch  102/269   train_loss = 4.358\n",
      "Epoch   5 Batch  103/269   train_loss = 4.437\n",
      "Epoch   5 Batch  104/269   train_loss = 4.455\n",
      "Epoch   5 Batch  105/269   train_loss = 4.503\n",
      "Epoch   5 Batch  106/269   train_loss = 4.538\n",
      "Epoch   5 Batch  107/269   train_loss = 4.611\n",
      "Epoch   5 Batch  108/269   train_loss = 4.496\n",
      "Epoch   5 Batch  109/269   train_loss = 4.498\n",
      "Epoch   5 Batch  110/269   train_loss = 4.441\n",
      "Epoch   5 Batch  111/269   train_loss = 4.546\n",
      "Epoch   5 Batch  112/269   train_loss = 4.522\n",
      "Epoch   5 Batch  113/269   train_loss = 4.249\n",
      "Epoch   5 Batch  114/269   train_loss = 4.345\n",
      "Epoch   5 Batch  115/269   train_loss = 4.288\n",
      "Epoch   5 Batch  116/269   train_loss = 4.290\n",
      "Epoch   5 Batch  117/269   train_loss = 4.160\n",
      "Epoch   5 Batch  118/269   train_loss = 4.526\n",
      "Epoch   5 Batch  119/269   train_loss = 4.335\n",
      "Epoch   5 Batch  120/269   train_loss = 4.512\n",
      "Epoch   5 Batch  121/269   train_loss = 4.386\n",
      "Epoch   5 Batch  122/269   train_loss = 4.432\n",
      "Epoch   5 Batch  123/269   train_loss = 4.200\n",
      "Epoch   5 Batch  124/269   train_loss = 4.556\n",
      "Epoch   5 Batch  125/269   train_loss = 4.122\n",
      "Epoch   5 Batch  126/269   train_loss = 4.032\n",
      "Epoch   5 Batch  127/269   train_loss = 4.317\n",
      "Epoch   5 Batch  128/269   train_loss = 4.329\n",
      "Epoch   5 Batch  129/269   train_loss = 4.447\n",
      "Epoch   5 Batch  130/269   train_loss = 4.663\n",
      "Epoch   5 Batch  131/269   train_loss = 4.331\n",
      "Epoch   5 Batch  132/269   train_loss = 4.438\n",
      "Epoch   5 Batch  133/269   train_loss = 4.212\n",
      "Epoch   5 Batch  134/269   train_loss = 4.533\n",
      "Epoch   5 Batch  135/269   train_loss = 4.184\n",
      "Epoch   5 Batch  136/269   train_loss = 4.125\n",
      "Epoch   5 Batch  137/269   train_loss = 4.462\n",
      "Epoch   5 Batch  138/269   train_loss = 4.306\n",
      "Epoch   5 Batch  139/269   train_loss = 4.387\n",
      "Epoch   5 Batch  140/269   train_loss = 4.535\n",
      "Epoch   5 Batch  141/269   train_loss = 4.450\n",
      "Epoch   5 Batch  142/269   train_loss = 4.139\n",
      "Epoch   5 Batch  143/269   train_loss = 4.389\n",
      "Epoch   5 Batch  144/269   train_loss = 4.553\n",
      "Epoch   5 Batch  145/269   train_loss = 4.234\n",
      "Epoch   5 Batch  146/269   train_loss = 4.490\n",
      "Epoch   5 Batch  147/269   train_loss = 4.140\n",
      "Epoch   5 Batch  148/269   train_loss = 4.525\n",
      "Epoch   5 Batch  149/269   train_loss = 4.340\n",
      "Epoch   5 Batch  150/269   train_loss = 4.327\n",
      "Epoch   5 Batch  151/269   train_loss = 4.408\n",
      "Epoch   5 Batch  152/269   train_loss = 4.258\n",
      "Epoch   5 Batch  153/269   train_loss = 4.498\n",
      "Epoch   5 Batch  154/269   train_loss = 4.395\n",
      "Epoch   5 Batch  155/269   train_loss = 4.312\n",
      "Epoch   5 Batch  156/269   train_loss = 4.829\n",
      "Epoch   5 Batch  157/269   train_loss = 4.829\n",
      "Epoch   5 Batch  158/269   train_loss = 4.447\n",
      "Epoch   5 Batch  159/269   train_loss = 4.611\n",
      "Epoch   5 Batch  160/269   train_loss = 4.331\n",
      "Epoch   5 Batch  161/269   train_loss = 4.462\n",
      "Epoch   5 Batch  162/269   train_loss = 4.326\n",
      "Epoch   5 Batch  163/269   train_loss = 4.597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch  164/269   train_loss = 4.636\n",
      "Epoch   5 Batch  165/269   train_loss = 4.675\n",
      "Epoch   5 Batch  166/269   train_loss = 4.437\n",
      "Epoch   5 Batch  167/269   train_loss = 4.399\n",
      "Epoch   5 Batch  168/269   train_loss = 4.298\n",
      "Epoch   5 Batch  169/269   train_loss = 4.677\n",
      "Epoch   5 Batch  170/269   train_loss = 4.268\n",
      "Epoch   5 Batch  171/269   train_loss = 4.800\n",
      "Epoch   5 Batch  172/269   train_loss = 4.337\n",
      "Epoch   5 Batch  173/269   train_loss = 4.521\n",
      "Epoch   5 Batch  174/269   train_loss = 4.609\n",
      "Epoch   5 Batch  175/269   train_loss = 4.324\n",
      "Epoch   5 Batch  176/269   train_loss = 4.319\n",
      "Epoch   5 Batch  177/269   train_loss = 4.390\n",
      "Epoch   5 Batch  178/269   train_loss = 4.636\n",
      "Epoch   5 Batch  179/269   train_loss = 4.388\n",
      "Epoch   5 Batch  180/269   train_loss = 4.197\n",
      "Epoch   5 Batch  181/269   train_loss = 4.404\n",
      "Epoch   5 Batch  182/269   train_loss = 4.128\n",
      "Epoch   5 Batch  183/269   train_loss = 4.465\n",
      "Epoch   5 Batch  184/269   train_loss = 4.464\n",
      "Epoch   5 Batch  185/269   train_loss = 4.344\n",
      "Epoch   5 Batch  186/269   train_loss = 4.205\n",
      "Epoch   5 Batch  187/269   train_loss = 4.618\n",
      "Epoch   5 Batch  188/269   train_loss = 4.294\n",
      "Epoch   5 Batch  189/269   train_loss = 4.397\n",
      "Epoch   5 Batch  190/269   train_loss = 4.632\n",
      "Epoch   5 Batch  191/269   train_loss = 4.352\n",
      "Epoch   5 Batch  192/269   train_loss = 4.289\n",
      "Epoch   5 Batch  193/269   train_loss = 4.228\n",
      "Epoch   5 Batch  194/269   train_loss = 4.441\n",
      "Epoch   5 Batch  195/269   train_loss = 4.235\n",
      "Epoch   5 Batch  196/269   train_loss = 4.538\n",
      "Epoch   5 Batch  197/269   train_loss = 4.562\n",
      "Epoch   5 Batch  198/269   train_loss = 4.581\n",
      "Epoch   5 Batch  199/269   train_loss = 4.744\n",
      "Epoch   5 Batch  200/269   train_loss = 4.670\n",
      "Epoch   5 Batch  201/269   train_loss = 4.238\n",
      "Epoch   5 Batch  202/269   train_loss = 4.153\n",
      "Epoch   5 Batch  203/269   train_loss = 4.318\n",
      "Epoch   5 Batch  204/269   train_loss = 4.294\n",
      "Epoch   5 Batch  205/269   train_loss = 4.518\n",
      "Epoch   5 Batch  206/269   train_loss = 4.408\n",
      "Epoch   5 Batch  207/269   train_loss = 4.304\n",
      "Epoch   5 Batch  208/269   train_loss = 4.417\n",
      "Epoch   5 Batch  209/269   train_loss = 4.496\n",
      "Epoch   5 Batch  210/269   train_loss = 4.316\n",
      "Epoch   5 Batch  211/269   train_loss = 4.252\n",
      "Epoch   5 Batch  212/269   train_loss = 4.816\n",
      "Epoch   5 Batch  213/269   train_loss = 4.417\n",
      "Epoch   5 Batch  214/269   train_loss = 4.474\n",
      "Epoch   5 Batch  215/269   train_loss = 4.634\n",
      "Epoch   5 Batch  216/269   train_loss = 4.572\n",
      "Epoch   5 Batch  217/269   train_loss = 4.358\n",
      "Epoch   5 Batch  218/269   train_loss = 4.347\n",
      "Epoch   5 Batch  219/269   train_loss = 4.092\n",
      "Epoch   5 Batch  220/269   train_loss = 4.379\n",
      "Epoch   5 Batch  221/269   train_loss = 4.296\n",
      "Epoch   5 Batch  222/269   train_loss = 4.373\n",
      "Epoch   5 Batch  223/269   train_loss = 4.385\n",
      "Epoch   5 Batch  224/269   train_loss = 4.477\n",
      "Epoch   5 Batch  225/269   train_loss = 4.646\n",
      "Epoch   5 Batch  226/269   train_loss = 4.568\n",
      "Epoch   5 Batch  227/269   train_loss = 4.164\n",
      "Epoch   5 Batch  228/269   train_loss = 4.357\n",
      "Epoch   5 Batch  229/269   train_loss = 4.497\n",
      "Epoch   5 Batch  230/269   train_loss = 4.601\n",
      "Epoch   5 Batch  231/269   train_loss = 4.376\n",
      "Epoch   5 Batch  232/269   train_loss = 4.249\n",
      "Epoch   5 Batch  233/269   train_loss = 4.405\n",
      "Epoch   5 Batch  234/269   train_loss = 4.346\n",
      "Epoch   5 Batch  235/269   train_loss = 4.630\n",
      "Epoch   5 Batch  236/269   train_loss = 4.277\n",
      "Epoch   5 Batch  237/269   train_loss = 4.036\n",
      "Epoch   5 Batch  238/269   train_loss = 4.304\n",
      "Epoch   5 Batch  239/269   train_loss = 4.712\n",
      "Epoch   5 Batch  240/269   train_loss = 4.398\n",
      "Epoch   5 Batch  241/269   train_loss = 4.580\n",
      "Epoch   5 Batch  242/269   train_loss = 4.091\n",
      "Epoch   5 Batch  243/269   train_loss = 4.377\n",
      "Epoch   5 Batch  244/269   train_loss = 4.128\n",
      "Epoch   5 Batch  245/269   train_loss = 4.256\n",
      "Epoch   5 Batch  246/269   train_loss = 4.036\n",
      "Epoch   5 Batch  247/269   train_loss = 4.480\n",
      "Epoch   5 Batch  248/269   train_loss = 4.481\n",
      "Epoch   5 Batch  249/269   train_loss = 4.388\n",
      "Epoch   5 Batch  250/269   train_loss = 3.915\n",
      "Epoch   5 Batch  251/269   train_loss = 4.329\n",
      "Epoch   5 Batch  252/269   train_loss = 4.529\n",
      "Epoch   5 Batch  253/269   train_loss = 4.398\n",
      "Epoch   5 Batch  254/269   train_loss = 4.131\n",
      "Epoch   5 Batch  255/269   train_loss = 4.473\n",
      "Epoch   5 Batch  256/269   train_loss = 4.317\n",
      "Epoch   5 Batch  257/269   train_loss = 4.034\n",
      "Epoch   5 Batch  258/269   train_loss = 4.077\n",
      "Epoch   5 Batch  259/269   train_loss = 4.078\n",
      "Epoch   5 Batch  260/269   train_loss = 4.338\n",
      "Epoch   5 Batch  261/269   train_loss = 4.393\n",
      "Epoch   5 Batch  262/269   train_loss = 4.210\n",
      "Epoch   5 Batch  263/269   train_loss = 4.097\n",
      "Epoch   5 Batch  264/269   train_loss = 4.749\n",
      "Epoch   5 Batch  265/269   train_loss = 4.384\n",
      "Epoch   5 Batch  266/269   train_loss = 4.305\n",
      "Epoch   5 Batch  267/269   train_loss = 4.393\n",
      "Epoch   5 Batch  268/269   train_loss = 4.623\n",
      "Epoch   6 Batch    0/269   train_loss = 4.282\n",
      "Epoch   6 Batch    1/269   train_loss = 3.967\n",
      "Epoch   6 Batch    2/269   train_loss = 4.126\n",
      "Epoch   6 Batch    3/269   train_loss = 4.255\n",
      "Epoch   6 Batch    4/269   train_loss = 4.722\n",
      "Epoch   6 Batch    5/269   train_loss = 4.449\n",
      "Epoch   6 Batch    6/269   train_loss = 4.332\n",
      "Epoch   6 Batch    7/269   train_loss = 4.293\n",
      "Epoch   6 Batch    8/269   train_loss = 4.441\n",
      "Epoch   6 Batch    9/269   train_loss = 4.200\n",
      "Epoch   6 Batch   10/269   train_loss = 4.296\n",
      "Epoch   6 Batch   11/269   train_loss = 4.355\n",
      "Epoch   6 Batch   12/269   train_loss = 4.323\n",
      "Epoch   6 Batch   13/269   train_loss = 4.432\n",
      "Epoch   6 Batch   14/269   train_loss = 4.246\n",
      "Epoch   6 Batch   15/269   train_loss = 4.551\n",
      "Epoch   6 Batch   16/269   train_loss = 4.415\n",
      "Epoch   6 Batch   17/269   train_loss = 4.510\n",
      "Epoch   6 Batch   18/269   train_loss = 4.407\n",
      "Epoch   6 Batch   19/269   train_loss = 4.320\n",
      "Epoch   6 Batch   20/269   train_loss = 4.721\n",
      "Epoch   6 Batch   21/269   train_loss = 4.485\n",
      "Epoch   6 Batch   22/269   train_loss = 4.123\n",
      "Epoch   6 Batch   23/269   train_loss = 4.400\n",
      "Epoch   6 Batch   24/269   train_loss = 4.323\n",
      "Epoch   6 Batch   25/269   train_loss = 4.530\n",
      "Epoch   6 Batch   26/269   train_loss = 4.446\n",
      "Epoch   6 Batch   27/269   train_loss = 4.323\n",
      "Epoch   6 Batch   28/269   train_loss = 4.439\n",
      "Epoch   6 Batch   29/269   train_loss = 4.539\n",
      "Epoch   6 Batch   30/269   train_loss = 4.491\n",
      "Epoch   6 Batch   31/269   train_loss = 4.325\n",
      "Epoch   6 Batch   32/269   train_loss = 4.156\n",
      "Epoch   6 Batch   33/269   train_loss = 4.141\n",
      "Epoch   6 Batch   34/269   train_loss = 4.414\n",
      "Epoch   6 Batch   35/269   train_loss = 4.261\n",
      "Epoch   6 Batch   36/269   train_loss = 4.207\n",
      "Epoch   6 Batch   37/269   train_loss = 4.265\n",
      "Epoch   6 Batch   38/269   train_loss = 4.266\n",
      "Epoch   6 Batch   39/269   train_loss = 4.537\n",
      "Epoch   6 Batch   40/269   train_loss = 4.208\n",
      "Epoch   6 Batch   41/269   train_loss = 4.225\n",
      "Epoch   6 Batch   42/269   train_loss = 4.777\n",
      "Epoch   6 Batch   43/269   train_loss = 4.502\n",
      "Epoch   6 Batch   44/269   train_loss = 4.349\n",
      "Epoch   6 Batch   45/269   train_loss = 4.151\n",
      "Epoch   6 Batch   46/269   train_loss = 4.589\n",
      "Epoch   6 Batch   47/269   train_loss = 4.026\n",
      "Epoch   6 Batch   48/269   train_loss = 4.694\n",
      "Epoch   6 Batch   49/269   train_loss = 4.461\n",
      "Epoch   6 Batch   50/269   train_loss = 4.617\n",
      "Epoch   6 Batch   51/269   train_loss = 4.335\n",
      "Epoch   6 Batch   52/269   train_loss = 4.524\n",
      "Epoch   6 Batch   53/269   train_loss = 4.510\n",
      "Epoch   6 Batch   54/269   train_loss = 4.392\n",
      "Epoch   6 Batch   55/269   train_loss = 4.489\n",
      "Epoch   6 Batch   56/269   train_loss = 4.319\n",
      "Epoch   6 Batch   57/269   train_loss = 4.384\n",
      "Epoch   6 Batch   58/269   train_loss = 4.477\n",
      "Epoch   6 Batch   59/269   train_loss = 4.151\n",
      "Epoch   6 Batch   60/269   train_loss = 4.435\n",
      "Epoch   6 Batch   61/269   train_loss = 4.651\n",
      "Epoch   6 Batch   62/269   train_loss = 4.326\n",
      "Epoch   6 Batch   63/269   train_loss = 4.618\n",
      "Epoch   6 Batch   64/269   train_loss = 4.288\n",
      "Epoch   6 Batch   65/269   train_loss = 4.392\n",
      "Epoch   6 Batch   66/269   train_loss = 4.429\n",
      "Epoch   6 Batch   67/269   train_loss = 4.311\n",
      "Epoch   6 Batch   68/269   train_loss = 4.476\n",
      "Epoch   6 Batch   69/269   train_loss = 4.535\n",
      "Epoch   6 Batch   70/269   train_loss = 4.118\n",
      "Epoch   6 Batch   71/269   train_loss = 4.466\n",
      "Epoch   6 Batch   72/269   train_loss = 4.128\n",
      "Epoch   6 Batch   73/269   train_loss = 4.331\n",
      "Epoch   6 Batch   74/269   train_loss = 4.361\n",
      "Epoch   6 Batch   75/269   train_loss = 4.574\n",
      "Epoch   6 Batch   76/269   train_loss = 4.435\n",
      "Epoch   6 Batch   77/269   train_loss = 4.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6 Batch   78/269   train_loss = 4.418\n",
      "Epoch   6 Batch   79/269   train_loss = 4.428\n",
      "Epoch   6 Batch   80/269   train_loss = 4.197\n",
      "Epoch   6 Batch   81/269   train_loss = 4.079\n",
      "Epoch   6 Batch   82/269   train_loss = 4.447\n",
      "Epoch   6 Batch   83/269   train_loss = 4.387\n",
      "Epoch   6 Batch   84/269   train_loss = 4.548\n",
      "Epoch   6 Batch   85/269   train_loss = 4.727\n",
      "Epoch   6 Batch   86/269   train_loss = 4.511\n",
      "Epoch   6 Batch   87/269   train_loss = 4.385\n",
      "Epoch   6 Batch   88/269   train_loss = 4.231\n",
      "Epoch   6 Batch   89/269   train_loss = 4.348\n",
      "Epoch   6 Batch   90/269   train_loss = 4.410\n",
      "Epoch   6 Batch   91/269   train_loss = 4.577\n",
      "Epoch   6 Batch   92/269   train_loss = 4.481\n",
      "Epoch   6 Batch   93/269   train_loss = 3.986\n",
      "Epoch   6 Batch   94/269   train_loss = 4.585\n",
      "Epoch   6 Batch   95/269   train_loss = 4.190\n",
      "Epoch   6 Batch   96/269   train_loss = 4.259\n",
      "Epoch   6 Batch   97/269   train_loss = 4.373\n",
      "Epoch   6 Batch   98/269   train_loss = 4.302\n",
      "Epoch   6 Batch   99/269   train_loss = 4.467\n",
      "Epoch   6 Batch  100/269   train_loss = 4.326\n",
      "Epoch   6 Batch  101/269   train_loss = 4.389\n",
      "Epoch   6 Batch  102/269   train_loss = 4.265\n",
      "Epoch   6 Batch  103/269   train_loss = 4.316\n",
      "Epoch   6 Batch  104/269   train_loss = 4.335\n",
      "Epoch   6 Batch  105/269   train_loss = 4.373\n",
      "Epoch   6 Batch  106/269   train_loss = 4.475\n",
      "Epoch   6 Batch  107/269   train_loss = 4.477\n",
      "Epoch   6 Batch  108/269   train_loss = 4.399\n",
      "Epoch   6 Batch  109/269   train_loss = 4.382\n",
      "Epoch   6 Batch  110/269   train_loss = 4.348\n",
      "Epoch   6 Batch  111/269   train_loss = 4.468\n",
      "Epoch   6 Batch  112/269   train_loss = 4.395\n",
      "Epoch   6 Batch  113/269   train_loss = 4.168\n",
      "Epoch   6 Batch  114/269   train_loss = 4.222\n",
      "Epoch   6 Batch  115/269   train_loss = 4.198\n",
      "Epoch   6 Batch  116/269   train_loss = 4.203\n",
      "Epoch   6 Batch  117/269   train_loss = 4.096\n",
      "Epoch   6 Batch  118/269   train_loss = 4.424\n",
      "Epoch   6 Batch  119/269   train_loss = 4.244\n",
      "Epoch   6 Batch  120/269   train_loss = 4.444\n",
      "Epoch   6 Batch  121/269   train_loss = 4.265\n",
      "Epoch   6 Batch  122/269   train_loss = 4.337\n",
      "Epoch   6 Batch  123/269   train_loss = 4.115\n",
      "Epoch   6 Batch  124/269   train_loss = 4.475\n",
      "Epoch   6 Batch  125/269   train_loss = 4.045\n",
      "Epoch   6 Batch  126/269   train_loss = 3.949\n",
      "Epoch   6 Batch  127/269   train_loss = 4.249\n",
      "Epoch   6 Batch  128/269   train_loss = 4.232\n",
      "Epoch   6 Batch  129/269   train_loss = 4.384\n",
      "Epoch   6 Batch  130/269   train_loss = 4.547\n",
      "Epoch   6 Batch  131/269   train_loss = 4.266\n",
      "Epoch   6 Batch  132/269   train_loss = 4.351\n",
      "Epoch   6 Batch  133/269   train_loss = 4.132\n",
      "Epoch   6 Batch  134/269   train_loss = 4.414\n",
      "Epoch   6 Batch  135/269   train_loss = 4.072\n",
      "Epoch   6 Batch  136/269   train_loss = 4.045\n",
      "Epoch   6 Batch  137/269   train_loss = 4.386\n",
      "Epoch   6 Batch  138/269   train_loss = 4.197\n",
      "Epoch   6 Batch  139/269   train_loss = 4.328\n",
      "Epoch   6 Batch  140/269   train_loss = 4.441\n",
      "Epoch   6 Batch  141/269   train_loss = 4.401\n",
      "Epoch   6 Batch  142/269   train_loss = 4.065\n",
      "Epoch   6 Batch  143/269   train_loss = 4.328\n",
      "Epoch   6 Batch  144/269   train_loss = 4.471\n",
      "Epoch   6 Batch  145/269   train_loss = 4.199\n",
      "Epoch   6 Batch  146/269   train_loss = 4.392\n",
      "Epoch   6 Batch  147/269   train_loss = 4.049\n",
      "Epoch   6 Batch  148/269   train_loss = 4.417\n",
      "Epoch   6 Batch  149/269   train_loss = 4.258\n",
      "Epoch   6 Batch  150/269   train_loss = 4.269\n",
      "Epoch   6 Batch  151/269   train_loss = 4.294\n",
      "Epoch   6 Batch  152/269   train_loss = 4.175\n",
      "Epoch   6 Batch  153/269   train_loss = 4.422\n",
      "Epoch   6 Batch  154/269   train_loss = 4.295\n",
      "Epoch   6 Batch  155/269   train_loss = 4.233\n",
      "Epoch   6 Batch  156/269   train_loss = 4.742\n",
      "Epoch   6 Batch  157/269   train_loss = 4.711\n",
      "Epoch   6 Batch  158/269   train_loss = 4.363\n",
      "Epoch   6 Batch  159/269   train_loss = 4.474\n",
      "Epoch   6 Batch  160/269   train_loss = 4.245\n",
      "Epoch   6 Batch  161/269   train_loss = 4.383\n",
      "Epoch   6 Batch  162/269   train_loss = 4.243\n",
      "Epoch   6 Batch  163/269   train_loss = 4.512\n",
      "Epoch   6 Batch  164/269   train_loss = 4.559\n",
      "Epoch   6 Batch  165/269   train_loss = 4.534\n",
      "Epoch   6 Batch  166/269   train_loss = 4.354\n",
      "Epoch   6 Batch  167/269   train_loss = 4.329\n",
      "Epoch   6 Batch  168/269   train_loss = 4.240\n",
      "Epoch   6 Batch  169/269   train_loss = 4.607\n",
      "Epoch   6 Batch  170/269   train_loss = 4.202\n",
      "Epoch   6 Batch  171/269   train_loss = 4.701\n",
      "Epoch   6 Batch  172/269   train_loss = 4.262\n",
      "Epoch   6 Batch  173/269   train_loss = 4.429\n",
      "Epoch   6 Batch  174/269   train_loss = 4.523\n",
      "Epoch   6 Batch  175/269   train_loss = 4.253\n",
      "Epoch   6 Batch  176/269   train_loss = 4.243\n",
      "Epoch   6 Batch  177/269   train_loss = 4.305\n",
      "Epoch   6 Batch  178/269   train_loss = 4.569\n",
      "Epoch   6 Batch  179/269   train_loss = 4.298\n",
      "Epoch   6 Batch  180/269   train_loss = 4.127\n",
      "Epoch   6 Batch  181/269   train_loss = 4.330\n",
      "Epoch   6 Batch  182/269   train_loss = 4.077\n",
      "Epoch   6 Batch  183/269   train_loss = 4.388\n",
      "Epoch   6 Batch  184/269   train_loss = 4.347\n",
      "Epoch   6 Batch  185/269   train_loss = 4.284\n",
      "Epoch   6 Batch  186/269   train_loss = 4.114\n",
      "Epoch   6 Batch  187/269   train_loss = 4.539\n",
      "Epoch   6 Batch  188/269   train_loss = 4.223\n",
      "Epoch   6 Batch  189/269   train_loss = 4.326\n",
      "Epoch   6 Batch  190/269   train_loss = 4.569\n",
      "Epoch   6 Batch  191/269   train_loss = 4.281\n",
      "Epoch   6 Batch  192/269   train_loss = 4.207\n",
      "Epoch   6 Batch  193/269   train_loss = 4.163\n",
      "Epoch   6 Batch  194/269   train_loss = 4.383\n",
      "Epoch   6 Batch  195/269   train_loss = 4.167\n",
      "Epoch   6 Batch  196/269   train_loss = 4.459\n",
      "Epoch   6 Batch  197/269   train_loss = 4.486\n",
      "Epoch   6 Batch  198/269   train_loss = 4.501\n",
      "Epoch   6 Batch  199/269   train_loss = 4.612\n",
      "Epoch   6 Batch  200/269   train_loss = 4.577\n",
      "Epoch   6 Batch  201/269   train_loss = 4.154\n",
      "Epoch   6 Batch  202/269   train_loss = 4.116\n",
      "Epoch   6 Batch  203/269   train_loss = 4.251\n",
      "Epoch   6 Batch  204/269   train_loss = 4.217\n",
      "Epoch   6 Batch  205/269   train_loss = 4.407\n",
      "Epoch   6 Batch  206/269   train_loss = 4.289\n",
      "Epoch   6 Batch  207/269   train_loss = 4.240\n",
      "Epoch   6 Batch  208/269   train_loss = 4.343\n",
      "Epoch   6 Batch  209/269   train_loss = 4.378\n",
      "Epoch   6 Batch  210/269   train_loss = 4.236\n",
      "Epoch   6 Batch  211/269   train_loss = 4.159\n",
      "Epoch   6 Batch  212/269   train_loss = 4.757\n",
      "Epoch   6 Batch  213/269   train_loss = 4.326\n",
      "Epoch   6 Batch  214/269   train_loss = 4.391\n",
      "Epoch   6 Batch  215/269   train_loss = 4.560\n",
      "Epoch   6 Batch  216/269   train_loss = 4.459\n",
      "Epoch   6 Batch  217/269   train_loss = 4.272\n",
      "Epoch   6 Batch  218/269   train_loss = 4.246\n",
      "Epoch   6 Batch  219/269   train_loss = 4.028\n",
      "Epoch   6 Batch  220/269   train_loss = 4.292\n",
      "Epoch   6 Batch  221/269   train_loss = 4.210\n",
      "Epoch   6 Batch  222/269   train_loss = 4.301\n",
      "Epoch   6 Batch  223/269   train_loss = 4.274\n",
      "Epoch   6 Batch  224/269   train_loss = 4.393\n",
      "Epoch   6 Batch  225/269   train_loss = 4.536\n",
      "Epoch   6 Batch  226/269   train_loss = 4.459\n",
      "Epoch   6 Batch  227/269   train_loss = 4.121\n",
      "Epoch   6 Batch  228/269   train_loss = 4.271\n",
      "Epoch   6 Batch  229/269   train_loss = 4.421\n",
      "Epoch   6 Batch  230/269   train_loss = 4.504\n",
      "Epoch   6 Batch  231/269   train_loss = 4.254\n",
      "Epoch   6 Batch  232/269   train_loss = 4.209\n",
      "Epoch   6 Batch  233/269   train_loss = 4.333\n",
      "Epoch   6 Batch  234/269   train_loss = 4.271\n",
      "Epoch   6 Batch  235/269   train_loss = 4.526\n",
      "Epoch   6 Batch  236/269   train_loss = 4.196\n",
      "Epoch   6 Batch  237/269   train_loss = 3.940\n",
      "Epoch   6 Batch  238/269   train_loss = 4.217\n",
      "Epoch   6 Batch  239/269   train_loss = 4.639\n",
      "Epoch   6 Batch  240/269   train_loss = 4.283\n",
      "Epoch   6 Batch  241/269   train_loss = 4.487\n",
      "Epoch   6 Batch  242/269   train_loss = 4.020\n",
      "Epoch   6 Batch  243/269   train_loss = 4.308\n",
      "Epoch   6 Batch  244/269   train_loss = 4.060\n",
      "Epoch   6 Batch  245/269   train_loss = 4.148\n",
      "Epoch   6 Batch  246/269   train_loss = 3.958\n",
      "Epoch   6 Batch  247/269   train_loss = 4.393\n",
      "Epoch   6 Batch  248/269   train_loss = 4.379\n",
      "Epoch   6 Batch  249/269   train_loss = 4.287\n",
      "Epoch   6 Batch  250/269   train_loss = 3.879\n",
      "Epoch   6 Batch  251/269   train_loss = 4.269\n",
      "Epoch   6 Batch  252/269   train_loss = 4.444\n",
      "Epoch   6 Batch  253/269   train_loss = 4.287\n",
      "Epoch   6 Batch  254/269   train_loss = 4.068\n",
      "Epoch   6 Batch  255/269   train_loss = 4.376\n",
      "Epoch   6 Batch  256/269   train_loss = 4.234\n",
      "Epoch   6 Batch  257/269   train_loss = 3.978\n",
      "Epoch   6 Batch  258/269   train_loss = 3.995\n",
      "Epoch   6 Batch  259/269   train_loss = 4.001\n",
      "Epoch   6 Batch  260/269   train_loss = 4.247\n",
      "Epoch   6 Batch  261/269   train_loss = 4.333\n",
      "Epoch   6 Batch  262/269   train_loss = 4.136\n",
      "Epoch   6 Batch  263/269   train_loss = 4.039\n",
      "Epoch   6 Batch  264/269   train_loss = 4.661\n",
      "Epoch   6 Batch  265/269   train_loss = 4.318\n",
      "Epoch   6 Batch  266/269   train_loss = 4.222\n",
      "Epoch   6 Batch  267/269   train_loss = 4.312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6 Batch  268/269   train_loss = 4.534\n",
      "Epoch   7 Batch    0/269   train_loss = 4.219\n",
      "Epoch   7 Batch    1/269   train_loss = 3.935\n",
      "Epoch   7 Batch    2/269   train_loss = 4.063\n",
      "Epoch   7 Batch    3/269   train_loss = 4.176\n",
      "Epoch   7 Batch    4/269   train_loss = 4.631\n",
      "Epoch   7 Batch    5/269   train_loss = 4.346\n",
      "Epoch   7 Batch    6/269   train_loss = 4.221\n",
      "Epoch   7 Batch    7/269   train_loss = 4.211\n",
      "Epoch   7 Batch    8/269   train_loss = 4.389\n",
      "Epoch   7 Batch    9/269   train_loss = 4.097\n",
      "Epoch   7 Batch   10/269   train_loss = 4.238\n",
      "Epoch   7 Batch   11/269   train_loss = 4.255\n",
      "Epoch   7 Batch   12/269   train_loss = 4.208\n",
      "Epoch   7 Batch   13/269   train_loss = 4.327\n",
      "Epoch   7 Batch   14/269   train_loss = 4.226\n",
      "Epoch   7 Batch   15/269   train_loss = 4.476\n",
      "Epoch   7 Batch   16/269   train_loss = 4.332\n",
      "Epoch   7 Batch   17/269   train_loss = 4.441\n",
      "Epoch   7 Batch   18/269   train_loss = 4.312\n",
      "Epoch   7 Batch   19/269   train_loss = 4.234\n",
      "Epoch   7 Batch   20/269   train_loss = 4.642\n",
      "Epoch   7 Batch   21/269   train_loss = 4.407\n",
      "Epoch   7 Batch   22/269   train_loss = 4.069\n",
      "Epoch   7 Batch   23/269   train_loss = 4.312\n",
      "Epoch   7 Batch   24/269   train_loss = 4.227\n",
      "Epoch   7 Batch   25/269   train_loss = 4.465\n",
      "Epoch   7 Batch   26/269   train_loss = 4.404\n",
      "Epoch   7 Batch   27/269   train_loss = 4.211\n",
      "Epoch   7 Batch   28/269   train_loss = 4.335\n",
      "Epoch   7 Batch   29/269   train_loss = 4.444\n",
      "Epoch   7 Batch   30/269   train_loss = 4.433\n",
      "Epoch   7 Batch   31/269   train_loss = 4.230\n",
      "Epoch   7 Batch   32/269   train_loss = 4.098\n",
      "Epoch   7 Batch   33/269   train_loss = 4.076\n",
      "Epoch   7 Batch   34/269   train_loss = 4.313\n",
      "Epoch   7 Batch   35/269   train_loss = 4.193\n",
      "Epoch   7 Batch   36/269   train_loss = 4.126\n",
      "Epoch   7 Batch   37/269   train_loss = 4.182\n",
      "Epoch   7 Batch   38/269   train_loss = 4.186\n",
      "Epoch   7 Batch   39/269   train_loss = 4.444\n",
      "Epoch   7 Batch   40/269   train_loss = 4.119\n",
      "Epoch   7 Batch   41/269   train_loss = 4.161\n",
      "Epoch   7 Batch   42/269   train_loss = 4.693\n",
      "Epoch   7 Batch   43/269   train_loss = 4.413\n",
      "Epoch   7 Batch   44/269   train_loss = 4.283\n",
      "Epoch   7 Batch   45/269   train_loss = 4.101\n",
      "Epoch   7 Batch   46/269   train_loss = 4.492\n",
      "Epoch   7 Batch   47/269   train_loss = 3.958\n",
      "Epoch   7 Batch   48/269   train_loss = 4.579\n",
      "Epoch   7 Batch   49/269   train_loss = 4.409\n",
      "Epoch   7 Batch   50/269   train_loss = 4.516\n",
      "Epoch   7 Batch   51/269   train_loss = 4.292\n",
      "Epoch   7 Batch   52/269   train_loss = 4.435\n",
      "Epoch   7 Batch   53/269   train_loss = 4.429\n",
      "Epoch   7 Batch   54/269   train_loss = 4.316\n",
      "Epoch   7 Batch   55/269   train_loss = 4.386\n",
      "Epoch   7 Batch   56/269   train_loss = 4.282\n",
      "Epoch   7 Batch   57/269   train_loss = 4.327\n",
      "Epoch   7 Batch   58/269   train_loss = 4.404\n",
      "Epoch   7 Batch   59/269   train_loss = 4.088\n",
      "Epoch   7 Batch   60/269   train_loss = 4.390\n",
      "Epoch   7 Batch   61/269   train_loss = 4.576\n",
      "Epoch   7 Batch   62/269   train_loss = 4.238\n",
      "Epoch   7 Batch   63/269   train_loss = 4.522\n",
      "Epoch   7 Batch   64/269   train_loss = 4.220\n",
      "Epoch   7 Batch   65/269   train_loss = 4.317\n",
      "Epoch   7 Batch   66/269   train_loss = 4.346\n",
      "Epoch   7 Batch   67/269   train_loss = 4.250\n",
      "Epoch   7 Batch   68/269   train_loss = 4.393\n",
      "Epoch   7 Batch   69/269   train_loss = 4.441\n",
      "Epoch   7 Batch   70/269   train_loss = 4.022\n",
      "Epoch   7 Batch   71/269   train_loss = 4.388\n",
      "Epoch   7 Batch   72/269   train_loss = 4.038\n",
      "Epoch   7 Batch   73/269   train_loss = 4.231\n",
      "Epoch   7 Batch   74/269   train_loss = 4.278\n",
      "Epoch   7 Batch   75/269   train_loss = 4.460\n",
      "Epoch   7 Batch   76/269   train_loss = 4.355\n",
      "Epoch   7 Batch   77/269   train_loss = 3.940\n",
      "Epoch   7 Batch   78/269   train_loss = 4.322\n",
      "Epoch   7 Batch   79/269   train_loss = 4.361\n",
      "Epoch   7 Batch   80/269   train_loss = 4.131\n",
      "Epoch   7 Batch   81/269   train_loss = 4.011\n",
      "Epoch   7 Batch   82/269   train_loss = 4.395\n",
      "Epoch   7 Batch   83/269   train_loss = 4.293\n",
      "Epoch   7 Batch   84/269   train_loss = 4.481\n",
      "Epoch   7 Batch   85/269   train_loss = 4.626\n",
      "Epoch   7 Batch   86/269   train_loss = 4.452\n",
      "Epoch   7 Batch   87/269   train_loss = 4.289\n",
      "Epoch   7 Batch   88/269   train_loss = 4.182\n",
      "Epoch   7 Batch   89/269   train_loss = 4.279\n",
      "Epoch   7 Batch   90/269   train_loss = 4.331\n",
      "Epoch   7 Batch   91/269   train_loss = 4.479\n",
      "Epoch   7 Batch   92/269   train_loss = 4.409\n",
      "Epoch   7 Batch   93/269   train_loss = 3.960\n",
      "Epoch   7 Batch   94/269   train_loss = 4.522\n",
      "Epoch   7 Batch   95/269   train_loss = 4.115\n",
      "Epoch   7 Batch   96/269   train_loss = 4.203\n",
      "Epoch   7 Batch   97/269   train_loss = 4.289\n",
      "Epoch   7 Batch   98/269   train_loss = 4.214\n",
      "Epoch   7 Batch   99/269   train_loss = 4.399\n",
      "Epoch   7 Batch  100/269   train_loss = 4.255\n",
      "Epoch   7 Batch  101/269   train_loss = 4.322\n",
      "Epoch   7 Batch  102/269   train_loss = 4.193\n",
      "Epoch   7 Batch  103/269   train_loss = 4.253\n",
      "Epoch   7 Batch  104/269   train_loss = 4.244\n",
      "Epoch   7 Batch  105/269   train_loss = 4.328\n",
      "Epoch   7 Batch  106/269   train_loss = 4.408\n",
      "Epoch   7 Batch  107/269   train_loss = 4.441\n",
      "Epoch   7 Batch  108/269   train_loss = 4.339\n",
      "Epoch   7 Batch  109/269   train_loss = 4.315\n",
      "Epoch   7 Batch  110/269   train_loss = 4.294\n",
      "Epoch   7 Batch  111/269   train_loss = 4.408\n",
      "Epoch   7 Batch  112/269   train_loss = 4.348\n",
      "Epoch   7 Batch  113/269   train_loss = 4.128\n",
      "Epoch   7 Batch  114/269   train_loss = 4.190\n",
      "Epoch   7 Batch  115/269   train_loss = 4.126\n",
      "Epoch   7 Batch  116/269   train_loss = 4.139\n",
      "Epoch   7 Batch  117/269   train_loss = 4.013\n",
      "Epoch   7 Batch  118/269   train_loss = 4.356\n",
      "Epoch   7 Batch  119/269   train_loss = 4.118\n",
      "Epoch   7 Batch  120/269   train_loss = 4.331\n",
      "Epoch   7 Batch  121/269   train_loss = 4.180\n",
      "Epoch   7 Batch  122/269   train_loss = 4.240\n",
      "Epoch   7 Batch  123/269   train_loss = 4.037\n",
      "Epoch   7 Batch  124/269   train_loss = 4.425\n",
      "Epoch   7 Batch  125/269   train_loss = 3.982\n",
      "Epoch   7 Batch  126/269   train_loss = 3.875\n",
      "Epoch   7 Batch  127/269   train_loss = 4.191\n",
      "Epoch   7 Batch  128/269   train_loss = 4.164\n",
      "Epoch   7 Batch  129/269   train_loss = 4.276\n",
      "Epoch   7 Batch  130/269   train_loss = 4.467\n",
      "Epoch   7 Batch  131/269   train_loss = 4.133\n",
      "Epoch   7 Batch  132/269   train_loss = 4.272\n",
      "Epoch   7 Batch  133/269   train_loss = 4.038\n",
      "Epoch   7 Batch  134/269   train_loss = 4.350\n",
      "Epoch   7 Batch  135/269   train_loss = 3.996\n",
      "Epoch   7 Batch  136/269   train_loss = 3.991\n",
      "Epoch   7 Batch  137/269   train_loss = 4.327\n",
      "Epoch   7 Batch  138/269   train_loss = 4.087\n",
      "Epoch   7 Batch  139/269   train_loss = 4.249\n",
      "Epoch   7 Batch  140/269   train_loss = 4.370\n",
      "Epoch   7 Batch  141/269   train_loss = 4.324\n",
      "Epoch   7 Batch  142/269   train_loss = 3.993\n",
      "Epoch   7 Batch  143/269   train_loss = 4.228\n",
      "Epoch   7 Batch  144/269   train_loss = 4.419\n",
      "Epoch   7 Batch  145/269   train_loss = 4.109\n",
      "Epoch   7 Batch  146/269   train_loss = 4.345\n",
      "Epoch   7 Batch  147/269   train_loss = 3.980\n",
      "Epoch   7 Batch  148/269   train_loss = 4.326\n",
      "Epoch   7 Batch  149/269   train_loss = 4.196\n",
      "Epoch   7 Batch  150/269   train_loss = 4.206\n",
      "Epoch   7 Batch  151/269   train_loss = 4.212\n",
      "Epoch   7 Batch  152/269   train_loss = 4.089\n",
      "Epoch   7 Batch  153/269   train_loss = 4.333\n",
      "Epoch   7 Batch  154/269   train_loss = 4.224\n",
      "Epoch   7 Batch  155/269   train_loss = 4.172\n",
      "Epoch   7 Batch  156/269   train_loss = 4.656\n",
      "Epoch   7 Batch  157/269   train_loss = 4.622\n",
      "Epoch   7 Batch  158/269   train_loss = 4.321\n",
      "Epoch   7 Batch  159/269   train_loss = 4.385\n",
      "Epoch   7 Batch  160/269   train_loss = 4.149\n",
      "Epoch   7 Batch  161/269   train_loss = 4.293\n",
      "Epoch   7 Batch  162/269   train_loss = 4.184\n",
      "Epoch   7 Batch  163/269   train_loss = 4.421\n",
      "Epoch   7 Batch  164/269   train_loss = 4.502\n",
      "Epoch   7 Batch  165/269   train_loss = 4.422\n",
      "Epoch   7 Batch  166/269   train_loss = 4.274\n",
      "Epoch   7 Batch  167/269   train_loss = 4.257\n",
      "Epoch   7 Batch  168/269   train_loss = 4.160\n",
      "Epoch   7 Batch  169/269   train_loss = 4.536\n",
      "Epoch   7 Batch  170/269   train_loss = 4.129\n",
      "Epoch   7 Batch  171/269   train_loss = 4.548\n",
      "Epoch   7 Batch  172/269   train_loss = 4.183\n",
      "Epoch   7 Batch  173/269   train_loss = 4.334\n",
      "Epoch   7 Batch  174/269   train_loss = 4.427\n",
      "Epoch   7 Batch  175/269   train_loss = 4.182\n",
      "Epoch   7 Batch  176/269   train_loss = 4.162\n",
      "Epoch   7 Batch  177/269   train_loss = 4.231\n",
      "Epoch   7 Batch  178/269   train_loss = 4.509\n",
      "Epoch   7 Batch  179/269   train_loss = 4.241\n",
      "Epoch   7 Batch  180/269   train_loss = 4.050\n",
      "Epoch   7 Batch  181/269   train_loss = 4.266\n",
      "Epoch   7 Batch  182/269   train_loss = 4.032\n",
      "Epoch   7 Batch  183/269   train_loss = 4.326\n",
      "Epoch   7 Batch  184/269   train_loss = 4.271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7 Batch  185/269   train_loss = 4.224\n",
      "Epoch   7 Batch  186/269   train_loss = 4.083\n",
      "Epoch   7 Batch  187/269   train_loss = 4.485\n",
      "Epoch   7 Batch  188/269   train_loss = 4.157\n",
      "Epoch   7 Batch  189/269   train_loss = 4.249\n",
      "Epoch   7 Batch  190/269   train_loss = 4.520\n",
      "Epoch   7 Batch  191/269   train_loss = 4.236\n",
      "Epoch   7 Batch  192/269   train_loss = 4.164\n",
      "Epoch   7 Batch  193/269   train_loss = 4.104\n",
      "Epoch   7 Batch  194/269   train_loss = 4.316\n",
      "Epoch   7 Batch  195/269   train_loss = 4.095\n",
      "Epoch   7 Batch  196/269   train_loss = 4.390\n",
      "Epoch   7 Batch  197/269   train_loss = 4.423\n",
      "Epoch   7 Batch  198/269   train_loss = 4.456\n",
      "Epoch   7 Batch  199/269   train_loss = 4.489\n",
      "Epoch   7 Batch  200/269   train_loss = 4.509\n",
      "Epoch   7 Batch  201/269   train_loss = 4.106\n",
      "Epoch   7 Batch  202/269   train_loss = 4.045\n",
      "Epoch   7 Batch  203/269   train_loss = 4.165\n",
      "Epoch   7 Batch  204/269   train_loss = 4.200\n",
      "Epoch   7 Batch  205/269   train_loss = 4.337\n",
      "Epoch   7 Batch  206/269   train_loss = 4.192\n",
      "Epoch   7 Batch  207/269   train_loss = 4.175\n",
      "Epoch   7 Batch  208/269   train_loss = 4.269\n",
      "Epoch   7 Batch  209/269   train_loss = 4.346\n",
      "Epoch   7 Batch  210/269   train_loss = 4.165\n",
      "Epoch   7 Batch  211/269   train_loss = 4.133\n",
      "Epoch   7 Batch  212/269   train_loss = 4.689\n",
      "Epoch   7 Batch  213/269   train_loss = 4.268\n",
      "Epoch   7 Batch  214/269   train_loss = 4.329\n",
      "Epoch   7 Batch  215/269   train_loss = 4.453\n",
      "Epoch   7 Batch  216/269   train_loss = 4.407\n",
      "Epoch   7 Batch  217/269   train_loss = 4.191\n",
      "Epoch   7 Batch  218/269   train_loss = 4.186\n",
      "Epoch   7 Batch  219/269   train_loss = 3.943\n",
      "Epoch   7 Batch  220/269   train_loss = 4.244\n",
      "Epoch   7 Batch  221/269   train_loss = 4.138\n",
      "Epoch   7 Batch  222/269   train_loss = 4.236\n",
      "Epoch   7 Batch  223/269   train_loss = 4.173\n",
      "Epoch   7 Batch  224/269   train_loss = 4.331\n",
      "Epoch   7 Batch  225/269   train_loss = 4.461\n",
      "Epoch   7 Batch  226/269   train_loss = 4.360\n",
      "Epoch   7 Batch  227/269   train_loss = 4.002\n",
      "Epoch   7 Batch  228/269   train_loss = 4.234\n",
      "Epoch   7 Batch  229/269   train_loss = 4.346\n",
      "Epoch   7 Batch  230/269   train_loss = 4.434\n",
      "Epoch   7 Batch  231/269   train_loss = 4.184\n",
      "Epoch   7 Batch  232/269   train_loss = 4.143\n",
      "Epoch   7 Batch  233/269   train_loss = 4.281\n",
      "Epoch   7 Batch  234/269   train_loss = 4.185\n",
      "Epoch   7 Batch  235/269   train_loss = 4.488\n",
      "Epoch   7 Batch  236/269   train_loss = 4.120\n",
      "Epoch   7 Batch  237/269   train_loss = 3.897\n",
      "Epoch   7 Batch  238/269   train_loss = 4.125\n",
      "Epoch   7 Batch  239/269   train_loss = 4.563\n",
      "Epoch   7 Batch  240/269   train_loss = 4.230\n",
      "Epoch   7 Batch  241/269   train_loss = 4.413\n",
      "Epoch   7 Batch  242/269   train_loss = 3.930\n",
      "Epoch   7 Batch  243/269   train_loss = 4.263\n",
      "Epoch   7 Batch  244/269   train_loss = 4.002\n",
      "Epoch   7 Batch  245/269   train_loss = 4.118\n",
      "Epoch   7 Batch  246/269   train_loss = 3.890\n",
      "Epoch   7 Batch  247/269   train_loss = 4.302\n",
      "Epoch   7 Batch  248/269   train_loss = 4.323\n",
      "Epoch   7 Batch  249/269   train_loss = 4.221\n",
      "Epoch   7 Batch  250/269   train_loss = 3.825\n",
      "Epoch   7 Batch  251/269   train_loss = 4.228\n",
      "Epoch   7 Batch  252/269   train_loss = 4.396\n",
      "Epoch   7 Batch  253/269   train_loss = 4.228\n",
      "Epoch   7 Batch  254/269   train_loss = 4.010\n",
      "Epoch   7 Batch  255/269   train_loss = 4.312\n",
      "Epoch   7 Batch  256/269   train_loss = 4.160\n",
      "Epoch   7 Batch  257/269   train_loss = 3.918\n",
      "Epoch   7 Batch  258/269   train_loss = 3.917\n",
      "Epoch   7 Batch  259/269   train_loss = 3.950\n",
      "Epoch   7 Batch  260/269   train_loss = 4.225\n",
      "Epoch   7 Batch  261/269   train_loss = 4.269\n",
      "Epoch   7 Batch  262/269   train_loss = 4.064\n",
      "Epoch   7 Batch  263/269   train_loss = 3.982\n",
      "Epoch   7 Batch  264/269   train_loss = 4.589\n",
      "Epoch   7 Batch  265/269   train_loss = 4.240\n",
      "Epoch   7 Batch  266/269   train_loss = 4.185\n",
      "Epoch   7 Batch  267/269   train_loss = 4.247\n",
      "Epoch   7 Batch  268/269   train_loss = 4.461\n",
      "Epoch   8 Batch    0/269   train_loss = 4.147\n",
      "Epoch   8 Batch    1/269   train_loss = 3.907\n",
      "Epoch   8 Batch    2/269   train_loss = 4.004\n",
      "Epoch   8 Batch    3/269   train_loss = 4.125\n",
      "Epoch   8 Batch    4/269   train_loss = 4.585\n",
      "Epoch   8 Batch    5/269   train_loss = 4.266\n",
      "Epoch   8 Batch    6/269   train_loss = 4.175\n",
      "Epoch   8 Batch    7/269   train_loss = 4.104\n",
      "Epoch   8 Batch    8/269   train_loss = 4.324\n",
      "Epoch   8 Batch    9/269   train_loss = 4.049\n",
      "Epoch   8 Batch   10/269   train_loss = 4.150\n",
      "Epoch   8 Batch   11/269   train_loss = 4.199\n",
      "Epoch   8 Batch   12/269   train_loss = 4.108\n",
      "Epoch   8 Batch   13/269   train_loss = 4.277\n",
      "Epoch   8 Batch   14/269   train_loss = 4.105\n",
      "Epoch   8 Batch   15/269   train_loss = 4.419\n",
      "Epoch   8 Batch   16/269   train_loss = 4.246\n",
      "Epoch   8 Batch   17/269   train_loss = 4.360\n",
      "Epoch   8 Batch   18/269   train_loss = 4.235\n",
      "Epoch   8 Batch   19/269   train_loss = 4.151\n",
      "Epoch   8 Batch   20/269   train_loss = 4.578\n",
      "Epoch   8 Batch   21/269   train_loss = 4.324\n",
      "Epoch   8 Batch   22/269   train_loss = 3.992\n",
      "Epoch   8 Batch   23/269   train_loss = 4.238\n",
      "Epoch   8 Batch   24/269   train_loss = 4.134\n",
      "Epoch   8 Batch   25/269   train_loss = 4.390\n",
      "Epoch   8 Batch   26/269   train_loss = 4.332\n",
      "Epoch   8 Batch   27/269   train_loss = 4.147\n",
      "Epoch   8 Batch   28/269   train_loss = 4.252\n",
      "Epoch   8 Batch   29/269   train_loss = 4.385\n",
      "Epoch   8 Batch   30/269   train_loss = 4.398\n",
      "Epoch   8 Batch   31/269   train_loss = 4.183\n",
      "Epoch   8 Batch   32/269   train_loss = 4.027\n",
      "Epoch   8 Batch   33/269   train_loss = 4.013\n",
      "Epoch   8 Batch   34/269   train_loss = 4.256\n",
      "Epoch   8 Batch   35/269   train_loss = 4.122\n",
      "Epoch   8 Batch   36/269   train_loss = 4.065\n",
      "Epoch   8 Batch   37/269   train_loss = 4.098\n",
      "Epoch   8 Batch   38/269   train_loss = 4.106\n",
      "Epoch   8 Batch   39/269   train_loss = 4.374\n",
      "Epoch   8 Batch   40/269   train_loss = 4.086\n",
      "Epoch   8 Batch   41/269   train_loss = 4.091\n",
      "Epoch   8 Batch   42/269   train_loss = 4.633\n",
      "Epoch   8 Batch   43/269   train_loss = 4.339\n",
      "Epoch   8 Batch   44/269   train_loss = 4.166\n",
      "Epoch   8 Batch   45/269   train_loss = 4.024\n",
      "Epoch   8 Batch   46/269   train_loss = 4.369\n",
      "Epoch   8 Batch   47/269   train_loss = 3.881\n",
      "Epoch   8 Batch   48/269   train_loss = 4.519\n",
      "Epoch   8 Batch   49/269   train_loss = 4.369\n",
      "Epoch   8 Batch   50/269   train_loss = 4.417\n",
      "Epoch   8 Batch   51/269   train_loss = 4.261\n",
      "Epoch   8 Batch   52/269   train_loss = 4.353\n",
      "Epoch   8 Batch   53/269   train_loss = 4.392\n",
      "Epoch   8 Batch   54/269   train_loss = 4.246\n",
      "Epoch   8 Batch   55/269   train_loss = 4.305\n",
      "Epoch   8 Batch   56/269   train_loss = 4.218\n",
      "Epoch   8 Batch   57/269   train_loss = 4.250\n",
      "Epoch   8 Batch   58/269   train_loss = 4.303\n",
      "Epoch   8 Batch   59/269   train_loss = 4.026\n",
      "Epoch   8 Batch   60/269   train_loss = 4.306\n",
      "Epoch   8 Batch   61/269   train_loss = 4.559\n",
      "Epoch   8 Batch   62/269   train_loss = 4.218\n",
      "Epoch   8 Batch   63/269   train_loss = 4.485\n",
      "Epoch   8 Batch   64/269   train_loss = 4.169\n",
      "Epoch   8 Batch   65/269   train_loss = 4.232\n",
      "Epoch   8 Batch   66/269   train_loss = 4.262\n",
      "Epoch   8 Batch   67/269   train_loss = 4.208\n",
      "Epoch   8 Batch   68/269   train_loss = 4.323\n",
      "Epoch   8 Batch   69/269   train_loss = 4.351\n",
      "Epoch   8 Batch   70/269   train_loss = 3.974\n",
      "Epoch   8 Batch   71/269   train_loss = 4.334\n",
      "Epoch   8 Batch   72/269   train_loss = 3.976\n",
      "Epoch   8 Batch   73/269   train_loss = 4.180\n",
      "Epoch   8 Batch   74/269   train_loss = 4.179\n",
      "Epoch   8 Batch   75/269   train_loss = 4.389\n",
      "Epoch   8 Batch   76/269   train_loss = 4.323\n",
      "Epoch   8 Batch   77/269   train_loss = 3.880\n",
      "Epoch   8 Batch   78/269   train_loss = 4.257\n",
      "Epoch   8 Batch   79/269   train_loss = 4.309\n",
      "Epoch   8 Batch   80/269   train_loss = 4.084\n",
      "Epoch   8 Batch   81/269   train_loss = 3.961\n",
      "Epoch   8 Batch   82/269   train_loss = 4.356\n",
      "Epoch   8 Batch   83/269   train_loss = 4.251\n",
      "Epoch   8 Batch   84/269   train_loss = 4.427\n",
      "Epoch   8 Batch   85/269   train_loss = 4.599\n",
      "Epoch   8 Batch   86/269   train_loss = 4.410\n",
      "Epoch   8 Batch   87/269   train_loss = 4.236\n",
      "Epoch   8 Batch   88/269   train_loss = 4.130\n",
      "Epoch   8 Batch   89/269   train_loss = 4.217\n",
      "Epoch   8 Batch   90/269   train_loss = 4.259\n",
      "Epoch   8 Batch   91/269   train_loss = 4.465\n",
      "Epoch   8 Batch   92/269   train_loss = 4.325\n",
      "Epoch   8 Batch   93/269   train_loss = 3.933\n",
      "Epoch   8 Batch   94/269   train_loss = 4.449\n",
      "Epoch   8 Batch   95/269   train_loss = 4.060\n",
      "Epoch   8 Batch   96/269   train_loss = 4.110\n",
      "Epoch   8 Batch   97/269   train_loss = 4.241\n",
      "Epoch   8 Batch   98/269   train_loss = 4.177\n",
      "Epoch   8 Batch   99/269   train_loss = 4.303\n",
      "Epoch   8 Batch  100/269   train_loss = 4.186\n",
      "Epoch   8 Batch  101/269   train_loss = 4.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8 Batch  102/269   train_loss = 4.121\n",
      "Epoch   8 Batch  103/269   train_loss = 4.188\n",
      "Epoch   8 Batch  104/269   train_loss = 4.174\n",
      "Epoch   8 Batch  105/269   train_loss = 4.302\n",
      "Epoch   8 Batch  106/269   train_loss = 4.414\n",
      "Epoch   8 Batch  107/269   train_loss = 4.384\n",
      "Epoch   8 Batch  108/269   train_loss = 4.299\n",
      "Epoch   8 Batch  109/269   train_loss = 4.274\n",
      "Epoch   8 Batch  110/269   train_loss = 4.236\n",
      "Epoch   8 Batch  111/269   train_loss = 4.381\n",
      "Epoch   8 Batch  112/269   train_loss = 4.288\n",
      "Epoch   8 Batch  113/269   train_loss = 4.100\n",
      "Epoch   8 Batch  114/269   train_loss = 4.123\n",
      "Epoch   8 Batch  115/269   train_loss = 4.059\n",
      "Epoch   8 Batch  116/269   train_loss = 4.063\n",
      "Epoch   8 Batch  117/269   train_loss = 3.974\n",
      "Epoch   8 Batch  118/269   train_loss = 4.308\n",
      "Epoch   8 Batch  119/269   train_loss = 4.056\n",
      "Epoch   8 Batch  120/269   train_loss = 4.299\n",
      "Epoch   8 Batch  121/269   train_loss = 4.143\n",
      "Epoch   8 Batch  122/269   train_loss = 4.168\n",
      "Epoch   8 Batch  123/269   train_loss = 4.010\n",
      "Epoch   8 Batch  124/269   train_loss = 4.373\n",
      "Epoch   8 Batch  125/269   train_loss = 3.941\n",
      "Epoch   8 Batch  126/269   train_loss = 3.803\n",
      "Epoch   8 Batch  127/269   train_loss = 4.129\n",
      "Epoch   8 Batch  128/269   train_loss = 4.131\n",
      "Epoch   8 Batch  129/269   train_loss = 4.222\n",
      "Epoch   8 Batch  130/269   train_loss = 4.415\n",
      "Epoch   8 Batch  131/269   train_loss = 4.069\n",
      "Epoch   8 Batch  132/269   train_loss = 4.261\n",
      "Epoch   8 Batch  133/269   train_loss = 3.985\n",
      "Epoch   8 Batch  134/269   train_loss = 4.301\n",
      "Epoch   8 Batch  135/269   train_loss = 3.943\n",
      "Epoch   8 Batch  136/269   train_loss = 3.934\n",
      "Epoch   8 Batch  137/269   train_loss = 4.284\n",
      "Epoch   8 Batch  138/269   train_loss = 4.022\n",
      "Epoch   8 Batch  139/269   train_loss = 4.192\n",
      "Epoch   8 Batch  140/269   train_loss = 4.320\n",
      "Epoch   8 Batch  141/269   train_loss = 4.302\n",
      "Epoch   8 Batch  142/269   train_loss = 3.940\n",
      "Epoch   8 Batch  143/269   train_loss = 4.207\n",
      "Epoch   8 Batch  144/269   train_loss = 4.384\n",
      "Epoch   8 Batch  145/269   train_loss = 4.091\n",
      "Epoch   8 Batch  146/269   train_loss = 4.271\n",
      "Epoch   8 Batch  147/269   train_loss = 3.955\n",
      "Epoch   8 Batch  148/269   train_loss = 4.246\n",
      "Epoch   8 Batch  149/269   train_loss = 4.133\n",
      "Epoch   8 Batch  150/269   train_loss = 4.163\n",
      "Epoch   8 Batch  151/269   train_loss = 4.159\n",
      "Epoch   8 Batch  152/269   train_loss = 4.039\n",
      "Epoch   8 Batch  153/269   train_loss = 4.270\n",
      "Epoch   8 Batch  154/269   train_loss = 4.173\n",
      "Epoch   8 Batch  155/269   train_loss = 4.084\n",
      "Epoch   8 Batch  156/269   train_loss = 4.562\n",
      "Epoch   8 Batch  157/269   train_loss = 4.535\n",
      "Epoch   8 Batch  158/269   train_loss = 4.240\n",
      "Epoch   8 Batch  159/269   train_loss = 4.314\n",
      "Epoch   8 Batch  160/269   train_loss = 4.112\n",
      "Epoch   8 Batch  161/269   train_loss = 4.228\n",
      "Epoch   8 Batch  162/269   train_loss = 4.153\n",
      "Epoch   8 Batch  163/269   train_loss = 4.345\n",
      "Epoch   8 Batch  164/269   train_loss = 4.430\n",
      "Epoch   8 Batch  165/269   train_loss = 4.365\n",
      "Epoch   8 Batch  166/269   train_loss = 4.212\n",
      "Epoch   8 Batch  167/269   train_loss = 4.188\n",
      "Epoch   8 Batch  168/269   train_loss = 4.123\n",
      "Epoch   8 Batch  169/269   train_loss = 4.471\n",
      "Epoch   8 Batch  170/269   train_loss = 4.054\n",
      "Epoch   8 Batch  171/269   train_loss = 4.500\n",
      "Epoch   8 Batch  172/269   train_loss = 4.140\n",
      "Epoch   8 Batch  173/269   train_loss = 4.309\n",
      "Epoch   8 Batch  174/269   train_loss = 4.358\n",
      "Epoch   8 Batch  175/269   train_loss = 4.148\n",
      "Epoch   8 Batch  176/269   train_loss = 4.101\n",
      "Epoch   8 Batch  177/269   train_loss = 4.172\n",
      "Epoch   8 Batch  178/269   train_loss = 4.448\n",
      "Epoch   8 Batch  179/269   train_loss = 4.196\n",
      "Epoch   8 Batch  180/269   train_loss = 4.023\n",
      "Epoch   8 Batch  181/269   train_loss = 4.251\n",
      "Epoch   8 Batch  182/269   train_loss = 3.995\n",
      "Epoch   8 Batch  183/269   train_loss = 4.255\n",
      "Epoch   8 Batch  184/269   train_loss = 4.215\n",
      "Epoch   8 Batch  185/269   train_loss = 4.226\n",
      "Epoch   8 Batch  186/269   train_loss = 4.045\n",
      "Epoch   8 Batch  187/269   train_loss = 4.455\n",
      "Epoch   8 Batch  188/269   train_loss = 4.086\n",
      "Epoch   8 Batch  189/269   train_loss = 4.160\n",
      "Epoch   8 Batch  190/269   train_loss = 4.474\n",
      "Epoch   8 Batch  191/269   train_loss = 4.240\n",
      "Epoch   8 Batch  192/269   train_loss = 4.128\n",
      "Epoch   8 Batch  193/269   train_loss = 4.059\n",
      "Epoch   8 Batch  194/269   train_loss = 4.272\n",
      "Epoch   8 Batch  195/269   train_loss = 4.010\n",
      "Epoch   8 Batch  196/269   train_loss = 4.325\n",
      "Epoch   8 Batch  197/269   train_loss = 4.401\n",
      "Epoch   8 Batch  198/269   train_loss = 4.383\n",
      "Epoch   8 Batch  199/269   train_loss = 4.414\n",
      "Epoch   8 Batch  200/269   train_loss = 4.417\n",
      "Epoch   8 Batch  201/269   train_loss = 4.078\n",
      "Epoch   8 Batch  202/269   train_loss = 3.963\n",
      "Epoch   8 Batch  203/269   train_loss = 4.088\n",
      "Epoch   8 Batch  204/269   train_loss = 4.166\n",
      "Epoch   8 Batch  205/269   train_loss = 4.271\n",
      "Epoch   8 Batch  206/269   train_loss = 4.135\n",
      "Epoch   8 Batch  207/269   train_loss = 4.161\n",
      "Epoch   8 Batch  208/269   train_loss = 4.195\n",
      "Epoch   8 Batch  209/269   train_loss = 4.279\n",
      "Epoch   8 Batch  210/269   train_loss = 4.113\n",
      "Epoch   8 Batch  211/269   train_loss = 4.134\n",
      "Epoch   8 Batch  212/269   train_loss = 4.598\n",
      "Epoch   8 Batch  213/269   train_loss = 4.196\n",
      "Epoch   8 Batch  214/269   train_loss = 4.275\n",
      "Epoch   8 Batch  215/269   train_loss = 4.386\n",
      "Epoch   8 Batch  216/269   train_loss = 4.376\n",
      "Epoch   8 Batch  217/269   train_loss = 4.107\n",
      "Epoch   8 Batch  218/269   train_loss = 4.144\n",
      "Epoch   8 Batch  219/269   train_loss = 3.883\n",
      "Epoch   8 Batch  220/269   train_loss = 4.214\n",
      "Epoch   8 Batch  221/269   train_loss = 4.090\n",
      "Epoch   8 Batch  222/269   train_loss = 4.193\n",
      "Epoch   8 Batch  223/269   train_loss = 4.131\n",
      "Epoch   8 Batch  224/269   train_loss = 4.261\n",
      "Epoch   8 Batch  225/269   train_loss = 4.399\n",
      "Epoch   8 Batch  226/269   train_loss = 4.261\n",
      "Epoch   8 Batch  227/269   train_loss = 3.955\n",
      "Epoch   8 Batch  228/269   train_loss = 4.150\n",
      "Epoch   8 Batch  229/269   train_loss = 4.264\n",
      "Epoch   8 Batch  230/269   train_loss = 4.355\n",
      "Epoch   8 Batch  231/269   train_loss = 4.148\n",
      "Epoch   8 Batch  232/269   train_loss = 4.072\n",
      "Epoch   8 Batch  233/269   train_loss = 4.204\n",
      "Epoch   8 Batch  234/269   train_loss = 4.124\n",
      "Epoch   8 Batch  235/269   train_loss = 4.413\n",
      "Epoch   8 Batch  236/269   train_loss = 4.050\n",
      "Epoch   8 Batch  237/269   train_loss = 3.841\n",
      "Epoch   8 Batch  238/269   train_loss = 4.087\n",
      "Epoch   8 Batch  239/269   train_loss = 4.487\n",
      "Epoch   8 Batch  240/269   train_loss = 4.162\n",
      "Epoch   8 Batch  241/269   train_loss = 4.320\n",
      "Epoch   8 Batch  242/269   train_loss = 3.901\n",
      "Epoch   8 Batch  243/269   train_loss = 4.200\n",
      "Epoch   8 Batch  244/269   train_loss = 3.962\n",
      "Epoch   8 Batch  245/269   train_loss = 4.075\n",
      "Epoch   8 Batch  246/269   train_loss = 3.855\n",
      "Epoch   8 Batch  247/269   train_loss = 4.261\n",
      "Epoch   8 Batch  248/269   train_loss = 4.264\n",
      "Epoch   8 Batch  249/269   train_loss = 4.151\n",
      "Epoch   8 Batch  250/269   train_loss = 3.783\n",
      "Epoch   8 Batch  251/269   train_loss = 4.181\n",
      "Epoch   8 Batch  252/269   train_loss = 4.316\n",
      "Epoch   8 Batch  253/269   train_loss = 4.176\n",
      "Epoch   8 Batch  254/269   train_loss = 3.973\n",
      "Epoch   8 Batch  255/269   train_loss = 4.235\n",
      "Epoch   8 Batch  256/269   train_loss = 4.125\n",
      "Epoch   8 Batch  257/269   train_loss = 3.877\n",
      "Epoch   8 Batch  258/269   train_loss = 3.841\n",
      "Epoch   8 Batch  259/269   train_loss = 3.936\n",
      "Epoch   8 Batch  260/269   train_loss = 4.193\n",
      "Epoch   8 Batch  261/269   train_loss = 4.213\n",
      "Epoch   8 Batch  262/269   train_loss = 4.008\n",
      "Epoch   8 Batch  263/269   train_loss = 3.887\n",
      "Epoch   8 Batch  264/269   train_loss = 4.517\n",
      "Epoch   8 Batch  265/269   train_loss = 4.192\n",
      "Epoch   8 Batch  266/269   train_loss = 4.132\n",
      "Epoch   8 Batch  267/269   train_loss = 4.196\n",
      "Epoch   8 Batch  268/269   train_loss = 4.357\n",
      "Epoch   9 Batch    0/269   train_loss = 4.041\n",
      "Epoch   9 Batch    1/269   train_loss = 3.847\n",
      "Epoch   9 Batch    2/269   train_loss = 3.986\n",
      "Epoch   9 Batch    3/269   train_loss = 4.057\n",
      "Epoch   9 Batch    4/269   train_loss = 4.515\n",
      "Epoch   9 Batch    5/269   train_loss = 4.227\n",
      "Epoch   9 Batch    6/269   train_loss = 4.083\n",
      "Epoch   9 Batch    7/269   train_loss = 4.038\n",
      "Epoch   9 Batch    8/269   train_loss = 4.253\n",
      "Epoch   9 Batch    9/269   train_loss = 3.973\n",
      "Epoch   9 Batch   10/269   train_loss = 4.095\n",
      "Epoch   9 Batch   11/269   train_loss = 4.129\n",
      "Epoch   9 Batch   12/269   train_loss = 4.053\n",
      "Epoch   9 Batch   13/269   train_loss = 4.187\n",
      "Epoch   9 Batch   14/269   train_loss = 4.037\n",
      "Epoch   9 Batch   15/269   train_loss = 4.349\n",
      "Epoch   9 Batch   16/269   train_loss = 4.200\n",
      "Epoch   9 Batch   17/269   train_loss = 4.293\n",
      "Epoch   9 Batch   18/269   train_loss = 4.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Batch   19/269   train_loss = 4.084\n",
      "Epoch   9 Batch   20/269   train_loss = 4.529\n",
      "Epoch   9 Batch   21/269   train_loss = 4.277\n",
      "Epoch   9 Batch   22/269   train_loss = 3.961\n",
      "Epoch   9 Batch   23/269   train_loss = 4.183\n",
      "Epoch   9 Batch   24/269   train_loss = 4.048\n",
      "Epoch   9 Batch   25/269   train_loss = 4.346\n",
      "Epoch   9 Batch   26/269   train_loss = 4.270\n",
      "Epoch   9 Batch   27/269   train_loss = 4.046\n",
      "Epoch   9 Batch   28/269   train_loss = 4.184\n",
      "Epoch   9 Batch   29/269   train_loss = 4.301\n",
      "Epoch   9 Batch   30/269   train_loss = 4.344\n",
      "Epoch   9 Batch   31/269   train_loss = 4.106\n",
      "Epoch   9 Batch   32/269   train_loss = 3.986\n",
      "Epoch   9 Batch   33/269   train_loss = 3.954\n",
      "Epoch   9 Batch   34/269   train_loss = 4.213\n",
      "Epoch   9 Batch   35/269   train_loss = 4.054\n",
      "Epoch   9 Batch   36/269   train_loss = 3.983\n",
      "Epoch   9 Batch   37/269   train_loss = 4.080\n",
      "Epoch   9 Batch   38/269   train_loss = 4.045\n",
      "Epoch   9 Batch   39/269   train_loss = 4.312\n",
      "Epoch   9 Batch   40/269   train_loss = 4.011\n",
      "Epoch   9 Batch   41/269   train_loss = 4.001\n",
      "Epoch   9 Batch   42/269   train_loss = 4.553\n",
      "Epoch   9 Batch   43/269   train_loss = 4.299\n",
      "Epoch   9 Batch   44/269   train_loss = 4.100\n",
      "Epoch   9 Batch   45/269   train_loss = 3.950\n",
      "Epoch   9 Batch   46/269   train_loss = 4.304\n",
      "Epoch   9 Batch   47/269   train_loss = 3.809\n",
      "Epoch   9 Batch   48/269   train_loss = 4.454\n",
      "Epoch   9 Batch   49/269   train_loss = 4.312\n",
      "Epoch   9 Batch   50/269   train_loss = 4.340\n",
      "Epoch   9 Batch   51/269   train_loss = 4.190\n",
      "Epoch   9 Batch   52/269   train_loss = 4.302\n",
      "Epoch   9 Batch   53/269   train_loss = 4.321\n",
      "Epoch   9 Batch   54/269   train_loss = 4.168\n",
      "Epoch   9 Batch   55/269   train_loss = 4.257\n",
      "Epoch   9 Batch   56/269   train_loss = 4.138\n",
      "Epoch   9 Batch   57/269   train_loss = 4.198\n",
      "Epoch   9 Batch   58/269   train_loss = 4.244\n",
      "Epoch   9 Batch   59/269   train_loss = 3.958\n",
      "Epoch   9 Batch   60/269   train_loss = 4.234\n",
      "Epoch   9 Batch   61/269   train_loss = 4.471\n",
      "Epoch   9 Batch   62/269   train_loss = 4.160\n",
      "Epoch   9 Batch   63/269   train_loss = 4.416\n",
      "Epoch   9 Batch   64/269   train_loss = 4.090\n",
      "Epoch   9 Batch   65/269   train_loss = 4.171\n",
      "Epoch   9 Batch   66/269   train_loss = 4.187\n",
      "Epoch   9 Batch   67/269   train_loss = 4.157\n",
      "Epoch   9 Batch   68/269   train_loss = 4.258\n",
      "Epoch   9 Batch   69/269   train_loss = 4.287\n",
      "Epoch   9 Batch   70/269   train_loss = 3.892\n",
      "Epoch   9 Batch   71/269   train_loss = 4.272\n",
      "Epoch   9 Batch   72/269   train_loss = 3.928\n",
      "Epoch   9 Batch   73/269   train_loss = 4.124\n",
      "Epoch   9 Batch   74/269   train_loss = 4.166\n",
      "Epoch   9 Batch   75/269   train_loss = 4.327\n",
      "Epoch   9 Batch   76/269   train_loss = 4.258\n",
      "Epoch   9 Batch   77/269   train_loss = 3.831\n",
      "Epoch   9 Batch   78/269   train_loss = 4.227\n",
      "Epoch   9 Batch   79/269   train_loss = 4.258\n",
      "Epoch   9 Batch   80/269   train_loss = 4.046\n",
      "Epoch   9 Batch   81/269   train_loss = 3.912\n",
      "Epoch   9 Batch   82/269   train_loss = 4.247\n",
      "Epoch   9 Batch   83/269   train_loss = 4.211\n",
      "Epoch   9 Batch   84/269   train_loss = 4.350\n",
      "Epoch   9 Batch   85/269   train_loss = 4.517\n",
      "Epoch   9 Batch   86/269   train_loss = 4.335\n",
      "Epoch   9 Batch   87/269   train_loss = 4.182\n",
      "Epoch   9 Batch   88/269   train_loss = 4.106\n",
      "Epoch   9 Batch   89/269   train_loss = 4.118\n",
      "Epoch   9 Batch   90/269   train_loss = 4.181\n",
      "Epoch   9 Batch   91/269   train_loss = 4.382\n",
      "Epoch   9 Batch   92/269   train_loss = 4.240\n",
      "Epoch   9 Batch   93/269   train_loss = 3.871\n",
      "Epoch   9 Batch   94/269   train_loss = 4.422\n",
      "Epoch   9 Batch   95/269   train_loss = 3.995\n",
      "Epoch   9 Batch   96/269   train_loss = 4.071\n",
      "Epoch   9 Batch   97/269   train_loss = 4.176\n",
      "Epoch   9 Batch   98/269   train_loss = 4.140\n",
      "Epoch   9 Batch   99/269   train_loss = 4.251\n",
      "Epoch   9 Batch  100/269   train_loss = 4.108\n",
      "Epoch   9 Batch  101/269   train_loss = 4.200\n",
      "Epoch   9 Batch  102/269   train_loss = 4.060\n",
      "Epoch   9 Batch  103/269   train_loss = 4.167\n",
      "Epoch   9 Batch  104/269   train_loss = 4.079\n",
      "Epoch   9 Batch  105/269   train_loss = 4.225\n",
      "Epoch   9 Batch  106/269   train_loss = 4.314\n",
      "Epoch   9 Batch  107/269   train_loss = 4.330\n",
      "Epoch   9 Batch  108/269   train_loss = 4.272\n",
      "Epoch   9 Batch  109/269   train_loss = 4.210\n",
      "Epoch   9 Batch  110/269   train_loss = 4.166\n",
      "Epoch   9 Batch  111/269   train_loss = 4.333\n",
      "Epoch   9 Batch  112/269   train_loss = 4.257\n",
      "Epoch   9 Batch  113/269   train_loss = 4.034\n",
      "Epoch   9 Batch  114/269   train_loss = 4.052\n",
      "Epoch   9 Batch  115/269   train_loss = 3.989\n",
      "Epoch   9 Batch  116/269   train_loss = 4.021\n",
      "Epoch   9 Batch  117/269   train_loss = 3.916\n",
      "Epoch   9 Batch  118/269   train_loss = 4.235\n",
      "Epoch   9 Batch  119/269   train_loss = 3.998\n",
      "Epoch   9 Batch  120/269   train_loss = 4.218\n",
      "Epoch   9 Batch  121/269   train_loss = 4.096\n",
      "Epoch   9 Batch  122/269   train_loss = 4.122\n",
      "Epoch   9 Batch  123/269   train_loss = 3.958\n",
      "Epoch   9 Batch  124/269   train_loss = 4.273\n",
      "Epoch   9 Batch  125/269   train_loss = 3.888\n",
      "Epoch   9 Batch  126/269   train_loss = 3.751\n",
      "Epoch   9 Batch  127/269   train_loss = 4.103\n",
      "Epoch   9 Batch  128/269   train_loss = 4.090\n",
      "Epoch   9 Batch  129/269   train_loss = 4.184\n",
      "Epoch   9 Batch  130/269   train_loss = 4.317\n",
      "Epoch   9 Batch  131/269   train_loss = 4.015\n",
      "Epoch   9 Batch  132/269   train_loss = 4.183\n",
      "Epoch   9 Batch  133/269   train_loss = 3.920\n",
      "Epoch   9 Batch  134/269   train_loss = 4.255\n",
      "Epoch   9 Batch  135/269   train_loss = 3.880\n",
      "Epoch   9 Batch  136/269   train_loss = 3.898\n",
      "Epoch   9 Batch  137/269   train_loss = 4.181\n",
      "Epoch   9 Batch  138/269   train_loss = 3.982\n",
      "Epoch   9 Batch  139/269   train_loss = 4.126\n",
      "Epoch   9 Batch  140/269   train_loss = 4.278\n",
      "Epoch   9 Batch  141/269   train_loss = 4.303\n",
      "Epoch   9 Batch  142/269   train_loss = 3.893\n",
      "Epoch   9 Batch  143/269   train_loss = 4.131\n",
      "Epoch   9 Batch  144/269   train_loss = 4.339\n",
      "Epoch   9 Batch  145/269   train_loss = 4.019\n",
      "Epoch   9 Batch  146/269   train_loss = 4.202\n",
      "Epoch   9 Batch  147/269   train_loss = 3.930\n",
      "Epoch   9 Batch  148/269   train_loss = 4.187\n",
      "Epoch   9 Batch  149/269   train_loss = 4.071\n",
      "Epoch   9 Batch  150/269   train_loss = 4.112\n",
      "Epoch   9 Batch  151/269   train_loss = 4.116\n",
      "Epoch   9 Batch  152/269   train_loss = 3.999\n",
      "Epoch   9 Batch  153/269   train_loss = 4.225\n",
      "Epoch   9 Batch  154/269   train_loss = 4.115\n",
      "Epoch   9 Batch  155/269   train_loss = 4.020\n",
      "Epoch   9 Batch  156/269   train_loss = 4.531\n",
      "Epoch   9 Batch  157/269   train_loss = 4.453\n",
      "Epoch   9 Batch  158/269   train_loss = 4.241\n",
      "Epoch   9 Batch  159/269   train_loss = 4.265\n",
      "Epoch   9 Batch  160/269   train_loss = 4.044\n",
      "Epoch   9 Batch  161/269   train_loss = 4.172\n",
      "Epoch   9 Batch  162/269   train_loss = 4.090\n",
      "Epoch   9 Batch  163/269   train_loss = 4.330\n",
      "Epoch   9 Batch  164/269   train_loss = 4.365\n",
      "Epoch   9 Batch  165/269   train_loss = 4.273\n",
      "Epoch   9 Batch  166/269   train_loss = 4.151\n",
      "Epoch   9 Batch  167/269   train_loss = 4.150\n",
      "Epoch   9 Batch  168/269   train_loss = 4.047\n",
      "Epoch   9 Batch  169/269   train_loss = 4.406\n",
      "Epoch   9 Batch  170/269   train_loss = 3.996\n",
      "Epoch   9 Batch  171/269   train_loss = 4.414\n",
      "Epoch   9 Batch  172/269   train_loss = 4.077\n",
      "Epoch   9 Batch  173/269   train_loss = 4.259\n",
      "Epoch   9 Batch  174/269   train_loss = 4.325\n",
      "Epoch   9 Batch  175/269   train_loss = 4.089\n",
      "Epoch   9 Batch  176/269   train_loss = 4.037\n",
      "Epoch   9 Batch  177/269   train_loss = 4.166\n",
      "Epoch   9 Batch  178/269   train_loss = 4.401\n",
      "Epoch   9 Batch  179/269   train_loss = 4.161\n",
      "Epoch   9 Batch  180/269   train_loss = 3.983\n",
      "Epoch   9 Batch  181/269   train_loss = 4.198\n",
      "Epoch   9 Batch  182/269   train_loss = 3.960\n",
      "Epoch   9 Batch  183/269   train_loss = 4.178\n",
      "Epoch   9 Batch  184/269   train_loss = 4.197\n",
      "Epoch   9 Batch  185/269   train_loss = 4.148\n",
      "Epoch   9 Batch  186/269   train_loss = 3.958\n",
      "Epoch   9 Batch  187/269   train_loss = 4.411\n",
      "Epoch   9 Batch  188/269   train_loss = 4.020\n",
      "Epoch   9 Batch  189/269   train_loss = 4.089\n",
      "Epoch   9 Batch  190/269   train_loss = 4.412\n",
      "Epoch   9 Batch  191/269   train_loss = 4.192\n",
      "Epoch   9 Batch  192/269   train_loss = 4.074\n",
      "Epoch   9 Batch  193/269   train_loss = 3.973\n",
      "Epoch   9 Batch  194/269   train_loss = 4.215\n",
      "Epoch   9 Batch  195/269   train_loss = 3.977\n",
      "Epoch   9 Batch  196/269   train_loss = 4.280\n",
      "Epoch   9 Batch  197/269   train_loss = 4.340\n",
      "Epoch   9 Batch  198/269   train_loss = 4.327\n",
      "Epoch   9 Batch  199/269   train_loss = 4.366\n",
      "Epoch   9 Batch  200/269   train_loss = 4.351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Batch  201/269   train_loss = 4.015\n",
      "Epoch   9 Batch  202/269   train_loss = 3.924\n",
      "Epoch   9 Batch  203/269   train_loss = 4.048\n",
      "Epoch   9 Batch  204/269   train_loss = 4.108\n",
      "Epoch   9 Batch  205/269   train_loss = 4.207\n",
      "Epoch   9 Batch  206/269   train_loss = 4.080\n",
      "Epoch   9 Batch  207/269   train_loss = 4.115\n",
      "Epoch   9 Batch  208/269   train_loss = 4.112\n",
      "Epoch   9 Batch  209/269   train_loss = 4.256\n",
      "Epoch   9 Batch  210/269   train_loss = 4.056\n",
      "Epoch   9 Batch  211/269   train_loss = 4.082\n",
      "Epoch   9 Batch  212/269   train_loss = 4.544\n",
      "Epoch   9 Batch  213/269   train_loss = 4.133\n",
      "Epoch   9 Batch  214/269   train_loss = 4.217\n",
      "Epoch   9 Batch  215/269   train_loss = 4.366\n",
      "Epoch   9 Batch  216/269   train_loss = 4.353\n",
      "Epoch   9 Batch  217/269   train_loss = 4.085\n",
      "Epoch   9 Batch  218/269   train_loss = 4.064\n",
      "Epoch   9 Batch  219/269   train_loss = 3.843\n",
      "Epoch   9 Batch  220/269   train_loss = 4.156\n",
      "Epoch   9 Batch  221/269   train_loss = 4.021\n",
      "Epoch   9 Batch  222/269   train_loss = 4.111\n",
      "Epoch   9 Batch  223/269   train_loss = 4.055\n",
      "Epoch   9 Batch  224/269   train_loss = 4.218\n",
      "Epoch   9 Batch  225/269   train_loss = 4.328\n",
      "Epoch   9 Batch  226/269   train_loss = 4.213\n",
      "Epoch   9 Batch  227/269   train_loss = 3.909\n",
      "Epoch   9 Batch  228/269   train_loss = 4.104\n",
      "Epoch   9 Batch  229/269   train_loss = 4.209\n",
      "Epoch   9 Batch  230/269   train_loss = 4.300\n",
      "Epoch   9 Batch  231/269   train_loss = 4.101\n",
      "Epoch   9 Batch  232/269   train_loss = 4.038\n",
      "Epoch   9 Batch  233/269   train_loss = 4.166\n",
      "Epoch   9 Batch  234/269   train_loss = 4.058\n",
      "Epoch   9 Batch  235/269   train_loss = 4.342\n",
      "Epoch   9 Batch  236/269   train_loss = 4.010\n",
      "Epoch   9 Batch  237/269   train_loss = 3.788\n",
      "Epoch   9 Batch  238/269   train_loss = 3.994\n",
      "Epoch   9 Batch  239/269   train_loss = 4.476\n",
      "Epoch   9 Batch  240/269   train_loss = 4.089\n",
      "Epoch   9 Batch  241/269   train_loss = 4.311\n",
      "Epoch   9 Batch  242/269   train_loss = 3.881\n",
      "Epoch   9 Batch  243/269   train_loss = 4.158\n",
      "Epoch   9 Batch  244/269   train_loss = 3.893\n",
      "Epoch   9 Batch  245/269   train_loss = 4.012\n",
      "Epoch   9 Batch  246/269   train_loss = 3.766\n",
      "Epoch   9 Batch  247/269   train_loss = 4.205\n",
      "Epoch   9 Batch  248/269   train_loss = 4.198\n",
      "Epoch   9 Batch  249/269   train_loss = 4.115\n",
      "Epoch   9 Batch  250/269   train_loss = 3.775\n",
      "Epoch   9 Batch  251/269   train_loss = 4.152\n",
      "Epoch   9 Batch  252/269   train_loss = 4.281\n",
      "Epoch   9 Batch  253/269   train_loss = 4.151\n",
      "Epoch   9 Batch  254/269   train_loss = 3.956\n",
      "Epoch   9 Batch  255/269   train_loss = 4.181\n",
      "Epoch   9 Batch  256/269   train_loss = 4.077\n",
      "Epoch   9 Batch  257/269   train_loss = 3.825\n",
      "Epoch   9 Batch  258/269   train_loss = 3.813\n",
      "Epoch   9 Batch  259/269   train_loss = 3.894\n",
      "Epoch   9 Batch  260/269   train_loss = 4.145\n",
      "Epoch   9 Batch  261/269   train_loss = 4.204\n",
      "Epoch   9 Batch  262/269   train_loss = 3.960\n",
      "Epoch   9 Batch  263/269   train_loss = 3.863\n",
      "Epoch   9 Batch  264/269   train_loss = 4.470\n",
      "Epoch   9 Batch  265/269   train_loss = 4.194\n",
      "Epoch   9 Batch  266/269   train_loss = 4.060\n",
      "Epoch   9 Batch  267/269   train_loss = 4.148\n",
      "Epoch   9 Batch  268/269   train_loss = 4.248\n",
      "Epoch  10 Batch    0/269   train_loss = 4.009\n",
      "Epoch  10 Batch    1/269   train_loss = 3.780\n",
      "Epoch  10 Batch    2/269   train_loss = 3.957\n",
      "Epoch  10 Batch    3/269   train_loss = 4.028\n",
      "Epoch  10 Batch    4/269   train_loss = 4.455\n",
      "Epoch  10 Batch    5/269   train_loss = 4.168\n",
      "Epoch  10 Batch    6/269   train_loss = 4.021\n",
      "Epoch  10 Batch    7/269   train_loss = 3.984\n",
      "Epoch  10 Batch    8/269   train_loss = 4.224\n",
      "Epoch  10 Batch    9/269   train_loss = 3.944\n",
      "Epoch  10 Batch   10/269   train_loss = 4.007\n",
      "Epoch  10 Batch   11/269   train_loss = 4.091\n",
      "Epoch  10 Batch   12/269   train_loss = 3.992\n",
      "Epoch  10 Batch   13/269   train_loss = 4.120\n",
      "Epoch  10 Batch   14/269   train_loss = 3.964\n",
      "Epoch  10 Batch   15/269   train_loss = 4.320\n",
      "Epoch  10 Batch   16/269   train_loss = 4.137\n",
      "Epoch  10 Batch   17/269   train_loss = 4.223\n",
      "Epoch  10 Batch   18/269   train_loss = 4.105\n",
      "Epoch  10 Batch   19/269   train_loss = 4.050\n",
      "Epoch  10 Batch   20/269   train_loss = 4.448\n",
      "Epoch  10 Batch   21/269   train_loss = 4.234\n",
      "Epoch  10 Batch   22/269   train_loss = 3.919\n",
      "Epoch  10 Batch   23/269   train_loss = 4.192\n",
      "Epoch  10 Batch   24/269   train_loss = 3.996\n",
      "Epoch  10 Batch   25/269   train_loss = 4.311\n",
      "Epoch  10 Batch   26/269   train_loss = 4.226\n",
      "Epoch  10 Batch   27/269   train_loss = 4.028\n",
      "Epoch  10 Batch   28/269   train_loss = 4.155\n",
      "Epoch  10 Batch   29/269   train_loss = 4.269\n",
      "Epoch  10 Batch   30/269   train_loss = 4.311\n",
      "Epoch  10 Batch   31/269   train_loss = 4.056\n",
      "Epoch  10 Batch   32/269   train_loss = 3.952\n",
      "Epoch  10 Batch   33/269   train_loss = 3.901\n",
      "Epoch  10 Batch   34/269   train_loss = 4.163\n",
      "Epoch  10 Batch   35/269   train_loss = 3.985\n",
      "Epoch  10 Batch   36/269   train_loss = 3.915\n",
      "Epoch  10 Batch   37/269   train_loss = 4.019\n",
      "Epoch  10 Batch   38/269   train_loss = 3.979\n",
      "Epoch  10 Batch   39/269   train_loss = 4.256\n",
      "Epoch  10 Batch   40/269   train_loss = 3.951\n",
      "Epoch  10 Batch   41/269   train_loss = 3.970\n",
      "Epoch  10 Batch   42/269   train_loss = 4.507\n",
      "Epoch  10 Batch   43/269   train_loss = 4.291\n",
      "Epoch  10 Batch   44/269   train_loss = 4.089\n",
      "Epoch  10 Batch   45/269   train_loss = 3.935\n",
      "Epoch  10 Batch   46/269   train_loss = 4.261\n",
      "Epoch  10 Batch   47/269   train_loss = 3.801\n",
      "Epoch  10 Batch   48/269   train_loss = 4.385\n",
      "Epoch  10 Batch   49/269   train_loss = 4.277\n",
      "Epoch  10 Batch   50/269   train_loss = 4.296\n",
      "Epoch  10 Batch   51/269   train_loss = 4.161\n",
      "Epoch  10 Batch   52/269   train_loss = 4.258\n",
      "Epoch  10 Batch   53/269   train_loss = 4.271\n",
      "Epoch  10 Batch   54/269   train_loss = 4.131\n",
      "Epoch  10 Batch   55/269   train_loss = 4.212\n",
      "Epoch  10 Batch   56/269   train_loss = 4.074\n",
      "Epoch  10 Batch   57/269   train_loss = 4.139\n",
      "Epoch  10 Batch   58/269   train_loss = 4.199\n",
      "Epoch  10 Batch   59/269   train_loss = 3.943\n",
      "Epoch  10 Batch   60/269   train_loss = 4.169\n",
      "Epoch  10 Batch   61/269   train_loss = 4.407\n",
      "Epoch  10 Batch   62/269   train_loss = 4.121\n",
      "Epoch  10 Batch   63/269   train_loss = 4.352\n",
      "Epoch  10 Batch   64/269   train_loss = 4.055\n",
      "Epoch  10 Batch   65/269   train_loss = 4.154\n",
      "Epoch  10 Batch   66/269   train_loss = 4.152\n",
      "Epoch  10 Batch   67/269   train_loss = 4.091\n",
      "Epoch  10 Batch   68/269   train_loss = 4.186\n",
      "Epoch  10 Batch   69/269   train_loss = 4.232\n",
      "Epoch  10 Batch   70/269   train_loss = 3.862\n",
      "Epoch  10 Batch   71/269   train_loss = 4.213\n",
      "Epoch  10 Batch   72/269   train_loss = 3.877\n",
      "Epoch  10 Batch   73/269   train_loss = 4.093\n",
      "Epoch  10 Batch   74/269   train_loss = 4.100\n",
      "Epoch  10 Batch   75/269   train_loss = 4.274\n",
      "Epoch  10 Batch   76/269   train_loss = 4.213\n",
      "Epoch  10 Batch   77/269   train_loss = 3.804\n",
      "Epoch  10 Batch   78/269   train_loss = 4.152\n",
      "Epoch  10 Batch   79/269   train_loss = 4.237\n",
      "Epoch  10 Batch   80/269   train_loss = 3.997\n",
      "Epoch  10 Batch   81/269   train_loss = 3.907\n",
      "Epoch  10 Batch   82/269   train_loss = 4.281\n",
      "Epoch  10 Batch   83/269   train_loss = 4.163\n",
      "Epoch  10 Batch   84/269   train_loss = 4.302\n",
      "Epoch  10 Batch   85/269   train_loss = 4.484\n",
      "Epoch  10 Batch   86/269   train_loss = 4.313\n",
      "Epoch  10 Batch   87/269   train_loss = 4.154\n",
      "Epoch  10 Batch   88/269   train_loss = 4.060\n",
      "Epoch  10 Batch   89/269   train_loss = 4.073\n",
      "Epoch  10 Batch   90/269   train_loss = 4.149\n",
      "Epoch  10 Batch   91/269   train_loss = 4.320\n",
      "Epoch  10 Batch   92/269   train_loss = 4.239\n",
      "Epoch  10 Batch   93/269   train_loss = 3.895\n",
      "Epoch  10 Batch   94/269   train_loss = 4.385\n",
      "Epoch  10 Batch   95/269   train_loss = 3.993\n",
      "Epoch  10 Batch   96/269   train_loss = 4.001\n",
      "Epoch  10 Batch   97/269   train_loss = 4.150\n",
      "Epoch  10 Batch   98/269   train_loss = 4.097\n",
      "Epoch  10 Batch   99/269   train_loss = 4.218\n",
      "Epoch  10 Batch  100/269   train_loss = 4.100\n",
      "Epoch  10 Batch  101/269   train_loss = 4.160\n",
      "Epoch  10 Batch  102/269   train_loss = 4.040\n",
      "Epoch  10 Batch  103/269   train_loss = 4.143\n",
      "Epoch  10 Batch  104/269   train_loss = 4.051\n",
      "Epoch  10 Batch  105/269   train_loss = 4.180\n",
      "Epoch  10 Batch  106/269   train_loss = 4.246\n",
      "Epoch  10 Batch  107/269   train_loss = 4.320\n",
      "Epoch  10 Batch  108/269   train_loss = 4.202\n",
      "Epoch  10 Batch  109/269   train_loss = 4.172\n",
      "Epoch  10 Batch  110/269   train_loss = 4.140\n",
      "Epoch  10 Batch  111/269   train_loss = 4.308\n",
      "Epoch  10 Batch  112/269   train_loss = 4.201\n",
      "Epoch  10 Batch  113/269   train_loss = 4.014\n",
      "Epoch  10 Batch  114/269   train_loss = 4.047\n",
      "Epoch  10 Batch  115/269   train_loss = 3.959\n",
      "Epoch  10 Batch  116/269   train_loss = 4.013\n",
      "Epoch  10 Batch  117/269   train_loss = 3.929\n",
      "Epoch  10 Batch  118/269   train_loss = 4.219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 Batch  119/269   train_loss = 3.966\n",
      "Epoch  10 Batch  120/269   train_loss = 4.160\n",
      "Epoch  10 Batch  121/269   train_loss = 4.031\n",
      "Epoch  10 Batch  122/269   train_loss = 4.081\n",
      "Epoch  10 Batch  123/269   train_loss = 3.925\n",
      "Epoch  10 Batch  124/269   train_loss = 4.255\n",
      "Epoch  10 Batch  125/269   train_loss = 3.862\n",
      "Epoch  10 Batch  126/269   train_loss = 3.748\n",
      "Epoch  10 Batch  127/269   train_loss = 4.061\n",
      "Epoch  10 Batch  128/269   train_loss = 4.127\n",
      "Epoch  10 Batch  129/269   train_loss = 4.152\n",
      "Epoch  10 Batch  130/269   train_loss = 4.304\n",
      "Epoch  10 Batch  131/269   train_loss = 3.958\n",
      "Epoch  10 Batch  132/269   train_loss = 4.112\n",
      "Epoch  10 Batch  133/269   train_loss = 3.903\n",
      "Epoch  10 Batch  134/269   train_loss = 4.232\n",
      "Epoch  10 Batch  135/269   train_loss = 3.874\n",
      "Epoch  10 Batch  136/269   train_loss = 3.893\n",
      "Epoch  10 Batch  137/269   train_loss = 4.152\n",
      "Epoch  10 Batch  138/269   train_loss = 3.936\n",
      "Epoch  10 Batch  139/269   train_loss = 4.088\n",
      "Epoch  10 Batch  140/269   train_loss = 4.274\n",
      "Epoch  10 Batch  141/269   train_loss = 4.261\n",
      "Epoch  10 Batch  142/269   train_loss = 3.861\n",
      "Epoch  10 Batch  143/269   train_loss = 4.087\n",
      "Epoch  10 Batch  144/269   train_loss = 4.275\n",
      "Epoch  10 Batch  145/269   train_loss = 3.995\n",
      "Epoch  10 Batch  146/269   train_loss = 4.191\n",
      "Epoch  10 Batch  147/269   train_loss = 3.894\n",
      "Epoch  10 Batch  148/269   train_loss = 4.136\n",
      "Epoch  10 Batch  149/269   train_loss = 4.029\n",
      "Epoch  10 Batch  150/269   train_loss = 4.049\n",
      "Epoch  10 Batch  151/269   train_loss = 4.067\n",
      "Epoch  10 Batch  152/269   train_loss = 3.979\n",
      "Epoch  10 Batch  153/269   train_loss = 4.189\n",
      "Epoch  10 Batch  154/269   train_loss = 4.066\n",
      "Epoch  10 Batch  155/269   train_loss = 3.973\n",
      "Epoch  10 Batch  156/269   train_loss = 4.472\n",
      "Epoch  10 Batch  157/269   train_loss = 4.395\n",
      "Epoch  10 Batch  158/269   train_loss = 4.196\n",
      "Epoch  10 Batch  159/269   train_loss = 4.239\n",
      "Epoch  10 Batch  160/269   train_loss = 4.016\n",
      "Epoch  10 Batch  161/269   train_loss = 4.165\n",
      "Epoch  10 Batch  162/269   train_loss = 4.081\n",
      "Epoch  10 Batch  163/269   train_loss = 4.251\n",
      "Epoch  10 Batch  164/269   train_loss = 4.314\n",
      "Epoch  10 Batch  165/269   train_loss = 4.226\n",
      "Epoch  10 Batch  166/269   train_loss = 4.113\n",
      "Epoch  10 Batch  167/269   train_loss = 4.086\n",
      "Epoch  10 Batch  168/269   train_loss = 4.005\n",
      "Epoch  10 Batch  169/269   train_loss = 4.342\n",
      "Epoch  10 Batch  170/269   train_loss = 3.949\n",
      "Epoch  10 Batch  171/269   train_loss = 4.310\n",
      "Epoch  10 Batch  172/269   train_loss = 3.999\n",
      "Epoch  10 Batch  173/269   train_loss = 4.207\n",
      "Epoch  10 Batch  174/269   train_loss = 4.266\n",
      "Epoch  10 Batch  175/269   train_loss = 4.056\n",
      "Epoch  10 Batch  176/269   train_loss = 3.993\n",
      "Epoch  10 Batch  177/269   train_loss = 4.122\n",
      "Epoch  10 Batch  178/269   train_loss = 4.373\n",
      "Epoch  10 Batch  179/269   train_loss = 4.105\n",
      "Epoch  10 Batch  180/269   train_loss = 3.949\n",
      "Epoch  10 Batch  181/269   train_loss = 4.184\n",
      "Epoch  10 Batch  182/269   train_loss = 3.914\n",
      "Epoch  10 Batch  183/269   train_loss = 4.173\n",
      "Epoch  10 Batch  184/269   train_loss = 4.146\n",
      "Epoch  10 Batch  185/269   train_loss = 4.130\n",
      "Epoch  10 Batch  186/269   train_loss = 3.918\n",
      "Epoch  10 Batch  187/269   train_loss = 4.366\n",
      "Epoch  10 Batch  188/269   train_loss = 3.976\n",
      "Epoch  10 Batch  189/269   train_loss = 4.059\n",
      "Epoch  10 Batch  190/269   train_loss = 4.382\n",
      "Epoch  10 Batch  191/269   train_loss = 4.115\n",
      "Epoch  10 Batch  192/269   train_loss = 4.034\n",
      "Epoch  10 Batch  193/269   train_loss = 3.945\n",
      "Epoch  10 Batch  194/269   train_loss = 4.166\n",
      "Epoch  10 Batch  195/269   train_loss = 3.970\n",
      "Epoch  10 Batch  196/269   train_loss = 4.218\n",
      "Epoch  10 Batch  197/269   train_loss = 4.292\n",
      "Epoch  10 Batch  198/269   train_loss = 4.255\n",
      "Epoch  10 Batch  199/269   train_loss = 4.293\n",
      "Epoch  10 Batch  200/269   train_loss = 4.268\n",
      "Epoch  10 Batch  201/269   train_loss = 3.976\n",
      "Epoch  10 Batch  202/269   train_loss = 3.873\n",
      "Epoch  10 Batch  203/269   train_loss = 4.017\n",
      "Epoch  10 Batch  204/269   train_loss = 4.057\n",
      "Epoch  10 Batch  205/269   train_loss = 4.153\n",
      "Epoch  10 Batch  206/269   train_loss = 4.034\n",
      "Epoch  10 Batch  207/269   train_loss = 4.045\n",
      "Epoch  10 Batch  208/269   train_loss = 4.054\n",
      "Epoch  10 Batch  209/269   train_loss = 4.195\n",
      "Epoch  10 Batch  210/269   train_loss = 4.003\n",
      "Epoch  10 Batch  211/269   train_loss = 4.065\n",
      "Epoch  10 Batch  212/269   train_loss = 4.485\n",
      "Epoch  10 Batch  213/269   train_loss = 4.087\n",
      "Epoch  10 Batch  214/269   train_loss = 4.186\n",
      "Epoch  10 Batch  215/269   train_loss = 4.292\n",
      "Epoch  10 Batch  216/269   train_loss = 4.298\n",
      "Epoch  10 Batch  217/269   train_loss = 4.043\n",
      "Epoch  10 Batch  218/269   train_loss = 4.060\n",
      "Epoch  10 Batch  219/269   train_loss = 3.798\n",
      "Epoch  10 Batch  220/269   train_loss = 4.137\n",
      "Epoch  10 Batch  221/269   train_loss = 3.977\n",
      "Epoch  10 Batch  222/269   train_loss = 4.104\n",
      "Epoch  10 Batch  223/269   train_loss = 3.991\n",
      "Epoch  10 Batch  224/269   train_loss = 4.195\n",
      "Epoch  10 Batch  225/269   train_loss = 4.293\n",
      "Epoch  10 Batch  226/269   train_loss = 4.173\n",
      "Epoch  10 Batch  227/269   train_loss = 3.852\n",
      "Epoch  10 Batch  228/269   train_loss = 4.067\n",
      "Epoch  10 Batch  229/269   train_loss = 4.173\n",
      "Epoch  10 Batch  230/269   train_loss = 4.242\n",
      "Epoch  10 Batch  231/269   train_loss = 4.067\n",
      "Epoch  10 Batch  232/269   train_loss = 3.993\n",
      "Epoch  10 Batch  233/269   train_loss = 4.117\n",
      "Epoch  10 Batch  234/269   train_loss = 4.014\n",
      "Epoch  10 Batch  235/269   train_loss = 4.309\n",
      "Epoch  10 Batch  236/269   train_loss = 3.958\n",
      "Epoch  10 Batch  237/269   train_loss = 3.731\n",
      "Epoch  10 Batch  238/269   train_loss = 3.940\n",
      "Epoch  10 Batch  239/269   train_loss = 4.415\n",
      "Epoch  10 Batch  240/269   train_loss = 4.049\n",
      "Epoch  10 Batch  241/269   train_loss = 4.269\n",
      "Epoch  10 Batch  242/269   train_loss = 3.807\n",
      "Epoch  10 Batch  243/269   train_loss = 4.091\n",
      "Epoch  10 Batch  244/269   train_loss = 3.871\n",
      "Epoch  10 Batch  245/269   train_loss = 3.947\n",
      "Epoch  10 Batch  246/269   train_loss = 3.738\n",
      "Epoch  10 Batch  247/269   train_loss = 4.155\n",
      "Epoch  10 Batch  248/269   train_loss = 4.135\n",
      "Epoch  10 Batch  249/269   train_loss = 4.064\n",
      "Epoch  10 Batch  250/269   train_loss = 3.722\n",
      "Epoch  10 Batch  251/269   train_loss = 4.129\n",
      "Epoch  10 Batch  252/269   train_loss = 4.216\n",
      "Epoch  10 Batch  253/269   train_loss = 4.109\n",
      "Epoch  10 Batch  254/269   train_loss = 3.921\n",
      "Epoch  10 Batch  255/269   train_loss = 4.130\n",
      "Epoch  10 Batch  256/269   train_loss = 4.037\n",
      "Epoch  10 Batch  257/269   train_loss = 3.816\n",
      "Epoch  10 Batch  258/269   train_loss = 3.780\n",
      "Epoch  10 Batch  259/269   train_loss = 3.860\n",
      "Epoch  10 Batch  260/269   train_loss = 4.106\n",
      "Epoch  10 Batch  261/269   train_loss = 4.182\n",
      "Epoch  10 Batch  262/269   train_loss = 3.923\n",
      "Epoch  10 Batch  263/269   train_loss = 3.826\n",
      "Epoch  10 Batch  264/269   train_loss = 4.409\n",
      "Epoch  10 Batch  265/269   train_loss = 4.112\n",
      "Epoch  10 Batch  266/269   train_loss = 4.033\n",
      "Epoch  10 Batch  267/269   train_loss = 4.096\n",
      "Epoch  10 Batch  268/269   train_loss = 4.217\n",
      "Epoch  11 Batch    0/269   train_loss = 3.961\n",
      "Epoch  11 Batch    1/269   train_loss = 3.774\n",
      "Epoch  11 Batch    2/269   train_loss = 3.935\n",
      "Epoch  11 Batch    3/269   train_loss = 3.982\n",
      "Epoch  11 Batch    4/269   train_loss = 4.407\n",
      "Epoch  11 Batch    5/269   train_loss = 4.162\n",
      "Epoch  11 Batch    6/269   train_loss = 3.983\n",
      "Epoch  11 Batch    7/269   train_loss = 3.901\n",
      "Epoch  11 Batch    8/269   train_loss = 4.192\n",
      "Epoch  11 Batch    9/269   train_loss = 3.902\n",
      "Epoch  11 Batch   10/269   train_loss = 3.959\n",
      "Epoch  11 Batch   11/269   train_loss = 4.067\n",
      "Epoch  11 Batch   12/269   train_loss = 3.901\n",
      "Epoch  11 Batch   13/269   train_loss = 4.073\n",
      "Epoch  11 Batch   14/269   train_loss = 3.922\n",
      "Epoch  11 Batch   15/269   train_loss = 4.285\n",
      "Epoch  11 Batch   16/269   train_loss = 4.092\n",
      "Epoch  11 Batch   17/269   train_loss = 4.184\n",
      "Epoch  11 Batch   18/269   train_loss = 4.053\n",
      "Epoch  11 Batch   19/269   train_loss = 4.007\n",
      "Epoch  11 Batch   20/269   train_loss = 4.409\n",
      "Epoch  11 Batch   21/269   train_loss = 4.198\n",
      "Epoch  11 Batch   22/269   train_loss = 3.866\n",
      "Epoch  11 Batch   23/269   train_loss = 4.111\n",
      "Epoch  11 Batch   24/269   train_loss = 3.940\n",
      "Epoch  11 Batch   25/269   train_loss = 4.254\n",
      "Epoch  11 Batch   26/269   train_loss = 4.176\n",
      "Epoch  11 Batch   27/269   train_loss = 3.955\n",
      "Epoch  11 Batch   28/269   train_loss = 4.092\n",
      "Epoch  11 Batch   29/269   train_loss = 4.240\n",
      "Epoch  11 Batch   30/269   train_loss = 4.297\n",
      "Epoch  11 Batch   31/269   train_loss = 4.019\n",
      "Epoch  11 Batch   32/269   train_loss = 3.892\n",
      "Epoch  11 Batch   33/269   train_loss = 3.850\n",
      "Epoch  11 Batch   34/269   train_loss = 4.118\n",
      "Epoch  11 Batch   35/269   train_loss = 3.923\n",
      "Epoch  11 Batch   36/269   train_loss = 3.867\n",
      "Epoch  11 Batch   37/269   train_loss = 3.987\n",
      "Epoch  11 Batch   38/269   train_loss = 3.917\n",
      "Epoch  11 Batch   39/269   train_loss = 4.205\n",
      "Epoch  11 Batch   40/269   train_loss = 3.909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11 Batch   41/269   train_loss = 3.913\n",
      "Epoch  11 Batch   42/269   train_loss = 4.445\n",
      "Epoch  11 Batch   43/269   train_loss = 4.228\n",
      "Epoch  11 Batch   44/269   train_loss = 4.030\n",
      "Epoch  11 Batch   45/269   train_loss = 3.868\n",
      "Epoch  11 Batch   46/269   train_loss = 4.185\n",
      "Epoch  11 Batch   47/269   train_loss = 3.781\n",
      "Epoch  11 Batch   48/269   train_loss = 4.327\n",
      "Epoch  11 Batch   49/269   train_loss = 4.293\n",
      "Epoch  11 Batch   50/269   train_loss = 4.245\n",
      "Epoch  11 Batch   51/269   train_loss = 4.114\n",
      "Epoch  11 Batch   52/269   train_loss = 4.156\n",
      "Epoch  11 Batch   53/269   train_loss = 4.252\n",
      "Epoch  11 Batch   54/269   train_loss = 4.083\n",
      "Epoch  11 Batch   55/269   train_loss = 4.175\n",
      "Epoch  11 Batch   56/269   train_loss = 4.056\n",
      "Epoch  11 Batch   57/269   train_loss = 4.105\n",
      "Epoch  11 Batch   58/269   train_loss = 4.096\n",
      "Epoch  11 Batch   59/269   train_loss = 3.937\n",
      "Epoch  11 Batch   60/269   train_loss = 4.165\n",
      "Epoch  11 Batch   61/269   train_loss = 4.382\n",
      "Epoch  11 Batch   62/269   train_loss = 4.063\n",
      "Epoch  11 Batch   63/269   train_loss = 4.284\n",
      "Epoch  11 Batch   64/269   train_loss = 3.994\n",
      "Epoch  11 Batch   65/269   train_loss = 4.108\n",
      "Epoch  11 Batch   66/269   train_loss = 4.087\n",
      "Epoch  11 Batch   67/269   train_loss = 4.058\n",
      "Epoch  11 Batch   68/269   train_loss = 4.179\n",
      "Epoch  11 Batch   69/269   train_loss = 4.176\n",
      "Epoch  11 Batch   70/269   train_loss = 3.820\n",
      "Epoch  11 Batch   71/269   train_loss = 4.158\n",
      "Epoch  11 Batch   72/269   train_loss = 3.802\n",
      "Epoch  11 Batch   73/269   train_loss = 4.043\n",
      "Epoch  11 Batch   74/269   train_loss = 4.069\n",
      "Epoch  11 Batch   75/269   train_loss = 4.195\n",
      "Epoch  11 Batch   76/269   train_loss = 4.154\n",
      "Epoch  11 Batch   77/269   train_loss = 3.761\n",
      "Epoch  11 Batch   78/269   train_loss = 4.127\n",
      "Epoch  11 Batch   79/269   train_loss = 4.197\n",
      "Epoch  11 Batch   80/269   train_loss = 4.013\n",
      "Epoch  11 Batch   81/269   train_loss = 3.861\n",
      "Epoch  11 Batch   82/269   train_loss = 4.238\n",
      "Epoch  11 Batch   83/269   train_loss = 4.098\n",
      "Epoch  11 Batch   84/269   train_loss = 4.242\n",
      "Epoch  11 Batch   85/269   train_loss = 4.474\n",
      "Epoch  11 Batch   86/269   train_loss = 4.277\n",
      "Epoch  11 Batch   87/269   train_loss = 4.091\n",
      "Epoch  11 Batch   88/269   train_loss = 4.059\n",
      "Epoch  11 Batch   89/269   train_loss = 4.056\n",
      "Epoch  11 Batch   90/269   train_loss = 4.064\n",
      "Epoch  11 Batch   91/269   train_loss = 4.277\n",
      "Epoch  11 Batch   92/269   train_loss = 4.178\n",
      "Epoch  11 Batch   93/269   train_loss = 3.863\n",
      "Epoch  11 Batch   94/269   train_loss = 4.308\n",
      "Epoch  11 Batch   95/269   train_loss = 3.917\n",
      "Epoch  11 Batch   96/269   train_loss = 3.964\n",
      "Epoch  11 Batch   97/269   train_loss = 4.087\n",
      "Epoch  11 Batch   98/269   train_loss = 4.076\n",
      "Epoch  11 Batch   99/269   train_loss = 4.180\n",
      "Epoch  11 Batch  100/269   train_loss = 4.037\n",
      "Epoch  11 Batch  101/269   train_loss = 4.131\n",
      "Epoch  11 Batch  102/269   train_loss = 3.960\n",
      "Epoch  11 Batch  103/269   train_loss = 4.088\n",
      "Epoch  11 Batch  104/269   train_loss = 3.963\n",
      "Epoch  11 Batch  105/269   train_loss = 4.158\n",
      "Epoch  11 Batch  106/269   train_loss = 4.248\n",
      "Epoch  11 Batch  107/269   train_loss = 4.246\n",
      "Epoch  11 Batch  108/269   train_loss = 4.161\n",
      "Epoch  11 Batch  109/269   train_loss = 4.128\n",
      "Epoch  11 Batch  110/269   train_loss = 4.068\n",
      "Epoch  11 Batch  111/269   train_loss = 4.253\n",
      "Epoch  11 Batch  112/269   train_loss = 4.160\n",
      "Epoch  11 Batch  113/269   train_loss = 3.987\n",
      "Epoch  11 Batch  114/269   train_loss = 3.995\n",
      "Epoch  11 Batch  115/269   train_loss = 3.930\n",
      "Epoch  11 Batch  116/269   train_loss = 3.970\n",
      "Epoch  11 Batch  117/269   train_loss = 3.857\n",
      "Epoch  11 Batch  118/269   train_loss = 4.174\n",
      "Epoch  11 Batch  119/269   train_loss = 3.890\n",
      "Epoch  11 Batch  120/269   train_loss = 4.117\n",
      "Epoch  11 Batch  121/269   train_loss = 3.995\n",
      "Epoch  11 Batch  122/269   train_loss = 4.005\n",
      "Epoch  11 Batch  123/269   train_loss = 3.862\n",
      "Epoch  11 Batch  124/269   train_loss = 4.198\n",
      "Epoch  11 Batch  125/269   train_loss = 3.831\n",
      "Epoch  11 Batch  126/269   train_loss = 3.682\n",
      "Epoch  11 Batch  127/269   train_loss = 4.017\n",
      "Epoch  11 Batch  128/269   train_loss = 4.047\n",
      "Epoch  11 Batch  129/269   train_loss = 4.091\n",
      "Epoch  11 Batch  130/269   train_loss = 4.266\n",
      "Epoch  11 Batch  131/269   train_loss = 3.882\n",
      "Epoch  11 Batch  132/269   train_loss = 4.071\n",
      "Epoch  11 Batch  133/269   train_loss = 3.862\n",
      "Epoch  11 Batch  134/269   train_loss = 4.193\n",
      "Epoch  11 Batch  135/269   train_loss = 3.827\n",
      "Epoch  11 Batch  136/269   train_loss = 3.814\n",
      "Epoch  11 Batch  137/269   train_loss = 4.112\n",
      "Epoch  11 Batch  138/269   train_loss = 3.919\n",
      "Epoch  11 Batch  139/269   train_loss = 4.076\n",
      "Epoch  11 Batch  140/269   train_loss = 4.192\n",
      "Epoch  11 Batch  141/269   train_loss = 4.238\n",
      "Epoch  11 Batch  142/269   train_loss = 3.810\n",
      "Epoch  11 Batch  143/269   train_loss = 4.067\n",
      "Epoch  11 Batch  144/269   train_loss = 4.222\n",
      "Epoch  11 Batch  145/269   train_loss = 3.957\n",
      "Epoch  11 Batch  146/269   train_loss = 4.160\n",
      "Epoch  11 Batch  147/269   train_loss = 3.871\n",
      "Epoch  11 Batch  148/269   train_loss = 4.113\n",
      "Epoch  11 Batch  149/269   train_loss = 3.999\n",
      "Epoch  11 Batch  150/269   train_loss = 4.008\n",
      "Epoch  11 Batch  151/269   train_loss = 4.038\n",
      "Epoch  11 Batch  152/269   train_loss = 3.970\n",
      "Epoch  11 Batch  153/269   train_loss = 4.129\n",
      "Epoch  11 Batch  154/269   train_loss = 4.009\n",
      "Epoch  11 Batch  155/269   train_loss = 3.936\n",
      "Epoch  11 Batch  156/269   train_loss = 4.414\n",
      "Epoch  11 Batch  157/269   train_loss = 4.331\n",
      "Epoch  11 Batch  158/269   train_loss = 4.151\n",
      "Epoch  11 Batch  159/269   train_loss = 4.166\n",
      "Epoch  11 Batch  160/269   train_loss = 3.976\n",
      "Epoch  11 Batch  161/269   train_loss = 4.089\n",
      "Epoch  11 Batch  162/269   train_loss = 4.035\n",
      "Epoch  11 Batch  163/269   train_loss = 4.238\n",
      "Epoch  11 Batch  164/269   train_loss = 4.259\n",
      "Epoch  11 Batch  165/269   train_loss = 4.190\n",
      "Epoch  11 Batch  166/269   train_loss = 4.080\n",
      "Epoch  11 Batch  167/269   train_loss = 4.054\n",
      "Epoch  11 Batch  168/269   train_loss = 3.977\n",
      "Epoch  11 Batch  169/269   train_loss = 4.300\n",
      "Epoch  11 Batch  170/269   train_loss = 3.899\n",
      "Epoch  11 Batch  171/269   train_loss = 4.265\n",
      "Epoch  11 Batch  172/269   train_loss = 3.966\n",
      "Epoch  11 Batch  173/269   train_loss = 4.177\n",
      "Epoch  11 Batch  174/269   train_loss = 4.230\n",
      "Epoch  11 Batch  175/269   train_loss = 4.054\n",
      "Epoch  11 Batch  176/269   train_loss = 3.985\n",
      "Epoch  11 Batch  177/269   train_loss = 4.082\n",
      "Epoch  11 Batch  178/269   train_loss = 4.341\n",
      "Epoch  11 Batch  179/269   train_loss = 4.074\n",
      "Epoch  11 Batch  180/269   train_loss = 3.918\n",
      "Epoch  11 Batch  181/269   train_loss = 4.129\n",
      "Epoch  11 Batch  182/269   train_loss = 3.898\n",
      "Epoch  11 Batch  183/269   train_loss = 4.117\n",
      "Epoch  11 Batch  184/269   train_loss = 4.082\n",
      "Epoch  11 Batch  185/269   train_loss = 4.085\n",
      "Epoch  11 Batch  186/269   train_loss = 3.906\n",
      "Epoch  11 Batch  187/269   train_loss = 4.310\n",
      "Epoch  11 Batch  188/269   train_loss = 3.932\n",
      "Epoch  11 Batch  189/269   train_loss = 4.023\n",
      "Epoch  11 Batch  190/269   train_loss = 4.347\n",
      "Epoch  11 Batch  191/269   train_loss = 4.068\n",
      "Epoch  11 Batch  192/269   train_loss = 3.968\n",
      "Epoch  11 Batch  193/269   train_loss = 3.921\n",
      "Epoch  11 Batch  194/269   train_loss = 4.134\n",
      "Epoch  11 Batch  195/269   train_loss = 3.952\n",
      "Epoch  11 Batch  196/269   train_loss = 4.177\n",
      "Epoch  11 Batch  197/269   train_loss = 4.246\n",
      "Epoch  11 Batch  198/269   train_loss = 4.208\n",
      "Epoch  11 Batch  199/269   train_loss = 4.246\n",
      "Epoch  11 Batch  200/269   train_loss = 4.208\n",
      "Epoch  11 Batch  201/269   train_loss = 3.941\n",
      "Epoch  11 Batch  202/269   train_loss = 3.856\n",
      "Epoch  11 Batch  203/269   train_loss = 3.961\n",
      "Epoch  11 Batch  204/269   train_loss = 4.052\n",
      "Epoch  11 Batch  205/269   train_loss = 4.132\n",
      "Epoch  11 Batch  206/269   train_loss = 4.021\n",
      "Epoch  11 Batch  207/269   train_loss = 4.009\n",
      "Epoch  11 Batch  208/269   train_loss = 3.990\n",
      "Epoch  11 Batch  209/269   train_loss = 4.149\n",
      "Epoch  11 Batch  210/269   train_loss = 3.959\n",
      "Epoch  11 Batch  211/269   train_loss = 4.054\n",
      "Epoch  11 Batch  212/269   train_loss = 4.408\n",
      "Epoch  11 Batch  213/269   train_loss = 4.050\n",
      "Epoch  11 Batch  214/269   train_loss = 4.112\n",
      "Epoch  11 Batch  215/269   train_loss = 4.205\n",
      "Epoch  11 Batch  216/269   train_loss = 4.220\n",
      "Epoch  11 Batch  217/269   train_loss = 3.977\n",
      "Epoch  11 Batch  218/269   train_loss = 4.011\n",
      "Epoch  11 Batch  219/269   train_loss = 3.769\n",
      "Epoch  11 Batch  220/269   train_loss = 4.115\n",
      "Epoch  11 Batch  221/269   train_loss = 3.959\n",
      "Epoch  11 Batch  222/269   train_loss = 4.075\n",
      "Epoch  11 Batch  223/269   train_loss = 3.957\n",
      "Epoch  11 Batch  224/269   train_loss = 4.148\n",
      "Epoch  11 Batch  225/269   train_loss = 4.267\n",
      "Epoch  11 Batch  226/269   train_loss = 4.118\n",
      "Epoch  11 Batch  227/269   train_loss = 3.824\n",
      "Epoch  11 Batch  228/269   train_loss = 4.004\n",
      "Epoch  11 Batch  229/269   train_loss = 4.124\n",
      "Epoch  11 Batch  230/269   train_loss = 4.200\n",
      "Epoch  11 Batch  231/269   train_loss = 4.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11 Batch  232/269   train_loss = 3.978\n",
      "Epoch  11 Batch  233/269   train_loss = 4.101\n",
      "Epoch  11 Batch  234/269   train_loss = 3.991\n",
      "Epoch  11 Batch  235/269   train_loss = 4.282\n",
      "Epoch  11 Batch  236/269   train_loss = 3.919\n",
      "Epoch  11 Batch  237/269   train_loss = 3.660\n",
      "Epoch  11 Batch  238/269   train_loss = 3.936\n",
      "Epoch  11 Batch  239/269   train_loss = 4.382\n",
      "Epoch  11 Batch  240/269   train_loss = 4.000\n",
      "Epoch  11 Batch  241/269   train_loss = 4.207\n",
      "Epoch  11 Batch  242/269   train_loss = 3.772\n",
      "Epoch  11 Batch  243/269   train_loss = 4.098\n",
      "Epoch  11 Batch  244/269   train_loss = 3.844\n",
      "Epoch  11 Batch  245/269   train_loss = 3.935\n",
      "Epoch  11 Batch  246/269   train_loss = 3.690\n",
      "Epoch  11 Batch  247/269   train_loss = 4.132\n",
      "Epoch  11 Batch  248/269   train_loss = 4.072\n",
      "Epoch  11 Batch  249/269   train_loss = 4.012\n",
      "Epoch  11 Batch  250/269   train_loss = 3.688\n",
      "Epoch  11 Batch  251/269   train_loss = 4.094\n",
      "Epoch  11 Batch  252/269   train_loss = 4.158\n",
      "Epoch  11 Batch  253/269   train_loss = 4.056\n",
      "Epoch  11 Batch  254/269   train_loss = 3.890\n",
      "Epoch  11 Batch  255/269   train_loss = 4.098\n",
      "Epoch  11 Batch  256/269   train_loss = 3.999\n",
      "Epoch  11 Batch  257/269   train_loss = 3.774\n",
      "Epoch  11 Batch  258/269   train_loss = 3.750\n",
      "Epoch  11 Batch  259/269   train_loss = 3.834\n",
      "Epoch  11 Batch  260/269   train_loss = 4.072\n",
      "Epoch  11 Batch  261/269   train_loss = 4.111\n",
      "Epoch  11 Batch  262/269   train_loss = 3.883\n",
      "Epoch  11 Batch  263/269   train_loss = 3.778\n",
      "Epoch  11 Batch  264/269   train_loss = 4.371\n",
      "Epoch  11 Batch  265/269   train_loss = 4.075\n",
      "Epoch  11 Batch  266/269   train_loss = 3.997\n",
      "Epoch  11 Batch  267/269   train_loss = 4.035\n",
      "Epoch  11 Batch  268/269   train_loss = 4.160\n",
      "Epoch  12 Batch    0/269   train_loss = 3.925\n",
      "Epoch  12 Batch    1/269   train_loss = 3.734\n",
      "Epoch  12 Batch    2/269   train_loss = 3.902\n",
      "Epoch  12 Batch    3/269   train_loss = 3.956\n",
      "Epoch  12 Batch    4/269   train_loss = 4.386\n",
      "Epoch  12 Batch    5/269   train_loss = 4.121\n",
      "Epoch  12 Batch    6/269   train_loss = 3.928\n",
      "Epoch  12 Batch    7/269   train_loss = 3.872\n",
      "Epoch  12 Batch    8/269   train_loss = 4.155\n",
      "Epoch  12 Batch    9/269   train_loss = 3.858\n",
      "Epoch  12 Batch   10/269   train_loss = 3.890\n",
      "Epoch  12 Batch   11/269   train_loss = 4.021\n",
      "Epoch  12 Batch   12/269   train_loss = 3.842\n",
      "Epoch  12 Batch   13/269   train_loss = 4.048\n",
      "Epoch  12 Batch   14/269   train_loss = 3.867\n",
      "Epoch  12 Batch   15/269   train_loss = 4.260\n",
      "Epoch  12 Batch   16/269   train_loss = 4.032\n",
      "Epoch  12 Batch   17/269   train_loss = 4.141\n",
      "Epoch  12 Batch   18/269   train_loss = 4.017\n",
      "Epoch  12 Batch   19/269   train_loss = 3.987\n",
      "Epoch  12 Batch   20/269   train_loss = 4.383\n",
      "Epoch  12 Batch   21/269   train_loss = 4.130\n",
      "Epoch  12 Batch   22/269   train_loss = 3.835\n",
      "Epoch  12 Batch   23/269   train_loss = 4.060\n",
      "Epoch  12 Batch   24/269   train_loss = 3.921\n",
      "Epoch  12 Batch   25/269   train_loss = 4.188\n",
      "Epoch  12 Batch   26/269   train_loss = 4.123\n",
      "Epoch  12 Batch   27/269   train_loss = 3.939\n",
      "Epoch  12 Batch   28/269   train_loss = 4.026\n",
      "Epoch  12 Batch   29/269   train_loss = 4.186\n",
      "Epoch  12 Batch   30/269   train_loss = 4.280\n",
      "Epoch  12 Batch   31/269   train_loss = 3.980\n",
      "Epoch  12 Batch   32/269   train_loss = 3.900\n",
      "Epoch  12 Batch   33/269   train_loss = 3.838\n",
      "Epoch  12 Batch   34/269   train_loss = 4.093\n",
      "Epoch  12 Batch   35/269   train_loss = 3.883\n",
      "Epoch  12 Batch   36/269   train_loss = 3.828\n",
      "Epoch  12 Batch   37/269   train_loss = 3.952\n",
      "Epoch  12 Batch   38/269   train_loss = 3.903\n",
      "Epoch  12 Batch   39/269   train_loss = 4.164\n",
      "Epoch  12 Batch   40/269   train_loss = 3.870\n",
      "Epoch  12 Batch   41/269   train_loss = 3.867\n",
      "Epoch  12 Batch   42/269   train_loss = 4.420\n",
      "Epoch  12 Batch   43/269   train_loss = 4.181\n",
      "Epoch  12 Batch   44/269   train_loss = 3.994\n",
      "Epoch  12 Batch   45/269   train_loss = 3.802\n",
      "Epoch  12 Batch   46/269   train_loss = 4.157\n",
      "Epoch  12 Batch   47/269   train_loss = 3.759\n",
      "Epoch  12 Batch   48/269   train_loss = 4.270\n",
      "Epoch  12 Batch   49/269   train_loss = 4.252\n",
      "Epoch  12 Batch   50/269   train_loss = 4.201\n",
      "Epoch  12 Batch   51/269   train_loss = 4.083\n",
      "Epoch  12 Batch   52/269   train_loss = 4.118\n",
      "Epoch  12 Batch   53/269   train_loss = 4.214\n",
      "Epoch  12 Batch   54/269   train_loss = 4.037\n",
      "Epoch  12 Batch   55/269   train_loss = 4.159\n",
      "Epoch  12 Batch   56/269   train_loss = 3.990\n",
      "Epoch  12 Batch   57/269   train_loss = 4.099\n",
      "Epoch  12 Batch   58/269   train_loss = 4.024\n",
      "Epoch  12 Batch   59/269   train_loss = 3.904\n",
      "Epoch  12 Batch   60/269   train_loss = 4.108\n",
      "Epoch  12 Batch   61/269   train_loss = 4.317\n",
      "Epoch  12 Batch   62/269   train_loss = 4.050\n",
      "Epoch  12 Batch   63/269   train_loss = 4.260\n",
      "Epoch  12 Batch   64/269   train_loss = 3.982\n",
      "Epoch  12 Batch   65/269   train_loss = 4.043\n",
      "Epoch  12 Batch   66/269   train_loss = 4.058\n",
      "Epoch  12 Batch   67/269   train_loss = 4.024\n",
      "Epoch  12 Batch   68/269   train_loss = 4.102\n",
      "Epoch  12 Batch   69/269   train_loss = 4.122\n",
      "Epoch  12 Batch   70/269   train_loss = 3.760\n",
      "Epoch  12 Batch   71/269   train_loss = 4.105\n",
      "Epoch  12 Batch   72/269   train_loss = 3.749\n",
      "Epoch  12 Batch   73/269   train_loss = 4.013\n",
      "Epoch  12 Batch   74/269   train_loss = 4.013\n",
      "Epoch  12 Batch   75/269   train_loss = 4.190\n",
      "Epoch  12 Batch   76/269   train_loss = 4.111\n",
      "Epoch  12 Batch   77/269   train_loss = 3.732\n",
      "Epoch  12 Batch   78/269   train_loss = 4.102\n",
      "Epoch  12 Batch   79/269   train_loss = 4.189\n",
      "Epoch  12 Batch   80/269   train_loss = 3.979\n",
      "Epoch  12 Batch   81/269   train_loss = 3.865\n",
      "Epoch  12 Batch   82/269   train_loss = 4.191\n",
      "Epoch  12 Batch   83/269   train_loss = 4.071\n",
      "Epoch  12 Batch   84/269   train_loss = 4.211\n",
      "Epoch  12 Batch   85/269   train_loss = 4.459\n",
      "Epoch  12 Batch   86/269   train_loss = 4.253\n",
      "Epoch  12 Batch   87/269   train_loss = 4.035\n",
      "Epoch  12 Batch   88/269   train_loss = 4.038\n",
      "Epoch  12 Batch   89/269   train_loss = 4.007\n",
      "Epoch  12 Batch   90/269   train_loss = 4.019\n",
      "Epoch  12 Batch   91/269   train_loss = 4.253\n",
      "Epoch  12 Batch   92/269   train_loss = 4.149\n",
      "Epoch  12 Batch   93/269   train_loss = 3.823\n",
      "Epoch  12 Batch   94/269   train_loss = 4.286\n",
      "Epoch  12 Batch   95/269   train_loss = 3.864\n",
      "Epoch  12 Batch   96/269   train_loss = 3.896\n",
      "Epoch  12 Batch   97/269   train_loss = 4.046\n",
      "Epoch  12 Batch   98/269   train_loss = 4.033\n",
      "Epoch  12 Batch   99/269   train_loss = 4.112\n",
      "Epoch  12 Batch  100/269   train_loss = 3.968\n",
      "Epoch  12 Batch  101/269   train_loss = 4.124\n",
      "Epoch  12 Batch  102/269   train_loss = 3.922\n",
      "Epoch  12 Batch  103/269   train_loss = 4.044\n",
      "Epoch  12 Batch  104/269   train_loss = 3.888\n",
      "Epoch  12 Batch  105/269   train_loss = 4.117\n",
      "Epoch  12 Batch  106/269   train_loss = 4.210\n",
      "Epoch  12 Batch  107/269   train_loss = 4.223\n",
      "Epoch  12 Batch  108/269   train_loss = 4.124\n",
      "Epoch  12 Batch  109/269   train_loss = 4.112\n",
      "Epoch  12 Batch  110/269   train_loss = 4.034\n",
      "Epoch  12 Batch  111/269   train_loss = 4.235\n",
      "Epoch  12 Batch  112/269   train_loss = 4.147\n",
      "Epoch  12 Batch  113/269   train_loss = 3.954\n",
      "Epoch  12 Batch  114/269   train_loss = 3.943\n",
      "Epoch  12 Batch  115/269   train_loss = 3.879\n",
      "Epoch  12 Batch  116/269   train_loss = 3.922\n",
      "Epoch  12 Batch  117/269   train_loss = 3.848\n",
      "Epoch  12 Batch  118/269   train_loss = 4.147\n",
      "Epoch  12 Batch  119/269   train_loss = 3.865\n",
      "Epoch  12 Batch  120/269   train_loss = 4.092\n",
      "Epoch  12 Batch  121/269   train_loss = 4.021\n",
      "Epoch  12 Batch  122/269   train_loss = 3.979\n",
      "Epoch  12 Batch  123/269   train_loss = 3.801\n",
      "Epoch  12 Batch  124/269   train_loss = 4.162\n",
      "Epoch  12 Batch  125/269   train_loss = 3.779\n",
      "Epoch  12 Batch  126/269   train_loss = 3.661\n",
      "Epoch  12 Batch  127/269   train_loss = 3.977\n",
      "Epoch  12 Batch  128/269   train_loss = 4.006\n",
      "Epoch  12 Batch  129/269   train_loss = 4.004\n",
      "Epoch  12 Batch  130/269   train_loss = 4.195\n",
      "Epoch  12 Batch  131/269   train_loss = 3.810\n",
      "Epoch  12 Batch  132/269   train_loss = 4.035\n",
      "Epoch  12 Batch  133/269   train_loss = 3.838\n",
      "Epoch  12 Batch  134/269   train_loss = 4.157\n",
      "Epoch  12 Batch  135/269   train_loss = 3.806\n",
      "Epoch  12 Batch  136/269   train_loss = 3.775\n",
      "Epoch  12 Batch  137/269   train_loss = 4.064\n",
      "Epoch  12 Batch  138/269   train_loss = 3.866\n",
      "Epoch  12 Batch  139/269   train_loss = 4.011\n",
      "Epoch  12 Batch  140/269   train_loss = 4.165\n",
      "Epoch  12 Batch  141/269   train_loss = 4.207\n",
      "Epoch  12 Batch  142/269   train_loss = 3.781\n",
      "Epoch  12 Batch  143/269   train_loss = 4.027\n",
      "Epoch  12 Batch  144/269   train_loss = 4.153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12 Batch  145/269   train_loss = 3.881\n",
      "Epoch  12 Batch  146/269   train_loss = 4.099\n",
      "Epoch  12 Batch  147/269   train_loss = 3.802\n",
      "Epoch  12 Batch  148/269   train_loss = 4.100\n",
      "Epoch  12 Batch  149/269   train_loss = 3.978\n",
      "Epoch  12 Batch  150/269   train_loss = 3.991\n",
      "Epoch  12 Batch  151/269   train_loss = 4.044\n",
      "Epoch  12 Batch  152/269   train_loss = 3.956\n",
      "Epoch  12 Batch  153/269   train_loss = 4.128\n",
      "Epoch  12 Batch  154/269   train_loss = 3.988\n",
      "Epoch  12 Batch  155/269   train_loss = 3.916\n",
      "Epoch  12 Batch  156/269   train_loss = 4.377\n",
      "Epoch  12 Batch  157/269   train_loss = 4.283\n",
      "Epoch  12 Batch  158/269   train_loss = 4.104\n",
      "Epoch  12 Batch  159/269   train_loss = 4.118\n",
      "Epoch  12 Batch  160/269   train_loss = 3.955\n",
      "Epoch  12 Batch  161/269   train_loss = 4.087\n",
      "Epoch  12 Batch  162/269   train_loss = 4.000\n",
      "Epoch  12 Batch  163/269   train_loss = 4.177\n",
      "Epoch  12 Batch  164/269   train_loss = 4.205\n",
      "Epoch  12 Batch  165/269   train_loss = 4.156\n",
      "Epoch  12 Batch  166/269   train_loss = 4.042\n",
      "Epoch  12 Batch  167/269   train_loss = 3.975\n",
      "Epoch  12 Batch  168/269   train_loss = 3.935\n",
      "Epoch  12 Batch  169/269   train_loss = 4.269\n",
      "Epoch  12 Batch  170/269   train_loss = 3.853\n",
      "Epoch  12 Batch  171/269   train_loss = 4.191\n",
      "Epoch  12 Batch  172/269   train_loss = 3.920\n",
      "Epoch  12 Batch  173/269   train_loss = 4.118\n",
      "Epoch  12 Batch  174/269   train_loss = 4.176\n",
      "Epoch  12 Batch  175/269   train_loss = 3.992\n",
      "Epoch  12 Batch  176/269   train_loss = 3.929\n",
      "Epoch  12 Batch  177/269   train_loss = 4.022\n",
      "Epoch  12 Batch  178/269   train_loss = 4.290\n",
      "Epoch  12 Batch  179/269   train_loss = 4.026\n",
      "Epoch  12 Batch  180/269   train_loss = 3.850\n",
      "Epoch  12 Batch  181/269   train_loss = 4.095\n",
      "Epoch  12 Batch  182/269   train_loss = 3.888\n",
      "Epoch  12 Batch  183/269   train_loss = 4.073\n",
      "Epoch  12 Batch  184/269   train_loss = 4.047\n",
      "Epoch  12 Batch  185/269   train_loss = 4.027\n",
      "Epoch  12 Batch  186/269   train_loss = 3.867\n",
      "Epoch  12 Batch  187/269   train_loss = 4.281\n",
      "Epoch  12 Batch  188/269   train_loss = 3.898\n",
      "Epoch  12 Batch  189/269   train_loss = 3.977\n",
      "Epoch  12 Batch  190/269   train_loss = 4.326\n",
      "Epoch  12 Batch  191/269   train_loss = 4.047\n",
      "Epoch  12 Batch  192/269   train_loss = 3.945\n",
      "Epoch  12 Batch  193/269   train_loss = 3.890\n",
      "Epoch  12 Batch  194/269   train_loss = 4.099\n",
      "Epoch  12 Batch  195/269   train_loss = 3.882\n",
      "Epoch  12 Batch  196/269   train_loss = 4.130\n",
      "Epoch  12 Batch  197/269   train_loss = 4.230\n",
      "Epoch  12 Batch  198/269   train_loss = 4.169\n",
      "Epoch  12 Batch  199/269   train_loss = 4.181\n",
      "Epoch  12 Batch  200/269   train_loss = 4.153\n",
      "Epoch  12 Batch  201/269   train_loss = 3.911\n",
      "Epoch  12 Batch  202/269   train_loss = 3.816\n",
      "Epoch  12 Batch  203/269   train_loss = 3.897\n",
      "Epoch  12 Batch  204/269   train_loss = 4.000\n",
      "Epoch  12 Batch  205/269   train_loss = 4.072\n",
      "Epoch  12 Batch  206/269   train_loss = 3.957\n",
      "Epoch  12 Batch  207/269   train_loss = 3.989\n",
      "Epoch  12 Batch  208/269   train_loss = 3.967\n",
      "Epoch  12 Batch  209/269   train_loss = 4.104\n",
      "Epoch  12 Batch  210/269   train_loss = 3.970\n",
      "Epoch  12 Batch  211/269   train_loss = 3.991\n",
      "Epoch  12 Batch  212/269   train_loss = 4.352\n",
      "Epoch  12 Batch  213/269   train_loss = 3.985\n",
      "Epoch  12 Batch  214/269   train_loss = 4.076\n",
      "Epoch  12 Batch  215/269   train_loss = 4.180\n",
      "Epoch  12 Batch  216/269   train_loss = 4.198\n",
      "Epoch  12 Batch  217/269   train_loss = 3.927\n",
      "Epoch  12 Batch  218/269   train_loss = 3.971\n",
      "Epoch  12 Batch  219/269   train_loss = 3.763\n",
      "Epoch  12 Batch  220/269   train_loss = 4.077\n",
      "Epoch  12 Batch  221/269   train_loss = 3.919\n",
      "Epoch  12 Batch  222/269   train_loss = 4.051\n",
      "Epoch  12 Batch  223/269   train_loss = 3.914\n",
      "Epoch  12 Batch  224/269   train_loss = 4.126\n",
      "Epoch  12 Batch  225/269   train_loss = 4.223\n",
      "Epoch  12 Batch  226/269   train_loss = 4.063\n",
      "Epoch  12 Batch  227/269   train_loss = 3.768\n",
      "Epoch  12 Batch  228/269   train_loss = 3.971\n",
      "Epoch  12 Batch  229/269   train_loss = 4.071\n",
      "Epoch  12 Batch  230/269   train_loss = 4.162\n",
      "Epoch  12 Batch  231/269   train_loss = 3.962\n",
      "Epoch  12 Batch  232/269   train_loss = 3.954\n",
      "Epoch  12 Batch  233/269   train_loss = 4.013\n",
      "Epoch  12 Batch  234/269   train_loss = 3.977\n",
      "Epoch  12 Batch  235/269   train_loss = 4.235\n",
      "Epoch  12 Batch  236/269   train_loss = 3.881\n",
      "Epoch  12 Batch  237/269   train_loss = 3.641\n",
      "Epoch  12 Batch  238/269   train_loss = 3.881\n",
      "Epoch  12 Batch  239/269   train_loss = 4.380\n",
      "Epoch  12 Batch  240/269   train_loss = 3.948\n",
      "Epoch  12 Batch  241/269   train_loss = 4.166\n",
      "Epoch  12 Batch  242/269   train_loss = 3.745\n",
      "Epoch  12 Batch  243/269   train_loss = 4.084\n",
      "Epoch  12 Batch  244/269   train_loss = 3.809\n",
      "Epoch  12 Batch  245/269   train_loss = 3.915\n",
      "Epoch  12 Batch  246/269   train_loss = 3.674\n",
      "Epoch  12 Batch  247/269   train_loss = 4.088\n",
      "Epoch  12 Batch  248/269   train_loss = 4.038\n",
      "Epoch  12 Batch  249/269   train_loss = 3.970\n",
      "Epoch  12 Batch  250/269   train_loss = 3.659\n",
      "Epoch  12 Batch  251/269   train_loss = 4.088\n",
      "Epoch  12 Batch  252/269   train_loss = 4.117\n",
      "Epoch  12 Batch  253/269   train_loss = 4.042\n",
      "Epoch  12 Batch  254/269   train_loss = 3.854\n",
      "Epoch  12 Batch  255/269   train_loss = 4.070\n",
      "Epoch  12 Batch  256/269   train_loss = 3.971\n",
      "Epoch  12 Batch  257/269   train_loss = 3.771\n",
      "Epoch  12 Batch  258/269   train_loss = 3.678\n",
      "Epoch  12 Batch  259/269   train_loss = 3.788\n",
      "Epoch  12 Batch  260/269   train_loss = 4.047\n",
      "Epoch  12 Batch  261/269   train_loss = 4.064\n",
      "Epoch  12 Batch  262/269   train_loss = 3.849\n",
      "Epoch  12 Batch  263/269   train_loss = 3.736\n",
      "Epoch  12 Batch  264/269   train_loss = 4.295\n",
      "Epoch  12 Batch  265/269   train_loss = 4.016\n",
      "Epoch  12 Batch  266/269   train_loss = 3.994\n",
      "Epoch  12 Batch  267/269   train_loss = 4.012\n",
      "Epoch  12 Batch  268/269   train_loss = 4.111\n",
      "Epoch  13 Batch    0/269   train_loss = 3.882\n",
      "Epoch  13 Batch    1/269   train_loss = 3.709\n",
      "Epoch  13 Batch    2/269   train_loss = 3.886\n",
      "Epoch  13 Batch    3/269   train_loss = 3.925\n",
      "Epoch  13 Batch    4/269   train_loss = 4.361\n",
      "Epoch  13 Batch    5/269   train_loss = 4.085\n",
      "Epoch  13 Batch    6/269   train_loss = 3.884\n",
      "Epoch  13 Batch    7/269   train_loss = 3.820\n",
      "Epoch  13 Batch    8/269   train_loss = 4.114\n",
      "Epoch  13 Batch    9/269   train_loss = 3.827\n",
      "Epoch  13 Batch   10/269   train_loss = 3.827\n",
      "Epoch  13 Batch   11/269   train_loss = 4.001\n",
      "Epoch  13 Batch   12/269   train_loss = 3.798\n",
      "Epoch  13 Batch   13/269   train_loss = 4.042\n",
      "Epoch  13 Batch   14/269   train_loss = 3.815\n",
      "Epoch  13 Batch   15/269   train_loss = 4.229\n",
      "Epoch  13 Batch   16/269   train_loss = 3.994\n",
      "Epoch  13 Batch   17/269   train_loss = 4.082\n",
      "Epoch  13 Batch   18/269   train_loss = 4.002\n",
      "Epoch  13 Batch   19/269   train_loss = 3.958\n",
      "Epoch  13 Batch   20/269   train_loss = 4.315\n",
      "Epoch  13 Batch   21/269   train_loss = 4.098\n",
      "Epoch  13 Batch   22/269   train_loss = 3.800\n",
      "Epoch  13 Batch   23/269   train_loss = 4.018\n",
      "Epoch  13 Batch   24/269   train_loss = 3.861\n",
      "Epoch  13 Batch   25/269   train_loss = 4.173\n",
      "Epoch  13 Batch   26/269   train_loss = 4.077\n",
      "Epoch  13 Batch   27/269   train_loss = 3.902\n",
      "Epoch  13 Batch   28/269   train_loss = 4.009\n",
      "Epoch  13 Batch   29/269   train_loss = 4.125\n",
      "Epoch  13 Batch   30/269   train_loss = 4.237\n",
      "Epoch  13 Batch   31/269   train_loss = 3.961\n",
      "Epoch  13 Batch   32/269   train_loss = 3.853\n",
      "Epoch  13 Batch   33/269   train_loss = 3.773\n",
      "Epoch  13 Batch   34/269   train_loss = 3.998\n",
      "Epoch  13 Batch   35/269   train_loss = 3.855\n",
      "Epoch  13 Batch   36/269   train_loss = 3.807\n",
      "Epoch  13 Batch   37/269   train_loss = 3.924\n",
      "Epoch  13 Batch   38/269   train_loss = 3.863\n",
      "Epoch  13 Batch   39/269   train_loss = 4.133\n",
      "Epoch  13 Batch   40/269   train_loss = 3.856\n",
      "Epoch  13 Batch   41/269   train_loss = 3.859\n",
      "Epoch  13 Batch   42/269   train_loss = 4.356\n",
      "Epoch  13 Batch   43/269   train_loss = 4.134\n",
      "Epoch  13 Batch   44/269   train_loss = 3.973\n",
      "Epoch  13 Batch   45/269   train_loss = 3.800\n",
      "Epoch  13 Batch   46/269   train_loss = 4.103\n",
      "Epoch  13 Batch   47/269   train_loss = 3.683\n",
      "Epoch  13 Batch   48/269   train_loss = 4.226\n",
      "Epoch  13 Batch   49/269   train_loss = 4.228\n",
      "Epoch  13 Batch   50/269   train_loss = 4.173\n",
      "Epoch  13 Batch   51/269   train_loss = 4.040\n",
      "Epoch  13 Batch   52/269   train_loss = 4.091\n",
      "Epoch  13 Batch   53/269   train_loss = 4.190\n",
      "Epoch  13 Batch   54/269   train_loss = 4.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13 Batch   55/269   train_loss = 4.125\n",
      "Epoch  13 Batch   56/269   train_loss = 3.961\n",
      "Epoch  13 Batch   57/269   train_loss = 4.032\n",
      "Epoch  13 Batch   58/269   train_loss = 3.967\n",
      "Epoch  13 Batch   59/269   train_loss = 3.887\n",
      "Epoch  13 Batch   60/269   train_loss = 4.081\n",
      "Epoch  13 Batch   61/269   train_loss = 4.286\n",
      "Epoch  13 Batch   62/269   train_loss = 3.982\n",
      "Epoch  13 Batch   63/269   train_loss = 4.213\n",
      "Epoch  13 Batch   64/269   train_loss = 3.963\n",
      "Epoch  13 Batch   65/269   train_loss = 4.013\n",
      "Epoch  13 Batch   66/269   train_loss = 4.011\n",
      "Epoch  13 Batch   67/269   train_loss = 3.994\n",
      "Epoch  13 Batch   68/269   train_loss = 4.078\n",
      "Epoch  13 Batch   69/269   train_loss = 4.076\n",
      "Epoch  13 Batch   70/269   train_loss = 3.727\n",
      "Epoch  13 Batch   71/269   train_loss = 4.087\n",
      "Epoch  13 Batch   72/269   train_loss = 3.711\n",
      "Epoch  13 Batch   73/269   train_loss = 4.005\n",
      "Epoch  13 Batch   74/269   train_loss = 3.999\n",
      "Epoch  13 Batch   75/269   train_loss = 4.122\n",
      "Epoch  13 Batch   76/269   train_loss = 4.077\n",
      "Epoch  13 Batch   77/269   train_loss = 3.692\n",
      "Epoch  13 Batch   78/269   train_loss = 4.040\n",
      "Epoch  13 Batch   79/269   train_loss = 4.164\n",
      "Epoch  13 Batch   80/269   train_loss = 3.976\n",
      "Epoch  13 Batch   81/269   train_loss = 3.854\n",
      "Epoch  13 Batch   82/269   train_loss = 4.146\n",
      "Epoch  13 Batch   83/269   train_loss = 4.037\n",
      "Epoch  13 Batch   84/269   train_loss = 4.190\n",
      "Epoch  13 Batch   85/269   train_loss = 4.401\n",
      "Epoch  13 Batch   86/269   train_loss = 4.258\n",
      "Epoch  13 Batch   87/269   train_loss = 4.030\n",
      "Epoch  13 Batch   88/269   train_loss = 4.023\n",
      "Epoch  13 Batch   89/269   train_loss = 3.982\n",
      "Epoch  13 Batch   90/269   train_loss = 3.992\n",
      "Epoch  13 Batch   91/269   train_loss = 4.185\n",
      "Epoch  13 Batch   92/269   train_loss = 4.125\n",
      "Epoch  13 Batch   93/269   train_loss = 3.822\n",
      "Epoch  13 Batch   94/269   train_loss = 4.256\n",
      "Epoch  13 Batch   95/269   train_loss = 3.833\n",
      "Epoch  13 Batch   96/269   train_loss = 3.869\n",
      "Epoch  13 Batch   97/269   train_loss = 3.983\n",
      "Epoch  13 Batch   98/269   train_loss = 4.033\n",
      "Epoch  13 Batch   99/269   train_loss = 4.077\n",
      "Epoch  13 Batch  100/269   train_loss = 3.954\n",
      "Epoch  13 Batch  101/269   train_loss = 4.055\n",
      "Epoch  13 Batch  102/269   train_loss = 3.888\n",
      "Epoch  13 Batch  103/269   train_loss = 4.017\n",
      "Epoch  13 Batch  104/269   train_loss = 3.843\n",
      "Epoch  13 Batch  105/269   train_loss = 4.080\n",
      "Epoch  13 Batch  106/269   train_loss = 4.163\n",
      "Epoch  13 Batch  107/269   train_loss = 4.194\n",
      "Epoch  13 Batch  108/269   train_loss = 4.100\n",
      "Epoch  13 Batch  109/269   train_loss = 4.082\n",
      "Epoch  13 Batch  110/269   train_loss = 4.034\n",
      "Epoch  13 Batch  111/269   train_loss = 4.209\n",
      "Epoch  13 Batch  112/269   train_loss = 4.135\n",
      "Epoch  13 Batch  113/269   train_loss = 3.944\n",
      "Epoch  13 Batch  114/269   train_loss = 3.931\n",
      "Epoch  13 Batch  115/269   train_loss = 3.844\n",
      "Epoch  13 Batch  116/269   train_loss = 3.883\n",
      "Epoch  13 Batch  117/269   train_loss = 3.835\n",
      "Epoch  13 Batch  118/269   train_loss = 4.081\n",
      "Epoch  13 Batch  119/269   train_loss = 3.823\n",
      "Epoch  13 Batch  120/269   train_loss = 4.081\n",
      "Epoch  13 Batch  121/269   train_loss = 3.955\n",
      "Epoch  13 Batch  122/269   train_loss = 3.944\n",
      "Epoch  13 Batch  123/269   train_loss = 3.766\n",
      "Epoch  13 Batch  124/269   train_loss = 4.135\n",
      "Epoch  13 Batch  125/269   train_loss = 3.726\n",
      "Epoch  13 Batch  126/269   train_loss = 3.650\n",
      "Epoch  13 Batch  127/269   train_loss = 3.956\n",
      "Epoch  13 Batch  128/269   train_loss = 3.970\n",
      "Epoch  13 Batch  129/269   train_loss = 3.969\n",
      "Epoch  13 Batch  130/269   train_loss = 4.165\n",
      "Epoch  13 Batch  131/269   train_loss = 3.769\n",
      "Epoch  13 Batch  132/269   train_loss = 4.019\n",
      "Epoch  13 Batch  133/269   train_loss = 3.796\n",
      "Epoch  13 Batch  134/269   train_loss = 4.088\n",
      "Epoch  13 Batch  135/269   train_loss = 3.768\n",
      "Epoch  13 Batch  136/269   train_loss = 3.739\n",
      "Epoch  13 Batch  137/269   train_loss = 4.049\n",
      "Epoch  13 Batch  138/269   train_loss = 3.841\n",
      "Epoch  13 Batch  139/269   train_loss = 4.009\n",
      "Epoch  13 Batch  140/269   train_loss = 4.120\n",
      "Epoch  13 Batch  141/269   train_loss = 4.210\n",
      "Epoch  13 Batch  142/269   train_loss = 3.753\n",
      "Epoch  13 Batch  143/269   train_loss = 4.005\n",
      "Epoch  13 Batch  144/269   train_loss = 4.129\n",
      "Epoch  13 Batch  145/269   train_loss = 3.867\n",
      "Epoch  13 Batch  146/269   train_loss = 4.085\n",
      "Epoch  13 Batch  147/269   train_loss = 3.778\n",
      "Epoch  13 Batch  148/269   train_loss = 4.046\n",
      "Epoch  13 Batch  149/269   train_loss = 3.925\n",
      "Epoch  13 Batch  150/269   train_loss = 3.964\n",
      "Epoch  13 Batch  151/269   train_loss = 3.984\n",
      "Epoch  13 Batch  152/269   train_loss = 3.927\n",
      "Epoch  13 Batch  153/269   train_loss = 4.038\n",
      "Epoch  13 Batch  154/269   train_loss = 3.945\n",
      "Epoch  13 Batch  155/269   train_loss = 3.889\n",
      "Epoch  13 Batch  156/269   train_loss = 4.356\n",
      "Epoch  13 Batch  157/269   train_loss = 4.235\n",
      "Epoch  13 Batch  158/269   train_loss = 4.077\n",
      "Epoch  13 Batch  159/269   train_loss = 4.077\n",
      "Epoch  13 Batch  160/269   train_loss = 3.935\n",
      "Epoch  13 Batch  161/269   train_loss = 4.064\n",
      "Epoch  13 Batch  162/269   train_loss = 3.969\n",
      "Epoch  13 Batch  163/269   train_loss = 4.144\n",
      "Epoch  13 Batch  164/269   train_loss = 4.203\n",
      "Epoch  13 Batch  165/269   train_loss = 4.134\n",
      "Epoch  13 Batch  166/269   train_loss = 3.989\n",
      "Epoch  13 Batch  167/269   train_loss = 3.941\n",
      "Epoch  13 Batch  168/269   train_loss = 3.901\n",
      "Epoch  13 Batch  169/269   train_loss = 4.247\n",
      "Epoch  13 Batch  170/269   train_loss = 3.821\n",
      "Epoch  13 Batch  171/269   train_loss = 4.136\n",
      "Epoch  13 Batch  172/269   train_loss = 3.890\n",
      "Epoch  13 Batch  173/269   train_loss = 4.088\n",
      "Epoch  13 Batch  174/269   train_loss = 4.162\n",
      "Epoch  13 Batch  175/269   train_loss = 3.989\n",
      "Epoch  13 Batch  176/269   train_loss = 3.893\n",
      "Epoch  13 Batch  177/269   train_loss = 3.979\n",
      "Epoch  13 Batch  178/269   train_loss = 4.258\n",
      "Epoch  13 Batch  179/269   train_loss = 3.984\n",
      "Epoch  13 Batch  180/269   train_loss = 3.820\n",
      "Epoch  13 Batch  181/269   train_loss = 4.092\n",
      "Epoch  13 Batch  182/269   train_loss = 3.875\n",
      "Epoch  13 Batch  183/269   train_loss = 4.016\n",
      "Epoch  13 Batch  184/269   train_loss = 4.027\n",
      "Epoch  13 Batch  185/269   train_loss = 3.993\n",
      "Epoch  13 Batch  186/269   train_loss = 3.860\n",
      "Epoch  13 Batch  187/269   train_loss = 4.218\n",
      "Epoch  13 Batch  188/269   train_loss = 3.915\n",
      "Epoch  13 Batch  189/269   train_loss = 3.963\n",
      "Epoch  13 Batch  190/269   train_loss = 4.323\n",
      "Epoch  13 Batch  191/269   train_loss = 4.019\n",
      "Epoch  13 Batch  192/269   train_loss = 3.939\n",
      "Epoch  13 Batch  193/269   train_loss = 3.870\n",
      "Epoch  13 Batch  194/269   train_loss = 4.082\n",
      "Epoch  13 Batch  195/269   train_loss = 3.856\n",
      "Epoch  13 Batch  196/269   train_loss = 4.095\n",
      "Epoch  13 Batch  197/269   train_loss = 4.188\n",
      "Epoch  13 Batch  198/269   train_loss = 4.136\n",
      "Epoch  13 Batch  199/269   train_loss = 4.144\n",
      "Epoch  13 Batch  200/269   train_loss = 4.102\n",
      "Epoch  13 Batch  201/269   train_loss = 3.884\n",
      "Epoch  13 Batch  202/269   train_loss = 3.820\n",
      "Epoch  13 Batch  203/269   train_loss = 3.868\n",
      "Epoch  13 Batch  204/269   train_loss = 4.001\n",
      "Epoch  13 Batch  205/269   train_loss = 4.078\n",
      "Epoch  13 Batch  206/269   train_loss = 3.960\n",
      "Epoch  13 Batch  207/269   train_loss = 3.957\n",
      "Epoch  13 Batch  208/269   train_loss = 3.935\n",
      "Epoch  13 Batch  209/269   train_loss = 4.078\n",
      "Epoch  13 Batch  210/269   train_loss = 3.915\n",
      "Epoch  13 Batch  211/269   train_loss = 3.949\n",
      "Epoch  13 Batch  212/269   train_loss = 4.310\n",
      "Epoch  13 Batch  213/269   train_loss = 3.938\n",
      "Epoch  13 Batch  214/269   train_loss = 4.045\n",
      "Epoch  13 Batch  215/269   train_loss = 4.144\n",
      "Epoch  13 Batch  216/269   train_loss = 4.167\n",
      "Epoch  13 Batch  217/269   train_loss = 3.877\n",
      "Epoch  13 Batch  218/269   train_loss = 3.957\n",
      "Epoch  13 Batch  219/269   train_loss = 3.726\n",
      "Epoch  13 Batch  220/269   train_loss = 4.042\n",
      "Epoch  13 Batch  221/269   train_loss = 3.886\n",
      "Epoch  13 Batch  222/269   train_loss = 4.004\n",
      "Epoch  13 Batch  223/269   train_loss = 3.893\n",
      "Epoch  13 Batch  224/269   train_loss = 4.091\n",
      "Epoch  13 Batch  225/269   train_loss = 4.231\n",
      "Epoch  13 Batch  226/269   train_loss = 4.050\n",
      "Epoch  13 Batch  227/269   train_loss = 3.743\n",
      "Epoch  13 Batch  228/269   train_loss = 3.934\n",
      "Epoch  13 Batch  229/269   train_loss = 4.030\n",
      "Epoch  13 Batch  230/269   train_loss = 4.131\n",
      "Epoch  13 Batch  231/269   train_loss = 3.942\n",
      "Epoch  13 Batch  232/269   train_loss = 3.934\n",
      "Epoch  13 Batch  233/269   train_loss = 4.036\n",
      "Epoch  13 Batch  234/269   train_loss = 3.954\n",
      "Epoch  13 Batch  235/269   train_loss = 4.181\n",
      "Epoch  13 Batch  236/269   train_loss = 3.875\n",
      "Epoch  13 Batch  237/269   train_loss = 3.598\n",
      "Epoch  13 Batch  238/269   train_loss = 3.811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13 Batch  239/269   train_loss = 4.362\n",
      "Epoch  13 Batch  240/269   train_loss = 3.910\n",
      "Epoch  13 Batch  241/269   train_loss = 4.162\n",
      "Epoch  13 Batch  242/269   train_loss = 3.730\n",
      "Epoch  13 Batch  243/269   train_loss = 4.040\n",
      "Epoch  13 Batch  244/269   train_loss = 3.820\n",
      "Epoch  13 Batch  245/269   train_loss = 3.924\n",
      "Epoch  13 Batch  246/269   train_loss = 3.634\n",
      "Epoch  13 Batch  247/269   train_loss = 4.060\n",
      "Epoch  13 Batch  248/269   train_loss = 4.021\n",
      "Epoch  13 Batch  249/269   train_loss = 3.948\n",
      "Epoch  13 Batch  250/269   train_loss = 3.658\n",
      "Epoch  13 Batch  251/269   train_loss = 4.105\n",
      "Epoch  13 Batch  252/269   train_loss = 4.117\n",
      "Epoch  13 Batch  253/269   train_loss = 3.977\n",
      "Epoch  13 Batch  254/269   train_loss = 3.867\n",
      "Epoch  13 Batch  255/269   train_loss = 3.993\n",
      "Epoch  13 Batch  256/269   train_loss = 3.922\n",
      "Epoch  13 Batch  257/269   train_loss = 3.775\n",
      "Epoch  13 Batch  258/269   train_loss = 3.648\n",
      "Epoch  13 Batch  259/269   train_loss = 3.772\n",
      "Epoch  13 Batch  260/269   train_loss = 4.034\n",
      "Epoch  13 Batch  261/269   train_loss = 4.046\n",
      "Epoch  13 Batch  262/269   train_loss = 3.827\n",
      "Epoch  13 Batch  263/269   train_loss = 3.733\n",
      "Epoch  13 Batch  264/269   train_loss = 4.296\n",
      "Epoch  13 Batch  265/269   train_loss = 4.033\n",
      "Epoch  13 Batch  266/269   train_loss = 3.970\n",
      "Epoch  13 Batch  267/269   train_loss = 3.975\n",
      "Epoch  13 Batch  268/269   train_loss = 4.075\n",
      "Epoch  14 Batch    0/269   train_loss = 3.835\n",
      "Epoch  14 Batch    1/269   train_loss = 3.677\n",
      "Epoch  14 Batch    2/269   train_loss = 3.833\n",
      "Epoch  14 Batch    3/269   train_loss = 3.888\n",
      "Epoch  14 Batch    4/269   train_loss = 4.336\n",
      "Epoch  14 Batch    5/269   train_loss = 4.074\n",
      "Epoch  14 Batch    6/269   train_loss = 3.838\n",
      "Epoch  14 Batch    7/269   train_loss = 3.779\n",
      "Epoch  14 Batch    8/269   train_loss = 4.072\n",
      "Epoch  14 Batch    9/269   train_loss = 3.784\n",
      "Epoch  14 Batch   10/269   train_loss = 3.807\n",
      "Epoch  14 Batch   11/269   train_loss = 3.955\n",
      "Epoch  14 Batch   12/269   train_loss = 3.773\n",
      "Epoch  14 Batch   13/269   train_loss = 3.990\n",
      "Epoch  14 Batch   14/269   train_loss = 3.810\n",
      "Epoch  14 Batch   15/269   train_loss = 4.190\n",
      "Epoch  14 Batch   16/269   train_loss = 3.972\n",
      "Epoch  14 Batch   17/269   train_loss = 4.062\n",
      "Epoch  14 Batch   18/269   train_loss = 3.933\n",
      "Epoch  14 Batch   19/269   train_loss = 3.902\n",
      "Epoch  14 Batch   20/269   train_loss = 4.279\n",
      "Epoch  14 Batch   21/269   train_loss = 4.099\n",
      "Epoch  14 Batch   22/269   train_loss = 3.769\n",
      "Epoch  14 Batch   23/269   train_loss = 4.001\n",
      "Epoch  14 Batch   24/269   train_loss = 3.847\n",
      "Epoch  14 Batch   25/269   train_loss = 4.135\n",
      "Epoch  14 Batch   26/269   train_loss = 4.024\n",
      "Epoch  14 Batch   27/269   train_loss = 3.839\n",
      "Epoch  14 Batch   28/269   train_loss = 3.980\n",
      "Epoch  14 Batch   29/269   train_loss = 4.094\n",
      "Epoch  14 Batch   30/269   train_loss = 4.186\n",
      "Epoch  14 Batch   31/269   train_loss = 3.937\n",
      "Epoch  14 Batch   32/269   train_loss = 3.805\n",
      "Epoch  14 Batch   33/269   train_loss = 3.751\n",
      "Epoch  14 Batch   34/269   train_loss = 4.003\n",
      "Epoch  14 Batch   35/269   train_loss = 3.824\n",
      "Epoch  14 Batch   36/269   train_loss = 3.791\n",
      "Epoch  14 Batch   37/269   train_loss = 3.941\n",
      "Epoch  14 Batch   38/269   train_loss = 3.796\n",
      "Epoch  14 Batch   39/269   train_loss = 4.086\n",
      "Epoch  14 Batch   40/269   train_loss = 3.836\n",
      "Epoch  14 Batch   41/269   train_loss = 3.813\n",
      "Epoch  14 Batch   42/269   train_loss = 4.308\n",
      "Epoch  14 Batch   43/269   train_loss = 4.111\n",
      "Epoch  14 Batch   44/269   train_loss = 3.947\n",
      "Epoch  14 Batch   45/269   train_loss = 3.743\n",
      "Epoch  14 Batch   46/269   train_loss = 4.074\n",
      "Epoch  14 Batch   47/269   train_loss = 3.668\n",
      "Epoch  14 Batch   48/269   train_loss = 4.214\n",
      "Epoch  14 Batch   49/269   train_loss = 4.183\n",
      "Epoch  14 Batch   50/269   train_loss = 4.143\n",
      "Epoch  14 Batch   51/269   train_loss = 4.032\n",
      "Epoch  14 Batch   52/269   train_loss = 4.037\n",
      "Epoch  14 Batch   53/269   train_loss = 4.175\n",
      "Epoch  14 Batch   54/269   train_loss = 3.973\n",
      "Epoch  14 Batch   55/269   train_loss = 4.075\n",
      "Epoch  14 Batch   56/269   train_loss = 3.937\n",
      "Epoch  14 Batch   57/269   train_loss = 4.029\n",
      "Epoch  14 Batch   58/269   train_loss = 3.908\n",
      "Epoch  14 Batch   59/269   train_loss = 3.833\n",
      "Epoch  14 Batch   60/269   train_loss = 4.043\n",
      "Epoch  14 Batch   61/269   train_loss = 4.267\n",
      "Epoch  14 Batch   62/269   train_loss = 3.949\n",
      "Epoch  14 Batch   63/269   train_loss = 4.188\n",
      "Epoch  14 Batch   64/269   train_loss = 3.925\n",
      "Epoch  14 Batch   65/269   train_loss = 4.008\n",
      "Epoch  14 Batch   66/269   train_loss = 3.965\n",
      "Epoch  14 Batch   67/269   train_loss = 3.968\n",
      "Epoch  14 Batch   68/269   train_loss = 4.051\n",
      "Epoch  14 Batch   69/269   train_loss = 4.039\n",
      "Epoch  14 Batch   70/269   train_loss = 3.708\n",
      "Epoch  14 Batch   71/269   train_loss = 4.085\n",
      "Epoch  14 Batch   72/269   train_loss = 3.674\n",
      "Epoch  14 Batch   73/269   train_loss = 3.960\n",
      "Epoch  14 Batch   74/269   train_loss = 3.955\n",
      "Epoch  14 Batch   75/269   train_loss = 4.095\n",
      "Epoch  14 Batch   76/269   train_loss = 4.039\n",
      "Epoch  14 Batch   77/269   train_loss = 3.644\n",
      "Epoch  14 Batch   78/269   train_loss = 4.022\n",
      "Epoch  14 Batch   79/269   train_loss = 4.157\n",
      "Epoch  14 Batch   80/269   train_loss = 3.929\n",
      "Epoch  14 Batch   81/269   train_loss = 3.851\n",
      "Epoch  14 Batch   82/269   train_loss = 4.174\n",
      "Epoch  14 Batch   83/269   train_loss = 3.987\n",
      "Epoch  14 Batch   84/269   train_loss = 4.146\n",
      "Epoch  14 Batch   85/269   train_loss = 4.389\n",
      "Epoch  14 Batch   86/269   train_loss = 4.230\n",
      "Epoch  14 Batch   87/269   train_loss = 3.972\n",
      "Epoch  14 Batch   88/269   train_loss = 4.010\n",
      "Epoch  14 Batch   89/269   train_loss = 3.958\n",
      "Epoch  14 Batch   90/269   train_loss = 3.948\n",
      "Epoch  14 Batch   91/269   train_loss = 4.177\n",
      "Epoch  14 Batch   92/269   train_loss = 4.086\n",
      "Epoch  14 Batch   93/269   train_loss = 3.764\n",
      "Epoch  14 Batch   94/269   train_loss = 4.261\n",
      "Epoch  14 Batch   95/269   train_loss = 3.820\n",
      "Epoch  14 Batch   96/269   train_loss = 3.840\n",
      "Epoch  14 Batch   97/269   train_loss = 3.974\n",
      "Epoch  14 Batch   98/269   train_loss = 4.020\n",
      "Epoch  14 Batch   99/269   train_loss = 4.029\n",
      "Epoch  14 Batch  100/269   train_loss = 3.909\n",
      "Epoch  14 Batch  101/269   train_loss = 4.050\n",
      "Epoch  14 Batch  102/269   train_loss = 3.873\n",
      "Epoch  14 Batch  103/269   train_loss = 3.966\n",
      "Epoch  14 Batch  104/269   train_loss = 3.827\n",
      "Epoch  14 Batch  105/269   train_loss = 4.039\n",
      "Epoch  14 Batch  106/269   train_loss = 4.162\n",
      "Epoch  14 Batch  107/269   train_loss = 4.160\n",
      "Epoch  14 Batch  108/269   train_loss = 4.060\n",
      "Epoch  14 Batch  109/269   train_loss = 4.061\n",
      "Epoch  14 Batch  110/269   train_loss = 3.982\n",
      "Epoch  14 Batch  111/269   train_loss = 4.170\n",
      "Epoch  14 Batch  112/269   train_loss = 4.114\n",
      "Epoch  14 Batch  113/269   train_loss = 3.916\n",
      "Epoch  14 Batch  114/269   train_loss = 3.877\n",
      "Epoch  14 Batch  115/269   train_loss = 3.822\n",
      "Epoch  14 Batch  116/269   train_loss = 3.863\n",
      "Epoch  14 Batch  117/269   train_loss = 3.815\n",
      "Epoch  14 Batch  118/269   train_loss = 4.052\n",
      "Epoch  14 Batch  119/269   train_loss = 3.801\n",
      "Epoch  14 Batch  120/269   train_loss = 4.045\n",
      "Epoch  14 Batch  121/269   train_loss = 3.936\n",
      "Epoch  14 Batch  122/269   train_loss = 3.943\n",
      "Epoch  14 Batch  123/269   train_loss = 3.733\n",
      "Epoch  14 Batch  124/269   train_loss = 4.112\n",
      "Epoch  14 Batch  125/269   train_loss = 3.714\n",
      "Epoch  14 Batch  126/269   train_loss = 3.638\n",
      "Epoch  14 Batch  127/269   train_loss = 3.948\n",
      "Epoch  14 Batch  128/269   train_loss = 3.939\n",
      "Epoch  14 Batch  129/269   train_loss = 3.958\n",
      "Epoch  14 Batch  130/269   train_loss = 4.118\n",
      "Epoch  14 Batch  131/269   train_loss = 3.749\n",
      "Epoch  14 Batch  132/269   train_loss = 3.975\n",
      "Epoch  14 Batch  133/269   train_loss = 3.777\n",
      "Epoch  14 Batch  134/269   train_loss = 4.120\n",
      "Epoch  14 Batch  135/269   train_loss = 3.750\n",
      "Epoch  14 Batch  136/269   train_loss = 3.698\n",
      "Epoch  14 Batch  137/269   train_loss = 4.005\n",
      "Epoch  14 Batch  138/269   train_loss = 3.820\n",
      "Epoch  14 Batch  139/269   train_loss = 3.983\n",
      "Epoch  14 Batch  140/269   train_loss = 4.056\n",
      "Epoch  14 Batch  141/269   train_loss = 4.202\n",
      "Epoch  14 Batch  142/269   train_loss = 3.758\n",
      "Epoch  14 Batch  143/269   train_loss = 3.978\n",
      "Epoch  14 Batch  144/269   train_loss = 4.102\n",
      "Epoch  14 Batch  145/269   train_loss = 3.847\n",
      "Epoch  14 Batch  146/269   train_loss = 4.121\n",
      "Epoch  14 Batch  147/269   train_loss = 3.753\n",
      "Epoch  14 Batch  148/269   train_loss = 4.026\n",
      "Epoch  14 Batch  149/269   train_loss = 3.885\n",
      "Epoch  14 Batch  150/269   train_loss = 3.922\n",
      "Epoch  14 Batch  151/269   train_loss = 3.959\n",
      "Epoch  14 Batch  152/269   train_loss = 3.950\n",
      "Epoch  14 Batch  153/269   train_loss = 4.046\n",
      "Epoch  14 Batch  154/269   train_loss = 3.938\n",
      "Epoch  14 Batch  155/269   train_loss = 3.864\n",
      "Epoch  14 Batch  156/269   train_loss = 4.314\n",
      "Epoch  14 Batch  157/269   train_loss = 4.187\n",
      "Epoch  14 Batch  158/269   train_loss = 4.049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14 Batch  159/269   train_loss = 4.064\n",
      "Epoch  14 Batch  160/269   train_loss = 3.910\n",
      "Epoch  14 Batch  161/269   train_loss = 4.025\n",
      "Epoch  14 Batch  162/269   train_loss = 3.954\n",
      "Epoch  14 Batch  163/269   train_loss = 4.086\n",
      "Epoch  14 Batch  164/269   train_loss = 4.178\n",
      "Epoch  14 Batch  165/269   train_loss = 4.077\n",
      "Epoch  14 Batch  166/269   train_loss = 4.013\n",
      "Epoch  14 Batch  167/269   train_loss = 3.881\n",
      "Epoch  14 Batch  168/269   train_loss = 3.899\n",
      "Epoch  14 Batch  169/269   train_loss = 4.228\n",
      "Epoch  14 Batch  170/269   train_loss = 3.792\n",
      "Epoch  14 Batch  171/269   train_loss = 4.126\n",
      "Epoch  14 Batch  172/269   train_loss = 3.818\n",
      "Epoch  14 Batch  173/269   train_loss = 4.056\n",
      "Epoch  14 Batch  174/269   train_loss = 4.121\n",
      "Epoch  14 Batch  175/269   train_loss = 3.986\n",
      "Epoch  14 Batch  176/269   train_loss = 3.868\n",
      "Epoch  14 Batch  177/269   train_loss = 3.934\n",
      "Epoch  14 Batch  178/269   train_loss = 4.199\n",
      "Epoch  14 Batch  179/269   train_loss = 3.951\n",
      "Epoch  14 Batch  180/269   train_loss = 3.797\n",
      "Epoch  14 Batch  181/269   train_loss = 4.069\n",
      "Epoch  14 Batch  182/269   train_loss = 3.826\n",
      "Epoch  14 Batch  183/269   train_loss = 3.994\n",
      "Epoch  14 Batch  184/269   train_loss = 4.002\n",
      "Epoch  14 Batch  185/269   train_loss = 3.984\n",
      "Epoch  14 Batch  186/269   train_loss = 3.826\n",
      "Epoch  14 Batch  187/269   train_loss = 4.195\n",
      "Epoch  14 Batch  188/269   train_loss = 3.864\n",
      "Epoch  14 Batch  189/269   train_loss = 3.925\n",
      "Epoch  14 Batch  190/269   train_loss = 4.313\n",
      "Epoch  14 Batch  191/269   train_loss = 3.976\n",
      "Epoch  14 Batch  192/269   train_loss = 3.914\n",
      "Epoch  14 Batch  193/269   train_loss = 3.863\n",
      "Epoch  14 Batch  194/269   train_loss = 4.072\n",
      "Epoch  14 Batch  195/269   train_loss = 3.845\n",
      "Epoch  14 Batch  196/269   train_loss = 4.061\n",
      "Epoch  14 Batch  197/269   train_loss = 4.137\n",
      "Epoch  14 Batch  198/269   train_loss = 4.117\n",
      "Epoch  14 Batch  199/269   train_loss = 4.128\n",
      "Epoch  14 Batch  200/269   train_loss = 4.050\n",
      "Epoch  14 Batch  201/269   train_loss = 3.852\n",
      "Epoch  14 Batch  202/269   train_loss = 3.800\n",
      "Epoch  14 Batch  203/269   train_loss = 3.838\n",
      "Epoch  14 Batch  204/269   train_loss = 3.971\n",
      "Epoch  14 Batch  205/269   train_loss = 4.020\n",
      "Epoch  14 Batch  206/269   train_loss = 3.929\n",
      "Epoch  14 Batch  207/269   train_loss = 3.935\n",
      "Epoch  14 Batch  208/269   train_loss = 3.916\n",
      "Epoch  14 Batch  209/269   train_loss = 4.084\n",
      "Epoch  14 Batch  210/269   train_loss = 3.889\n",
      "Epoch  14 Batch  211/269   train_loss = 3.959\n",
      "Epoch  14 Batch  212/269   train_loss = 4.320\n",
      "Epoch  14 Batch  213/269   train_loss = 3.919\n",
      "Epoch  14 Batch  214/269   train_loss = 3.999\n",
      "Epoch  14 Batch  215/269   train_loss = 4.157\n",
      "Epoch  14 Batch  216/269   train_loss = 4.103\n",
      "Epoch  14 Batch  217/269   train_loss = 3.872\n",
      "Epoch  14 Batch  218/269   train_loss = 3.939\n",
      "Epoch  14 Batch  219/269   train_loss = 3.693\n",
      "Epoch  14 Batch  220/269   train_loss = 4.027\n",
      "Epoch  14 Batch  221/269   train_loss = 3.869\n",
      "Epoch  14 Batch  222/269   train_loss = 3.980\n",
      "Epoch  14 Batch  223/269   train_loss = 3.844\n",
      "Epoch  14 Batch  224/269   train_loss = 4.081\n",
      "Epoch  14 Batch  225/269   train_loss = 4.199\n",
      "Epoch  14 Batch  226/269   train_loss = 4.003\n",
      "Epoch  14 Batch  227/269   train_loss = 3.711\n",
      "Epoch  14 Batch  228/269   train_loss = 3.892\n",
      "Epoch  14 Batch  229/269   train_loss = 4.022\n",
      "Epoch  14 Batch  230/269   train_loss = 4.081\n",
      "Epoch  14 Batch  231/269   train_loss = 3.914\n",
      "Epoch  14 Batch  232/269   train_loss = 3.918\n",
      "Epoch  14 Batch  233/269   train_loss = 4.023\n",
      "Epoch  14 Batch  234/269   train_loss = 3.911\n",
      "Epoch  14 Batch  235/269   train_loss = 4.181\n",
      "Epoch  14 Batch  236/269   train_loss = 3.832\n",
      "Epoch  14 Batch  237/269   train_loss = 3.570\n",
      "Epoch  14 Batch  238/269   train_loss = 3.795\n",
      "Epoch  14 Batch  239/269   train_loss = 4.333\n",
      "Epoch  14 Batch  240/269   train_loss = 3.890\n",
      "Epoch  14 Batch  241/269   train_loss = 4.116\n",
      "Epoch  14 Batch  242/269   train_loss = 3.707\n",
      "Epoch  14 Batch  243/269   train_loss = 4.034\n",
      "Epoch  14 Batch  244/269   train_loss = 3.807\n",
      "Epoch  14 Batch  245/269   train_loss = 3.910\n",
      "Epoch  14 Batch  246/269   train_loss = 3.604\n",
      "Epoch  14 Batch  247/269   train_loss = 4.044\n",
      "Epoch  14 Batch  248/269   train_loss = 3.989\n",
      "Epoch  14 Batch  249/269   train_loss = 3.924\n",
      "Epoch  14 Batch  250/269   train_loss = 3.617\n",
      "Epoch  14 Batch  251/269   train_loss = 4.058\n",
      "Epoch  14 Batch  252/269   train_loss = 4.085\n",
      "Epoch  14 Batch  253/269   train_loss = 3.965\n",
      "Epoch  14 Batch  254/269   train_loss = 3.840\n",
      "Epoch  14 Batch  255/269   train_loss = 3.990\n",
      "Epoch  14 Batch  256/269   train_loss = 3.885\n",
      "Epoch  14 Batch  257/269   train_loss = 3.755\n",
      "Epoch  14 Batch  258/269   train_loss = 3.638\n",
      "Epoch  14 Batch  259/269   train_loss = 3.747\n",
      "Epoch  14 Batch  260/269   train_loss = 3.989\n",
      "Epoch  14 Batch  261/269   train_loss = 4.039\n",
      "Epoch  14 Batch  262/269   train_loss = 3.787\n",
      "Epoch  14 Batch  263/269   train_loss = 3.706\n",
      "Epoch  14 Batch  264/269   train_loss = 4.267\n",
      "Epoch  14 Batch  265/269   train_loss = 3.993\n",
      "Epoch  14 Batch  266/269   train_loss = 3.924\n",
      "Epoch  14 Batch  267/269   train_loss = 3.955\n",
      "Epoch  14 Batch  268/269   train_loss = 4.057\n",
      "Epoch  15 Batch    0/269   train_loss = 3.821\n",
      "Epoch  15 Batch    1/269   train_loss = 3.711\n",
      "Epoch  15 Batch    2/269   train_loss = 3.840\n",
      "Epoch  15 Batch    3/269   train_loss = 3.911\n",
      "Epoch  15 Batch    4/269   train_loss = 4.307\n",
      "Epoch  15 Batch    5/269   train_loss = 4.027\n",
      "Epoch  15 Batch    6/269   train_loss = 3.797\n",
      "Epoch  15 Batch    7/269   train_loss = 3.768\n",
      "Epoch  15 Batch    8/269   train_loss = 4.056\n",
      "Epoch  15 Batch    9/269   train_loss = 3.763\n",
      "Epoch  15 Batch   10/269   train_loss = 3.775\n",
      "Epoch  15 Batch   11/269   train_loss = 3.904\n",
      "Epoch  15 Batch   12/269   train_loss = 3.746\n",
      "Epoch  15 Batch   13/269   train_loss = 3.956\n",
      "Epoch  15 Batch   14/269   train_loss = 3.783\n",
      "Epoch  15 Batch   15/269   train_loss = 4.177\n",
      "Epoch  15 Batch   16/269   train_loss = 3.938\n",
      "Epoch  15 Batch   17/269   train_loss = 4.030\n",
      "Epoch  15 Batch   18/269   train_loss = 3.920\n",
      "Epoch  15 Batch   19/269   train_loss = 3.860\n",
      "Epoch  15 Batch   20/269   train_loss = 4.247\n",
      "Epoch  15 Batch   21/269   train_loss = 4.106\n",
      "Epoch  15 Batch   22/269   train_loss = 3.757\n",
      "Epoch  15 Batch   23/269   train_loss = 3.953\n",
      "Epoch  15 Batch   24/269   train_loss = 3.846\n",
      "Epoch  15 Batch   25/269   train_loss = 4.128\n",
      "Epoch  15 Batch   26/269   train_loss = 4.020\n",
      "Epoch  15 Batch   27/269   train_loss = 3.813\n",
      "Epoch  15 Batch   28/269   train_loss = 3.955\n",
      "Epoch  15 Batch   29/269   train_loss = 4.055\n",
      "Epoch  15 Batch   30/269   train_loss = 4.187\n",
      "Epoch  15 Batch   31/269   train_loss = 3.906\n",
      "Epoch  15 Batch   32/269   train_loss = 3.810\n",
      "Epoch  15 Batch   33/269   train_loss = 3.711\n",
      "Epoch  15 Batch   34/269   train_loss = 3.957\n",
      "Epoch  15 Batch   35/269   train_loss = 3.783\n",
      "Epoch  15 Batch   36/269   train_loss = 3.774\n",
      "Epoch  15 Batch   37/269   train_loss = 3.902\n",
      "Epoch  15 Batch   38/269   train_loss = 3.765\n",
      "Epoch  15 Batch   39/269   train_loss = 4.048\n",
      "Epoch  15 Batch   40/269   train_loss = 3.814\n",
      "Epoch  15 Batch   41/269   train_loss = 3.799\n",
      "Epoch  15 Batch   42/269   train_loss = 4.315\n",
      "Epoch  15 Batch   43/269   train_loss = 4.088\n",
      "Epoch  15 Batch   44/269   train_loss = 3.905\n",
      "Epoch  15 Batch   45/269   train_loss = 3.725\n",
      "Epoch  15 Batch   46/269   train_loss = 4.037\n",
      "Epoch  15 Batch   47/269   train_loss = 3.645\n",
      "Epoch  15 Batch   48/269   train_loss = 4.178\n",
      "Epoch  15 Batch   49/269   train_loss = 4.151\n",
      "Epoch  15 Batch   50/269   train_loss = 4.147\n",
      "Epoch  15 Batch   51/269   train_loss = 4.026\n",
      "Epoch  15 Batch   52/269   train_loss = 3.978\n",
      "Epoch  15 Batch   53/269   train_loss = 4.162\n",
      "Epoch  15 Batch   54/269   train_loss = 3.951\n",
      "Epoch  15 Batch   55/269   train_loss = 4.101\n",
      "Epoch  15 Batch   56/269   train_loss = 3.913\n",
      "Epoch  15 Batch   57/269   train_loss = 4.003\n",
      "Epoch  15 Batch   58/269   train_loss = 3.938\n",
      "Epoch  15 Batch   59/269   train_loss = 3.815\n",
      "Epoch  15 Batch   60/269   train_loss = 4.002\n",
      "Epoch  15 Batch   61/269   train_loss = 4.227\n",
      "Epoch  15 Batch   62/269   train_loss = 3.926\n",
      "Epoch  15 Batch   63/269   train_loss = 4.160\n",
      "Epoch  15 Batch   64/269   train_loss = 3.884\n",
      "Epoch  15 Batch   65/269   train_loss = 3.968\n",
      "Epoch  15 Batch   66/269   train_loss = 3.929\n",
      "Epoch  15 Batch   67/269   train_loss = 3.931\n",
      "Epoch  15 Batch   68/269   train_loss = 4.017\n",
      "Epoch  15 Batch   69/269   train_loss = 4.012\n",
      "Epoch  15 Batch   70/269   train_loss = 3.676\n",
      "Epoch  15 Batch   71/269   train_loss = 4.057\n",
      "Epoch  15 Batch   72/269   train_loss = 3.659\n",
      "Epoch  15 Batch   73/269   train_loss = 3.928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15 Batch   74/269   train_loss = 3.937\n",
      "Epoch  15 Batch   75/269   train_loss = 4.074\n",
      "Epoch  15 Batch   76/269   train_loss = 4.025\n",
      "Epoch  15 Batch   77/269   train_loss = 3.616\n",
      "Epoch  15 Batch   78/269   train_loss = 4.007\n",
      "Epoch  15 Batch   79/269   train_loss = 4.128\n",
      "Epoch  15 Batch   80/269   train_loss = 3.927\n",
      "Epoch  15 Batch   81/269   train_loss = 3.827\n",
      "Epoch  15 Batch   82/269   train_loss = 4.143\n",
      "Epoch  15 Batch   83/269   train_loss = 3.956\n",
      "Epoch  15 Batch   84/269   train_loss = 4.125\n",
      "Epoch  15 Batch   85/269   train_loss = 4.332\n",
      "Epoch  15 Batch   86/269   train_loss = 4.219\n",
      "Epoch  15 Batch   87/269   train_loss = 3.957\n",
      "Epoch  15 Batch   88/269   train_loss = 3.978\n",
      "Epoch  15 Batch   89/269   train_loss = 3.949\n",
      "Epoch  15 Batch   90/269   train_loss = 3.942\n",
      "Epoch  15 Batch   91/269   train_loss = 4.140\n",
      "Epoch  15 Batch   92/269   train_loss = 4.081\n",
      "Epoch  15 Batch   93/269   train_loss = 3.758\n",
      "Epoch  15 Batch   94/269   train_loss = 4.209\n",
      "Epoch  15 Batch   95/269   train_loss = 3.794\n",
      "Epoch  15 Batch   96/269   train_loss = 3.851\n",
      "Epoch  15 Batch   97/269   train_loss = 3.971\n",
      "Epoch  15 Batch   98/269   train_loss = 3.968\n",
      "Epoch  15 Batch   99/269   train_loss = 3.986\n",
      "Epoch  15 Batch  100/269   train_loss = 3.905\n",
      "Epoch  15 Batch  101/269   train_loss = 4.022\n",
      "Epoch  15 Batch  102/269   train_loss = 3.863\n",
      "Epoch  15 Batch  103/269   train_loss = 3.951\n",
      "Epoch  15 Batch  104/269   train_loss = 3.819\n",
      "Epoch  15 Batch  105/269   train_loss = 4.062\n",
      "Epoch  15 Batch  106/269   train_loss = 4.145\n",
      "Epoch  15 Batch  107/269   train_loss = 4.144\n",
      "Epoch  15 Batch  108/269   train_loss = 4.039\n",
      "Epoch  15 Batch  109/269   train_loss = 4.030\n",
      "Epoch  15 Batch  110/269   train_loss = 3.937\n",
      "Epoch  15 Batch  111/269   train_loss = 4.125\n",
      "Epoch  15 Batch  112/269   train_loss = 4.095\n",
      "Epoch  15 Batch  113/269   train_loss = 3.900\n",
      "Epoch  15 Batch  114/269   train_loss = 3.880\n",
      "Epoch  15 Batch  115/269   train_loss = 3.778\n",
      "Epoch  15 Batch  116/269   train_loss = 3.836\n",
      "Epoch  15 Batch  117/269   train_loss = 3.791\n",
      "Epoch  15 Batch  118/269   train_loss = 4.021\n",
      "Epoch  15 Batch  119/269   train_loss = 3.766\n",
      "Epoch  15 Batch  120/269   train_loss = 4.032\n",
      "Epoch  15 Batch  121/269   train_loss = 3.898\n",
      "Epoch  15 Batch  122/269   train_loss = 3.936\n",
      "Epoch  15 Batch  123/269   train_loss = 3.711\n",
      "Epoch  15 Batch  124/269   train_loss = 4.108\n",
      "Epoch  15 Batch  125/269   train_loss = 3.698\n",
      "Epoch  15 Batch  126/269   train_loss = 3.614\n",
      "Epoch  15 Batch  127/269   train_loss = 3.890\n",
      "Epoch  15 Batch  128/269   train_loss = 3.920\n",
      "Epoch  15 Batch  129/269   train_loss = 3.938\n",
      "Epoch  15 Batch  130/269   train_loss = 4.087\n",
      "Epoch  15 Batch  131/269   train_loss = 3.733\n",
      "Epoch  15 Batch  132/269   train_loss = 3.979\n",
      "Epoch  15 Batch  133/269   train_loss = 3.721\n",
      "Epoch  15 Batch  134/269   train_loss = 4.094\n",
      "Epoch  15 Batch  135/269   train_loss = 3.702\n",
      "Epoch  15 Batch  136/269   train_loss = 3.690\n",
      "Epoch  15 Batch  137/269   train_loss = 4.019\n",
      "Epoch  15 Batch  138/269   train_loss = 3.805\n",
      "Epoch  15 Batch  139/269   train_loss = 3.953\n",
      "Epoch  15 Batch  140/269   train_loss = 4.032\n",
      "Epoch  15 Batch  141/269   train_loss = 4.180\n",
      "Epoch  15 Batch  142/269   train_loss = 3.735\n",
      "Epoch  15 Batch  143/269   train_loss = 3.957\n",
      "Epoch  15 Batch  144/269   train_loss = 4.082\n",
      "Epoch  15 Batch  145/269   train_loss = 3.844\n",
      "Epoch  15 Batch  146/269   train_loss = 4.099\n",
      "Epoch  15 Batch  147/269   train_loss = 3.746\n",
      "Epoch  15 Batch  148/269   train_loss = 4.000\n",
      "Epoch  15 Batch  149/269   train_loss = 3.878\n",
      "Epoch  15 Batch  150/269   train_loss = 3.914\n",
      "Epoch  15 Batch  151/269   train_loss = 3.947\n",
      "Epoch  15 Batch  152/269   train_loss = 3.919\n",
      "Epoch  15 Batch  153/269   train_loss = 4.012\n",
      "Epoch  15 Batch  154/269   train_loss = 3.868\n",
      "Epoch  15 Batch  155/269   train_loss = 3.850\n",
      "Epoch  15 Batch  156/269   train_loss = 4.271\n",
      "Epoch  15 Batch  157/269   train_loss = 4.135\n",
      "Epoch  15 Batch  158/269   train_loss = 4.020\n",
      "Epoch  15 Batch  159/269   train_loss = 4.017\n",
      "Epoch  15 Batch  160/269   train_loss = 3.875\n",
      "Epoch  15 Batch  161/269   train_loss = 4.028\n",
      "Epoch  15 Batch  162/269   train_loss = 3.950\n",
      "Epoch  15 Batch  163/269   train_loss = 4.061\n",
      "Epoch  15 Batch  164/269   train_loss = 4.184\n",
      "Epoch  15 Batch  165/269   train_loss = 4.052\n",
      "Epoch  15 Batch  166/269   train_loss = 3.993\n",
      "Epoch  15 Batch  167/269   train_loss = 3.857\n",
      "Epoch  15 Batch  168/269   train_loss = 3.872\n",
      "Epoch  15 Batch  169/269   train_loss = 4.216\n",
      "Epoch  15 Batch  170/269   train_loss = 3.783\n",
      "Epoch  15 Batch  171/269   train_loss = 4.084\n",
      "Epoch  15 Batch  172/269   train_loss = 3.803\n",
      "Epoch  15 Batch  173/269   train_loss = 4.020\n",
      "Epoch  15 Batch  174/269   train_loss = 4.102\n",
      "Epoch  15 Batch  175/269   train_loss = 3.981\n",
      "Epoch  15 Batch  176/269   train_loss = 3.845\n",
      "Epoch  15 Batch  177/269   train_loss = 3.953\n",
      "Epoch  15 Batch  178/269   train_loss = 4.192\n",
      "Epoch  15 Batch  179/269   train_loss = 3.962\n",
      "Epoch  15 Batch  180/269   train_loss = 3.784\n",
      "Epoch  15 Batch  181/269   train_loss = 4.012\n",
      "Epoch  15 Batch  182/269   train_loss = 3.822\n",
      "Epoch  15 Batch  183/269   train_loss = 3.977\n",
      "Epoch  15 Batch  184/269   train_loss = 3.950\n",
      "Epoch  15 Batch  185/269   train_loss = 3.967\n",
      "Epoch  15 Batch  186/269   train_loss = 3.832\n",
      "Epoch  15 Batch  187/269   train_loss = 4.163\n",
      "Epoch  15 Batch  188/269   train_loss = 3.836\n",
      "Epoch  15 Batch  189/269   train_loss = 3.905\n",
      "Epoch  15 Batch  190/269   train_loss = 4.299\n",
      "Epoch  15 Batch  191/269   train_loss = 3.973\n",
      "Epoch  15 Batch  192/269   train_loss = 3.909\n",
      "Epoch  15 Batch  193/269   train_loss = 3.825\n",
      "Epoch  15 Batch  194/269   train_loss = 4.025\n",
      "Epoch  15 Batch  195/269   train_loss = 3.805\n",
      "Epoch  15 Batch  196/269   train_loss = 4.030\n",
      "Epoch  15 Batch  197/269   train_loss = 4.106\n",
      "Epoch  15 Batch  198/269   train_loss = 4.076\n",
      "Epoch  15 Batch  199/269   train_loss = 4.095\n",
      "Epoch  15 Batch  200/269   train_loss = 4.046\n",
      "Epoch  15 Batch  201/269   train_loss = 3.839\n",
      "Epoch  15 Batch  202/269   train_loss = 3.756\n",
      "Epoch  15 Batch  203/269   train_loss = 3.834\n",
      "Epoch  15 Batch  204/269   train_loss = 3.976\n",
      "Epoch  15 Batch  205/269   train_loss = 3.977\n",
      "Epoch  15 Batch  206/269   train_loss = 3.902\n",
      "Epoch  15 Batch  207/269   train_loss = 3.913\n",
      "Epoch  15 Batch  208/269   train_loss = 3.896\n",
      "Epoch  15 Batch  209/269   train_loss = 4.048\n",
      "Epoch  15 Batch  210/269   train_loss = 3.878\n",
      "Epoch  15 Batch  211/269   train_loss = 3.912\n",
      "Epoch  15 Batch  212/269   train_loss = 4.277\n",
      "Epoch  15 Batch  213/269   train_loss = 3.859\n",
      "Epoch  15 Batch  214/269   train_loss = 3.975\n",
      "Epoch  15 Batch  215/269   train_loss = 4.120\n",
      "Epoch  15 Batch  216/269   train_loss = 4.099\n",
      "Epoch  15 Batch  217/269   train_loss = 3.860\n",
      "Epoch  15 Batch  218/269   train_loss = 3.937\n",
      "Epoch  15 Batch  219/269   train_loss = 3.669\n",
      "Epoch  15 Batch  220/269   train_loss = 4.026\n",
      "Epoch  15 Batch  221/269   train_loss = 3.870\n",
      "Epoch  15 Batch  222/269   train_loss = 3.965\n",
      "Epoch  15 Batch  223/269   train_loss = 3.815\n",
      "Epoch  15 Batch  224/269   train_loss = 4.039\n",
      "Epoch  15 Batch  225/269   train_loss = 4.172\n",
      "Epoch  15 Batch  226/269   train_loss = 3.967\n",
      "Epoch  15 Batch  227/269   train_loss = 3.663\n",
      "Epoch  15 Batch  228/269   train_loss = 3.883\n",
      "Epoch  15 Batch  229/269   train_loss = 4.031\n",
      "Epoch  15 Batch  230/269   train_loss = 4.069\n",
      "Epoch  15 Batch  231/269   train_loss = 3.912\n",
      "Epoch  15 Batch  232/269   train_loss = 3.924\n",
      "Epoch  15 Batch  233/269   train_loss = 4.007\n",
      "Epoch  15 Batch  234/269   train_loss = 3.889\n",
      "Epoch  15 Batch  235/269   train_loss = 4.161\n",
      "Epoch  15 Batch  236/269   train_loss = 3.806\n",
      "Epoch  15 Batch  237/269   train_loss = 3.535\n",
      "Epoch  15 Batch  238/269   train_loss = 3.744\n",
      "Epoch  15 Batch  239/269   train_loss = 4.316\n",
      "Epoch  15 Batch  240/269   train_loss = 3.863\n",
      "Epoch  15 Batch  241/269   train_loss = 4.126\n",
      "Epoch  15 Batch  242/269   train_loss = 3.684\n",
      "Epoch  15 Batch  243/269   train_loss = 4.009\n",
      "Epoch  15 Batch  244/269   train_loss = 3.773\n",
      "Epoch  15 Batch  245/269   train_loss = 3.892\n",
      "Epoch  15 Batch  246/269   train_loss = 3.605\n",
      "Epoch  15 Batch  247/269   train_loss = 4.059\n",
      "Epoch  15 Batch  248/269   train_loss = 3.992\n",
      "Epoch  15 Batch  249/269   train_loss = 3.887\n",
      "Epoch  15 Batch  250/269   train_loss = 3.619\n",
      "Epoch  15 Batch  251/269   train_loss = 4.042\n",
      "Epoch  15 Batch  252/269   train_loss = 4.046\n",
      "Epoch  15 Batch  253/269   train_loss = 3.927\n",
      "Epoch  15 Batch  254/269   train_loss = 3.825\n",
      "Epoch  15 Batch  255/269   train_loss = 3.960\n",
      "Epoch  15 Batch  256/269   train_loss = 3.891\n",
      "Epoch  15 Batch  257/269   train_loss = 3.739\n",
      "Epoch  15 Batch  258/269   train_loss = 3.611\n",
      "Epoch  15 Batch  259/269   train_loss = 3.725\n",
      "Epoch  15 Batch  260/269   train_loss = 3.971\n",
      "Epoch  15 Batch  261/269   train_loss = 3.987\n",
      "Epoch  15 Batch  262/269   train_loss = 3.754\n",
      "Epoch  15 Batch  263/269   train_loss = 3.695\n",
      "Epoch  15 Batch  264/269   train_loss = 4.259\n",
      "Epoch  15 Batch  265/269   train_loss = 4.005\n",
      "Epoch  15 Batch  266/269   train_loss = 3.926\n",
      "Epoch  15 Batch  267/269   train_loss = 3.978\n",
      "Epoch  15 Batch  268/269   train_loss = 4.035\n",
      "Epoch  16 Batch    0/269   train_loss = 3.793\n",
      "Epoch  16 Batch    1/269   train_loss = 3.676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16 Batch    2/269   train_loss = 3.812\n",
      "Epoch  16 Batch    3/269   train_loss = 3.860\n",
      "Epoch  16 Batch    4/269   train_loss = 4.292\n",
      "Epoch  16 Batch    5/269   train_loss = 3.988\n",
      "Epoch  16 Batch    6/269   train_loss = 3.785\n",
      "Epoch  16 Batch    7/269   train_loss = 3.770\n",
      "Epoch  16 Batch    8/269   train_loss = 4.048\n",
      "Epoch  16 Batch    9/269   train_loss = 3.760\n",
      "Epoch  16 Batch   10/269   train_loss = 3.733\n",
      "Epoch  16 Batch   11/269   train_loss = 3.883\n",
      "Epoch  16 Batch   12/269   train_loss = 3.715\n",
      "Epoch  16 Batch   13/269   train_loss = 3.913\n",
      "Epoch  16 Batch   14/269   train_loss = 3.752\n",
      "Epoch  16 Batch   15/269   train_loss = 4.142\n",
      "Epoch  16 Batch   16/269   train_loss = 3.908\n",
      "Epoch  16 Batch   17/269   train_loss = 4.005\n",
      "Epoch  16 Batch   18/269   train_loss = 3.875\n",
      "Epoch  16 Batch   19/269   train_loss = 3.863\n",
      "Epoch  16 Batch   20/269   train_loss = 4.204\n",
      "Epoch  16 Batch   21/269   train_loss = 4.057\n",
      "Epoch  16 Batch   22/269   train_loss = 3.761\n",
      "Epoch  16 Batch   23/269   train_loss = 3.954\n",
      "Epoch  16 Batch   24/269   train_loss = 3.813\n",
      "Epoch  16 Batch   25/269   train_loss = 4.115\n",
      "Epoch  16 Batch   26/269   train_loss = 3.989\n",
      "Epoch  16 Batch   27/269   train_loss = 3.777\n",
      "Epoch  16 Batch   28/269   train_loss = 3.933\n",
      "Epoch  16 Batch   29/269   train_loss = 4.012\n",
      "Epoch  16 Batch   30/269   train_loss = 4.181\n",
      "Epoch  16 Batch   31/269   train_loss = 3.884\n",
      "Epoch  16 Batch   32/269   train_loss = 3.808\n",
      "Epoch  16 Batch   33/269   train_loss = 3.669\n",
      "Epoch  16 Batch   34/269   train_loss = 3.911\n",
      "Epoch  16 Batch   35/269   train_loss = 3.761\n",
      "Epoch  16 Batch   36/269   train_loss = 3.755\n",
      "Epoch  16 Batch   37/269   train_loss = 3.886\n",
      "Epoch  16 Batch   38/269   train_loss = 3.740\n",
      "Epoch  16 Batch   39/269   train_loss = 4.012\n",
      "Epoch  16 Batch   40/269   train_loss = 3.785\n",
      "Epoch  16 Batch   41/269   train_loss = 3.771\n",
      "Epoch  16 Batch   42/269   train_loss = 4.296\n",
      "Epoch  16 Batch   43/269   train_loss = 4.050\n",
      "Epoch  16 Batch   44/269   train_loss = 3.893\n",
      "Epoch  16 Batch   45/269   train_loss = 3.671\n",
      "Epoch  16 Batch   46/269   train_loss = 4.025\n",
      "Epoch  16 Batch   47/269   train_loss = 3.642\n",
      "Epoch  16 Batch   48/269   train_loss = 4.144\n",
      "Epoch  16 Batch   49/269   train_loss = 4.176\n",
      "Epoch  16 Batch   50/269   train_loss = 4.086\n",
      "Epoch  16 Batch   51/269   train_loss = 3.973\n",
      "Epoch  16 Batch   52/269   train_loss = 3.939\n",
      "Epoch  16 Batch   53/269   train_loss = 4.139\n",
      "Epoch  16 Batch   54/269   train_loss = 3.921\n",
      "Epoch  16 Batch   55/269   train_loss = 4.067\n",
      "Epoch  16 Batch   56/269   train_loss = 3.893\n",
      "Epoch  16 Batch   57/269   train_loss = 3.990\n",
      "Epoch  16 Batch   58/269   train_loss = 3.883\n",
      "Epoch  16 Batch   59/269   train_loss = 3.824\n",
      "Epoch  16 Batch   60/269   train_loss = 4.010\n",
      "Epoch  16 Batch   61/269   train_loss = 4.212\n",
      "Epoch  16 Batch   62/269   train_loss = 3.888\n",
      "Epoch  16 Batch   63/269   train_loss = 4.118\n",
      "Epoch  16 Batch   64/269   train_loss = 3.865\n",
      "Epoch  16 Batch   65/269   train_loss = 3.943\n",
      "Epoch  16 Batch   66/269   train_loss = 3.889\n",
      "Epoch  16 Batch   67/269   train_loss = 3.955\n",
      "Epoch  16 Batch   68/269   train_loss = 4.001\n",
      "Epoch  16 Batch   69/269   train_loss = 3.976\n",
      "Epoch  16 Batch   70/269   train_loss = 3.637\n",
      "Epoch  16 Batch   71/269   train_loss = 4.040\n",
      "Epoch  16 Batch   72/269   train_loss = 3.643\n",
      "Epoch  16 Batch   73/269   train_loss = 3.924\n",
      "Epoch  16 Batch   74/269   train_loss = 3.909\n",
      "Epoch  16 Batch   75/269   train_loss = 4.050\n",
      "Epoch  16 Batch   76/269   train_loss = 3.992\n",
      "Epoch  16 Batch   77/269   train_loss = 3.581\n",
      "Epoch  16 Batch   78/269   train_loss = 3.997\n",
      "Epoch  16 Batch   79/269   train_loss = 4.102\n",
      "Epoch  16 Batch   80/269   train_loss = 3.898\n",
      "Epoch  16 Batch   81/269   train_loss = 3.771\n",
      "Epoch  16 Batch   82/269   train_loss = 4.111\n",
      "Epoch  16 Batch   83/269   train_loss = 3.929\n",
      "Epoch  16 Batch   84/269   train_loss = 4.113\n",
      "Epoch  16 Batch   85/269   train_loss = 4.317\n",
      "Epoch  16 Batch   86/269   train_loss = 4.178\n",
      "Epoch  16 Batch   87/269   train_loss = 3.927\n",
      "Epoch  16 Batch   88/269   train_loss = 3.938\n",
      "Epoch  16 Batch   89/269   train_loss = 3.915\n",
      "Epoch  16 Batch   90/269   train_loss = 3.849\n",
      "Epoch  16 Batch   91/269   train_loss = 4.143\n",
      "Epoch  16 Batch   92/269   train_loss = 4.036\n",
      "Epoch  16 Batch   93/269   train_loss = 3.740\n",
      "Epoch  16 Batch   94/269   train_loss = 4.185\n",
      "Epoch  16 Batch   95/269   train_loss = 3.794\n",
      "Epoch  16 Batch   96/269   train_loss = 3.811\n",
      "Epoch  16 Batch   97/269   train_loss = 3.973\n",
      "Epoch  16 Batch   98/269   train_loss = 3.983\n",
      "Epoch  16 Batch   99/269   train_loss = 3.967\n",
      "Epoch  16 Batch  100/269   train_loss = 3.876\n",
      "Epoch  16 Batch  101/269   train_loss = 4.023\n",
      "Epoch  16 Batch  102/269   train_loss = 3.831\n",
      "Epoch  16 Batch  103/269   train_loss = 3.915\n",
      "Epoch  16 Batch  104/269   train_loss = 3.768\n",
      "Epoch  16 Batch  105/269   train_loss = 4.038\n",
      "Epoch  16 Batch  106/269   train_loss = 4.098\n",
      "Epoch  16 Batch  107/269   train_loss = 4.125\n",
      "Epoch  16 Batch  108/269   train_loss = 4.004\n",
      "Epoch  16 Batch  109/269   train_loss = 3.986\n",
      "Epoch  16 Batch  110/269   train_loss = 3.937\n",
      "Epoch  16 Batch  111/269   train_loss = 4.081\n",
      "Epoch  16 Batch  112/269   train_loss = 4.059\n",
      "Epoch  16 Batch  113/269   train_loss = 3.882\n",
      "Epoch  16 Batch  114/269   train_loss = 3.838\n",
      "Epoch  16 Batch  115/269   train_loss = 3.768\n",
      "Epoch  16 Batch  116/269   train_loss = 3.820\n",
      "Epoch  16 Batch  117/269   train_loss = 3.754\n",
      "Epoch  16 Batch  118/269   train_loss = 4.046\n",
      "Epoch  16 Batch  119/269   train_loss = 3.764\n",
      "Epoch  16 Batch  120/269   train_loss = 4.014\n",
      "Epoch  16 Batch  121/269   train_loss = 3.850\n",
      "Epoch  16 Batch  122/269   train_loss = 3.906\n",
      "Epoch  16 Batch  123/269   train_loss = 3.718\n",
      "Epoch  16 Batch  124/269   train_loss = 4.081\n",
      "Epoch  16 Batch  125/269   train_loss = 3.654\n",
      "Epoch  16 Batch  126/269   train_loss = 3.597\n",
      "Epoch  16 Batch  127/269   train_loss = 3.872\n",
      "Epoch  16 Batch  128/269   train_loss = 3.915\n",
      "Epoch  16 Batch  129/269   train_loss = 3.895\n",
      "Epoch  16 Batch  130/269   train_loss = 4.055\n",
      "Epoch  16 Batch  131/269   train_loss = 3.697\n",
      "Epoch  16 Batch  132/269   train_loss = 3.922\n",
      "Epoch  16 Batch  133/269   train_loss = 3.708\n",
      "Epoch  16 Batch  134/269   train_loss = 4.076\n",
      "Epoch  16 Batch  135/269   train_loss = 3.681\n",
      "Epoch  16 Batch  136/269   train_loss = 3.659\n",
      "Epoch  16 Batch  137/269   train_loss = 4.001\n",
      "Epoch  16 Batch  138/269   train_loss = 3.780\n",
      "Epoch  16 Batch  139/269   train_loss = 3.907\n",
      "Epoch  16 Batch  140/269   train_loss = 4.011\n",
      "Epoch  16 Batch  141/269   train_loss = 4.168\n",
      "Epoch  16 Batch  142/269   train_loss = 3.729\n",
      "Epoch  16 Batch  143/269   train_loss = 3.922\n",
      "Epoch  16 Batch  144/269   train_loss = 4.059\n",
      "Epoch  16 Batch  145/269   train_loss = 3.842\n",
      "Epoch  16 Batch  146/269   train_loss = 4.093\n",
      "Epoch  16 Batch  147/269   train_loss = 3.715\n",
      "Epoch  16 Batch  148/269   train_loss = 3.971\n",
      "Epoch  16 Batch  149/269   train_loss = 3.867\n",
      "Epoch  16 Batch  150/269   train_loss = 3.893\n",
      "Epoch  16 Batch  151/269   train_loss = 3.955\n",
      "Epoch  16 Batch  152/269   train_loss = 3.911\n",
      "Epoch  16 Batch  153/269   train_loss = 3.980\n",
      "Epoch  16 Batch  154/269   train_loss = 3.869\n",
      "Epoch  16 Batch  155/269   train_loss = 3.848\n",
      "Epoch  16 Batch  156/269   train_loss = 4.268\n",
      "Epoch  16 Batch  157/269   train_loss = 4.102\n",
      "Epoch  16 Batch  158/269   train_loss = 4.002\n",
      "Epoch  16 Batch  159/269   train_loss = 4.014\n",
      "Epoch  16 Batch  160/269   train_loss = 3.884\n",
      "Epoch  16 Batch  161/269   train_loss = 4.016\n",
      "Epoch  16 Batch  162/269   train_loss = 3.951\n",
      "Epoch  16 Batch  163/269   train_loss = 4.039\n",
      "Epoch  16 Batch  164/269   train_loss = 4.155\n",
      "Epoch  16 Batch  165/269   train_loss = 4.044\n",
      "Epoch  16 Batch  166/269   train_loss = 3.971\n",
      "Epoch  16 Batch  167/269   train_loss = 3.832\n",
      "Epoch  16 Batch  168/269   train_loss = 3.861\n",
      "Epoch  16 Batch  169/269   train_loss = 4.202\n",
      "Epoch  16 Batch  170/269   train_loss = 3.739\n",
      "Epoch  16 Batch  171/269   train_loss = 4.053\n",
      "Epoch  16 Batch  172/269   train_loss = 3.807\n",
      "Epoch  16 Batch  173/269   train_loss = 4.003\n",
      "Epoch  16 Batch  174/269   train_loss = 4.072\n",
      "Epoch  16 Batch  175/269   train_loss = 3.991\n",
      "Epoch  16 Batch  176/269   train_loss = 3.842\n",
      "Epoch  16 Batch  177/269   train_loss = 3.925\n",
      "Epoch  16 Batch  178/269   train_loss = 4.177\n",
      "Epoch  16 Batch  179/269   train_loss = 3.933\n",
      "Epoch  16 Batch  180/269   train_loss = 3.795\n",
      "Epoch  16 Batch  181/269   train_loss = 4.016\n",
      "Epoch  16 Batch  182/269   train_loss = 3.799\n",
      "Epoch  16 Batch  183/269   train_loss = 3.951\n",
      "Epoch  16 Batch  184/269   train_loss = 3.934\n",
      "Epoch  16 Batch  185/269   train_loss = 3.957\n",
      "Epoch  16 Batch  186/269   train_loss = 3.829\n",
      "Epoch  16 Batch  187/269   train_loss = 4.123\n",
      "Epoch  16 Batch  188/269   train_loss = 3.823\n",
      "Epoch  16 Batch  189/269   train_loss = 3.898\n",
      "Epoch  16 Batch  190/269   train_loss = 4.275\n",
      "Epoch  16 Batch  191/269   train_loss = 3.960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16 Batch  192/269   train_loss = 3.915\n",
      "Epoch  16 Batch  193/269   train_loss = 3.825\n",
      "Epoch  16 Batch  194/269   train_loss = 4.004\n",
      "Epoch  16 Batch  195/269   train_loss = 3.816\n",
      "Epoch  16 Batch  196/269   train_loss = 4.037\n",
      "Epoch  16 Batch  197/269   train_loss = 4.084\n",
      "Epoch  16 Batch  198/269   train_loss = 4.047\n",
      "Epoch  16 Batch  199/269   train_loss = 4.068\n",
      "Epoch  16 Batch  200/269   train_loss = 3.991\n",
      "Epoch  16 Batch  201/269   train_loss = 3.832\n",
      "Epoch  16 Batch  202/269   train_loss = 3.733\n",
      "Epoch  16 Batch  203/269   train_loss = 3.799\n",
      "Epoch  16 Batch  204/269   train_loss = 3.946\n",
      "Epoch  16 Batch  205/269   train_loss = 3.956\n",
      "Epoch  16 Batch  206/269   train_loss = 3.886\n",
      "Epoch  16 Batch  207/269   train_loss = 3.901\n",
      "Epoch  16 Batch  208/269   train_loss = 3.857\n",
      "Epoch  16 Batch  209/269   train_loss = 4.084\n",
      "Epoch  16 Batch  210/269   train_loss = 3.869\n",
      "Epoch  16 Batch  211/269   train_loss = 3.906\n",
      "Epoch  16 Batch  212/269   train_loss = 4.229\n",
      "Epoch  16 Batch  213/269   train_loss = 3.843\n",
      "Epoch  16 Batch  214/269   train_loss = 3.936\n",
      "Epoch  16 Batch  215/269   train_loss = 4.110\n",
      "Epoch  16 Batch  216/269   train_loss = 4.088\n",
      "Epoch  16 Batch  217/269   train_loss = 3.798\n",
      "Epoch  16 Batch  218/269   train_loss = 3.924\n",
      "Epoch  16 Batch  219/269   train_loss = 3.668\n",
      "Epoch  16 Batch  220/269   train_loss = 3.999\n",
      "Epoch  16 Batch  221/269   train_loss = 3.845\n",
      "Epoch  16 Batch  222/269   train_loss = 3.958\n",
      "Epoch  16 Batch  223/269   train_loss = 3.810\n",
      "Epoch  16 Batch  224/269   train_loss = 4.013\n",
      "Epoch  16 Batch  225/269   train_loss = 4.167\n",
      "Epoch  16 Batch  226/269   train_loss = 3.930\n",
      "Epoch  16 Batch  227/269   train_loss = 3.658\n",
      "Epoch  16 Batch  228/269   train_loss = 3.866\n",
      "Epoch  16 Batch  229/269   train_loss = 4.030\n",
      "Epoch  16 Batch  230/269   train_loss = 4.070\n",
      "Epoch  16 Batch  231/269   train_loss = 3.892\n",
      "Epoch  16 Batch  232/269   train_loss = 3.872\n",
      "Epoch  16 Batch  233/269   train_loss = 3.981\n",
      "Epoch  16 Batch  234/269   train_loss = 3.878\n",
      "Epoch  16 Batch  235/269   train_loss = 4.133\n",
      "Epoch  16 Batch  236/269   train_loss = 3.776\n",
      "Epoch  16 Batch  237/269   train_loss = 3.513\n",
      "Epoch  16 Batch  238/269   train_loss = 3.723\n",
      "Epoch  16 Batch  239/269   train_loss = 4.278\n",
      "Epoch  16 Batch  240/269   train_loss = 3.823\n",
      "Epoch  16 Batch  241/269   train_loss = 4.099\n",
      "Epoch  16 Batch  242/269   train_loss = 3.711\n",
      "Epoch  16 Batch  243/269   train_loss = 3.989\n",
      "Epoch  16 Batch  244/269   train_loss = 3.764\n",
      "Epoch  16 Batch  245/269   train_loss = 3.873\n",
      "Epoch  16 Batch  246/269   train_loss = 3.593\n",
      "Epoch  16 Batch  247/269   train_loss = 4.011\n",
      "Epoch  16 Batch  248/269   train_loss = 3.976\n",
      "Epoch  16 Batch  249/269   train_loss = 3.892\n",
      "Epoch  16 Batch  250/269   train_loss = 3.642\n",
      "Epoch  16 Batch  251/269   train_loss = 4.035\n",
      "Epoch  16 Batch  252/269   train_loss = 4.037\n",
      "Epoch  16 Batch  253/269   train_loss = 3.917\n",
      "Epoch  16 Batch  254/269   train_loss = 3.795\n",
      "Epoch  16 Batch  255/269   train_loss = 3.932\n",
      "Epoch  16 Batch  256/269   train_loss = 3.877\n",
      "Epoch  16 Batch  257/269   train_loss = 3.717\n",
      "Epoch  16 Batch  258/269   train_loss = 3.621\n",
      "Epoch  16 Batch  259/269   train_loss = 3.738\n",
      "Epoch  16 Batch  260/269   train_loss = 4.000\n",
      "Epoch  16 Batch  261/269   train_loss = 3.992\n",
      "Epoch  16 Batch  262/269   train_loss = 3.762\n",
      "Epoch  16 Batch  263/269   train_loss = 3.674\n",
      "Epoch  16 Batch  264/269   train_loss = 4.240\n",
      "Epoch  16 Batch  265/269   train_loss = 3.968\n",
      "Epoch  16 Batch  266/269   train_loss = 3.907\n",
      "Epoch  16 Batch  267/269   train_loss = 3.955\n",
      "Epoch  16 Batch  268/269   train_loss = 4.032\n",
      "Epoch  17 Batch    0/269   train_loss = 3.792\n",
      "Epoch  17 Batch    1/269   train_loss = 3.658\n",
      "Epoch  17 Batch    2/269   train_loss = 3.819\n",
      "Epoch  17 Batch    3/269   train_loss = 3.847\n",
      "Epoch  17 Batch    4/269   train_loss = 4.284\n",
      "Epoch  17 Batch    5/269   train_loss = 3.981\n",
      "Epoch  17 Batch    6/269   train_loss = 3.764\n",
      "Epoch  17 Batch    7/269   train_loss = 3.725\n",
      "Epoch  17 Batch    8/269   train_loss = 4.018\n",
      "Epoch  17 Batch    9/269   train_loss = 3.728\n",
      "Epoch  17 Batch   10/269   train_loss = 3.707\n",
      "Epoch  17 Batch   11/269   train_loss = 3.882\n",
      "Epoch  17 Batch   12/269   train_loss = 3.706\n",
      "Epoch  17 Batch   13/269   train_loss = 3.942\n",
      "Epoch  17 Batch   14/269   train_loss = 3.724\n",
      "Epoch  17 Batch   15/269   train_loss = 4.116\n",
      "Epoch  17 Batch   16/269   train_loss = 3.866\n",
      "Epoch  17 Batch   17/269   train_loss = 3.952\n",
      "Epoch  17 Batch   18/269   train_loss = 3.864\n",
      "Epoch  17 Batch   19/269   train_loss = 3.852\n",
      "Epoch  17 Batch   20/269   train_loss = 4.164\n",
      "Epoch  17 Batch   21/269   train_loss = 4.067\n",
      "Epoch  17 Batch   22/269   train_loss = 3.752\n",
      "Epoch  17 Batch   23/269   train_loss = 3.954\n",
      "Epoch  17 Batch   24/269   train_loss = 3.803\n",
      "Epoch  17 Batch   25/269   train_loss = 4.089\n",
      "Epoch  17 Batch   26/269   train_loss = 3.961\n",
      "Epoch  17 Batch   27/269   train_loss = 3.766\n",
      "Epoch  17 Batch   28/269   train_loss = 3.925\n",
      "Epoch  17 Batch   29/269   train_loss = 3.973\n",
      "Epoch  17 Batch   30/269   train_loss = 4.172\n",
      "Epoch  17 Batch   31/269   train_loss = 3.882\n",
      "Epoch  17 Batch   32/269   train_loss = 3.785\n",
      "Epoch  17 Batch   33/269   train_loss = 3.641\n",
      "Epoch  17 Batch   34/269   train_loss = 3.901\n",
      "Epoch  17 Batch   35/269   train_loss = 3.736\n",
      "Epoch  17 Batch   36/269   train_loss = 3.766\n",
      "Epoch  17 Batch   37/269   train_loss = 3.868\n",
      "Epoch  17 Batch   38/269   train_loss = 3.749\n",
      "Epoch  17 Batch   39/269   train_loss = 4.008\n",
      "Epoch  17 Batch   40/269   train_loss = 3.773\n",
      "Epoch  17 Batch   41/269   train_loss = 3.753\n",
      "Epoch  17 Batch   42/269   train_loss = 4.279\n",
      "Epoch  17 Batch   43/269   train_loss = 4.067\n",
      "Epoch  17 Batch   44/269   train_loss = 3.889\n",
      "Epoch  17 Batch   45/269   train_loss = 3.643\n",
      "Epoch  17 Batch   46/269   train_loss = 3.991\n",
      "Epoch  17 Batch   47/269   train_loss = 3.619\n",
      "Epoch  17 Batch   48/269   train_loss = 4.164\n",
      "Epoch  17 Batch   49/269   train_loss = 4.096\n",
      "Epoch  17 Batch   50/269   train_loss = 4.062\n",
      "Epoch  17 Batch   51/269   train_loss = 3.936\n",
      "Epoch  17 Batch   52/269   train_loss = 3.930\n",
      "Epoch  17 Batch   53/269   train_loss = 4.137\n",
      "Epoch  17 Batch   54/269   train_loss = 3.915\n",
      "Epoch  17 Batch   55/269   train_loss = 4.056\n",
      "Epoch  17 Batch   56/269   train_loss = 3.881\n",
      "Epoch  17 Batch   57/269   train_loss = 3.976\n",
      "Epoch  17 Batch   58/269   train_loss = 3.850\n",
      "Epoch  17 Batch   59/269   train_loss = 3.815\n",
      "Epoch  17 Batch   60/269   train_loss = 3.980\n",
      "Epoch  17 Batch   61/269   train_loss = 4.220\n",
      "Epoch  17 Batch   62/269   train_loss = 3.865\n",
      "Epoch  17 Batch   63/269   train_loss = 4.157\n",
      "Epoch  17 Batch   64/269   train_loss = 3.870\n",
      "Epoch  17 Batch   65/269   train_loss = 3.958\n",
      "Epoch  17 Batch   66/269   train_loss = 3.897\n",
      "Epoch  17 Batch   67/269   train_loss = 3.934\n",
      "Epoch  17 Batch   68/269   train_loss = 3.979\n",
      "Epoch  17 Batch   69/269   train_loss = 3.985\n",
      "Epoch  17 Batch   70/269   train_loss = 3.641\n",
      "Epoch  17 Batch   71/269   train_loss = 4.025\n",
      "Epoch  17 Batch   72/269   train_loss = 3.672\n",
      "Epoch  17 Batch   73/269   train_loss = 3.918\n",
      "Epoch  17 Batch   74/269   train_loss = 3.924\n",
      "Epoch  17 Batch   75/269   train_loss = 4.044\n",
      "Epoch  17 Batch   76/269   train_loss = 3.966\n",
      "Epoch  17 Batch   77/269   train_loss = 3.586\n",
      "Epoch  17 Batch   78/269   train_loss = 3.996\n",
      "Epoch  17 Batch   79/269   train_loss = 4.084\n",
      "Epoch  17 Batch   80/269   train_loss = 3.910\n",
      "Epoch  17 Batch   81/269   train_loss = 3.750\n",
      "Epoch  17 Batch   82/269   train_loss = 4.091\n",
      "Epoch  17 Batch   83/269   train_loss = 3.912\n",
      "Epoch  17 Batch   84/269   train_loss = 4.126\n",
      "Epoch  17 Batch   85/269   train_loss = 4.268\n",
      "Epoch  17 Batch   86/269   train_loss = 4.150\n",
      "Epoch  17 Batch   87/269   train_loss = 3.968\n",
      "Epoch  17 Batch   88/269   train_loss = 3.927\n",
      "Epoch  17 Batch   89/269   train_loss = 3.926\n",
      "Epoch  17 Batch   90/269   train_loss = 3.837\n",
      "Epoch  17 Batch   91/269   train_loss = 4.125\n",
      "Epoch  17 Batch   92/269   train_loss = 4.046\n",
      "Epoch  17 Batch   93/269   train_loss = 3.722\n",
      "Epoch  17 Batch   94/269   train_loss = 4.189\n",
      "Epoch  17 Batch   95/269   train_loss = 3.745\n",
      "Epoch  17 Batch   96/269   train_loss = 3.795\n",
      "Epoch  17 Batch   97/269   train_loss = 3.935\n",
      "Epoch  17 Batch   98/269   train_loss = 3.991\n",
      "Epoch  17 Batch   99/269   train_loss = 3.989\n",
      "Epoch  17 Batch  100/269   train_loss = 3.849\n",
      "Epoch  17 Batch  101/269   train_loss = 4.007\n",
      "Epoch  17 Batch  102/269   train_loss = 3.788\n",
      "Epoch  17 Batch  103/269   train_loss = 3.935\n",
      "Epoch  17 Batch  104/269   train_loss = 3.741\n",
      "Epoch  17 Batch  105/269   train_loss = 4.046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17 Batch  106/269   train_loss = 4.072\n",
      "Epoch  17 Batch  107/269   train_loss = 4.143\n",
      "Epoch  17 Batch  108/269   train_loss = 3.964\n",
      "Epoch  17 Batch  109/269   train_loss = 3.988\n",
      "Epoch  17 Batch  110/269   train_loss = 3.959\n",
      "Epoch  17 Batch  111/269   train_loss = 4.076\n",
      "Epoch  17 Batch  112/269   train_loss = 4.035\n",
      "Epoch  17 Batch  113/269   train_loss = 3.899\n",
      "Epoch  17 Batch  114/269   train_loss = 3.838\n",
      "Epoch  17 Batch  115/269   train_loss = 3.765\n",
      "Epoch  17 Batch  116/269   train_loss = 3.773\n",
      "Epoch  17 Batch  117/269   train_loss = 3.727\n",
      "Epoch  17 Batch  118/269   train_loss = 4.052\n",
      "Epoch  17 Batch  119/269   train_loss = 3.713\n",
      "Epoch  17 Batch  120/269   train_loss = 3.994\n",
      "Epoch  17 Batch  121/269   train_loss = 3.832\n",
      "Epoch  17 Batch  122/269   train_loss = 3.917\n",
      "Epoch  17 Batch  123/269   train_loss = 3.741\n",
      "Epoch  17 Batch  124/269   train_loss = 4.079\n",
      "Epoch  17 Batch  125/269   train_loss = 3.669\n",
      "Epoch  17 Batch  126/269   train_loss = 3.592\n",
      "Epoch  17 Batch  127/269   train_loss = 3.880\n",
      "Epoch  17 Batch  128/269   train_loss = 3.923\n",
      "Epoch  17 Batch  129/269   train_loss = 3.906\n",
      "Epoch  17 Batch  130/269   train_loss = 4.050\n",
      "Epoch  17 Batch  131/269   train_loss = 3.720\n",
      "Epoch  17 Batch  132/269   train_loss = 3.906\n",
      "Epoch  17 Batch  133/269   train_loss = 3.718\n",
      "Epoch  17 Batch  134/269   train_loss = 4.084\n",
      "Epoch  17 Batch  135/269   train_loss = 3.661\n",
      "Epoch  17 Batch  136/269   train_loss = 3.635\n",
      "Epoch  17 Batch  137/269   train_loss = 4.007\n",
      "Epoch  17 Batch  138/269   train_loss = 3.757\n",
      "Epoch  17 Batch  139/269   train_loss = 3.949\n",
      "Epoch  17 Batch  140/269   train_loss = 4.017\n",
      "Epoch  17 Batch  141/269   train_loss = 4.152\n",
      "Epoch  17 Batch  142/269   train_loss = 3.751\n",
      "Epoch  17 Batch  143/269   train_loss = 3.947\n",
      "Epoch  17 Batch  144/269   train_loss = 4.040\n",
      "Epoch  17 Batch  145/269   train_loss = 3.825\n",
      "Epoch  17 Batch  146/269   train_loss = 4.099\n",
      "Epoch  17 Batch  147/269   train_loss = 3.696\n",
      "Epoch  17 Batch  148/269   train_loss = 3.992\n",
      "Epoch  17 Batch  149/269   train_loss = 3.842\n",
      "Epoch  17 Batch  150/269   train_loss = 3.912\n",
      "Epoch  17 Batch  151/269   train_loss = 3.936\n",
      "Epoch  17 Batch  152/269   train_loss = 3.903\n",
      "Epoch  17 Batch  153/269   train_loss = 3.971\n",
      "Epoch  17 Batch  154/269   train_loss = 3.827\n",
      "Epoch  17 Batch  155/269   train_loss = 3.849\n",
      "Epoch  17 Batch  156/269   train_loss = 4.281\n",
      "Epoch  17 Batch  157/269   train_loss = 4.105\n",
      "Epoch  17 Batch  158/269   train_loss = 4.000\n",
      "Epoch  17 Batch  159/269   train_loss = 4.010\n",
      "Epoch  17 Batch  160/269   train_loss = 3.863\n",
      "Epoch  17 Batch  161/269   train_loss = 4.019\n",
      "Epoch  17 Batch  162/269   train_loss = 3.956\n",
      "Epoch  17 Batch  163/269   train_loss = 4.029\n",
      "Epoch  17 Batch  164/269   train_loss = 4.140\n",
      "Epoch  17 Batch  165/269   train_loss = 4.067\n",
      "Epoch  17 Batch  166/269   train_loss = 3.953\n",
      "Epoch  17 Batch  167/269   train_loss = 3.839\n",
      "Epoch  17 Batch  168/269   train_loss = 3.861\n",
      "Epoch  17 Batch  169/269   train_loss = 4.201\n",
      "Epoch  17 Batch  170/269   train_loss = 3.743\n",
      "Epoch  17 Batch  171/269   train_loss = 4.071\n",
      "Epoch  17 Batch  172/269   train_loss = 3.761\n",
      "Epoch  17 Batch  173/269   train_loss = 4.002\n",
      "Epoch  17 Batch  174/269   train_loss = 4.094\n",
      "Epoch  17 Batch  175/269   train_loss = 3.965\n",
      "Epoch  17 Batch  176/269   train_loss = 3.812\n",
      "Epoch  17 Batch  177/269   train_loss = 3.895\n",
      "Epoch  17 Batch  178/269   train_loss = 4.191\n",
      "Epoch  17 Batch  179/269   train_loss = 3.935\n",
      "Epoch  17 Batch  180/269   train_loss = 3.793\n",
      "Epoch  17 Batch  181/269   train_loss = 4.028\n",
      "Epoch  17 Batch  182/269   train_loss = 3.795\n",
      "Epoch  17 Batch  183/269   train_loss = 3.952\n",
      "Epoch  17 Batch  184/269   train_loss = 3.918\n",
      "Epoch  17 Batch  185/269   train_loss = 3.969\n",
      "Epoch  17 Batch  186/269   train_loss = 3.809\n",
      "Epoch  17 Batch  187/269   train_loss = 4.129\n",
      "Epoch  17 Batch  188/269   train_loss = 3.786\n",
      "Epoch  17 Batch  189/269   train_loss = 3.878\n",
      "Epoch  17 Batch  190/269   train_loss = 4.262\n",
      "Epoch  17 Batch  191/269   train_loss = 3.984\n",
      "Epoch  17 Batch  192/269   train_loss = 3.876\n",
      "Epoch  17 Batch  193/269   train_loss = 3.795\n",
      "Epoch  17 Batch  194/269   train_loss = 4.013\n",
      "Epoch  17 Batch  195/269   train_loss = 3.798\n",
      "Epoch  17 Batch  196/269   train_loss = 4.001\n",
      "Epoch  17 Batch  197/269   train_loss = 4.093\n",
      "Epoch  17 Batch  198/269   train_loss = 4.037\n",
      "Epoch  17 Batch  199/269   train_loss = 4.072\n",
      "Epoch  17 Batch  200/269   train_loss = 3.960\n",
      "Epoch  17 Batch  201/269   train_loss = 3.820\n",
      "Epoch  17 Batch  202/269   train_loss = 3.740\n",
      "Epoch  17 Batch  203/269   train_loss = 3.785\n",
      "Epoch  17 Batch  204/269   train_loss = 3.948\n",
      "Epoch  17 Batch  205/269   train_loss = 3.949\n",
      "Epoch  17 Batch  206/269   train_loss = 3.885\n",
      "Epoch  17 Batch  207/269   train_loss = 3.929\n",
      "Epoch  17 Batch  208/269   train_loss = 3.850\n",
      "Epoch  17 Batch  209/269   train_loss = 4.031\n",
      "Epoch  17 Batch  210/269   train_loss = 3.839\n",
      "Epoch  17 Batch  211/269   train_loss = 3.855\n",
      "Epoch  17 Batch  212/269   train_loss = 4.194\n",
      "Epoch  17 Batch  213/269   train_loss = 3.845\n",
      "Epoch  17 Batch  214/269   train_loss = 3.959\n",
      "Epoch  17 Batch  215/269   train_loss = 4.123\n",
      "Epoch  17 Batch  216/269   train_loss = 4.102\n",
      "Epoch  17 Batch  217/269   train_loss = 3.803\n",
      "Epoch  17 Batch  218/269   train_loss = 3.909\n",
      "Epoch  17 Batch  219/269   train_loss = 3.691\n",
      "Epoch  17 Batch  220/269   train_loss = 3.976\n",
      "Epoch  17 Batch  221/269   train_loss = 3.803\n",
      "Epoch  17 Batch  222/269   train_loss = 3.976\n",
      "Epoch  17 Batch  223/269   train_loss = 3.755\n",
      "Epoch  17 Batch  224/269   train_loss = 4.031\n",
      "Epoch  17 Batch  225/269   train_loss = 4.181\n",
      "Epoch  17 Batch  226/269   train_loss = 3.946\n",
      "Epoch  17 Batch  227/269   train_loss = 3.647\n",
      "Epoch  17 Batch  228/269   train_loss = 3.869\n",
      "Epoch  17 Batch  229/269   train_loss = 4.044\n",
      "Epoch  17 Batch  230/269   train_loss = 3.997\n",
      "Epoch  17 Batch  231/269   train_loss = 3.871\n",
      "Epoch  17 Batch  232/269   train_loss = 3.866\n",
      "Epoch  17 Batch  233/269   train_loss = 3.933\n",
      "Epoch  17 Batch  234/269   train_loss = 3.883\n",
      "Epoch  17 Batch  235/269   train_loss = 4.138\n",
      "Epoch  17 Batch  236/269   train_loss = 3.790\n",
      "Epoch  17 Batch  237/269   train_loss = 3.478\n",
      "Epoch  17 Batch  238/269   train_loss = 3.719\n",
      "Epoch  17 Batch  239/269   train_loss = 4.299\n",
      "Epoch  17 Batch  240/269   train_loss = 3.863\n",
      "Epoch  17 Batch  241/269   train_loss = 4.085\n",
      "Epoch  17 Batch  242/269   train_loss = 3.700\n",
      "Epoch  17 Batch  243/269   train_loss = 4.019\n",
      "Epoch  17 Batch  244/269   train_loss = 3.775\n",
      "Epoch  17 Batch  245/269   train_loss = 3.869\n",
      "Epoch  17 Batch  246/269   train_loss = 3.609\n",
      "Epoch  17 Batch  247/269   train_loss = 3.995\n",
      "Epoch  17 Batch  248/269   train_loss = 3.962\n",
      "Epoch  17 Batch  249/269   train_loss = 3.848\n",
      "Epoch  17 Batch  250/269   train_loss = 3.666\n",
      "Epoch  17 Batch  251/269   train_loss = 4.064\n",
      "Epoch  17 Batch  252/269   train_loss = 4.036\n",
      "Epoch  17 Batch  253/269   train_loss = 3.881\n",
      "Epoch  17 Batch  254/269   train_loss = 3.824\n",
      "Epoch  17 Batch  255/269   train_loss = 3.942\n",
      "Epoch  17 Batch  256/269   train_loss = 3.896\n",
      "Epoch  17 Batch  257/269   train_loss = 3.702\n",
      "Epoch  17 Batch  258/269   train_loss = 3.601\n",
      "Epoch  17 Batch  259/269   train_loss = 3.692\n",
      "Epoch  17 Batch  260/269   train_loss = 3.966\n",
      "Epoch  17 Batch  261/269   train_loss = 3.971\n",
      "Epoch  17 Batch  262/269   train_loss = 3.763\n",
      "Epoch  17 Batch  263/269   train_loss = 3.647\n",
      "Epoch  17 Batch  264/269   train_loss = 4.232\n",
      "Epoch  17 Batch  265/269   train_loss = 3.987\n",
      "Epoch  17 Batch  266/269   train_loss = 3.881\n",
      "Epoch  17 Batch  267/269   train_loss = 3.934\n",
      "Epoch  17 Batch  268/269   train_loss = 4.035\n",
      "Epoch  18 Batch    0/269   train_loss = 3.745\n",
      "Epoch  18 Batch    1/269   train_loss = 3.643\n",
      "Epoch  18 Batch    2/269   train_loss = 3.786\n",
      "Epoch  18 Batch    3/269   train_loss = 3.867\n",
      "Epoch  18 Batch    4/269   train_loss = 4.273\n",
      "Epoch  18 Batch    5/269   train_loss = 3.981\n",
      "Epoch  18 Batch    6/269   train_loss = 3.795\n",
      "Epoch  18 Batch    7/269   train_loss = 3.701\n",
      "Epoch  18 Batch    8/269   train_loss = 3.994\n",
      "Epoch  18 Batch    9/269   train_loss = 3.704\n",
      "Epoch  18 Batch   10/269   train_loss = 3.685\n",
      "Epoch  18 Batch   11/269   train_loss = 3.868\n",
      "Epoch  18 Batch   12/269   train_loss = 3.637\n",
      "Epoch  18 Batch   13/269   train_loss = 3.930\n",
      "Epoch  18 Batch   14/269   train_loss = 3.739\n",
      "Epoch  18 Batch   15/269   train_loss = 4.113\n",
      "Epoch  18 Batch   16/269   train_loss = 3.846\n",
      "Epoch  18 Batch   17/269   train_loss = 3.968\n",
      "Epoch  18 Batch   18/269   train_loss = 3.843\n",
      "Epoch  18 Batch   19/269   train_loss = 3.840\n",
      "Epoch  18 Batch   20/269   train_loss = 4.141\n",
      "Epoch  18 Batch   21/269   train_loss = 4.014\n",
      "Epoch  18 Batch   22/269   train_loss = 3.742\n",
      "Epoch  18 Batch   23/269   train_loss = 3.946\n",
      "Epoch  18 Batch   24/269   train_loss = 3.792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18 Batch   25/269   train_loss = 4.058\n",
      "Epoch  18 Batch   26/269   train_loss = 3.944\n",
      "Epoch  18 Batch   27/269   train_loss = 3.767\n",
      "Epoch  18 Batch   28/269   train_loss = 3.948\n",
      "Epoch  18 Batch   29/269   train_loss = 3.958\n",
      "Epoch  18 Batch   30/269   train_loss = 4.150\n",
      "Epoch  18 Batch   31/269   train_loss = 3.879\n",
      "Epoch  18 Batch   32/269   train_loss = 3.764\n",
      "Epoch  18 Batch   33/269   train_loss = 3.662\n",
      "Epoch  18 Batch   34/269   train_loss = 3.891\n",
      "Epoch  18 Batch   35/269   train_loss = 3.704\n",
      "Epoch  18 Batch   36/269   train_loss = 3.715\n",
      "Epoch  18 Batch   37/269   train_loss = 3.869\n",
      "Epoch  18 Batch   38/269   train_loss = 3.752\n",
      "Epoch  18 Batch   39/269   train_loss = 4.005\n",
      "Epoch  18 Batch   40/269   train_loss = 3.749\n",
      "Epoch  18 Batch   41/269   train_loss = 3.763\n",
      "Epoch  18 Batch   42/269   train_loss = 4.246\n",
      "Epoch  18 Batch   43/269   train_loss = 4.030\n",
      "Epoch  18 Batch   44/269   train_loss = 3.895\n",
      "Epoch  18 Batch   45/269   train_loss = 3.624\n",
      "Epoch  18 Batch   46/269   train_loss = 3.990\n",
      "Epoch  18 Batch   47/269   train_loss = 3.613\n",
      "Epoch  18 Batch   48/269   train_loss = 4.124\n",
      "Epoch  18 Batch   49/269   train_loss = 4.083\n",
      "Epoch  18 Batch   50/269   train_loss = 4.039\n",
      "Epoch  18 Batch   51/269   train_loss = 3.964\n",
      "Epoch  18 Batch   52/269   train_loss = 3.932\n",
      "Epoch  18 Batch   53/269   train_loss = 4.111\n",
      "Epoch  18 Batch   54/269   train_loss = 3.934\n",
      "Epoch  18 Batch   55/269   train_loss = 4.031\n",
      "Epoch  18 Batch   56/269   train_loss = 3.886\n",
      "Epoch  18 Batch   57/269   train_loss = 3.960\n",
      "Epoch  18 Batch   58/269   train_loss = 3.847\n",
      "Epoch  18 Batch   59/269   train_loss = 3.783\n",
      "Epoch  18 Batch   60/269   train_loss = 3.974\n",
      "Epoch  18 Batch   61/269   train_loss = 4.190\n",
      "Epoch  18 Batch   62/269   train_loss = 3.845\n",
      "Epoch  18 Batch   63/269   train_loss = 4.091\n",
      "Epoch  18 Batch   64/269   train_loss = 3.868\n",
      "Epoch  18 Batch   65/269   train_loss = 3.926\n",
      "Epoch  18 Batch   66/269   train_loss = 3.852\n",
      "Epoch  18 Batch   67/269   train_loss = 3.881\n",
      "Epoch  18 Batch   68/269   train_loss = 3.945\n",
      "Epoch  18 Batch   69/269   train_loss = 3.970\n",
      "Epoch  18 Batch   70/269   train_loss = 3.603\n",
      "Epoch  18 Batch   71/269   train_loss = 4.016\n",
      "Epoch  18 Batch   72/269   train_loss = 3.629\n",
      "Epoch  18 Batch   73/269   train_loss = 3.890\n",
      "Epoch  18 Batch   74/269   train_loss = 3.918\n",
      "Epoch  18 Batch   75/269   train_loss = 4.005\n",
      "Epoch  18 Batch   76/269   train_loss = 3.958\n",
      "Epoch  18 Batch   77/269   train_loss = 3.544\n",
      "Epoch  18 Batch   78/269   train_loss = 3.979\n",
      "Epoch  18 Batch   79/269   train_loss = 4.060\n",
      "Epoch  18 Batch   80/269   train_loss = 3.895\n",
      "Epoch  18 Batch   81/269   train_loss = 3.715\n",
      "Epoch  18 Batch   82/269   train_loss = 4.068\n",
      "Epoch  18 Batch   83/269   train_loss = 3.922\n",
      "Epoch  18 Batch   84/269   train_loss = 4.111\n",
      "Epoch  18 Batch   85/269   train_loss = 4.249\n",
      "Epoch  18 Batch   86/269   train_loss = 4.146\n",
      "Epoch  18 Batch   87/269   train_loss = 3.907\n",
      "Epoch  18 Batch   88/269   train_loss = 3.899\n",
      "Epoch  18 Batch   89/269   train_loss = 3.933\n",
      "Epoch  18 Batch   90/269   train_loss = 3.798\n",
      "Epoch  18 Batch   91/269   train_loss = 4.110\n",
      "Epoch  18 Batch   92/269   train_loss = 4.042\n",
      "Epoch  18 Batch   93/269   train_loss = 3.729\n",
      "Epoch  18 Batch   94/269   train_loss = 4.156\n",
      "Epoch  18 Batch   95/269   train_loss = 3.707\n",
      "Epoch  18 Batch   96/269   train_loss = 3.765\n",
      "Epoch  18 Batch   97/269   train_loss = 3.909\n",
      "Epoch  18 Batch   98/269   train_loss = 3.969\n",
      "Epoch  18 Batch   99/269   train_loss = 3.966\n",
      "Epoch  18 Batch  100/269   train_loss = 3.845\n",
      "Epoch  18 Batch  101/269   train_loss = 4.001\n",
      "Epoch  18 Batch  102/269   train_loss = 3.745\n",
      "Epoch  18 Batch  103/269   train_loss = 3.882\n",
      "Epoch  18 Batch  104/269   train_loss = 3.745\n",
      "Epoch  18 Batch  105/269   train_loss = 4.006\n",
      "Epoch  18 Batch  106/269   train_loss = 4.075\n",
      "Epoch  18 Batch  107/269   train_loss = 4.112\n",
      "Epoch  18 Batch  108/269   train_loss = 3.942\n",
      "Epoch  18 Batch  109/269   train_loss = 3.954\n",
      "Epoch  18 Batch  110/269   train_loss = 3.920\n",
      "Epoch  18 Batch  111/269   train_loss = 4.076\n",
      "Epoch  18 Batch  112/269   train_loss = 4.034\n",
      "Epoch  18 Batch  113/269   train_loss = 3.907\n",
      "Epoch  18 Batch  114/269   train_loss = 3.797\n",
      "Epoch  18 Batch  115/269   train_loss = 3.736\n",
      "Epoch  18 Batch  116/269   train_loss = 3.752\n",
      "Epoch  18 Batch  117/269   train_loss = 3.727\n",
      "Epoch  18 Batch  118/269   train_loss = 4.023\n",
      "Epoch  18 Batch  119/269   train_loss = 3.683\n",
      "Epoch  18 Batch  120/269   train_loss = 3.976\n",
      "Epoch  18 Batch  121/269   train_loss = 3.818\n",
      "Epoch  18 Batch  122/269   train_loss = 3.931\n",
      "Epoch  18 Batch  123/269   train_loss = 3.725\n",
      "Epoch  18 Batch  124/269   train_loss = 4.062\n",
      "Epoch  18 Batch  125/269   train_loss = 3.645\n",
      "Epoch  18 Batch  126/269   train_loss = 3.558\n",
      "Epoch  18 Batch  127/269   train_loss = 3.865\n",
      "Epoch  18 Batch  128/269   train_loss = 3.920\n",
      "Epoch  18 Batch  129/269   train_loss = 3.865\n",
      "Epoch  18 Batch  130/269   train_loss = 4.035\n",
      "Epoch  18 Batch  131/269   train_loss = 3.683\n",
      "Epoch  18 Batch  132/269   train_loss = 3.863\n",
      "Epoch  18 Batch  133/269   train_loss = 3.705\n",
      "Epoch  18 Batch  134/269   train_loss = 4.059\n",
      "Epoch  18 Batch  135/269   train_loss = 3.684\n",
      "Epoch  18 Batch  136/269   train_loss = 3.625\n",
      "Epoch  18 Batch  137/269   train_loss = 3.985\n",
      "Epoch  18 Batch  138/269   train_loss = 3.734\n",
      "Epoch  18 Batch  139/269   train_loss = 3.903\n",
      "Epoch  18 Batch  140/269   train_loss = 3.956\n",
      "Epoch  18 Batch  141/269   train_loss = 4.160\n",
      "Epoch  18 Batch  142/269   train_loss = 3.746\n",
      "Epoch  18 Batch  143/269   train_loss = 3.921\n",
      "Epoch  18 Batch  144/269   train_loss = 4.060\n",
      "Epoch  18 Batch  145/269   train_loss = 3.844\n",
      "Epoch  18 Batch  146/269   train_loss = 4.084\n",
      "Epoch  18 Batch  147/269   train_loss = 3.687\n",
      "Epoch  18 Batch  148/269   train_loss = 3.967\n",
      "Epoch  18 Batch  149/269   train_loss = 3.832\n",
      "Epoch  18 Batch  150/269   train_loss = 3.912\n",
      "Epoch  18 Batch  151/269   train_loss = 3.887\n",
      "Epoch  18 Batch  152/269   train_loss = 3.874\n",
      "Epoch  18 Batch  153/269   train_loss = 3.965\n",
      "Epoch  18 Batch  154/269   train_loss = 3.824\n",
      "Epoch  18 Batch  155/269   train_loss = 3.774\n",
      "Epoch  18 Batch  156/269   train_loss = 4.273\n",
      "Epoch  18 Batch  157/269   train_loss = 4.098\n",
      "Epoch  18 Batch  158/269   train_loss = 3.968\n",
      "Epoch  18 Batch  159/269   train_loss = 3.995\n",
      "Epoch  18 Batch  160/269   train_loss = 3.839\n",
      "Epoch  18 Batch  161/269   train_loss = 4.021\n",
      "Epoch  18 Batch  162/269   train_loss = 3.954\n",
      "Epoch  18 Batch  163/269   train_loss = 4.003\n",
      "Epoch  18 Batch  164/269   train_loss = 4.139\n",
      "Epoch  18 Batch  165/269   train_loss = 4.055\n",
      "Epoch  18 Batch  166/269   train_loss = 3.942\n",
      "Epoch  18 Batch  167/269   train_loss = 3.807\n",
      "Epoch  18 Batch  168/269   train_loss = 3.834\n",
      "Epoch  18 Batch  169/269   train_loss = 4.184\n",
      "Epoch  18 Batch  170/269   train_loss = 3.702\n",
      "Epoch  18 Batch  171/269   train_loss = 4.031\n",
      "Epoch  18 Batch  172/269   train_loss = 3.724\n",
      "Epoch  18 Batch  173/269   train_loss = 3.973\n",
      "Epoch  18 Batch  174/269   train_loss = 4.059\n",
      "Epoch  18 Batch  175/269   train_loss = 3.948\n",
      "Epoch  18 Batch  176/269   train_loss = 3.802\n",
      "Epoch  18 Batch  177/269   train_loss = 3.907\n",
      "Epoch  18 Batch  178/269   train_loss = 4.157\n",
      "Epoch  18 Batch  179/269   train_loss = 3.931\n",
      "Epoch  18 Batch  180/269   train_loss = 3.766\n",
      "Epoch  18 Batch  181/269   train_loss = 4.004\n",
      "Epoch  18 Batch  182/269   train_loss = 3.804\n",
      "Epoch  18 Batch  183/269   train_loss = 3.913\n",
      "Epoch  18 Batch  184/269   train_loss = 3.935\n",
      "Epoch  18 Batch  185/269   train_loss = 3.942\n",
      "Epoch  18 Batch  186/269   train_loss = 3.792\n",
      "Epoch  18 Batch  187/269   train_loss = 4.101\n",
      "Epoch  18 Batch  188/269   train_loss = 3.760\n",
      "Epoch  18 Batch  189/269   train_loss = 3.855\n",
      "Epoch  18 Batch  190/269   train_loss = 4.259\n",
      "Epoch  18 Batch  191/269   train_loss = 3.936\n",
      "Epoch  18 Batch  192/269   train_loss = 3.887\n",
      "Epoch  18 Batch  193/269   train_loss = 3.804\n",
      "Epoch  18 Batch  194/269   train_loss = 3.976\n",
      "Epoch  18 Batch  195/269   train_loss = 3.825\n",
      "Epoch  18 Batch  196/269   train_loss = 4.001\n",
      "Epoch  18 Batch  197/269   train_loss = 4.087\n",
      "Epoch  18 Batch  198/269   train_loss = 4.034\n",
      "Epoch  18 Batch  199/269   train_loss = 4.045\n",
      "Epoch  18 Batch  200/269   train_loss = 3.968\n",
      "Epoch  18 Batch  201/269   train_loss = 3.829\n",
      "Epoch  18 Batch  202/269   train_loss = 3.718\n",
      "Epoch  18 Batch  203/269   train_loss = 3.794\n",
      "Epoch  18 Batch  204/269   train_loss = 3.936\n",
      "Epoch  18 Batch  205/269   train_loss = 3.958\n",
      "Epoch  18 Batch  206/269   train_loss = 3.874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18 Batch  207/269   train_loss = 3.886\n",
      "Epoch  18 Batch  208/269   train_loss = 3.856\n",
      "Epoch  18 Batch  209/269   train_loss = 4.014\n",
      "Epoch  18 Batch  210/269   train_loss = 3.801\n",
      "Epoch  18 Batch  211/269   train_loss = 3.862\n",
      "Epoch  18 Batch  212/269   train_loss = 4.205\n",
      "Epoch  18 Batch  213/269   train_loss = 3.820\n",
      "Epoch  18 Batch  214/269   train_loss = 3.886\n",
      "Epoch  18 Batch  215/269   train_loss = 4.087\n",
      "Epoch  18 Batch  216/269   train_loss = 4.067\n",
      "Epoch  18 Batch  217/269   train_loss = 3.809\n",
      "Epoch  18 Batch  218/269   train_loss = 3.915\n",
      "Epoch  18 Batch  219/269   train_loss = 3.616\n",
      "Epoch  18 Batch  220/269   train_loss = 3.990\n",
      "Epoch  18 Batch  221/269   train_loss = 3.755\n",
      "Epoch  18 Batch  222/269   train_loss = 3.951\n",
      "Epoch  18 Batch  223/269   train_loss = 3.769\n",
      "Epoch  18 Batch  224/269   train_loss = 4.002\n",
      "Epoch  18 Batch  225/269   train_loss = 4.097\n",
      "Epoch  18 Batch  226/269   train_loss = 3.902\n",
      "Epoch  18 Batch  227/269   train_loss = 3.609\n",
      "Epoch  18 Batch  228/269   train_loss = 3.903\n",
      "Epoch  18 Batch  229/269   train_loss = 3.977\n",
      "Epoch  18 Batch  230/269   train_loss = 4.021\n",
      "Epoch  18 Batch  231/269   train_loss = 3.881\n",
      "Epoch  18 Batch  232/269   train_loss = 3.894\n",
      "Epoch  18 Batch  233/269   train_loss = 3.948\n",
      "Epoch  18 Batch  234/269   train_loss = 3.880\n",
      "Epoch  18 Batch  235/269   train_loss = 4.129\n",
      "Epoch  18 Batch  236/269   train_loss = 3.740\n",
      "Epoch  18 Batch  237/269   train_loss = 3.526\n",
      "Epoch  18 Batch  238/269   train_loss = 3.656\n",
      "Epoch  18 Batch  239/269   train_loss = 4.258\n",
      "Epoch  18 Batch  240/269   train_loss = 3.818\n",
      "Epoch  18 Batch  241/269   train_loss = 4.074\n",
      "Epoch  18 Batch  242/269   train_loss = 3.681\n",
      "Epoch  18 Batch  243/269   train_loss = 3.993\n",
      "Epoch  18 Batch  244/269   train_loss = 3.745\n",
      "Epoch  18 Batch  245/269   train_loss = 3.858\n",
      "Epoch  18 Batch  246/269   train_loss = 3.572\n",
      "Epoch  18 Batch  247/269   train_loss = 3.987\n",
      "Epoch  18 Batch  248/269   train_loss = 3.898\n",
      "Epoch  18 Batch  249/269   train_loss = 3.806\n",
      "Epoch  18 Batch  250/269   train_loss = 3.650\n",
      "Epoch  18 Batch  251/269   train_loss = 4.026\n",
      "Epoch  18 Batch  252/269   train_loss = 4.026\n",
      "Epoch  18 Batch  253/269   train_loss = 3.867\n",
      "Epoch  18 Batch  254/269   train_loss = 3.783\n",
      "Epoch  18 Batch  255/269   train_loss = 3.945\n",
      "Epoch  18 Batch  256/269   train_loss = 3.851\n",
      "Epoch  18 Batch  257/269   train_loss = 3.674\n",
      "Epoch  18 Batch  258/269   train_loss = 3.559\n",
      "Epoch  18 Batch  259/269   train_loss = 3.674\n",
      "Epoch  18 Batch  260/269   train_loss = 3.931\n",
      "Epoch  18 Batch  261/269   train_loss = 3.954\n",
      "Epoch  18 Batch  262/269   train_loss = 3.723\n",
      "Epoch  18 Batch  263/269   train_loss = 3.642\n",
      "Epoch  18 Batch  264/269   train_loss = 4.193\n",
      "Epoch  18 Batch  265/269   train_loss = 3.928\n",
      "Epoch  18 Batch  266/269   train_loss = 3.851\n",
      "Epoch  18 Batch  267/269   train_loss = 3.915\n",
      "Epoch  18 Batch  268/269   train_loss = 3.993\n",
      "Epoch  19 Batch    0/269   train_loss = 3.763\n",
      "Epoch  19 Batch    1/269   train_loss = 3.623\n",
      "Epoch  19 Batch    2/269   train_loss = 3.774\n",
      "Epoch  19 Batch    3/269   train_loss = 3.802\n",
      "Epoch  19 Batch    4/269   train_loss = 4.227\n",
      "Epoch  19 Batch    5/269   train_loss = 3.925\n",
      "Epoch  19 Batch    6/269   train_loss = 3.778\n",
      "Epoch  19 Batch    7/269   train_loss = 3.674\n",
      "Epoch  19 Batch    8/269   train_loss = 3.966\n",
      "Epoch  19 Batch    9/269   train_loss = 3.670\n",
      "Epoch  19 Batch   10/269   train_loss = 3.673\n",
      "Epoch  19 Batch   11/269   train_loss = 3.825\n",
      "Epoch  19 Batch   12/269   train_loss = 3.637\n",
      "Epoch  19 Batch   13/269   train_loss = 3.896\n",
      "Epoch  19 Batch   14/269   train_loss = 3.715\n",
      "Epoch  19 Batch   15/269   train_loss = 4.116\n",
      "Epoch  19 Batch   16/269   train_loss = 3.814\n",
      "Epoch  19 Batch   17/269   train_loss = 3.947\n",
      "Epoch  19 Batch   18/269   train_loss = 3.849\n",
      "Epoch  19 Batch   19/269   train_loss = 3.834\n",
      "Epoch  19 Batch   20/269   train_loss = 4.142\n",
      "Epoch  19 Batch   21/269   train_loss = 3.989\n",
      "Epoch  19 Batch   22/269   train_loss = 3.695\n",
      "Epoch  19 Batch   23/269   train_loss = 3.913\n",
      "Epoch  19 Batch   24/269   train_loss = 3.780\n",
      "Epoch  19 Batch   25/269   train_loss = 4.050\n",
      "Epoch  19 Batch   26/269   train_loss = 3.945\n",
      "Epoch  19 Batch   27/269   train_loss = 3.713\n",
      "Epoch  19 Batch   28/269   train_loss = 3.917\n",
      "Epoch  19 Batch   29/269   train_loss = 3.924\n",
      "Epoch  19 Batch   30/269   train_loss = 4.152\n",
      "Epoch  19 Batch   31/269   train_loss = 3.840\n",
      "Epoch  19 Batch   32/269   train_loss = 3.718\n",
      "Epoch  19 Batch   33/269   train_loss = 3.630\n",
      "Epoch  19 Batch   34/269   train_loss = 3.846\n",
      "Epoch  19 Batch   35/269   train_loss = 3.680\n",
      "Epoch  19 Batch   36/269   train_loss = 3.692\n",
      "Epoch  19 Batch   37/269   train_loss = 3.855\n",
      "Epoch  19 Batch   38/269   train_loss = 3.744\n",
      "Epoch  19 Batch   39/269   train_loss = 3.960\n",
      "Epoch  19 Batch   40/269   train_loss = 3.714\n",
      "Epoch  19 Batch   41/269   train_loss = 3.726\n",
      "Epoch  19 Batch   42/269   train_loss = 4.263\n",
      "Epoch  19 Batch   43/269   train_loss = 4.037\n",
      "Epoch  19 Batch   44/269   train_loss = 3.880\n",
      "Epoch  19 Batch   45/269   train_loss = 3.601\n",
      "Epoch  19 Batch   46/269   train_loss = 3.954\n",
      "Epoch  19 Batch   47/269   train_loss = 3.605\n",
      "Epoch  19 Batch   48/269   train_loss = 4.097\n",
      "Epoch  19 Batch   49/269   train_loss = 4.094\n",
      "Epoch  19 Batch   50/269   train_loss = 4.026\n",
      "Epoch  19 Batch   51/269   train_loss = 3.943\n",
      "Epoch  19 Batch   52/269   train_loss = 3.905\n",
      "Epoch  19 Batch   53/269   train_loss = 4.084\n",
      "Epoch  19 Batch   54/269   train_loss = 3.911\n",
      "Epoch  19 Batch   55/269   train_loss = 3.975\n",
      "Epoch  19 Batch   56/269   train_loss = 3.835\n",
      "Epoch  19 Batch   57/269   train_loss = 3.962\n",
      "Epoch  19 Batch   58/269   train_loss = 3.806\n",
      "Epoch  19 Batch   59/269   train_loss = 3.774\n",
      "Epoch  19 Batch   60/269   train_loss = 3.933\n",
      "Epoch  19 Batch   61/269   train_loss = 4.141\n",
      "Epoch  19 Batch   62/269   train_loss = 3.773\n",
      "Epoch  19 Batch   63/269   train_loss = 4.060\n",
      "Epoch  19 Batch   64/269   train_loss = 3.837\n",
      "Epoch  19 Batch   65/269   train_loss = 3.909\n",
      "Epoch  19 Batch   66/269   train_loss = 3.835\n",
      "Epoch  19 Batch   67/269   train_loss = 3.842\n",
      "Epoch  19 Batch   68/269   train_loss = 3.927\n",
      "Epoch  19 Batch   69/269   train_loss = 3.950\n",
      "Epoch  19 Batch   70/269   train_loss = 3.584\n",
      "Epoch  19 Batch   71/269   train_loss = 3.984\n",
      "Epoch  19 Batch   72/269   train_loss = 3.586\n",
      "Epoch  19 Batch   73/269   train_loss = 3.849\n",
      "Epoch  19 Batch   74/269   train_loss = 3.853\n",
      "Epoch  19 Batch   75/269   train_loss = 3.994\n",
      "Epoch  19 Batch   76/269   train_loss = 3.907\n",
      "Epoch  19 Batch   77/269   train_loss = 3.527\n",
      "Epoch  19 Batch   78/269   train_loss = 3.966\n",
      "Epoch  19 Batch   79/269   train_loss = 4.049\n",
      "Epoch  19 Batch   80/269   train_loss = 3.891\n",
      "Epoch  19 Batch   81/269   train_loss = 3.714\n",
      "Epoch  19 Batch   82/269   train_loss = 4.063\n",
      "Epoch  19 Batch   83/269   train_loss = 3.859\n",
      "Epoch  19 Batch   84/269   train_loss = 4.059\n",
      "Epoch  19 Batch   85/269   train_loss = 4.248\n",
      "Epoch  19 Batch   86/269   train_loss = 4.122\n",
      "Epoch  19 Batch   87/269   train_loss = 3.877\n",
      "Epoch  19 Batch   88/269   train_loss = 3.866\n",
      "Epoch  19 Batch   89/269   train_loss = 3.879\n",
      "Epoch  19 Batch   90/269   train_loss = 3.772\n",
      "Epoch  19 Batch   91/269   train_loss = 4.097\n",
      "Epoch  19 Batch   92/269   train_loss = 4.040\n",
      "Epoch  19 Batch   93/269   train_loss = 3.709\n",
      "Epoch  19 Batch   94/269   train_loss = 4.108\n",
      "Epoch  19 Batch   95/269   train_loss = 3.713\n",
      "Epoch  19 Batch   96/269   train_loss = 3.720\n",
      "Epoch  19 Batch   97/269   train_loss = 3.895\n",
      "Epoch  19 Batch   98/269   train_loss = 3.938\n",
      "Epoch  19 Batch   99/269   train_loss = 3.910\n",
      "Epoch  19 Batch  100/269   train_loss = 3.844\n",
      "Epoch  19 Batch  101/269   train_loss = 3.971\n",
      "Epoch  19 Batch  102/269   train_loss = 3.738\n",
      "Epoch  19 Batch  103/269   train_loss = 3.849\n",
      "Epoch  19 Batch  104/269   train_loss = 3.690\n",
      "Epoch  19 Batch  105/269   train_loss = 3.986\n",
      "Epoch  19 Batch  106/269   train_loss = 4.064\n",
      "Epoch  19 Batch  107/269   train_loss = 4.057\n",
      "Epoch  19 Batch  108/269   train_loss = 3.937\n",
      "Epoch  19 Batch  109/269   train_loss = 3.954\n",
      "Epoch  19 Batch  110/269   train_loss = 3.898\n",
      "Epoch  19 Batch  111/269   train_loss = 4.067\n",
      "Epoch  19 Batch  112/269   train_loss = 4.102\n",
      "Epoch  19 Batch  113/269   train_loss = 3.848\n",
      "Epoch  19 Batch  114/269   train_loss = 3.792\n",
      "Epoch  19 Batch  115/269   train_loss = 3.692\n",
      "Epoch  19 Batch  116/269   train_loss = 3.739\n",
      "Epoch  19 Batch  117/269   train_loss = 3.694\n",
      "Epoch  19 Batch  118/269   train_loss = 4.000\n",
      "Epoch  19 Batch  119/269   train_loss = 3.700\n",
      "Epoch  19 Batch  120/269   train_loss = 3.969\n",
      "Epoch  19 Batch  121/269   train_loss = 3.780\n",
      "Epoch  19 Batch  122/269   train_loss = 3.906\n",
      "Epoch  19 Batch  123/269   train_loss = 3.700\n",
      "Epoch  19 Batch  124/269   train_loss = 4.038\n",
      "Epoch  19 Batch  125/269   train_loss = 3.611\n",
      "Epoch  19 Batch  126/269   train_loss = 3.549\n",
      "Epoch  19 Batch  127/269   train_loss = 3.838\n",
      "Epoch  19 Batch  128/269   train_loss = 3.902\n",
      "Epoch  19 Batch  129/269   train_loss = 3.833\n",
      "Epoch  19 Batch  130/269   train_loss = 3.984\n",
      "Epoch  19 Batch  131/269   train_loss = 3.619\n",
      "Epoch  19 Batch  132/269   train_loss = 3.879\n",
      "Epoch  19 Batch  133/269   train_loss = 3.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19 Batch  134/269   train_loss = 4.027\n",
      "Epoch  19 Batch  135/269   train_loss = 3.666\n",
      "Epoch  19 Batch  136/269   train_loss = 3.606\n",
      "Epoch  19 Batch  137/269   train_loss = 3.953\n",
      "Epoch  19 Batch  138/269   train_loss = 3.738\n",
      "Epoch  19 Batch  139/269   train_loss = 3.891\n",
      "Epoch  19 Batch  140/269   train_loss = 3.984\n",
      "Epoch  19 Batch  141/269   train_loss = 4.191\n",
      "Epoch  19 Batch  142/269   train_loss = 3.699\n",
      "Epoch  19 Batch  143/269   train_loss = 3.913\n",
      "Epoch  19 Batch  144/269   train_loss = 4.017\n",
      "Epoch  19 Batch  145/269   train_loss = 3.829\n",
      "Epoch  19 Batch  146/269   train_loss = 4.053\n",
      "Epoch  19 Batch  147/269   train_loss = 3.677\n",
      "Epoch  19 Batch  148/269   train_loss = 3.913\n",
      "Epoch  19 Batch  149/269   train_loss = 3.801\n",
      "Epoch  19 Batch  150/269   train_loss = 3.877\n",
      "Epoch  19 Batch  151/269   train_loss = 3.863\n",
      "Epoch  19 Batch  152/269   train_loss = 3.858\n",
      "Epoch  19 Batch  153/269   train_loss = 3.949\n",
      "Epoch  19 Batch  154/269   train_loss = 3.774\n",
      "Epoch  19 Batch  155/269   train_loss = 3.745\n",
      "Epoch  19 Batch  156/269   train_loss = 4.226\n",
      "Epoch  19 Batch  157/269   train_loss = 4.066\n",
      "Epoch  19 Batch  158/269   train_loss = 3.956\n",
      "Epoch  19 Batch  159/269   train_loss = 3.940\n",
      "Epoch  19 Batch  160/269   train_loss = 3.828\n",
      "Epoch  19 Batch  161/269   train_loss = 3.961\n",
      "Epoch  19 Batch  162/269   train_loss = 3.922\n",
      "Epoch  19 Batch  163/269   train_loss = 3.980\n",
      "Epoch  19 Batch  164/269   train_loss = 4.097\n",
      "Epoch  19 Batch  165/269   train_loss = 3.989\n",
      "Epoch  19 Batch  166/269   train_loss = 3.918\n",
      "Epoch  19 Batch  167/269   train_loss = 3.809\n",
      "Epoch  19 Batch  168/269   train_loss = 3.815\n",
      "Epoch  19 Batch  169/269   train_loss = 4.163\n",
      "Epoch  19 Batch  170/269   train_loss = 3.682\n",
      "Epoch  19 Batch  171/269   train_loss = 3.986\n",
      "Epoch  19 Batch  172/269   train_loss = 3.753\n",
      "Epoch  19 Batch  173/269   train_loss = 3.955\n",
      "Epoch  19 Batch  174/269   train_loss = 4.030\n",
      "Epoch  19 Batch  175/269   train_loss = 3.947\n",
      "Epoch  19 Batch  176/269   train_loss = 3.797\n",
      "Epoch  19 Batch  177/269   train_loss = 3.877\n",
      "Epoch  19 Batch  178/269   train_loss = 4.147\n",
      "Epoch  19 Batch  179/269   train_loss = 3.906\n",
      "Epoch  19 Batch  180/269   train_loss = 3.762\n",
      "Epoch  19 Batch  181/269   train_loss = 3.984\n",
      "Epoch  19 Batch  182/269   train_loss = 3.773\n",
      "Epoch  19 Batch  183/269   train_loss = 3.932\n",
      "Epoch  19 Batch  184/269   train_loss = 3.889\n",
      "Epoch  19 Batch  185/269   train_loss = 3.897\n",
      "Epoch  19 Batch  186/269   train_loss = 3.787\n",
      "Epoch  19 Batch  187/269   train_loss = 4.097\n",
      "Epoch  19 Batch  188/269   train_loss = 3.778\n",
      "Epoch  19 Batch  189/269   train_loss = 3.841\n",
      "Epoch  19 Batch  190/269   train_loss = 4.226\n",
      "Epoch  19 Batch  191/269   train_loss = 3.909\n",
      "Epoch  19 Batch  192/269   train_loss = 3.881\n",
      "Epoch  19 Batch  193/269   train_loss = 3.771\n",
      "Epoch  19 Batch  194/269   train_loss = 3.952\n",
      "Epoch  19 Batch  195/269   train_loss = 3.777\n",
      "Epoch  19 Batch  196/269   train_loss = 3.958\n",
      "Epoch  19 Batch  197/269   train_loss = 4.053\n",
      "Epoch  19 Batch  198/269   train_loss = 4.014\n",
      "Epoch  19 Batch  199/269   train_loss = 3.991\n",
      "Epoch  19 Batch  200/269   train_loss = 3.941\n",
      "Epoch  19 Batch  201/269   train_loss = 3.833\n",
      "Epoch  19 Batch  202/269   train_loss = 3.666\n",
      "Epoch  19 Batch  203/269   train_loss = 3.768\n",
      "Epoch  19 Batch  204/269   train_loss = 3.922\n",
      "Epoch  19 Batch  205/269   train_loss = 3.916\n",
      "Epoch  19 Batch  206/269   train_loss = 3.848\n",
      "Epoch  19 Batch  207/269   train_loss = 3.839\n",
      "Epoch  19 Batch  208/269   train_loss = 3.848\n",
      "Epoch  19 Batch  209/269   train_loss = 4.005\n",
      "Epoch  19 Batch  210/269   train_loss = 3.788\n",
      "Epoch  19 Batch  211/269   train_loss = 3.863\n",
      "Epoch  19 Batch  212/269   train_loss = 4.175\n",
      "Epoch  19 Batch  213/269   train_loss = 3.780\n",
      "Epoch  19 Batch  214/269   train_loss = 3.907\n",
      "Epoch  19 Batch  215/269   train_loss = 4.063\n",
      "Epoch  19 Batch  216/269   train_loss = 4.047\n",
      "Epoch  19 Batch  217/269   train_loss = 3.775\n",
      "Epoch  19 Batch  218/269   train_loss = 3.875\n",
      "Epoch  19 Batch  219/269   train_loss = 3.624\n",
      "Epoch  19 Batch  220/269   train_loss = 3.968\n",
      "Epoch  19 Batch  221/269   train_loss = 3.783\n",
      "Epoch  19 Batch  222/269   train_loss = 3.924\n",
      "Epoch  19 Batch  223/269   train_loss = 3.755\n",
      "Epoch  19 Batch  224/269   train_loss = 3.955\n",
      "Epoch  19 Batch  225/269   train_loss = 4.082\n",
      "Epoch  19 Batch  226/269   train_loss = 3.884\n",
      "Epoch  19 Batch  227/269   train_loss = 3.591\n",
      "Epoch  19 Batch  228/269   train_loss = 3.848\n",
      "Epoch  19 Batch  229/269   train_loss = 3.951\n",
      "Epoch  19 Batch  230/269   train_loss = 3.999\n",
      "Epoch  19 Batch  231/269   train_loss = 3.860\n",
      "Epoch  19 Batch  232/269   train_loss = 3.846\n",
      "Epoch  19 Batch  233/269   train_loss = 3.931\n",
      "Epoch  19 Batch  234/269   train_loss = 3.871\n",
      "Epoch  19 Batch  235/269   train_loss = 4.102\n",
      "Epoch  19 Batch  236/269   train_loss = 3.763\n",
      "Epoch  19 Batch  237/269   train_loss = 3.500\n",
      "Epoch  19 Batch  238/269   train_loss = 3.631\n",
      "Epoch  19 Batch  239/269   train_loss = 4.267\n",
      "Epoch  19 Batch  240/269   train_loss = 3.842\n",
      "Epoch  19 Batch  241/269   train_loss = 4.082\n",
      "Epoch  19 Batch  242/269   train_loss = 3.651\n",
      "Epoch  19 Batch  243/269   train_loss = 3.964\n",
      "Epoch  19 Batch  244/269   train_loss = 3.785\n",
      "Epoch  19 Batch  245/269   train_loss = 3.826\n",
      "Epoch  19 Batch  246/269   train_loss = 3.543\n",
      "Epoch  19 Batch  247/269   train_loss = 3.956\n",
      "Epoch  19 Batch  248/269   train_loss = 3.912\n",
      "Epoch  19 Batch  249/269   train_loss = 3.805\n",
      "Epoch  19 Batch  250/269   train_loss = 3.598\n",
      "Epoch  19 Batch  251/269   train_loss = 4.026\n",
      "Epoch  19 Batch  252/269   train_loss = 4.004\n",
      "Epoch  19 Batch  253/269   train_loss = 3.847\n",
      "Epoch  19 Batch  254/269   train_loss = 3.775\n",
      "Epoch  19 Batch  255/269   train_loss = 3.912\n",
      "Epoch  19 Batch  256/269   train_loss = 3.833\n",
      "Epoch  19 Batch  257/269   train_loss = 3.674\n",
      "Epoch  19 Batch  258/269   train_loss = 3.552\n",
      "Epoch  19 Batch  259/269   train_loss = 3.664\n",
      "Epoch  19 Batch  260/269   train_loss = 3.914\n",
      "Epoch  19 Batch  261/269   train_loss = 3.983\n",
      "Epoch  19 Batch  262/269   train_loss = 3.706\n",
      "Epoch  19 Batch  263/269   train_loss = 3.597\n",
      "Epoch  19 Batch  264/269   train_loss = 4.154\n",
      "Epoch  19 Batch  265/269   train_loss = 3.904\n",
      "Epoch  19 Batch  266/269   train_loss = 3.834\n",
      "Epoch  19 Batch  267/269   train_loss = 3.893\n",
      "Epoch  19 Batch  268/269   train_loss = 3.987\n",
      "Epoch  20 Batch    0/269   train_loss = 3.736\n",
      "Epoch  20 Batch    1/269   train_loss = 3.628\n",
      "Epoch  20 Batch    2/269   train_loss = 3.762\n",
      "Epoch  20 Batch    3/269   train_loss = 3.792\n",
      "Epoch  20 Batch    4/269   train_loss = 4.211\n",
      "Epoch  20 Batch    5/269   train_loss = 3.894\n",
      "Epoch  20 Batch    6/269   train_loss = 3.753\n",
      "Epoch  20 Batch    7/269   train_loss = 3.667\n",
      "Epoch  20 Batch    8/269   train_loss = 3.947\n",
      "Epoch  20 Batch    9/269   train_loss = 3.687\n",
      "Epoch  20 Batch   10/269   train_loss = 3.674\n",
      "Epoch  20 Batch   11/269   train_loss = 3.811\n",
      "Epoch  20 Batch   12/269   train_loss = 3.598\n",
      "Epoch  20 Batch   13/269   train_loss = 3.902\n",
      "Epoch  20 Batch   14/269   train_loss = 3.716\n",
      "Epoch  20 Batch   15/269   train_loss = 4.068\n",
      "Epoch  20 Batch   16/269   train_loss = 3.817\n",
      "Epoch  20 Batch   17/269   train_loss = 3.918\n",
      "Epoch  20 Batch   18/269   train_loss = 3.822\n",
      "Epoch  20 Batch   19/269   train_loss = 3.803\n",
      "Epoch  20 Batch   20/269   train_loss = 4.124\n",
      "Epoch  20 Batch   21/269   train_loss = 3.982\n",
      "Epoch  20 Batch   22/269   train_loss = 3.676\n",
      "Epoch  20 Batch   23/269   train_loss = 3.903\n",
      "Epoch  20 Batch   24/269   train_loss = 3.764\n",
      "Epoch  20 Batch   25/269   train_loss = 4.036\n",
      "Epoch  20 Batch   26/269   train_loss = 3.914\n",
      "Epoch  20 Batch   27/269   train_loss = 3.695\n",
      "Epoch  20 Batch   28/269   train_loss = 3.875\n",
      "Epoch  20 Batch   29/269   train_loss = 3.911\n",
      "Epoch  20 Batch   30/269   train_loss = 4.137\n",
      "Epoch  20 Batch   31/269   train_loss = 3.841\n",
      "Epoch  20 Batch   32/269   train_loss = 3.755\n",
      "Epoch  20 Batch   33/269   train_loss = 3.569\n",
      "Epoch  20 Batch   34/269   train_loss = 3.817\n",
      "Epoch  20 Batch   35/269   train_loss = 3.687\n",
      "Epoch  20 Batch   36/269   train_loss = 3.678\n",
      "Epoch  20 Batch   37/269   train_loss = 3.845\n",
      "Epoch  20 Batch   38/269   train_loss = 3.704\n",
      "Epoch  20 Batch   39/269   train_loss = 3.946\n",
      "Epoch  20 Batch   40/269   train_loss = 3.690\n",
      "Epoch  20 Batch   41/269   train_loss = 3.721\n",
      "Epoch  20 Batch   42/269   train_loss = 4.245\n",
      "Epoch  20 Batch   43/269   train_loss = 4.037\n",
      "Epoch  20 Batch   44/269   train_loss = 3.846\n",
      "Epoch  20 Batch   45/269   train_loss = 3.586\n",
      "Epoch  20 Batch   46/269   train_loss = 3.913\n",
      "Epoch  20 Batch   47/269   train_loss = 3.577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20 Batch   48/269   train_loss = 4.063\n",
      "Epoch  20 Batch   49/269   train_loss = 4.071\n",
      "Epoch  20 Batch   50/269   train_loss = 4.000\n",
      "Epoch  20 Batch   51/269   train_loss = 3.912\n",
      "Epoch  20 Batch   52/269   train_loss = 3.873\n",
      "Epoch  20 Batch   53/269   train_loss = 4.054\n",
      "Epoch  20 Batch   54/269   train_loss = 3.885\n",
      "Epoch  20 Batch   55/269   train_loss = 3.995\n",
      "Epoch  20 Batch   56/269   train_loss = 3.830\n",
      "Epoch  20 Batch   57/269   train_loss = 3.972\n",
      "Epoch  20 Batch   58/269   train_loss = 3.759\n",
      "Epoch  20 Batch   59/269   train_loss = 3.758\n",
      "Epoch  20 Batch   60/269   train_loss = 3.923\n",
      "Epoch  20 Batch   61/269   train_loss = 4.128\n",
      "Epoch  20 Batch   62/269   train_loss = 3.764\n",
      "Epoch  20 Batch   63/269   train_loss = 4.033\n",
      "Epoch  20 Batch   64/269   train_loss = 3.819\n",
      "Epoch  20 Batch   65/269   train_loss = 3.866\n",
      "Epoch  20 Batch   66/269   train_loss = 3.797\n",
      "Epoch  20 Batch   67/269   train_loss = 3.817\n",
      "Epoch  20 Batch   68/269   train_loss = 3.913\n",
      "Epoch  20 Batch   69/269   train_loss = 3.925\n",
      "Epoch  20 Batch   70/269   train_loss = 3.546\n",
      "Epoch  20 Batch   71/269   train_loss = 3.963\n",
      "Epoch  20 Batch   72/269   train_loss = 3.570\n",
      "Epoch  20 Batch   73/269   train_loss = 3.837\n",
      "Epoch  20 Batch   74/269   train_loss = 3.824\n",
      "Epoch  20 Batch   75/269   train_loss = 3.991\n",
      "Epoch  20 Batch   76/269   train_loss = 3.904\n",
      "Epoch  20 Batch   77/269   train_loss = 3.516\n",
      "Epoch  20 Batch   78/269   train_loss = 3.939\n",
      "Epoch  20 Batch   79/269   train_loss = 4.037\n",
      "Epoch  20 Batch   80/269   train_loss = 3.886\n",
      "Epoch  20 Batch   81/269   train_loss = 3.713\n",
      "Epoch  20 Batch   82/269   train_loss = 4.033\n",
      "Epoch  20 Batch   83/269   train_loss = 3.880\n",
      "Epoch  20 Batch   84/269   train_loss = 4.045\n",
      "Epoch  20 Batch   85/269   train_loss = 4.258\n",
      "Epoch  20 Batch   86/269   train_loss = 4.105\n",
      "Epoch  20 Batch   87/269   train_loss = 3.853\n",
      "Epoch  20 Batch   88/269   train_loss = 3.869\n",
      "Epoch  20 Batch   89/269   train_loss = 3.857\n",
      "Epoch  20 Batch   90/269   train_loss = 3.766\n",
      "Epoch  20 Batch   91/269   train_loss = 4.054\n",
      "Epoch  20 Batch   92/269   train_loss = 4.008\n",
      "Epoch  20 Batch   93/269   train_loss = 3.711\n",
      "Epoch  20 Batch   94/269   train_loss = 4.134\n",
      "Epoch  20 Batch   95/269   train_loss = 3.672\n",
      "Epoch  20 Batch   96/269   train_loss = 3.707\n",
      "Epoch  20 Batch   97/269   train_loss = 3.864\n",
      "Epoch  20 Batch   98/269   train_loss = 3.937\n",
      "Epoch  20 Batch   99/269   train_loss = 3.904\n",
      "Epoch  20 Batch  100/269   train_loss = 3.811\n",
      "Epoch  20 Batch  101/269   train_loss = 3.949\n",
      "Epoch  20 Batch  102/269   train_loss = 3.711\n",
      "Epoch  20 Batch  103/269   train_loss = 3.842\n",
      "Epoch  20 Batch  104/269   train_loss = 3.700\n",
      "Epoch  20 Batch  105/269   train_loss = 3.951\n",
      "Epoch  20 Batch  106/269   train_loss = 4.035\n",
      "Epoch  20 Batch  107/269   train_loss = 4.050\n",
      "Epoch  20 Batch  108/269   train_loss = 3.939\n",
      "Epoch  20 Batch  109/269   train_loss = 3.921\n",
      "Epoch  20 Batch  110/269   train_loss = 3.884\n",
      "Epoch  20 Batch  111/269   train_loss = 4.028\n",
      "Epoch  20 Batch  112/269   train_loss = 4.053\n",
      "Epoch  20 Batch  113/269   train_loss = 3.859\n",
      "Epoch  20 Batch  114/269   train_loss = 3.812\n",
      "Epoch  20 Batch  115/269   train_loss = 3.696\n",
      "Epoch  20 Batch  116/269   train_loss = 3.713\n",
      "Epoch  20 Batch  117/269   train_loss = 3.704\n",
      "Epoch  20 Batch  118/269   train_loss = 3.989\n",
      "Epoch  20 Batch  119/269   train_loss = 3.682\n",
      "Epoch  20 Batch  120/269   train_loss = 3.943\n",
      "Epoch  20 Batch  121/269   train_loss = 3.788\n",
      "Epoch  20 Batch  122/269   train_loss = 3.885\n",
      "Epoch  20 Batch  123/269   train_loss = 3.672\n",
      "Epoch  20 Batch  124/269   train_loss = 4.012\n",
      "Epoch  20 Batch  125/269   train_loss = 3.586\n",
      "Epoch  20 Batch  126/269   train_loss = 3.528\n",
      "Epoch  20 Batch  127/269   train_loss = 3.834\n",
      "Epoch  20 Batch  128/269   train_loss = 3.877\n",
      "Epoch  20 Batch  129/269   train_loss = 3.783\n",
      "Epoch  20 Batch  130/269   train_loss = 3.979\n",
      "Epoch  20 Batch  131/269   train_loss = 3.565\n",
      "Epoch  20 Batch  132/269   train_loss = 3.863\n",
      "Epoch  20 Batch  133/269   train_loss = 3.662\n",
      "Epoch  20 Batch  134/269   train_loss = 4.034\n",
      "Epoch  20 Batch  135/269   train_loss = 3.654\n",
      "Epoch  20 Batch  136/269   train_loss = 3.592\n",
      "Epoch  20 Batch  137/269   train_loss = 3.947\n",
      "Epoch  20 Batch  138/269   train_loss = 3.700\n",
      "Epoch  20 Batch  139/269   train_loss = 3.894\n",
      "Epoch  20 Batch  140/269   train_loss = 3.958\n",
      "Epoch  20 Batch  141/269   train_loss = 4.158\n",
      "Epoch  20 Batch  142/269   train_loss = 3.702\n",
      "Epoch  20 Batch  143/269   train_loss = 3.892\n",
      "Epoch  20 Batch  144/269   train_loss = 4.007\n",
      "Epoch  20 Batch  145/269   train_loss = 3.830\n",
      "Epoch  20 Batch  146/269   train_loss = 4.055\n",
      "Epoch  20 Batch  147/269   train_loss = 3.663\n",
      "Epoch  20 Batch  148/269   train_loss = 3.921\n",
      "Epoch  20 Batch  149/269   train_loss = 3.807\n",
      "Epoch  20 Batch  150/269   train_loss = 3.858\n",
      "Epoch  20 Batch  151/269   train_loss = 3.890\n",
      "Epoch  20 Batch  152/269   train_loss = 3.828\n",
      "Epoch  20 Batch  153/269   train_loss = 3.940\n",
      "Epoch  20 Batch  154/269   train_loss = 3.779\n",
      "Epoch  20 Batch  155/269   train_loss = 3.718\n",
      "Epoch  20 Batch  156/269   train_loss = 4.228\n",
      "Epoch  20 Batch  157/269   train_loss = 4.055\n",
      "Epoch  20 Batch  158/269   train_loss = 3.943\n",
      "Epoch  20 Batch  159/269   train_loss = 3.975\n",
      "Epoch  20 Batch  160/269   train_loss = 3.841\n",
      "Epoch  20 Batch  161/269   train_loss = 3.956\n",
      "Epoch  20 Batch  162/269   train_loss = 3.932\n",
      "Epoch  20 Batch  163/269   train_loss = 3.991\n",
      "Epoch  20 Batch  164/269   train_loss = 4.069\n",
      "Epoch  20 Batch  165/269   train_loss = 4.007\n",
      "Epoch  20 Batch  166/269   train_loss = 3.946\n",
      "Epoch  20 Batch  167/269   train_loss = 3.788\n",
      "Epoch  20 Batch  168/269   train_loss = 3.806\n",
      "Epoch  20 Batch  169/269   train_loss = 4.128\n",
      "Epoch  20 Batch  170/269   train_loss = 3.696\n",
      "Epoch  20 Batch  171/269   train_loss = 3.959\n",
      "Epoch  20 Batch  172/269   train_loss = 3.720\n",
      "Epoch  20 Batch  173/269   train_loss = 3.941\n",
      "Epoch  20 Batch  174/269   train_loss = 4.004\n",
      "Epoch  20 Batch  175/269   train_loss = 3.911\n",
      "Epoch  20 Batch  176/269   train_loss = 3.816\n",
      "Epoch  20 Batch  177/269   train_loss = 3.894\n",
      "Epoch  20 Batch  178/269   train_loss = 4.149\n",
      "Epoch  20 Batch  179/269   train_loss = 3.895\n",
      "Epoch  20 Batch  180/269   train_loss = 3.772\n",
      "Epoch  20 Batch  181/269   train_loss = 4.034\n",
      "Epoch  20 Batch  182/269   train_loss = 3.785\n",
      "Epoch  20 Batch  183/269   train_loss = 3.916\n",
      "Epoch  20 Batch  184/269   train_loss = 3.863\n",
      "Epoch  20 Batch  185/269   train_loss = 3.892\n",
      "Epoch  20 Batch  186/269   train_loss = 3.777\n",
      "Epoch  20 Batch  187/269   train_loss = 4.081\n",
      "Epoch  20 Batch  188/269   train_loss = 3.767\n",
      "Epoch  20 Batch  189/269   train_loss = 3.822\n",
      "Epoch  20 Batch  190/269   train_loss = 4.215\n",
      "Epoch  20 Batch  191/269   train_loss = 3.898\n",
      "Epoch  20 Batch  192/269   train_loss = 3.834\n",
      "Epoch  20 Batch  193/269   train_loss = 3.769\n",
      "Epoch  20 Batch  194/269   train_loss = 3.923\n",
      "Epoch  20 Batch  195/269   train_loss = 3.770\n",
      "Epoch  20 Batch  196/269   train_loss = 3.945\n",
      "Epoch  20 Batch  197/269   train_loss = 4.071\n",
      "Epoch  20 Batch  198/269   train_loss = 3.959\n",
      "Epoch  20 Batch  199/269   train_loss = 3.958\n",
      "Epoch  20 Batch  200/269   train_loss = 3.911\n",
      "Epoch  20 Batch  201/269   train_loss = 3.853\n",
      "Epoch  20 Batch  202/269   train_loss = 3.700\n",
      "Epoch  20 Batch  203/269   train_loss = 3.751\n",
      "Epoch  20 Batch  204/269   train_loss = 3.903\n",
      "Epoch  20 Batch  205/269   train_loss = 3.903\n",
      "Epoch  20 Batch  206/269   train_loss = 3.830\n",
      "Epoch  20 Batch  207/269   train_loss = 3.848\n",
      "Epoch  20 Batch  208/269   train_loss = 3.817\n",
      "Epoch  20 Batch  209/269   train_loss = 4.005\n",
      "Epoch  20 Batch  210/269   train_loss = 3.750\n",
      "Epoch  20 Batch  211/269   train_loss = 3.811\n",
      "Epoch  20 Batch  212/269   train_loss = 4.165\n",
      "Epoch  20 Batch  213/269   train_loss = 3.753\n",
      "Epoch  20 Batch  214/269   train_loss = 3.865\n",
      "Epoch  20 Batch  215/269   train_loss = 4.048\n",
      "Epoch  20 Batch  216/269   train_loss = 4.053\n",
      "Epoch  20 Batch  217/269   train_loss = 3.767\n",
      "Epoch  20 Batch  218/269   train_loss = 3.882\n",
      "Epoch  20 Batch  219/269   train_loss = 3.598\n",
      "Epoch  20 Batch  220/269   train_loss = 3.963\n",
      "Epoch  20 Batch  221/269   train_loss = 3.781\n",
      "Epoch  20 Batch  222/269   train_loss = 3.896\n",
      "Epoch  20 Batch  223/269   train_loss = 3.700\n",
      "Epoch  20 Batch  224/269   train_loss = 3.957\n",
      "Epoch  20 Batch  225/269   train_loss = 4.068\n",
      "Epoch  20 Batch  226/269   train_loss = 3.853\n",
      "Epoch  20 Batch  227/269   train_loss = 3.581\n",
      "Epoch  20 Batch  228/269   train_loss = 3.808\n",
      "Epoch  20 Batch  229/269   train_loss = 3.923\n",
      "Epoch  20 Batch  230/269   train_loss = 3.998\n",
      "Epoch  20 Batch  231/269   train_loss = 3.871\n",
      "Epoch  20 Batch  232/269   train_loss = 3.833\n",
      "Epoch  20 Batch  233/269   train_loss = 3.948\n",
      "Epoch  20 Batch  234/269   train_loss = 3.879\n",
      "Epoch  20 Batch  235/269   train_loss = 4.066\n",
      "Epoch  20 Batch  236/269   train_loss = 3.735\n",
      "Epoch  20 Batch  237/269   train_loss = 3.457\n",
      "Epoch  20 Batch  238/269   train_loss = 3.601\n",
      "Epoch  20 Batch  239/269   train_loss = 4.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20 Batch  240/269   train_loss = 3.831\n",
      "Epoch  20 Batch  241/269   train_loss = 4.085\n",
      "Epoch  20 Batch  242/269   train_loss = 3.652\n",
      "Epoch  20 Batch  243/269   train_loss = 3.943\n",
      "Epoch  20 Batch  244/269   train_loss = 3.752\n",
      "Epoch  20 Batch  245/269   train_loss = 3.830\n",
      "Epoch  20 Batch  246/269   train_loss = 3.525\n",
      "Epoch  20 Batch  247/269   train_loss = 3.917\n",
      "Epoch  20 Batch  248/269   train_loss = 3.900\n",
      "Epoch  20 Batch  249/269   train_loss = 3.795\n",
      "Epoch  20 Batch  250/269   train_loss = 3.561\n",
      "Epoch  20 Batch  251/269   train_loss = 4.005\n",
      "Epoch  20 Batch  252/269   train_loss = 3.987\n",
      "Epoch  20 Batch  253/269   train_loss = 3.801\n",
      "Epoch  20 Batch  254/269   train_loss = 3.764\n",
      "Epoch  20 Batch  255/269   train_loss = 3.900\n",
      "Epoch  20 Batch  256/269   train_loss = 3.795\n",
      "Epoch  20 Batch  257/269   train_loss = 3.646\n",
      "Epoch  20 Batch  258/269   train_loss = 3.556\n",
      "Epoch  20 Batch  259/269   train_loss = 3.654\n",
      "Epoch  20 Batch  260/269   train_loss = 3.924\n",
      "Epoch  20 Batch  261/269   train_loss = 3.963\n",
      "Epoch  20 Batch  262/269   train_loss = 3.717\n",
      "Epoch  20 Batch  263/269   train_loss = 3.584\n",
      "Epoch  20 Batch  264/269   train_loss = 4.151\n",
      "Epoch  20 Batch  265/269   train_loss = 3.878\n",
      "Epoch  20 Batch  266/269   train_loss = 3.825\n",
      "Epoch  20 Batch  267/269   train_loss = 3.889\n",
      "Epoch  20 Batch  268/269   train_loss = 3.971\n",
      "Epoch  21 Batch    0/269   train_loss = 3.737\n",
      "Epoch  21 Batch    1/269   train_loss = 3.582\n",
      "Epoch  21 Batch    2/269   train_loss = 3.734\n",
      "Epoch  21 Batch    3/269   train_loss = 3.779\n",
      "Epoch  21 Batch    4/269   train_loss = 4.188\n",
      "Epoch  21 Batch    5/269   train_loss = 3.899\n",
      "Epoch  21 Batch    6/269   train_loss = 3.733\n",
      "Epoch  21 Batch    7/269   train_loss = 3.647\n",
      "Epoch  21 Batch    8/269   train_loss = 3.959\n",
      "Epoch  21 Batch    9/269   train_loss = 3.673\n",
      "Epoch  21 Batch   10/269   train_loss = 3.691\n",
      "Epoch  21 Batch   11/269   train_loss = 3.783\n",
      "Epoch  21 Batch   12/269   train_loss = 3.591\n",
      "Epoch  21 Batch   13/269   train_loss = 3.874\n",
      "Epoch  21 Batch   14/269   train_loss = 3.706\n",
      "Epoch  21 Batch   15/269   train_loss = 4.050\n",
      "Epoch  21 Batch   16/269   train_loss = 3.798\n",
      "Epoch  21 Batch   17/269   train_loss = 3.907\n",
      "Epoch  21 Batch   18/269   train_loss = 3.815\n",
      "Epoch  21 Batch   19/269   train_loss = 3.794\n",
      "Epoch  21 Batch   20/269   train_loss = 4.076\n",
      "Epoch  21 Batch   21/269   train_loss = 3.954\n",
      "Epoch  21 Batch   22/269   train_loss = 3.678\n",
      "Epoch  21 Batch   23/269   train_loss = 3.911\n",
      "Epoch  21 Batch   24/269   train_loss = 3.781\n",
      "Epoch  21 Batch   25/269   train_loss = 4.034\n",
      "Epoch  21 Batch   26/269   train_loss = 3.913\n",
      "Epoch  21 Batch   27/269   train_loss = 3.684\n",
      "Epoch  21 Batch   28/269   train_loss = 3.884\n",
      "Epoch  21 Batch   29/269   train_loss = 3.894\n",
      "Epoch  21 Batch   30/269   train_loss = 4.143\n",
      "Epoch  21 Batch   31/269   train_loss = 3.821\n",
      "Epoch  21 Batch   32/269   train_loss = 3.732\n",
      "Epoch  21 Batch   33/269   train_loss = 3.533\n",
      "Epoch  21 Batch   34/269   train_loss = 3.798\n",
      "Epoch  21 Batch   35/269   train_loss = 3.656\n",
      "Epoch  21 Batch   36/269   train_loss = 3.645\n",
      "Epoch  21 Batch   37/269   train_loss = 3.823\n",
      "Epoch  21 Batch   38/269   train_loss = 3.656\n",
      "Epoch  21 Batch   39/269   train_loss = 3.933\n",
      "Epoch  21 Batch   40/269   train_loss = 3.652\n",
      "Epoch  21 Batch   41/269   train_loss = 3.689\n",
      "Epoch  21 Batch   42/269   train_loss = 4.239\n",
      "Epoch  21 Batch   43/269   train_loss = 3.988\n",
      "Epoch  21 Batch   44/269   train_loss = 3.857\n",
      "Epoch  21 Batch   45/269   train_loss = 3.570\n",
      "Epoch  21 Batch   46/269   train_loss = 3.922\n",
      "Epoch  21 Batch   47/269   train_loss = 3.544\n",
      "Epoch  21 Batch   48/269   train_loss = 4.055\n",
      "Epoch  21 Batch   49/269   train_loss = 4.047\n",
      "Epoch  21 Batch   50/269   train_loss = 3.972\n",
      "Epoch  21 Batch   51/269   train_loss = 3.878\n",
      "Epoch  21 Batch   52/269   train_loss = 3.868\n",
      "Epoch  21 Batch   53/269   train_loss = 4.034\n",
      "Epoch  21 Batch   54/269   train_loss = 3.888\n",
      "Epoch  21 Batch   55/269   train_loss = 3.956\n",
      "Epoch  21 Batch   56/269   train_loss = 3.801\n",
      "Epoch  21 Batch   57/269   train_loss = 3.995\n",
      "Epoch  21 Batch   58/269   train_loss = 3.778\n",
      "Epoch  21 Batch   59/269   train_loss = 3.766\n",
      "Epoch  21 Batch   60/269   train_loss = 3.875\n",
      "Epoch  21 Batch   61/269   train_loss = 4.109\n",
      "Epoch  21 Batch   62/269   train_loss = 3.749\n",
      "Epoch  21 Batch   63/269   train_loss = 4.050\n",
      "Epoch  21 Batch   64/269   train_loss = 3.814\n",
      "Epoch  21 Batch   65/269   train_loss = 3.859\n",
      "Epoch  21 Batch   66/269   train_loss = 3.785\n",
      "Epoch  21 Batch   67/269   train_loss = 3.827\n",
      "Epoch  21 Batch   68/269   train_loss = 3.893\n",
      "Epoch  21 Batch   69/269   train_loss = 3.905\n",
      "Epoch  21 Batch   70/269   train_loss = 3.532\n",
      "Epoch  21 Batch   71/269   train_loss = 3.960\n",
      "Epoch  21 Batch   72/269   train_loss = 3.568\n",
      "Epoch  21 Batch   73/269   train_loss = 3.808\n",
      "Epoch  21 Batch   74/269   train_loss = 3.851\n",
      "Epoch  21 Batch   75/269   train_loss = 3.976\n",
      "Epoch  21 Batch   76/269   train_loss = 3.859\n",
      "Epoch  21 Batch   77/269   train_loss = 3.480\n",
      "Epoch  21 Batch   78/269   train_loss = 3.918\n",
      "Epoch  21 Batch   79/269   train_loss = 4.036\n",
      "Epoch  21 Batch   80/269   train_loss = 3.850\n",
      "Epoch  21 Batch   81/269   train_loss = 3.742\n",
      "Epoch  21 Batch   82/269   train_loss = 4.040\n",
      "Epoch  21 Batch   83/269   train_loss = 3.867\n",
      "Epoch  21 Batch   84/269   train_loss = 4.027\n",
      "Epoch  21 Batch   85/269   train_loss = 4.257\n",
      "Epoch  21 Batch   86/269   train_loss = 4.109\n",
      "Epoch  21 Batch   87/269   train_loss = 3.842\n",
      "Epoch  21 Batch   88/269   train_loss = 3.854\n",
      "Epoch  21 Batch   89/269   train_loss = 3.845\n",
      "Epoch  21 Batch   90/269   train_loss = 3.698\n",
      "Epoch  21 Batch   91/269   train_loss = 4.031\n",
      "Epoch  21 Batch   92/269   train_loss = 3.995\n",
      "Epoch  21 Batch   93/269   train_loss = 3.677\n",
      "Epoch  21 Batch   94/269   train_loss = 4.112\n",
      "Epoch  21 Batch   95/269   train_loss = 3.665\n",
      "Epoch  21 Batch   96/269   train_loss = 3.718\n",
      "Epoch  21 Batch   97/269   train_loss = 3.888\n",
      "Epoch  21 Batch   98/269   train_loss = 3.928\n",
      "Epoch  21 Batch   99/269   train_loss = 3.892\n",
      "Epoch  21 Batch  100/269   train_loss = 3.778\n",
      "Epoch  21 Batch  101/269   train_loss = 3.952\n",
      "Epoch  21 Batch  102/269   train_loss = 3.723\n",
      "Epoch  21 Batch  103/269   train_loss = 3.824\n",
      "Epoch  21 Batch  104/269   train_loss = 3.704\n",
      "Epoch  21 Batch  105/269   train_loss = 3.975\n",
      "Epoch  21 Batch  106/269   train_loss = 4.042\n",
      "Epoch  21 Batch  107/269   train_loss = 4.048\n",
      "Epoch  21 Batch  108/269   train_loss = 3.925\n",
      "Epoch  21 Batch  109/269   train_loss = 3.890\n",
      "Epoch  21 Batch  110/269   train_loss = 3.881\n",
      "Epoch  21 Batch  111/269   train_loss = 4.024\n",
      "Epoch  21 Batch  112/269   train_loss = 3.992\n",
      "Epoch  21 Batch  113/269   train_loss = 3.856\n",
      "Epoch  21 Batch  114/269   train_loss = 3.786\n",
      "Epoch  21 Batch  115/269   train_loss = 3.684\n",
      "Epoch  21 Batch  116/269   train_loss = 3.700\n",
      "Epoch  21 Batch  117/269   train_loss = 3.691\n",
      "Epoch  21 Batch  118/269   train_loss = 3.973\n",
      "Epoch  21 Batch  119/269   train_loss = 3.696\n",
      "Epoch  21 Batch  120/269   train_loss = 3.941\n",
      "Epoch  21 Batch  121/269   train_loss = 3.787\n",
      "Epoch  21 Batch  122/269   train_loss = 3.894\n",
      "Epoch  21 Batch  123/269   train_loss = 3.652\n",
      "Epoch  21 Batch  124/269   train_loss = 4.022\n",
      "Epoch  21 Batch  125/269   train_loss = 3.603\n",
      "Epoch  21 Batch  126/269   train_loss = 3.571\n",
      "Epoch  21 Batch  127/269   train_loss = 3.815\n",
      "Epoch  21 Batch  128/269   train_loss = 3.867\n",
      "Epoch  21 Batch  129/269   train_loss = 3.765\n",
      "Epoch  21 Batch  130/269   train_loss = 3.948\n",
      "Epoch  21 Batch  131/269   train_loss = 3.581\n",
      "Epoch  21 Batch  132/269   train_loss = 3.867\n",
      "Epoch  21 Batch  133/269   train_loss = 3.620\n",
      "Epoch  21 Batch  134/269   train_loss = 4.005\n",
      "Epoch  21 Batch  135/269   train_loss = 3.635\n",
      "Epoch  21 Batch  136/269   train_loss = 3.585\n",
      "Epoch  21 Batch  137/269   train_loss = 3.920\n",
      "Epoch  21 Batch  138/269   train_loss = 3.705\n",
      "Epoch  21 Batch  139/269   train_loss = 3.877\n",
      "Epoch  21 Batch  140/269   train_loss = 3.940\n",
      "Epoch  21 Batch  141/269   train_loss = 4.139\n",
      "Epoch  21 Batch  142/269   train_loss = 3.666\n",
      "Epoch  21 Batch  143/269   train_loss = 3.890\n",
      "Epoch  21 Batch  144/269   train_loss = 3.968\n",
      "Epoch  21 Batch  145/269   train_loss = 3.796\n",
      "Epoch  21 Batch  146/269   train_loss = 4.026\n",
      "Epoch  21 Batch  147/269   train_loss = 3.657\n",
      "Epoch  21 Batch  148/269   train_loss = 3.946\n",
      "Epoch  21 Batch  149/269   train_loss = 3.756\n",
      "Epoch  21 Batch  150/269   train_loss = 3.853\n",
      "Epoch  21 Batch  151/269   train_loss = 3.877\n",
      "Epoch  21 Batch  152/269   train_loss = 3.814\n",
      "Epoch  21 Batch  153/269   train_loss = 3.904\n",
      "Epoch  21 Batch  154/269   train_loss = 3.764\n",
      "Epoch  21 Batch  155/269   train_loss = 3.715\n",
      "Epoch  21 Batch  156/269   train_loss = 4.188\n",
      "Epoch  21 Batch  157/269   train_loss = 4.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21 Batch  158/269   train_loss = 3.886\n",
      "Epoch  21 Batch  159/269   train_loss = 3.974\n",
      "Epoch  21 Batch  160/269   train_loss = 3.796\n",
      "Epoch  21 Batch  161/269   train_loss = 3.928\n",
      "Epoch  21 Batch  162/269   train_loss = 3.881\n",
      "Epoch  21 Batch  163/269   train_loss = 3.982\n",
      "Epoch  21 Batch  164/269   train_loss = 4.051\n",
      "Epoch  21 Batch  165/269   train_loss = 3.983\n",
      "Epoch  21 Batch  166/269   train_loss = 3.908\n",
      "Epoch  21 Batch  167/269   train_loss = 3.753\n",
      "Epoch  21 Batch  168/269   train_loss = 3.795\n",
      "Epoch  21 Batch  169/269   train_loss = 4.106\n",
      "Epoch  21 Batch  170/269   train_loss = 3.662\n",
      "Epoch  21 Batch  171/269   train_loss = 3.953\n",
      "Epoch  21 Batch  172/269   train_loss = 3.682\n",
      "Epoch  21 Batch  173/269   train_loss = 3.920\n",
      "Epoch  21 Batch  174/269   train_loss = 3.971\n",
      "Epoch  21 Batch  175/269   train_loss = 3.860\n",
      "Epoch  21 Batch  176/269   train_loss = 3.802\n",
      "Epoch  21 Batch  177/269   train_loss = 3.901\n",
      "Epoch  21 Batch  178/269   train_loss = 4.121\n",
      "Epoch  21 Batch  179/269   train_loss = 3.858\n",
      "Epoch  21 Batch  180/269   train_loss = 3.752\n",
      "Epoch  21 Batch  181/269   train_loss = 4.011\n",
      "Epoch  21 Batch  182/269   train_loss = 3.750\n",
      "Epoch  21 Batch  183/269   train_loss = 3.856\n",
      "Epoch  21 Batch  184/269   train_loss = 3.850\n",
      "Epoch  21 Batch  185/269   train_loss = 3.882\n",
      "Epoch  21 Batch  186/269   train_loss = 3.766\n",
      "Epoch  21 Batch  187/269   train_loss = 4.038\n",
      "Epoch  21 Batch  188/269   train_loss = 3.757\n",
      "Epoch  21 Batch  189/269   train_loss = 3.797\n",
      "Epoch  21 Batch  190/269   train_loss = 4.204\n",
      "Epoch  21 Batch  191/269   train_loss = 3.890\n",
      "Epoch  21 Batch  192/269   train_loss = 3.828\n",
      "Epoch  21 Batch  193/269   train_loss = 3.735\n",
      "Epoch  21 Batch  194/269   train_loss = 3.933\n",
      "Epoch  21 Batch  195/269   train_loss = 3.764\n",
      "Epoch  21 Batch  196/269   train_loss = 3.942\n",
      "Epoch  21 Batch  197/269   train_loss = 4.068\n",
      "Epoch  21 Batch  198/269   train_loss = 3.968\n",
      "Epoch  21 Batch  199/269   train_loss = 3.947\n",
      "Epoch  21 Batch  200/269   train_loss = 3.880\n",
      "Epoch  21 Batch  201/269   train_loss = 3.831\n",
      "Epoch  21 Batch  202/269   train_loss = 3.684\n",
      "Epoch  21 Batch  203/269   train_loss = 3.725\n",
      "Epoch  21 Batch  204/269   train_loss = 3.881\n",
      "Epoch  21 Batch  205/269   train_loss = 3.885\n",
      "Epoch  21 Batch  206/269   train_loss = 3.792\n",
      "Epoch  21 Batch  207/269   train_loss = 3.831\n",
      "Epoch  21 Batch  208/269   train_loss = 3.795\n",
      "Epoch  21 Batch  209/269   train_loss = 3.990\n",
      "Epoch  21 Batch  210/269   train_loss = 3.727\n",
      "Epoch  21 Batch  211/269   train_loss = 3.803\n",
      "Epoch  21 Batch  212/269   train_loss = 4.138\n",
      "Epoch  21 Batch  213/269   train_loss = 3.721\n",
      "Epoch  21 Batch  214/269   train_loss = 3.831\n",
      "Epoch  21 Batch  215/269   train_loss = 4.039\n",
      "Epoch  21 Batch  216/269   train_loss = 4.036\n",
      "Epoch  21 Batch  217/269   train_loss = 3.732\n",
      "Epoch  21 Batch  218/269   train_loss = 3.854\n",
      "Epoch  21 Batch  219/269   train_loss = 3.549\n",
      "Epoch  21 Batch  220/269   train_loss = 3.950\n",
      "Epoch  21 Batch  221/269   train_loss = 3.746\n",
      "Epoch  21 Batch  222/269   train_loss = 3.875\n",
      "Epoch  21 Batch  223/269   train_loss = 3.684\n",
      "Epoch  21 Batch  224/269   train_loss = 3.965\n",
      "Epoch  21 Batch  225/269   train_loss = 4.087\n",
      "Epoch  21 Batch  226/269   train_loss = 3.842\n",
      "Epoch  21 Batch  227/269   train_loss = 3.575\n",
      "Epoch  21 Batch  228/269   train_loss = 3.798\n",
      "Epoch  21 Batch  229/269   train_loss = 3.934\n",
      "Epoch  21 Batch  230/269   train_loss = 3.945\n",
      "Epoch  21 Batch  231/269   train_loss = 3.856\n",
      "Epoch  21 Batch  232/269   train_loss = 3.823\n",
      "Epoch  21 Batch  233/269   train_loss = 3.911\n",
      "Epoch  21 Batch  234/269   train_loss = 3.866\n",
      "Epoch  21 Batch  235/269   train_loss = 4.068\n",
      "Epoch  21 Batch  236/269   train_loss = 3.715\n",
      "Epoch  21 Batch  237/269   train_loss = 3.455\n",
      "Epoch  21 Batch  238/269   train_loss = 3.603\n",
      "Epoch  21 Batch  239/269   train_loss = 4.220\n",
      "Epoch  21 Batch  240/269   train_loss = 3.795\n",
      "Epoch  21 Batch  241/269   train_loss = 4.054\n",
      "Epoch  21 Batch  242/269   train_loss = 3.619\n",
      "Epoch  21 Batch  243/269   train_loss = 3.930\n",
      "Epoch  21 Batch  244/269   train_loss = 3.726\n",
      "Epoch  21 Batch  245/269   train_loss = 3.835\n",
      "Epoch  21 Batch  246/269   train_loss = 3.517\n",
      "Epoch  21 Batch  247/269   train_loss = 3.937\n",
      "Epoch  21 Batch  248/269   train_loss = 3.888\n",
      "Epoch  21 Batch  249/269   train_loss = 3.783\n",
      "Epoch  21 Batch  250/269   train_loss = 3.557\n",
      "Epoch  21 Batch  251/269   train_loss = 3.995\n",
      "Epoch  21 Batch  252/269   train_loss = 3.980\n",
      "Epoch  21 Batch  253/269   train_loss = 3.794\n",
      "Epoch  21 Batch  254/269   train_loss = 3.733\n",
      "Epoch  21 Batch  255/269   train_loss = 3.878\n",
      "Epoch  21 Batch  256/269   train_loss = 3.773\n",
      "Epoch  21 Batch  257/269   train_loss = 3.634\n",
      "Epoch  21 Batch  258/269   train_loss = 3.564\n",
      "Epoch  21 Batch  259/269   train_loss = 3.631\n",
      "Epoch  21 Batch  260/269   train_loss = 3.910\n",
      "Epoch  21 Batch  261/269   train_loss = 3.928\n",
      "Epoch  21 Batch  262/269   train_loss = 3.708\n",
      "Epoch  21 Batch  263/269   train_loss = 3.585\n",
      "Epoch  21 Batch  264/269   train_loss = 4.156\n",
      "Epoch  21 Batch  265/269   train_loss = 3.886\n",
      "Epoch  21 Batch  266/269   train_loss = 3.816\n",
      "Epoch  21 Batch  267/269   train_loss = 3.888\n",
      "Epoch  21 Batch  268/269   train_loss = 3.947\n",
      "Epoch  22 Batch    0/269   train_loss = 3.707\n",
      "Epoch  22 Batch    1/269   train_loss = 3.593\n",
      "Epoch  22 Batch    2/269   train_loss = 3.711\n",
      "Epoch  22 Batch    3/269   train_loss = 3.753\n",
      "Epoch  22 Batch    4/269   train_loss = 4.174\n",
      "Epoch  22 Batch    5/269   train_loss = 3.859\n",
      "Epoch  22 Batch    6/269   train_loss = 3.723\n",
      "Epoch  22 Batch    7/269   train_loss = 3.630\n",
      "Epoch  22 Batch    8/269   train_loss = 3.957\n",
      "Epoch  22 Batch    9/269   train_loss = 3.621\n",
      "Epoch  22 Batch   10/269   train_loss = 3.647\n",
      "Epoch  22 Batch   11/269   train_loss = 3.782\n",
      "Epoch  22 Batch   12/269   train_loss = 3.568\n",
      "Epoch  22 Batch   13/269   train_loss = 3.867\n",
      "Epoch  22 Batch   14/269   train_loss = 3.692\n",
      "Epoch  22 Batch   15/269   train_loss = 4.042\n",
      "Epoch  22 Batch   16/269   train_loss = 3.799\n",
      "Epoch  22 Batch   17/269   train_loss = 3.885\n",
      "Epoch  22 Batch   18/269   train_loss = 3.807\n",
      "Epoch  22 Batch   19/269   train_loss = 3.804\n",
      "Epoch  22 Batch   20/269   train_loss = 4.079\n",
      "Epoch  22 Batch   21/269   train_loss = 3.950\n",
      "Epoch  22 Batch   22/269   train_loss = 3.679\n",
      "Epoch  22 Batch   23/269   train_loss = 3.904\n",
      "Epoch  22 Batch   24/269   train_loss = 3.755\n",
      "Epoch  22 Batch   25/269   train_loss = 4.020\n",
      "Epoch  22 Batch   26/269   train_loss = 3.900\n",
      "Epoch  22 Batch   27/269   train_loss = 3.697\n",
      "Epoch  22 Batch   28/269   train_loss = 3.872\n",
      "Epoch  22 Batch   29/269   train_loss = 3.901\n",
      "Epoch  22 Batch   30/269   train_loss = 4.151\n",
      "Epoch  22 Batch   31/269   train_loss = 3.809\n",
      "Epoch  22 Batch   32/269   train_loss = 3.745\n",
      "Epoch  22 Batch   33/269   train_loss = 3.511\n",
      "Epoch  22 Batch   34/269   train_loss = 3.773\n",
      "Epoch  22 Batch   35/269   train_loss = 3.646\n",
      "Epoch  22 Batch   36/269   train_loss = 3.620\n",
      "Epoch  22 Batch   37/269   train_loss = 3.838\n",
      "Epoch  22 Batch   38/269   train_loss = 3.679\n",
      "Epoch  22 Batch   39/269   train_loss = 3.935\n",
      "Epoch  22 Batch   40/269   train_loss = 3.671\n",
      "Epoch  22 Batch   41/269   train_loss = 3.709\n",
      "Epoch  22 Batch   42/269   train_loss = 4.219\n",
      "Epoch  22 Batch   43/269   train_loss = 3.978\n",
      "Epoch  22 Batch   44/269   train_loss = 3.839\n",
      "Epoch  22 Batch   45/269   train_loss = 3.567\n",
      "Epoch  22 Batch   46/269   train_loss = 3.887\n",
      "Epoch  22 Batch   47/269   train_loss = 3.517\n",
      "Epoch  22 Batch   48/269   train_loss = 4.064\n",
      "Epoch  22 Batch   49/269   train_loss = 4.019\n",
      "Epoch  22 Batch   50/269   train_loss = 3.956\n",
      "Epoch  22 Batch   51/269   train_loss = 3.892\n",
      "Epoch  22 Batch   52/269   train_loss = 3.864\n",
      "Epoch  22 Batch   53/269   train_loss = 4.029\n",
      "Epoch  22 Batch   54/269   train_loss = 3.847\n",
      "Epoch  22 Batch   55/269   train_loss = 3.957\n",
      "Epoch  22 Batch   56/269   train_loss = 3.779\n",
      "Epoch  22 Batch   57/269   train_loss = 3.959\n",
      "Epoch  22 Batch   58/269   train_loss = 3.761\n",
      "Epoch  22 Batch   59/269   train_loss = 3.729\n",
      "Epoch  22 Batch   60/269   train_loss = 3.871\n",
      "Epoch  22 Batch   61/269   train_loss = 4.106\n",
      "Epoch  22 Batch   62/269   train_loss = 3.728\n",
      "Epoch  22 Batch   63/269   train_loss = 4.007\n",
      "Epoch  22 Batch   64/269   train_loss = 3.813\n",
      "Epoch  22 Batch   65/269   train_loss = 3.878\n",
      "Epoch  22 Batch   66/269   train_loss = 3.782\n",
      "Epoch  22 Batch   67/269   train_loss = 3.841\n",
      "Epoch  22 Batch   68/269   train_loss = 3.870\n",
      "Epoch  22 Batch   69/269   train_loss = 3.876\n",
      "Epoch  22 Batch   70/269   train_loss = 3.515\n",
      "Epoch  22 Batch   71/269   train_loss = 3.934\n",
      "Epoch  22 Batch   72/269   train_loss = 3.589\n",
      "Epoch  22 Batch   73/269   train_loss = 3.788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22 Batch   74/269   train_loss = 3.838\n",
      "Epoch  22 Batch   75/269   train_loss = 3.941\n",
      "Epoch  22 Batch   76/269   train_loss = 3.846\n",
      "Epoch  22 Batch   77/269   train_loss = 3.495\n",
      "Epoch  22 Batch   78/269   train_loss = 3.912\n",
      "Epoch  22 Batch   79/269   train_loss = 4.016\n",
      "Epoch  22 Batch   80/269   train_loss = 3.857\n",
      "Epoch  22 Batch   81/269   train_loss = 3.688\n",
      "Epoch  22 Batch   82/269   train_loss = 4.019\n",
      "Epoch  22 Batch   83/269   train_loss = 3.853\n",
      "Epoch  22 Batch   84/269   train_loss = 4.008\n",
      "Epoch  22 Batch   85/269   train_loss = 4.217\n",
      "Epoch  22 Batch   86/269   train_loss = 4.106\n",
      "Epoch  22 Batch   87/269   train_loss = 3.821\n",
      "Epoch  22 Batch   88/269   train_loss = 3.843\n",
      "Epoch  22 Batch   89/269   train_loss = 3.827\n",
      "Epoch  22 Batch   90/269   train_loss = 3.685\n",
      "Epoch  22 Batch   91/269   train_loss = 4.014\n",
      "Epoch  22 Batch   92/269   train_loss = 4.037\n",
      "Epoch  22 Batch   93/269   train_loss = 3.669\n",
      "Epoch  22 Batch   94/269   train_loss = 4.102\n",
      "Epoch  22 Batch   95/269   train_loss = 3.645\n",
      "Epoch  22 Batch   96/269   train_loss = 3.705\n",
      "Epoch  22 Batch   97/269   train_loss = 3.866\n",
      "Epoch  22 Batch   98/269   train_loss = 3.920\n",
      "Epoch  22 Batch   99/269   train_loss = 3.872\n",
      "Epoch  22 Batch  100/269   train_loss = 3.795\n",
      "Epoch  22 Batch  101/269   train_loss = 3.933\n",
      "Epoch  22 Batch  102/269   train_loss = 3.663\n",
      "Epoch  22 Batch  103/269   train_loss = 3.792\n",
      "Epoch  22 Batch  104/269   train_loss = 3.669\n",
      "Epoch  22 Batch  105/269   train_loss = 3.970\n",
      "Epoch  22 Batch  106/269   train_loss = 4.074\n",
      "Epoch  22 Batch  107/269   train_loss = 4.036\n",
      "Epoch  22 Batch  108/269   train_loss = 3.908\n",
      "Epoch  22 Batch  109/269   train_loss = 3.878\n",
      "Epoch  22 Batch  110/269   train_loss = 3.871\n",
      "Epoch  22 Batch  111/269   train_loss = 4.013\n",
      "Epoch  22 Batch  112/269   train_loss = 3.981\n",
      "Epoch  22 Batch  113/269   train_loss = 3.825\n",
      "Epoch  22 Batch  114/269   train_loss = 3.758\n",
      "Epoch  22 Batch  115/269   train_loss = 3.668\n",
      "Epoch  22 Batch  116/269   train_loss = 3.673\n",
      "Epoch  22 Batch  117/269   train_loss = 3.676\n",
      "Epoch  22 Batch  118/269   train_loss = 3.963\n",
      "Epoch  22 Batch  119/269   train_loss = 3.668\n",
      "Epoch  22 Batch  120/269   train_loss = 3.911\n",
      "Epoch  22 Batch  121/269   train_loss = 3.783\n",
      "Epoch  22 Batch  122/269   train_loss = 3.866\n",
      "Epoch  22 Batch  123/269   train_loss = 3.634\n",
      "Epoch  22 Batch  124/269   train_loss = 4.040\n",
      "Epoch  22 Batch  125/269   train_loss = 3.585\n",
      "Epoch  22 Batch  126/269   train_loss = 3.515\n",
      "Epoch  22 Batch  127/269   train_loss = 3.812\n",
      "Epoch  22 Batch  128/269   train_loss = 3.832\n",
      "Epoch  22 Batch  129/269   train_loss = 3.772\n",
      "Epoch  22 Batch  130/269   train_loss = 3.940\n",
      "Epoch  22 Batch  131/269   train_loss = 3.550\n",
      "Epoch  22 Batch  132/269   train_loss = 3.852\n",
      "Epoch  22 Batch  133/269   train_loss = 3.616\n",
      "Epoch  22 Batch  134/269   train_loss = 3.981\n",
      "Epoch  22 Batch  135/269   train_loss = 3.607\n",
      "Epoch  22 Batch  136/269   train_loss = 3.570\n",
      "Epoch  22 Batch  137/269   train_loss = 3.887\n",
      "Epoch  22 Batch  138/269   train_loss = 3.685\n",
      "Epoch  22 Batch  139/269   train_loss = 3.855\n",
      "Epoch  22 Batch  140/269   train_loss = 3.946\n",
      "Epoch  22 Batch  141/269   train_loss = 4.155\n",
      "Epoch  22 Batch  142/269   train_loss = 3.667\n",
      "Epoch  22 Batch  143/269   train_loss = 3.859\n",
      "Epoch  22 Batch  144/269   train_loss = 3.941\n",
      "Epoch  22 Batch  145/269   train_loss = 3.797\n",
      "Epoch  22 Batch  146/269   train_loss = 3.982\n",
      "Epoch  22 Batch  147/269   train_loss = 3.618\n",
      "Epoch  22 Batch  148/269   train_loss = 3.946\n",
      "Epoch  22 Batch  149/269   train_loss = 3.759\n",
      "Epoch  22 Batch  150/269   train_loss = 3.872\n",
      "Epoch  22 Batch  151/269   train_loss = 3.864\n",
      "Epoch  22 Batch  152/269   train_loss = 3.813\n",
      "Epoch  22 Batch  153/269   train_loss = 3.895\n",
      "Epoch  22 Batch  154/269   train_loss = 3.762\n",
      "Epoch  22 Batch  155/269   train_loss = 3.703\n",
      "Epoch  22 Batch  156/269   train_loss = 4.197\n",
      "Epoch  22 Batch  157/269   train_loss = 3.988\n",
      "Epoch  22 Batch  158/269   train_loss = 3.912\n",
      "Epoch  22 Batch  159/269   train_loss = 3.944\n",
      "Epoch  22 Batch  160/269   train_loss = 3.795\n",
      "Epoch  22 Batch  161/269   train_loss = 3.937\n",
      "Epoch  22 Batch  162/269   train_loss = 3.888\n",
      "Epoch  22 Batch  163/269   train_loss = 3.959\n",
      "Epoch  22 Batch  164/269   train_loss = 4.051\n",
      "Epoch  22 Batch  165/269   train_loss = 3.955\n",
      "Epoch  22 Batch  166/269   train_loss = 3.873\n",
      "Epoch  22 Batch  167/269   train_loss = 3.739\n",
      "Epoch  22 Batch  168/269   train_loss = 3.817\n",
      "Epoch  22 Batch  169/269   train_loss = 4.090\n",
      "Epoch  22 Batch  170/269   train_loss = 3.651\n",
      "Epoch  22 Batch  171/269   train_loss = 3.931\n",
      "Epoch  22 Batch  172/269   train_loss = 3.684\n",
      "Epoch  22 Batch  173/269   train_loss = 3.883\n",
      "Epoch  22 Batch  174/269   train_loss = 3.953\n",
      "Epoch  22 Batch  175/269   train_loss = 3.879\n",
      "Epoch  22 Batch  176/269   train_loss = 3.784\n",
      "Epoch  22 Batch  177/269   train_loss = 3.838\n",
      "Epoch  22 Batch  178/269   train_loss = 4.101\n",
      "Epoch  22 Batch  179/269   train_loss = 3.875\n",
      "Epoch  22 Batch  180/269   train_loss = 3.731\n",
      "Epoch  22 Batch  181/269   train_loss = 3.961\n",
      "Epoch  22 Batch  182/269   train_loss = 3.744\n",
      "Epoch  22 Batch  183/269   train_loss = 3.838\n",
      "Epoch  22 Batch  184/269   train_loss = 3.863\n",
      "Epoch  22 Batch  185/269   train_loss = 3.889\n",
      "Epoch  22 Batch  186/269   train_loss = 3.764\n",
      "Epoch  22 Batch  187/269   train_loss = 4.006\n",
      "Epoch  22 Batch  188/269   train_loss = 3.747\n",
      "Epoch  22 Batch  189/269   train_loss = 3.792\n",
      "Epoch  22 Batch  190/269   train_loss = 4.187\n",
      "Epoch  22 Batch  191/269   train_loss = 3.875\n",
      "Epoch  22 Batch  192/269   train_loss = 3.798\n",
      "Epoch  22 Batch  193/269   train_loss = 3.722\n",
      "Epoch  22 Batch  194/269   train_loss = 3.900\n",
      "Epoch  22 Batch  195/269   train_loss = 3.766\n",
      "Epoch  22 Batch  196/269   train_loss = 3.925\n",
      "Epoch  22 Batch  197/269   train_loss = 4.051\n",
      "Epoch  22 Batch  198/269   train_loss = 3.936\n",
      "Epoch  22 Batch  199/269   train_loss = 3.919\n",
      "Epoch  22 Batch  200/269   train_loss = 3.824\n",
      "Epoch  22 Batch  201/269   train_loss = 3.829\n",
      "Epoch  22 Batch  202/269   train_loss = 3.677\n",
      "Epoch  22 Batch  203/269   train_loss = 3.723\n",
      "Epoch  22 Batch  204/269   train_loss = 3.893\n",
      "Epoch  22 Batch  205/269   train_loss = 3.871\n",
      "Epoch  22 Batch  206/269   train_loss = 3.803\n",
      "Epoch  22 Batch  207/269   train_loss = 3.850\n",
      "Epoch  22 Batch  208/269   train_loss = 3.818\n",
      "Epoch  22 Batch  209/269   train_loss = 3.970\n",
      "Epoch  22 Batch  210/269   train_loss = 3.735\n",
      "Epoch  22 Batch  211/269   train_loss = 3.840\n",
      "Epoch  22 Batch  212/269   train_loss = 4.109\n",
      "Epoch  22 Batch  213/269   train_loss = 3.719\n",
      "Epoch  22 Batch  214/269   train_loss = 3.840\n",
      "Epoch  22 Batch  215/269   train_loss = 3.993\n",
      "Epoch  22 Batch  216/269   train_loss = 4.019\n",
      "Epoch  22 Batch  217/269   train_loss = 3.722\n",
      "Epoch  22 Batch  218/269   train_loss = 3.855\n",
      "Epoch  22 Batch  219/269   train_loss = 3.548\n",
      "Epoch  22 Batch  220/269   train_loss = 3.940\n",
      "Epoch  22 Batch  221/269   train_loss = 3.726\n",
      "Epoch  22 Batch  222/269   train_loss = 3.843\n",
      "Epoch  22 Batch  223/269   train_loss = 3.702\n",
      "Epoch  22 Batch  224/269   train_loss = 3.931\n",
      "Epoch  22 Batch  225/269   train_loss = 4.043\n",
      "Epoch  22 Batch  226/269   train_loss = 3.826\n",
      "Epoch  22 Batch  227/269   train_loss = 3.566\n",
      "Epoch  22 Batch  228/269   train_loss = 3.773\n",
      "Epoch  22 Batch  229/269   train_loss = 3.919\n",
      "Epoch  22 Batch  230/269   train_loss = 3.981\n",
      "Epoch  22 Batch  231/269   train_loss = 3.830\n",
      "Epoch  22 Batch  232/269   train_loss = 3.816\n",
      "Epoch  22 Batch  233/269   train_loss = 3.917\n",
      "Epoch  22 Batch  234/269   train_loss = 3.842\n",
      "Epoch  22 Batch  235/269   train_loss = 4.076\n",
      "Epoch  22 Batch  236/269   train_loss = 3.691\n",
      "Epoch  22 Batch  237/269   train_loss = 3.418\n",
      "Epoch  22 Batch  238/269   train_loss = 3.592\n",
      "Epoch  22 Batch  239/269   train_loss = 4.233\n",
      "Epoch  22 Batch  240/269   train_loss = 3.790\n",
      "Epoch  22 Batch  241/269   train_loss = 4.063\n",
      "Epoch  22 Batch  242/269   train_loss = 3.627\n",
      "Epoch  22 Batch  243/269   train_loss = 3.942\n",
      "Epoch  22 Batch  244/269   train_loss = 3.731\n",
      "Epoch  22 Batch  245/269   train_loss = 3.816\n",
      "Epoch  22 Batch  246/269   train_loss = 3.481\n",
      "Epoch  22 Batch  247/269   train_loss = 3.876\n",
      "Epoch  22 Batch  248/269   train_loss = 3.872\n",
      "Epoch  22 Batch  249/269   train_loss = 3.781\n",
      "Epoch  22 Batch  250/269   train_loss = 3.580\n",
      "Epoch  22 Batch  251/269   train_loss = 3.965\n",
      "Epoch  22 Batch  252/269   train_loss = 3.951\n",
      "Epoch  22 Batch  253/269   train_loss = 3.773\n",
      "Epoch  22 Batch  254/269   train_loss = 3.732\n",
      "Epoch  22 Batch  255/269   train_loss = 3.901\n",
      "Epoch  22 Batch  256/269   train_loss = 3.759\n",
      "Epoch  22 Batch  257/269   train_loss = 3.645\n",
      "Epoch  22 Batch  258/269   train_loss = 3.576\n",
      "Epoch  22 Batch  259/269   train_loss = 3.635\n",
      "Epoch  22 Batch  260/269   train_loss = 3.890\n",
      "Epoch  22 Batch  261/269   train_loss = 3.951\n",
      "Epoch  22 Batch  262/269   train_loss = 3.705\n",
      "Epoch  22 Batch  263/269   train_loss = 3.584\n",
      "Epoch  22 Batch  264/269   train_loss = 4.115\n",
      "Epoch  22 Batch  265/269   train_loss = 3.881\n",
      "Epoch  22 Batch  266/269   train_loss = 3.801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22 Batch  267/269   train_loss = 3.876\n",
      "Epoch  22 Batch  268/269   train_loss = 3.948\n",
      "Epoch  23 Batch    0/269   train_loss = 3.708\n",
      "Epoch  23 Batch    1/269   train_loss = 3.555\n",
      "Epoch  23 Batch    2/269   train_loss = 3.699\n",
      "Epoch  23 Batch    3/269   train_loss = 3.776\n",
      "Epoch  23 Batch    4/269   train_loss = 4.163\n",
      "Epoch  23 Batch    5/269   train_loss = 3.845\n",
      "Epoch  23 Batch    6/269   train_loss = 3.740\n",
      "Epoch  23 Batch    7/269   train_loss = 3.614\n",
      "Epoch  23 Batch    8/269   train_loss = 3.955\n",
      "Epoch  23 Batch    9/269   train_loss = 3.596\n",
      "Epoch  23 Batch   10/269   train_loss = 3.627\n",
      "Epoch  23 Batch   11/269   train_loss = 3.789\n",
      "Epoch  23 Batch   12/269   train_loss = 3.525\n",
      "Epoch  23 Batch   13/269   train_loss = 3.876\n",
      "Epoch  23 Batch   14/269   train_loss = 3.691\n",
      "Epoch  23 Batch   15/269   train_loss = 4.043\n",
      "Epoch  23 Batch   16/269   train_loss = 3.774\n",
      "Epoch  23 Batch   17/269   train_loss = 3.842\n",
      "Epoch  23 Batch   18/269   train_loss = 3.777\n",
      "Epoch  23 Batch   19/269   train_loss = 3.803\n",
      "Epoch  23 Batch   20/269   train_loss = 4.086\n",
      "Epoch  23 Batch   21/269   train_loss = 3.958\n",
      "Epoch  23 Batch   22/269   train_loss = 3.669\n",
      "Epoch  23 Batch   23/269   train_loss = 3.904\n",
      "Epoch  23 Batch   24/269   train_loss = 3.746\n",
      "Epoch  23 Batch   25/269   train_loss = 4.011\n",
      "Epoch  23 Batch   26/269   train_loss = 3.897\n",
      "Epoch  23 Batch   27/269   train_loss = 3.715\n",
      "Epoch  23 Batch   28/269   train_loss = 3.880\n",
      "Epoch  23 Batch   29/269   train_loss = 3.887\n",
      "Epoch  23 Batch   30/269   train_loss = 4.123\n",
      "Epoch  23 Batch   31/269   train_loss = 3.838\n",
      "Epoch  23 Batch   32/269   train_loss = 3.708\n",
      "Epoch  23 Batch   33/269   train_loss = 3.505\n",
      "Epoch  23 Batch   34/269   train_loss = 3.766\n",
      "Epoch  23 Batch   35/269   train_loss = 3.654\n",
      "Epoch  23 Batch   36/269   train_loss = 3.635\n",
      "Epoch  23 Batch   37/269   train_loss = 3.822\n",
      "Epoch  23 Batch   38/269   train_loss = 3.647\n",
      "Epoch  23 Batch   39/269   train_loss = 3.920\n",
      "Epoch  23 Batch   40/269   train_loss = 3.665\n",
      "Epoch  23 Batch   41/269   train_loss = 3.705\n",
      "Epoch  23 Batch   42/269   train_loss = 4.200\n",
      "Epoch  23 Batch   43/269   train_loss = 3.982\n",
      "Epoch  23 Batch   44/269   train_loss = 3.837\n",
      "Epoch  23 Batch   45/269   train_loss = 3.546\n",
      "Epoch  23 Batch   46/269   train_loss = 3.910\n",
      "Epoch  23 Batch   47/269   train_loss = 3.512\n",
      "Epoch  23 Batch   48/269   train_loss = 4.071\n",
      "Epoch  23 Batch   49/269   train_loss = 4.003\n",
      "Epoch  23 Batch   50/269   train_loss = 3.948\n",
      "Epoch  23 Batch   51/269   train_loss = 3.876\n",
      "Epoch  23 Batch   52/269   train_loss = 3.883\n",
      "Epoch  23 Batch   53/269   train_loss = 4.044\n",
      "Epoch  23 Batch   54/269   train_loss = 3.847\n",
      "Epoch  23 Batch   55/269   train_loss = 3.935\n",
      "Epoch  23 Batch   56/269   train_loss = 3.775\n",
      "Epoch  23 Batch   57/269   train_loss = 3.973\n",
      "Epoch  23 Batch   58/269   train_loss = 3.731\n",
      "Epoch  23 Batch   59/269   train_loss = 3.724\n",
      "Epoch  23 Batch   60/269   train_loss = 3.864\n",
      "Epoch  23 Batch   61/269   train_loss = 4.094\n",
      "Epoch  23 Batch   62/269   train_loss = 3.691\n",
      "Epoch  23 Batch   63/269   train_loss = 3.992\n",
      "Epoch  23 Batch   64/269   train_loss = 3.784\n",
      "Epoch  23 Batch   65/269   train_loss = 3.847\n",
      "Epoch  23 Batch   66/269   train_loss = 3.767\n",
      "Epoch  23 Batch   67/269   train_loss = 3.838\n",
      "Epoch  23 Batch   68/269   train_loss = 3.871\n",
      "Epoch  23 Batch   69/269   train_loss = 3.855\n",
      "Epoch  23 Batch   70/269   train_loss = 3.509\n",
      "Epoch  23 Batch   71/269   train_loss = 3.930\n",
      "Epoch  23 Batch   72/269   train_loss = 3.566\n",
      "Epoch  23 Batch   73/269   train_loss = 3.806\n",
      "Epoch  23 Batch   74/269   train_loss = 3.826\n",
      "Epoch  23 Batch   75/269   train_loss = 3.948\n",
      "Epoch  23 Batch   76/269   train_loss = 3.813\n",
      "Epoch  23 Batch   77/269   train_loss = 3.469\n",
      "Epoch  23 Batch   78/269   train_loss = 3.920\n",
      "Epoch  23 Batch   79/269   train_loss = 4.002\n",
      "Epoch  23 Batch   80/269   train_loss = 3.851\n",
      "Epoch  23 Batch   81/269   train_loss = 3.670\n",
      "Epoch  23 Batch   82/269   train_loss = 4.006\n",
      "Epoch  23 Batch   83/269   train_loss = 3.839\n",
      "Epoch  23 Batch   84/269   train_loss = 4.003\n",
      "Epoch  23 Batch   85/269   train_loss = 4.215\n",
      "Epoch  23 Batch   86/269   train_loss = 4.097\n",
      "Epoch  23 Batch   87/269   train_loss = 3.797\n",
      "Epoch  23 Batch   88/269   train_loss = 3.845\n",
      "Epoch  23 Batch   89/269   train_loss = 3.814\n",
      "Epoch  23 Batch   90/269   train_loss = 3.688\n",
      "Epoch  23 Batch   91/269   train_loss = 4.001\n",
      "Epoch  23 Batch   92/269   train_loss = 3.989\n",
      "Epoch  23 Batch   93/269   train_loss = 3.652\n",
      "Epoch  23 Batch   94/269   train_loss = 4.090\n",
      "Epoch  23 Batch   95/269   train_loss = 3.644\n",
      "Epoch  23 Batch   96/269   train_loss = 3.699\n",
      "Epoch  23 Batch   97/269   train_loss = 3.819\n",
      "Epoch  23 Batch   98/269   train_loss = 3.925\n",
      "Epoch  23 Batch   99/269   train_loss = 3.873\n",
      "Epoch  23 Batch  100/269   train_loss = 3.745\n",
      "Epoch  23 Batch  101/269   train_loss = 3.924\n",
      "Epoch  23 Batch  102/269   train_loss = 3.672\n",
      "Epoch  23 Batch  103/269   train_loss = 3.782\n",
      "Epoch  23 Batch  104/269   train_loss = 3.657\n",
      "Epoch  23 Batch  105/269   train_loss = 3.969\n",
      "Epoch  23 Batch  106/269   train_loss = 4.081\n",
      "Epoch  23 Batch  107/269   train_loss = 4.042\n",
      "Epoch  23 Batch  108/269   train_loss = 3.866\n",
      "Epoch  23 Batch  109/269   train_loss = 3.865\n",
      "Epoch  23 Batch  110/269   train_loss = 3.867\n",
      "Epoch  23 Batch  111/269   train_loss = 3.999\n",
      "Epoch  23 Batch  112/269   train_loss = 3.986\n",
      "Epoch  23 Batch  113/269   train_loss = 3.811\n",
      "Epoch  23 Batch  114/269   train_loss = 3.775\n",
      "Epoch  23 Batch  115/269   train_loss = 3.659\n",
      "Epoch  23 Batch  116/269   train_loss = 3.669\n",
      "Epoch  23 Batch  117/269   train_loss = 3.675\n",
      "Epoch  23 Batch  118/269   train_loss = 3.958\n",
      "Epoch  23 Batch  119/269   train_loss = 3.682\n",
      "Epoch  23 Batch  120/269   train_loss = 3.922\n",
      "Epoch  23 Batch  121/269   train_loss = 3.761\n",
      "Epoch  23 Batch  122/269   train_loss = 3.862\n",
      "Epoch  23 Batch  123/269   train_loss = 3.653\n",
      "Epoch  23 Batch  124/269   train_loss = 4.020\n",
      "Epoch  23 Batch  125/269   train_loss = 3.592\n",
      "Epoch  23 Batch  126/269   train_loss = 3.524\n",
      "Epoch  23 Batch  127/269   train_loss = 3.820\n",
      "Epoch  23 Batch  128/269   train_loss = 3.826\n",
      "Epoch  23 Batch  129/269   train_loss = 3.793\n",
      "Epoch  23 Batch  130/269   train_loss = 3.943\n",
      "Epoch  23 Batch  131/269   train_loss = 3.550\n",
      "Epoch  23 Batch  132/269   train_loss = 3.855\n",
      "Epoch  23 Batch  133/269   train_loss = 3.611\n",
      "Epoch  23 Batch  134/269   train_loss = 3.981\n",
      "Epoch  23 Batch  135/269   train_loss = 3.601\n",
      "Epoch  23 Batch  136/269   train_loss = 3.577\n",
      "Epoch  23 Batch  137/269   train_loss = 3.897\n",
      "Epoch  23 Batch  138/269   train_loss = 3.674\n",
      "Epoch  23 Batch  139/269   train_loss = 3.852\n",
      "Epoch  23 Batch  140/269   train_loss = 3.879\n",
      "Epoch  23 Batch  141/269   train_loss = 4.135\n",
      "Epoch  23 Batch  142/269   train_loss = 3.656\n",
      "Epoch  23 Batch  143/269   train_loss = 3.845\n",
      "Epoch  23 Batch  144/269   train_loss = 3.927\n",
      "Epoch  23 Batch  145/269   train_loss = 3.799\n",
      "Epoch  23 Batch  146/269   train_loss = 3.972\n",
      "Epoch  23 Batch  147/269   train_loss = 3.618\n",
      "Epoch  23 Batch  148/269   train_loss = 3.914\n",
      "Epoch  23 Batch  149/269   train_loss = 3.752\n",
      "Epoch  23 Batch  150/269   train_loss = 3.875\n",
      "Epoch  23 Batch  151/269   train_loss = 3.870\n",
      "Epoch  23 Batch  152/269   train_loss = 3.796\n",
      "Epoch  23 Batch  153/269   train_loss = 3.886\n",
      "Epoch  23 Batch  154/269   train_loss = 3.760\n",
      "Epoch  23 Batch  155/269   train_loss = 3.691\n",
      "Epoch  23 Batch  156/269   train_loss = 4.191\n",
      "Epoch  23 Batch  157/269   train_loss = 3.982\n",
      "Epoch  23 Batch  158/269   train_loss = 3.898\n",
      "Epoch  23 Batch  159/269   train_loss = 3.909\n",
      "Epoch  23 Batch  160/269   train_loss = 3.770\n",
      "Epoch  23 Batch  161/269   train_loss = 3.917\n",
      "Epoch  23 Batch  162/269   train_loss = 3.879\n",
      "Epoch  23 Batch  163/269   train_loss = 3.900\n",
      "Epoch  23 Batch  164/269   train_loss = 4.021\n",
      "Epoch  23 Batch  165/269   train_loss = 3.942\n",
      "Epoch  23 Batch  166/269   train_loss = 3.866\n",
      "Epoch  23 Batch  167/269   train_loss = 3.722\n",
      "Epoch  23 Batch  168/269   train_loss = 3.786\n",
      "Epoch  23 Batch  169/269   train_loss = 4.068\n",
      "Epoch  23 Batch  170/269   train_loss = 3.648\n",
      "Epoch  23 Batch  171/269   train_loss = 3.918\n",
      "Epoch  23 Batch  172/269   train_loss = 3.643\n",
      "Epoch  23 Batch  173/269   train_loss = 3.890\n",
      "Epoch  23 Batch  174/269   train_loss = 3.961\n",
      "Epoch  23 Batch  175/269   train_loss = 3.871\n",
      "Epoch  23 Batch  176/269   train_loss = 3.778\n",
      "Epoch  23 Batch  177/269   train_loss = 3.821\n",
      "Epoch  23 Batch  178/269   train_loss = 4.093\n",
      "Epoch  23 Batch  179/269   train_loss = 3.857\n",
      "Epoch  23 Batch  180/269   train_loss = 3.711\n",
      "Epoch  23 Batch  181/269   train_loss = 3.983\n",
      "Epoch  23 Batch  182/269   train_loss = 3.752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23 Batch  183/269   train_loss = 3.811\n",
      "Epoch  23 Batch  184/269   train_loss = 3.844\n",
      "Epoch  23 Batch  185/269   train_loss = 3.923\n",
      "Epoch  23 Batch  186/269   train_loss = 3.753\n",
      "Epoch  23 Batch  187/269   train_loss = 3.990\n",
      "Epoch  23 Batch  188/269   train_loss = 3.749\n",
      "Epoch  23 Batch  189/269   train_loss = 3.790\n",
      "Epoch  23 Batch  190/269   train_loss = 4.202\n",
      "Epoch  23 Batch  191/269   train_loss = 3.871\n",
      "Epoch  23 Batch  192/269   train_loss = 3.808\n",
      "Epoch  23 Batch  193/269   train_loss = 3.712\n",
      "Epoch  23 Batch  194/269   train_loss = 3.911\n",
      "Epoch  23 Batch  195/269   train_loss = 3.755\n",
      "Epoch  23 Batch  196/269   train_loss = 3.892\n",
      "Epoch  23 Batch  197/269   train_loss = 4.056\n",
      "Epoch  23 Batch  198/269   train_loss = 3.928\n",
      "Epoch  23 Batch  199/269   train_loss = 3.913\n",
      "Epoch  23 Batch  200/269   train_loss = 3.772\n",
      "Epoch  23 Batch  201/269   train_loss = 3.782\n",
      "Epoch  23 Batch  202/269   train_loss = 3.661\n",
      "Epoch  23 Batch  203/269   train_loss = 3.698\n",
      "Epoch  23 Batch  204/269   train_loss = 3.912\n",
      "Epoch  23 Batch  205/269   train_loss = 3.876\n",
      "Epoch  23 Batch  206/269   train_loss = 3.752\n",
      "Epoch  23 Batch  207/269   train_loss = 3.824\n",
      "Epoch  23 Batch  208/269   train_loss = 3.846\n",
      "Epoch  23 Batch  209/269   train_loss = 3.937\n",
      "Epoch  23 Batch  210/269   train_loss = 3.729\n",
      "Epoch  23 Batch  211/269   train_loss = 3.823\n",
      "Epoch  23 Batch  212/269   train_loss = 4.139\n",
      "Epoch  23 Batch  213/269   train_loss = 3.712\n",
      "Epoch  23 Batch  214/269   train_loss = 3.799\n",
      "Epoch  23 Batch  215/269   train_loss = 3.984\n",
      "Epoch  23 Batch  216/269   train_loss = 4.029\n",
      "Epoch  23 Batch  217/269   train_loss = 3.697\n",
      "Epoch  23 Batch  218/269   train_loss = 3.851\n",
      "Epoch  23 Batch  219/269   train_loss = 3.545\n",
      "Epoch  23 Batch  220/269   train_loss = 3.928\n",
      "Epoch  23 Batch  221/269   train_loss = 3.719\n",
      "Epoch  23 Batch  222/269   train_loss = 3.889\n",
      "Epoch  23 Batch  223/269   train_loss = 3.683\n",
      "Epoch  23 Batch  224/269   train_loss = 3.937\n",
      "Epoch  23 Batch  225/269   train_loss = 4.032\n",
      "Epoch  23 Batch  226/269   train_loss = 3.846\n",
      "Epoch  23 Batch  227/269   train_loss = 3.558\n",
      "Epoch  23 Batch  228/269   train_loss = 3.777\n",
      "Epoch  23 Batch  229/269   train_loss = 3.914\n",
      "Epoch  23 Batch  230/269   train_loss = 3.948\n",
      "Epoch  23 Batch  231/269   train_loss = 3.832\n",
      "Epoch  23 Batch  232/269   train_loss = 3.809\n",
      "Epoch  23 Batch  233/269   train_loss = 3.918\n",
      "Epoch  23 Batch  234/269   train_loss = 3.821\n",
      "Epoch  23 Batch  235/269   train_loss = 4.055\n",
      "Epoch  23 Batch  236/269   train_loss = 3.673\n",
      "Epoch  23 Batch  237/269   train_loss = 3.448\n",
      "Epoch  23 Batch  238/269   train_loss = 3.592\n",
      "Epoch  23 Batch  239/269   train_loss = 4.225\n",
      "Epoch  23 Batch  240/269   train_loss = 3.798\n",
      "Epoch  23 Batch  241/269   train_loss = 4.017\n",
      "Epoch  23 Batch  242/269   train_loss = 3.623\n",
      "Epoch  23 Batch  243/269   train_loss = 3.928\n",
      "Epoch  23 Batch  244/269   train_loss = 3.734\n",
      "Epoch  23 Batch  245/269   train_loss = 3.824\n",
      "Epoch  23 Batch  246/269   train_loss = 3.460\n",
      "Epoch  23 Batch  247/269   train_loss = 3.857\n",
      "Epoch  23 Batch  248/269   train_loss = 3.866\n",
      "Epoch  23 Batch  249/269   train_loss = 3.766\n",
      "Epoch  23 Batch  250/269   train_loss = 3.544\n",
      "Epoch  23 Batch  251/269   train_loss = 3.947\n",
      "Epoch  23 Batch  252/269   train_loss = 3.969\n",
      "Epoch  23 Batch  253/269   train_loss = 3.753\n",
      "Epoch  23 Batch  254/269   train_loss = 3.725\n",
      "Epoch  23 Batch  255/269   train_loss = 3.858\n",
      "Epoch  23 Batch  256/269   train_loss = 3.724\n",
      "Epoch  23 Batch  257/269   train_loss = 3.621\n",
      "Epoch  23 Batch  258/269   train_loss = 3.534\n",
      "Epoch  23 Batch  259/269   train_loss = 3.617\n",
      "Epoch  23 Batch  260/269   train_loss = 3.888\n",
      "Epoch  23 Batch  261/269   train_loss = 3.983\n",
      "Epoch  23 Batch  262/269   train_loss = 3.688\n",
      "Epoch  23 Batch  263/269   train_loss = 3.557\n",
      "Epoch  23 Batch  264/269   train_loss = 4.114\n",
      "Epoch  23 Batch  265/269   train_loss = 3.875\n",
      "Epoch  23 Batch  266/269   train_loss = 3.774\n",
      "Epoch  23 Batch  267/269   train_loss = 3.860\n",
      "Epoch  23 Batch  268/269   train_loss = 3.948\n",
      "Epoch  24 Batch    0/269   train_loss = 3.705\n",
      "Epoch  24 Batch    1/269   train_loss = 3.562\n",
      "Epoch  24 Batch    2/269   train_loss = 3.702\n",
      "Epoch  24 Batch    3/269   train_loss = 3.741\n",
      "Epoch  24 Batch    4/269   train_loss = 4.179\n",
      "Epoch  24 Batch    5/269   train_loss = 3.847\n",
      "Epoch  24 Batch    6/269   train_loss = 3.733\n",
      "Epoch  24 Batch    7/269   train_loss = 3.574\n",
      "Epoch  24 Batch    8/269   train_loss = 3.961\n",
      "Epoch  24 Batch    9/269   train_loss = 3.588\n",
      "Epoch  24 Batch   10/269   train_loss = 3.622\n",
      "Epoch  24 Batch   11/269   train_loss = 3.766\n",
      "Epoch  24 Batch   12/269   train_loss = 3.497\n",
      "Epoch  24 Batch   13/269   train_loss = 3.838\n",
      "Epoch  24 Batch   14/269   train_loss = 3.671\n",
      "Epoch  24 Batch   15/269   train_loss = 4.022\n",
      "Epoch  24 Batch   16/269   train_loss = 3.758\n",
      "Epoch  24 Batch   17/269   train_loss = 3.839\n",
      "Epoch  24 Batch   18/269   train_loss = 3.782\n",
      "Epoch  24 Batch   19/269   train_loss = 3.781\n",
      "Epoch  24 Batch   20/269   train_loss = 4.102\n",
      "Epoch  24 Batch   21/269   train_loss = 3.940\n",
      "Epoch  24 Batch   22/269   train_loss = 3.693\n",
      "Epoch  24 Batch   23/269   train_loss = 3.862\n",
      "Epoch  24 Batch   24/269   train_loss = 3.736\n",
      "Epoch  24 Batch   25/269   train_loss = 3.995\n",
      "Epoch  24 Batch   26/269   train_loss = 3.874\n",
      "Epoch  24 Batch   27/269   train_loss = 3.673\n",
      "Epoch  24 Batch   28/269   train_loss = 3.901\n",
      "Epoch  24 Batch   29/269   train_loss = 3.889\n",
      "Epoch  24 Batch   30/269   train_loss = 4.148\n",
      "Epoch  24 Batch   31/269   train_loss = 3.789\n",
      "Epoch  24 Batch   32/269   train_loss = 3.711\n",
      "Epoch  24 Batch   33/269   train_loss = 3.478\n",
      "Epoch  24 Batch   34/269   train_loss = 3.782\n",
      "Epoch  24 Batch   35/269   train_loss = 3.647\n",
      "Epoch  24 Batch   36/269   train_loss = 3.621\n",
      "Epoch  24 Batch   37/269   train_loss = 3.812\n",
      "Epoch  24 Batch   38/269   train_loss = 3.619\n",
      "Epoch  24 Batch   39/269   train_loss = 3.878\n",
      "Epoch  24 Batch   40/269   train_loss = 3.658\n",
      "Epoch  24 Batch   41/269   train_loss = 3.673\n",
      "Epoch  24 Batch   42/269   train_loss = 4.201\n",
      "Epoch  24 Batch   43/269   train_loss = 3.954\n",
      "Epoch  24 Batch   44/269   train_loss = 3.808\n",
      "Epoch  24 Batch   45/269   train_loss = 3.527\n",
      "Epoch  24 Batch   46/269   train_loss = 3.911\n",
      "Epoch  24 Batch   47/269   train_loss = 3.489\n",
      "Epoch  24 Batch   48/269   train_loss = 4.054\n",
      "Epoch  24 Batch   49/269   train_loss = 3.993\n",
      "Epoch  24 Batch   50/269   train_loss = 3.954\n",
      "Epoch  24 Batch   51/269   train_loss = 3.900\n",
      "Epoch  24 Batch   52/269   train_loss = 3.882\n",
      "Epoch  24 Batch   53/269   train_loss = 4.020\n",
      "Epoch  24 Batch   54/269   train_loss = 3.851\n",
      "Epoch  24 Batch   55/269   train_loss = 3.934\n",
      "Epoch  24 Batch   56/269   train_loss = 3.749\n",
      "Epoch  24 Batch   57/269   train_loss = 3.958\n",
      "Epoch  24 Batch   58/269   train_loss = 3.741\n",
      "Epoch  24 Batch   59/269   train_loss = 3.695\n",
      "Epoch  24 Batch   60/269   train_loss = 3.834\n",
      "Epoch  24 Batch   61/269   train_loss = 4.095\n",
      "Epoch  24 Batch   62/269   train_loss = 3.662\n",
      "Epoch  24 Batch   63/269   train_loss = 3.985\n",
      "Epoch  24 Batch   64/269   train_loss = 3.762\n",
      "Epoch  24 Batch   65/269   train_loss = 3.853\n",
      "Epoch  24 Batch   66/269   train_loss = 3.762\n",
      "Epoch  24 Batch   67/269   train_loss = 3.825\n",
      "Epoch  24 Batch   68/269   train_loss = 3.831\n",
      "Epoch  24 Batch   69/269   train_loss = 3.852\n",
      "Epoch  24 Batch   70/269   train_loss = 3.513\n",
      "Epoch  24 Batch   71/269   train_loss = 3.927\n",
      "Epoch  24 Batch   72/269   train_loss = 3.551\n",
      "Epoch  24 Batch   73/269   train_loss = 3.799\n",
      "Epoch  24 Batch   74/269   train_loss = 3.817\n",
      "Epoch  24 Batch   75/269   train_loss = 3.930\n",
      "Epoch  24 Batch   76/269   train_loss = 3.814\n",
      "Epoch  24 Batch   77/269   train_loss = 3.470\n",
      "Epoch  24 Batch   78/269   train_loss = 3.917\n",
      "Epoch  24 Batch   79/269   train_loss = 3.979\n",
      "Epoch  24 Batch   80/269   train_loss = 3.855\n",
      "Epoch  24 Batch   81/269   train_loss = 3.667\n",
      "Epoch  24 Batch   82/269   train_loss = 4.005\n",
      "Epoch  24 Batch   83/269   train_loss = 3.809\n",
      "Epoch  24 Batch   84/269   train_loss = 4.000\n",
      "Epoch  24 Batch   85/269   train_loss = 4.201\n",
      "Epoch  24 Batch   86/269   train_loss = 4.061\n",
      "Epoch  24 Batch   87/269   train_loss = 3.800\n",
      "Epoch  24 Batch   88/269   train_loss = 3.831\n",
      "Epoch  24 Batch   89/269   train_loss = 3.779\n",
      "Epoch  24 Batch   90/269   train_loss = 3.642\n",
      "Epoch  24 Batch   91/269   train_loss = 3.974\n",
      "Epoch  24 Batch   92/269   train_loss = 4.006\n",
      "Epoch  24 Batch   93/269   train_loss = 3.656\n",
      "Epoch  24 Batch   94/269   train_loss = 4.109\n",
      "Epoch  24 Batch   95/269   train_loss = 3.640\n",
      "Epoch  24 Batch   96/269   train_loss = 3.681\n",
      "Epoch  24 Batch   97/269   train_loss = 3.813\n",
      "Epoch  24 Batch   98/269   train_loss = 3.915\n",
      "Epoch  24 Batch   99/269   train_loss = 3.817\n",
      "Epoch  24 Batch  100/269   train_loss = 3.743\n",
      "Epoch  24 Batch  101/269   train_loss = 3.908\n",
      "Epoch  24 Batch  102/269   train_loss = 3.652\n",
      "Epoch  24 Batch  103/269   train_loss = 3.770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  24 Batch  104/269   train_loss = 3.653\n",
      "Epoch  24 Batch  105/269   train_loss = 3.934\n",
      "Epoch  24 Batch  106/269   train_loss = 4.033\n",
      "Epoch  24 Batch  107/269   train_loss = 4.015\n",
      "Epoch  24 Batch  108/269   train_loss = 3.854\n",
      "Epoch  24 Batch  109/269   train_loss = 3.857\n",
      "Epoch  24 Batch  110/269   train_loss = 3.847\n",
      "Epoch  24 Batch  111/269   train_loss = 3.992\n",
      "Epoch  24 Batch  112/269   train_loss = 3.965\n",
      "Epoch  24 Batch  113/269   train_loss = 3.817\n",
      "Epoch  24 Batch  114/269   train_loss = 3.771\n",
      "Epoch  24 Batch  115/269   train_loss = 3.690\n",
      "Epoch  24 Batch  116/269   train_loss = 3.671\n",
      "Epoch  24 Batch  117/269   train_loss = 3.662\n",
      "Epoch  24 Batch  118/269   train_loss = 3.976\n",
      "Epoch  24 Batch  119/269   train_loss = 3.651\n",
      "Epoch  24 Batch  120/269   train_loss = 3.896\n",
      "Epoch  24 Batch  121/269   train_loss = 3.767\n",
      "Epoch  24 Batch  122/269   train_loss = 3.832\n",
      "Epoch  24 Batch  123/269   train_loss = 3.615\n",
      "Epoch  24 Batch  124/269   train_loss = 4.015\n",
      "Epoch  24 Batch  125/269   train_loss = 3.618\n",
      "Epoch  24 Batch  126/269   train_loss = 3.504\n",
      "Epoch  24 Batch  127/269   train_loss = 3.792\n",
      "Epoch  24 Batch  128/269   train_loss = 3.777\n",
      "Epoch  24 Batch  129/269   train_loss = 3.762\n",
      "Epoch  24 Batch  130/269   train_loss = 3.940\n",
      "Epoch  24 Batch  131/269   train_loss = 3.563\n",
      "Epoch  24 Batch  132/269   train_loss = 3.824\n",
      "Epoch  24 Batch  133/269   train_loss = 3.604\n",
      "Epoch  24 Batch  134/269   train_loss = 3.982\n",
      "Epoch  24 Batch  135/269   train_loss = 3.613\n",
      "Epoch  24 Batch  136/269   train_loss = 3.560\n",
      "Epoch  24 Batch  137/269   train_loss = 3.883\n",
      "Epoch  24 Batch  138/269   train_loss = 3.687\n",
      "Epoch  24 Batch  139/269   train_loss = 3.833\n",
      "Epoch  24 Batch  140/269   train_loss = 3.874\n",
      "Epoch  24 Batch  141/269   train_loss = 4.129\n",
      "Epoch  24 Batch  142/269   train_loss = 3.636\n",
      "Epoch  24 Batch  143/269   train_loss = 3.828\n",
      "Epoch  24 Batch  144/269   train_loss = 3.923\n",
      "Epoch  24 Batch  145/269   train_loss = 3.779\n",
      "Epoch  24 Batch  146/269   train_loss = 3.973\n",
      "Epoch  24 Batch  147/269   train_loss = 3.586\n",
      "Epoch  24 Batch  148/269   train_loss = 3.933\n",
      "Epoch  24 Batch  149/269   train_loss = 3.747\n",
      "Epoch  24 Batch  150/269   train_loss = 3.874\n",
      "Epoch  24 Batch  151/269   train_loss = 3.851\n",
      "Epoch  24 Batch  152/269   train_loss = 3.795\n",
      "Epoch  24 Batch  153/269   train_loss = 3.890\n",
      "Epoch  24 Batch  154/269   train_loss = 3.742\n",
      "Epoch  24 Batch  155/269   train_loss = 3.666\n",
      "Epoch  24 Batch  156/269   train_loss = 4.164\n",
      "Epoch  24 Batch  157/269   train_loss = 3.984\n",
      "Epoch  24 Batch  158/269   train_loss = 3.885\n",
      "Epoch  24 Batch  159/269   train_loss = 3.883\n",
      "Epoch  24 Batch  160/269   train_loss = 3.757\n",
      "Epoch  24 Batch  161/269   train_loss = 3.911\n",
      "Epoch  24 Batch  162/269   train_loss = 3.866\n",
      "Epoch  24 Batch  163/269   train_loss = 3.867\n",
      "Epoch  24 Batch  164/269   train_loss = 4.015\n",
      "Epoch  24 Batch  165/269   train_loss = 3.929\n",
      "Epoch  24 Batch  166/269   train_loss = 3.867\n",
      "Epoch  24 Batch  167/269   train_loss = 3.714\n",
      "Epoch  24 Batch  168/269   train_loss = 3.807\n",
      "Epoch  24 Batch  169/269   train_loss = 4.051\n",
      "Epoch  24 Batch  170/269   train_loss = 3.624\n",
      "Epoch  24 Batch  171/269   train_loss = 3.917\n",
      "Epoch  24 Batch  172/269   train_loss = 3.622\n",
      "Epoch  24 Batch  173/269   train_loss = 3.867\n",
      "Epoch  24 Batch  174/269   train_loss = 3.938\n",
      "Epoch  24 Batch  175/269   train_loss = 3.862\n",
      "Epoch  24 Batch  176/269   train_loss = 3.768\n",
      "Epoch  24 Batch  177/269   train_loss = 3.824\n",
      "Epoch  24 Batch  178/269   train_loss = 4.109\n",
      "Epoch  24 Batch  179/269   train_loss = 3.876\n",
      "Epoch  24 Batch  180/269   train_loss = 3.712\n",
      "Epoch  24 Batch  181/269   train_loss = 3.969\n",
      "Epoch  24 Batch  182/269   train_loss = 3.749\n",
      "Epoch  24 Batch  183/269   train_loss = 3.766\n",
      "Epoch  24 Batch  184/269   train_loss = 3.841\n",
      "Epoch  24 Batch  185/269   train_loss = 3.877\n",
      "Epoch  24 Batch  186/269   train_loss = 3.748\n",
      "Epoch  24 Batch  187/269   train_loss = 3.983\n",
      "Epoch  24 Batch  188/269   train_loss = 3.729\n",
      "Epoch  24 Batch  189/269   train_loss = 3.785\n",
      "Epoch  24 Batch  190/269   train_loss = 4.163\n",
      "Epoch  24 Batch  191/269   train_loss = 3.873\n",
      "Epoch  24 Batch  192/269   train_loss = 3.800\n",
      "Epoch  24 Batch  193/269   train_loss = 3.722\n",
      "Epoch  24 Batch  194/269   train_loss = 3.932\n",
      "Epoch  24 Batch  195/269   train_loss = 3.738\n",
      "Epoch  24 Batch  196/269   train_loss = 3.876\n",
      "Epoch  24 Batch  197/269   train_loss = 4.060\n",
      "Epoch  24 Batch  198/269   train_loss = 3.950\n",
      "Epoch  24 Batch  199/269   train_loss = 3.892\n",
      "Epoch  24 Batch  200/269   train_loss = 3.747\n",
      "Epoch  24 Batch  201/269   train_loss = 3.775\n",
      "Epoch  24 Batch  202/269   train_loss = 3.651\n",
      "Epoch  24 Batch  203/269   train_loss = 3.702\n",
      "Epoch  24 Batch  204/269   train_loss = 3.882\n",
      "Epoch  24 Batch  205/269   train_loss = 3.866\n",
      "Epoch  24 Batch  206/269   train_loss = 3.758\n",
      "Epoch  24 Batch  207/269   train_loss = 3.816\n",
      "Epoch  24 Batch  208/269   train_loss = 3.804\n",
      "Epoch  24 Batch  209/269   train_loss = 3.941\n",
      "Epoch  24 Batch  210/269   train_loss = 3.712\n",
      "Epoch  24 Batch  211/269   train_loss = 3.789\n",
      "Epoch  24 Batch  212/269   train_loss = 4.099\n",
      "Epoch  24 Batch  213/269   train_loss = 3.679\n",
      "Epoch  24 Batch  214/269   train_loss = 3.801\n",
      "Epoch  24 Batch  215/269   train_loss = 3.966\n",
      "Epoch  24 Batch  216/269   train_loss = 4.004\n",
      "Epoch  24 Batch  217/269   train_loss = 3.694\n",
      "Epoch  24 Batch  218/269   train_loss = 3.836\n",
      "Epoch  24 Batch  219/269   train_loss = 3.537\n",
      "Epoch  24 Batch  220/269   train_loss = 3.944\n",
      "Epoch  24 Batch  221/269   train_loss = 3.719\n",
      "Epoch  24 Batch  222/269   train_loss = 3.873\n",
      "Epoch  24 Batch  223/269   train_loss = 3.670\n",
      "Epoch  24 Batch  224/269   train_loss = 3.940\n",
      "Epoch  24 Batch  225/269   train_loss = 4.037\n",
      "Epoch  24 Batch  226/269   train_loss = 3.839\n",
      "Epoch  24 Batch  227/269   train_loss = 3.515\n",
      "Epoch  24 Batch  228/269   train_loss = 3.765\n",
      "Epoch  24 Batch  229/269   train_loss = 3.936\n",
      "Epoch  24 Batch  230/269   train_loss = 3.928\n",
      "Epoch  24 Batch  231/269   train_loss = 3.772\n",
      "Epoch  24 Batch  232/269   train_loss = 3.822\n",
      "Epoch  24 Batch  233/269   train_loss = 3.904\n",
      "Epoch  24 Batch  234/269   train_loss = 3.815\n",
      "Epoch  24 Batch  235/269   train_loss = 4.040\n",
      "Epoch  24 Batch  236/269   train_loss = 3.651\n",
      "Epoch  24 Batch  237/269   train_loss = 3.415\n",
      "Epoch  24 Batch  238/269   train_loss = 3.575\n",
      "Epoch  24 Batch  239/269   train_loss = 4.209\n",
      "Epoch  24 Batch  240/269   train_loss = 3.776\n",
      "Epoch  24 Batch  241/269   train_loss = 4.023\n",
      "Epoch  24 Batch  242/269   train_loss = 3.610\n",
      "Epoch  24 Batch  243/269   train_loss = 3.917\n",
      "Epoch  24 Batch  244/269   train_loss = 3.735\n",
      "Epoch  24 Batch  245/269   train_loss = 3.813\n",
      "Epoch  24 Batch  246/269   train_loss = 3.470\n",
      "Epoch  24 Batch  247/269   train_loss = 3.850\n",
      "Epoch  24 Batch  248/269   train_loss = 3.884\n",
      "Epoch  24 Batch  249/269   train_loss = 3.741\n",
      "Epoch  24 Batch  250/269   train_loss = 3.538\n",
      "Epoch  24 Batch  251/269   train_loss = 3.955\n",
      "Epoch  24 Batch  252/269   train_loss = 3.960\n",
      "Epoch  24 Batch  253/269   train_loss = 3.754\n",
      "Epoch  24 Batch  254/269   train_loss = 3.747\n",
      "Epoch  24 Batch  255/269   train_loss = 3.867\n",
      "Epoch  24 Batch  256/269   train_loss = 3.737\n",
      "Epoch  24 Batch  257/269   train_loss = 3.617\n",
      "Epoch  24 Batch  258/269   train_loss = 3.532\n",
      "Epoch  24 Batch  259/269   train_loss = 3.645\n",
      "Epoch  24 Batch  260/269   train_loss = 3.890\n",
      "Epoch  24 Batch  261/269   train_loss = 3.928\n",
      "Epoch  24 Batch  262/269   train_loss = 3.676\n",
      "Epoch  24 Batch  263/269   train_loss = 3.543\n",
      "Epoch  24 Batch  264/269   train_loss = 4.078\n",
      "Epoch  24 Batch  265/269   train_loss = 3.881\n",
      "Epoch  24 Batch  266/269   train_loss = 3.775\n",
      "Epoch  24 Batch  267/269   train_loss = 3.845\n",
      "Epoch  24 Batch  268/269   train_loss = 3.912\n",
      "Epoch  25 Batch    0/269   train_loss = 3.684\n",
      "Epoch  25 Batch    1/269   train_loss = 3.558\n",
      "Epoch  25 Batch    2/269   train_loss = 3.702\n",
      "Epoch  25 Batch    3/269   train_loss = 3.748\n",
      "Epoch  25 Batch    4/269   train_loss = 4.162\n",
      "Epoch  25 Batch    5/269   train_loss = 3.855\n",
      "Epoch  25 Batch    6/269   train_loss = 3.719\n",
      "Epoch  25 Batch    7/269   train_loss = 3.586\n",
      "Epoch  25 Batch    8/269   train_loss = 3.946\n",
      "Epoch  25 Batch    9/269   train_loss = 3.563\n",
      "Epoch  25 Batch   10/269   train_loss = 3.585\n",
      "Epoch  25 Batch   11/269   train_loss = 3.747\n",
      "Epoch  25 Batch   12/269   train_loss = 3.463\n",
      "Epoch  25 Batch   13/269   train_loss = 3.830\n",
      "Epoch  25 Batch   14/269   train_loss = 3.651\n",
      "Epoch  25 Batch   15/269   train_loss = 4.039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25 Batch   16/269   train_loss = 3.741\n",
      "Epoch  25 Batch   17/269   train_loss = 3.870\n",
      "Epoch  25 Batch   18/269   train_loss = 3.758\n",
      "Epoch  25 Batch   19/269   train_loss = 3.748\n",
      "Epoch  25 Batch   20/269   train_loss = 4.078\n",
      "Epoch  25 Batch   21/269   train_loss = 3.912\n",
      "Epoch  25 Batch   22/269   train_loss = 3.671\n",
      "Epoch  25 Batch   23/269   train_loss = 3.838\n",
      "Epoch  25 Batch   24/269   train_loss = 3.725\n",
      "Epoch  25 Batch   25/269   train_loss = 3.985\n",
      "Epoch  25 Batch   26/269   train_loss = 3.889\n",
      "Epoch  25 Batch   27/269   train_loss = 3.687\n",
      "Epoch  25 Batch   28/269   train_loss = 3.843\n",
      "Epoch  25 Batch   29/269   train_loss = 3.835\n",
      "Epoch  25 Batch   30/269   train_loss = 4.147\n",
      "Epoch  25 Batch   31/269   train_loss = 3.801\n",
      "Epoch  25 Batch   32/269   train_loss = 3.734\n",
      "Epoch  25 Batch   33/269   train_loss = 3.491\n",
      "Epoch  25 Batch   34/269   train_loss = 3.776\n",
      "Epoch  25 Batch   35/269   train_loss = 3.627\n",
      "Epoch  25 Batch   36/269   train_loss = 3.616\n",
      "Epoch  25 Batch   37/269   train_loss = 3.832\n",
      "Epoch  25 Batch   38/269   train_loss = 3.591\n",
      "Epoch  25 Batch   39/269   train_loss = 3.885\n",
      "Epoch  25 Batch   40/269   train_loss = 3.648\n",
      "Epoch  25 Batch   41/269   train_loss = 3.647\n",
      "Epoch  25 Batch   42/269   train_loss = 4.208\n",
      "Epoch  25 Batch   43/269   train_loss = 3.943\n",
      "Epoch  25 Batch   44/269   train_loss = 3.803\n",
      "Epoch  25 Batch   45/269   train_loss = 3.535\n",
      "Epoch  25 Batch   46/269   train_loss = 3.896\n",
      "Epoch  25 Batch   47/269   train_loss = 3.468\n",
      "Epoch  25 Batch   48/269   train_loss = 4.039\n",
      "Epoch  25 Batch   49/269   train_loss = 4.004\n",
      "Epoch  25 Batch   50/269   train_loss = 3.932\n",
      "Epoch  25 Batch   51/269   train_loss = 3.834\n",
      "Epoch  25 Batch   52/269   train_loss = 3.830\n",
      "Epoch  25 Batch   53/269   train_loss = 4.014\n",
      "Epoch  25 Batch   54/269   train_loss = 3.820\n",
      "Epoch  25 Batch   55/269   train_loss = 3.885\n",
      "Epoch  25 Batch   56/269   train_loss = 3.720\n",
      "Epoch  25 Batch   57/269   train_loss = 3.943\n",
      "Epoch  25 Batch   58/269   train_loss = 3.718\n",
      "Epoch  25 Batch   59/269   train_loss = 3.688\n",
      "Epoch  25 Batch   60/269   train_loss = 3.811\n",
      "Epoch  25 Batch   61/269   train_loss = 4.069\n",
      "Epoch  25 Batch   62/269   train_loss = 3.654\n",
      "Epoch  25 Batch   63/269   train_loss = 3.973\n",
      "Epoch  25 Batch   64/269   train_loss = 3.766\n",
      "Epoch  25 Batch   65/269   train_loss = 3.851\n",
      "Epoch  25 Batch   66/269   train_loss = 3.744\n",
      "Epoch  25 Batch   67/269   train_loss = 3.820\n",
      "Epoch  25 Batch   68/269   train_loss = 3.826\n",
      "Epoch  25 Batch   69/269   train_loss = 3.830\n",
      "Epoch  25 Batch   70/269   train_loss = 3.507\n",
      "Epoch  25 Batch   71/269   train_loss = 3.903\n",
      "Epoch  25 Batch   72/269   train_loss = 3.522\n",
      "Epoch  25 Batch   73/269   train_loss = 3.741\n",
      "Epoch  25 Batch   74/269   train_loss = 3.821\n",
      "Epoch  25 Batch   75/269   train_loss = 3.929\n",
      "Epoch  25 Batch   76/269   train_loss = 3.823\n",
      "Epoch  25 Batch   77/269   train_loss = 3.454\n",
      "Epoch  25 Batch   78/269   train_loss = 3.912\n",
      "Epoch  25 Batch   79/269   train_loss = 3.977\n",
      "Epoch  25 Batch   80/269   train_loss = 3.876\n",
      "Epoch  25 Batch   81/269   train_loss = 3.675\n",
      "Epoch  25 Batch   82/269   train_loss = 3.990\n",
      "Epoch  25 Batch   83/269   train_loss = 3.768\n",
      "Epoch  25 Batch   84/269   train_loss = 3.992\n",
      "Epoch  25 Batch   85/269   train_loss = 4.183\n",
      "Epoch  25 Batch   86/269   train_loss = 4.063\n",
      "Epoch  25 Batch   87/269   train_loss = 3.796\n",
      "Epoch  25 Batch   88/269   train_loss = 3.834\n",
      "Epoch  25 Batch   89/269   train_loss = 3.788\n",
      "Epoch  25 Batch   90/269   train_loss = 3.663\n",
      "Epoch  25 Batch   91/269   train_loss = 3.950\n",
      "Epoch  25 Batch   92/269   train_loss = 3.990\n",
      "Epoch  25 Batch   93/269   train_loss = 3.637\n",
      "Epoch  25 Batch   94/269   train_loss = 4.066\n",
      "Epoch  25 Batch   95/269   train_loss = 3.608\n",
      "Epoch  25 Batch   96/269   train_loss = 3.688\n",
      "Epoch  25 Batch   97/269   train_loss = 3.787\n",
      "Epoch  25 Batch   98/269   train_loss = 3.951\n",
      "Epoch  25 Batch   99/269   train_loss = 3.821\n",
      "Epoch  25 Batch  100/269   train_loss = 3.736\n",
      "Epoch  25 Batch  101/269   train_loss = 3.930\n",
      "Epoch  25 Batch  102/269   train_loss = 3.657\n",
      "Epoch  25 Batch  103/269   train_loss = 3.767\n",
      "Epoch  25 Batch  104/269   train_loss = 3.669\n",
      "Epoch  25 Batch  105/269   train_loss = 3.919\n",
      "Epoch  25 Batch  106/269   train_loss = 4.040\n",
      "Epoch  25 Batch  107/269   train_loss = 4.014\n",
      "Epoch  25 Batch  108/269   train_loss = 3.840\n",
      "Epoch  25 Batch  109/269   train_loss = 3.900\n",
      "Epoch  25 Batch  110/269   train_loss = 3.831\n",
      "Epoch  25 Batch  111/269   train_loss = 3.953\n",
      "Epoch  25 Batch  112/269   train_loss = 3.949\n",
      "Epoch  25 Batch  113/269   train_loss = 3.816\n",
      "Epoch  25 Batch  114/269   train_loss = 3.753\n",
      "Epoch  25 Batch  115/269   train_loss = 3.672\n",
      "Epoch  25 Batch  116/269   train_loss = 3.667\n",
      "Epoch  25 Batch  117/269   train_loss = 3.661\n",
      "Epoch  25 Batch  118/269   train_loss = 3.981\n",
      "Epoch  25 Batch  119/269   train_loss = 3.658\n",
      "Epoch  25 Batch  120/269   train_loss = 3.875\n",
      "Epoch  25 Batch  121/269   train_loss = 3.724\n",
      "Epoch  25 Batch  122/269   train_loss = 3.859\n",
      "Epoch  25 Batch  123/269   train_loss = 3.600\n",
      "Epoch  25 Batch  124/269   train_loss = 3.986\n",
      "Epoch  25 Batch  125/269   train_loss = 3.601\n",
      "Epoch  25 Batch  126/269   train_loss = 3.513\n",
      "Epoch  25 Batch  127/269   train_loss = 3.790\n",
      "Epoch  25 Batch  128/269   train_loss = 3.783\n",
      "Epoch  25 Batch  129/269   train_loss = 3.735\n",
      "Epoch  25 Batch  130/269   train_loss = 3.887\n",
      "Epoch  25 Batch  131/269   train_loss = 3.539\n",
      "Epoch  25 Batch  132/269   train_loss = 3.838\n",
      "Epoch  25 Batch  133/269   train_loss = 3.577\n",
      "Epoch  25 Batch  134/269   train_loss = 3.939\n",
      "Epoch  25 Batch  135/269   train_loss = 3.594\n",
      "Epoch  25 Batch  136/269   train_loss = 3.546\n",
      "Epoch  25 Batch  137/269   train_loss = 3.875\n",
      "Epoch  25 Batch  138/269   train_loss = 3.647\n",
      "Epoch  25 Batch  139/269   train_loss = 3.816\n",
      "Epoch  25 Batch  140/269   train_loss = 3.866\n",
      "Epoch  25 Batch  141/269   train_loss = 4.126\n",
      "Epoch  25 Batch  142/269   train_loss = 3.624\n",
      "Epoch  25 Batch  143/269   train_loss = 3.811\n",
      "Epoch  25 Batch  144/269   train_loss = 3.924\n",
      "Epoch  25 Batch  145/269   train_loss = 3.771\n",
      "Epoch  25 Batch  146/269   train_loss = 3.977\n",
      "Epoch  25 Batch  147/269   train_loss = 3.574\n",
      "Epoch  25 Batch  148/269   train_loss = 3.932\n",
      "Epoch  25 Batch  149/269   train_loss = 3.733\n",
      "Epoch  25 Batch  150/269   train_loss = 3.842\n",
      "Epoch  25 Batch  151/269   train_loss = 3.839\n",
      "Epoch  25 Batch  152/269   train_loss = 3.798\n",
      "Epoch  25 Batch  153/269   train_loss = 3.893\n",
      "Epoch  25 Batch  154/269   train_loss = 3.740\n",
      "Epoch  25 Batch  155/269   train_loss = 3.700\n",
      "Epoch  25 Batch  156/269   train_loss = 4.170\n",
      "Epoch  25 Batch  157/269   train_loss = 3.953\n",
      "Epoch  25 Batch  158/269   train_loss = 3.898\n",
      "Epoch  25 Batch  159/269   train_loss = 3.881\n",
      "Epoch  25 Batch  160/269   train_loss = 3.729\n",
      "Epoch  25 Batch  161/269   train_loss = 3.897\n",
      "Epoch  25 Batch  162/269   train_loss = 3.844\n",
      "Epoch  25 Batch  163/269   train_loss = 3.873\n",
      "Epoch  25 Batch  164/269   train_loss = 4.018\n",
      "Epoch  25 Batch  165/269   train_loss = 3.913\n",
      "Epoch  25 Batch  166/269   train_loss = 3.887\n",
      "Epoch  25 Batch  167/269   train_loss = 3.703\n",
      "Epoch  25 Batch  168/269   train_loss = 3.760\n",
      "Epoch  25 Batch  169/269   train_loss = 4.038\n",
      "Epoch  25 Batch  170/269   train_loss = 3.626\n",
      "Epoch  25 Batch  171/269   train_loss = 3.877\n",
      "Epoch  25 Batch  172/269   train_loss = 3.592\n",
      "Epoch  25 Batch  173/269   train_loss = 3.853\n",
      "Epoch  25 Batch  174/269   train_loss = 3.943\n",
      "Epoch  25 Batch  175/269   train_loss = 3.860\n",
      "Epoch  25 Batch  176/269   train_loss = 3.757\n",
      "Epoch  25 Batch  177/269   train_loss = 3.800\n",
      "Epoch  25 Batch  178/269   train_loss = 4.101\n",
      "Epoch  25 Batch  179/269   train_loss = 3.858\n",
      "Epoch  25 Batch  180/269   train_loss = 3.714\n",
      "Epoch  25 Batch  181/269   train_loss = 4.006\n",
      "Epoch  25 Batch  182/269   train_loss = 3.728\n",
      "Epoch  25 Batch  183/269   train_loss = 3.779\n",
      "Epoch  25 Batch  184/269   train_loss = 3.817\n",
      "Epoch  25 Batch  185/269   train_loss = 3.888\n",
      "Epoch  25 Batch  186/269   train_loss = 3.734\n",
      "Epoch  25 Batch  187/269   train_loss = 3.974\n",
      "Epoch  25 Batch  188/269   train_loss = 3.701\n",
      "Epoch  25 Batch  189/269   train_loss = 3.743\n",
      "Epoch  25 Batch  190/269   train_loss = 4.151\n",
      "Epoch  25 Batch  191/269   train_loss = 3.880\n",
      "Epoch  25 Batch  192/269   train_loss = 3.765\n",
      "Epoch  25 Batch  193/269   train_loss = 3.737\n",
      "Epoch  25 Batch  194/269   train_loss = 3.901\n",
      "Epoch  25 Batch  195/269   train_loss = 3.716\n",
      "Epoch  25 Batch  196/269   train_loss = 3.879\n",
      "Epoch  25 Batch  197/269   train_loss = 4.051\n",
      "Epoch  25 Batch  198/269   train_loss = 3.916\n",
      "Epoch  25 Batch  199/269   train_loss = 3.861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25 Batch  200/269   train_loss = 3.763\n",
      "Epoch  25 Batch  201/269   train_loss = 3.772\n",
      "Epoch  25 Batch  202/269   train_loss = 3.664\n",
      "Epoch  25 Batch  203/269   train_loss = 3.691\n",
      "Epoch  25 Batch  204/269   train_loss = 3.888\n",
      "Epoch  25 Batch  205/269   train_loss = 3.862\n",
      "Epoch  25 Batch  206/269   train_loss = 3.742\n",
      "Epoch  25 Batch  207/269   train_loss = 3.837\n",
      "Epoch  25 Batch  208/269   train_loss = 3.793\n",
      "Epoch  25 Batch  209/269   train_loss = 3.962\n",
      "Epoch  25 Batch  210/269   train_loss = 3.690\n",
      "Epoch  25 Batch  211/269   train_loss = 3.804\n",
      "Epoch  25 Batch  212/269   train_loss = 4.068\n",
      "Epoch  25 Batch  213/269   train_loss = 3.691\n",
      "Epoch  25 Batch  214/269   train_loss = 3.797\n",
      "Epoch  25 Batch  215/269   train_loss = 3.973\n",
      "Epoch  25 Batch  216/269   train_loss = 4.012\n",
      "Epoch  25 Batch  217/269   train_loss = 3.667\n",
      "Epoch  25 Batch  218/269   train_loss = 3.854\n",
      "Epoch  25 Batch  219/269   train_loss = 3.520\n",
      "Epoch  25 Batch  220/269   train_loss = 3.931\n",
      "Epoch  25 Batch  221/269   train_loss = 3.729\n",
      "Epoch  25 Batch  222/269   train_loss = 3.872\n",
      "Epoch  25 Batch  223/269   train_loss = 3.665\n",
      "Epoch  25 Batch  224/269   train_loss = 3.907\n",
      "Epoch  25 Batch  225/269   train_loss = 4.023\n",
      "Epoch  25 Batch  226/269   train_loss = 3.838\n",
      "Epoch  25 Batch  227/269   train_loss = 3.518\n",
      "Epoch  25 Batch  228/269   train_loss = 3.744\n",
      "Epoch  25 Batch  229/269   train_loss = 3.919\n",
      "Epoch  25 Batch  230/269   train_loss = 3.950\n",
      "Epoch  25 Batch  231/269   train_loss = 3.778\n",
      "Epoch  25 Batch  232/269   train_loss = 3.794\n",
      "Epoch  25 Batch  233/269   train_loss = 3.902\n",
      "Epoch  25 Batch  234/269   train_loss = 3.822\n",
      "Epoch  25 Batch  235/269   train_loss = 4.047\n",
      "Epoch  25 Batch  236/269   train_loss = 3.663\n",
      "Epoch  25 Batch  237/269   train_loss = 3.403\n",
      "Epoch  25 Batch  238/269   train_loss = 3.586\n",
      "Epoch  25 Batch  239/269   train_loss = 4.204\n",
      "Epoch  25 Batch  240/269   train_loss = 3.760\n",
      "Epoch  25 Batch  241/269   train_loss = 4.005\n",
      "Epoch  25 Batch  242/269   train_loss = 3.608\n",
      "Epoch  25 Batch  243/269   train_loss = 3.933\n",
      "Epoch  25 Batch  244/269   train_loss = 3.713\n",
      "Epoch  25 Batch  245/269   train_loss = 3.826\n",
      "Epoch  25 Batch  246/269   train_loss = 3.432\n",
      "Epoch  25 Batch  247/269   train_loss = 3.855\n",
      "Epoch  25 Batch  248/269   train_loss = 3.880\n",
      "Epoch  25 Batch  249/269   train_loss = 3.736\n",
      "Epoch  25 Batch  250/269   train_loss = 3.542\n",
      "Epoch  25 Batch  251/269   train_loss = 3.960\n",
      "Epoch  25 Batch  252/269   train_loss = 3.966\n",
      "Epoch  25 Batch  253/269   train_loss = 3.736\n",
      "Epoch  25 Batch  254/269   train_loss = 3.701\n",
      "Epoch  25 Batch  255/269   train_loss = 3.857\n",
      "Epoch  25 Batch  256/269   train_loss = 3.717\n",
      "Epoch  25 Batch  257/269   train_loss = 3.599\n",
      "Epoch  25 Batch  258/269   train_loss = 3.530\n",
      "Epoch  25 Batch  259/269   train_loss = 3.602\n",
      "Epoch  25 Batch  260/269   train_loss = 3.878\n",
      "Epoch  25 Batch  261/269   train_loss = 3.951\n",
      "Epoch  25 Batch  262/269   train_loss = 3.681\n",
      "Epoch  25 Batch  263/269   train_loss = 3.545\n",
      "Epoch  25 Batch  264/269   train_loss = 4.075\n",
      "Epoch  25 Batch  265/269   train_loss = 3.872\n",
      "Epoch  25 Batch  266/269   train_loss = 3.778\n",
      "Epoch  25 Batch  267/269   train_loss = 3.849\n",
      "Epoch  25 Batch  268/269   train_loss = 3.918\n",
      "Epoch  26 Batch    0/269   train_loss = 3.704\n",
      "Epoch  26 Batch    1/269   train_loss = 3.574\n",
      "Epoch  26 Batch    2/269   train_loss = 3.723\n",
      "Epoch  26 Batch    3/269   train_loss = 3.693\n",
      "Epoch  26 Batch    4/269   train_loss = 4.138\n",
      "Epoch  26 Batch    5/269   train_loss = 3.850\n",
      "Epoch  26 Batch    6/269   train_loss = 3.719\n",
      "Epoch  26 Batch    7/269   train_loss = 3.548\n",
      "Epoch  26 Batch    8/269   train_loss = 3.980\n",
      "Epoch  26 Batch    9/269   train_loss = 3.571\n",
      "Epoch  26 Batch   10/269   train_loss = 3.596\n",
      "Epoch  26 Batch   11/269   train_loss = 3.751\n",
      "Epoch  26 Batch   12/269   train_loss = 3.463\n",
      "Epoch  26 Batch   13/269   train_loss = 3.816\n",
      "Epoch  26 Batch   14/269   train_loss = 3.650\n",
      "Epoch  26 Batch   15/269   train_loss = 4.027\n",
      "Epoch  26 Batch   16/269   train_loss = 3.761\n",
      "Epoch  26 Batch   17/269   train_loss = 3.874\n",
      "Epoch  26 Batch   18/269   train_loss = 3.728\n",
      "Epoch  26 Batch   19/269   train_loss = 3.741\n",
      "Epoch  26 Batch   20/269   train_loss = 4.071\n",
      "Epoch  26 Batch   21/269   train_loss = 3.910\n",
      "Epoch  26 Batch   22/269   train_loss = 3.682\n",
      "Epoch  26 Batch   23/269   train_loss = 3.844\n",
      "Epoch  26 Batch   24/269   train_loss = 3.727\n",
      "Epoch  26 Batch   25/269   train_loss = 3.995\n",
      "Epoch  26 Batch   26/269   train_loss = 3.872\n",
      "Epoch  26 Batch   27/269   train_loss = 3.655\n",
      "Epoch  26 Batch   28/269   train_loss = 3.863\n",
      "Epoch  26 Batch   29/269   train_loss = 3.856\n",
      "Epoch  26 Batch   30/269   train_loss = 4.131\n",
      "Epoch  26 Batch   31/269   train_loss = 3.798\n",
      "Epoch  26 Batch   32/269   train_loss = 3.708\n",
      "Epoch  26 Batch   33/269   train_loss = 3.474\n",
      "Epoch  26 Batch   34/269   train_loss = 3.763\n",
      "Epoch  26 Batch   35/269   train_loss = 3.608\n",
      "Epoch  26 Batch   36/269   train_loss = 3.594\n",
      "Epoch  26 Batch   37/269   train_loss = 3.806\n",
      "Epoch  26 Batch   38/269   train_loss = 3.576\n",
      "Epoch  26 Batch   39/269   train_loss = 3.892\n",
      "Epoch  26 Batch   40/269   train_loss = 3.646\n",
      "Epoch  26 Batch   41/269   train_loss = 3.655\n",
      "Epoch  26 Batch   42/269   train_loss = 4.186\n",
      "Epoch  26 Batch   43/269   train_loss = 3.932\n",
      "Epoch  26 Batch   44/269   train_loss = 3.797\n",
      "Epoch  26 Batch   45/269   train_loss = 3.528\n",
      "Epoch  26 Batch   46/269   train_loss = 3.885\n",
      "Epoch  26 Batch   47/269   train_loss = 3.456\n",
      "Epoch  26 Batch   48/269   train_loss = 4.038\n",
      "Epoch  26 Batch   49/269   train_loss = 3.995\n",
      "Epoch  26 Batch   50/269   train_loss = 3.919\n",
      "Epoch  26 Batch   51/269   train_loss = 3.848\n",
      "Epoch  26 Batch   52/269   train_loss = 3.849\n",
      "Epoch  26 Batch   53/269   train_loss = 4.020\n",
      "Epoch  26 Batch   54/269   train_loss = 3.792\n",
      "Epoch  26 Batch   55/269   train_loss = 3.937\n",
      "Epoch  26 Batch   56/269   train_loss = 3.728\n",
      "Epoch  26 Batch   57/269   train_loss = 3.932\n",
      "Epoch  26 Batch   58/269   train_loss = 3.701\n",
      "Epoch  26 Batch   59/269   train_loss = 3.678\n",
      "Epoch  26 Batch   60/269   train_loss = 3.813\n",
      "Epoch  26 Batch   61/269   train_loss = 4.090\n",
      "Epoch  26 Batch   62/269   train_loss = 3.654\n",
      "Epoch  26 Batch   63/269   train_loss = 3.950\n",
      "Epoch  26 Batch   64/269   train_loss = 3.751\n",
      "Epoch  26 Batch   65/269   train_loss = 3.863\n",
      "Epoch  26 Batch   66/269   train_loss = 3.725\n",
      "Epoch  26 Batch   67/269   train_loss = 3.832\n",
      "Epoch  26 Batch   68/269   train_loss = 3.831\n",
      "Epoch  26 Batch   69/269   train_loss = 3.850\n",
      "Epoch  26 Batch   70/269   train_loss = 3.486\n",
      "Epoch  26 Batch   71/269   train_loss = 3.910\n",
      "Epoch  26 Batch   72/269   train_loss = 3.550\n",
      "Epoch  26 Batch   73/269   train_loss = 3.742\n",
      "Epoch  26 Batch   74/269   train_loss = 3.811\n",
      "Epoch  26 Batch   75/269   train_loss = 3.938\n",
      "Epoch  26 Batch   76/269   train_loss = 3.823\n",
      "Epoch  26 Batch   77/269   train_loss = 3.486\n",
      "Epoch  26 Batch   78/269   train_loss = 3.889\n",
      "Epoch  26 Batch   79/269   train_loss = 3.974\n",
      "Epoch  26 Batch   80/269   train_loss = 3.872\n",
      "Epoch  26 Batch   81/269   train_loss = 3.648\n",
      "Epoch  26 Batch   82/269   train_loss = 3.990\n",
      "Epoch  26 Batch   83/269   train_loss = 3.769\n",
      "Epoch  26 Batch   84/269   train_loss = 3.995\n",
      "Epoch  26 Batch   85/269   train_loss = 4.181\n",
      "Epoch  26 Batch   86/269   train_loss = 4.079\n",
      "Epoch  26 Batch   87/269   train_loss = 3.811\n",
      "Epoch  26 Batch   88/269   train_loss = 3.832\n",
      "Epoch  26 Batch   89/269   train_loss = 3.775\n",
      "Epoch  26 Batch   90/269   train_loss = 3.670\n",
      "Epoch  26 Batch   91/269   train_loss = 3.963\n",
      "Epoch  26 Batch   92/269   train_loss = 3.977\n",
      "Epoch  26 Batch   93/269   train_loss = 3.638\n",
      "Epoch  26 Batch   94/269   train_loss = 4.082\n",
      "Epoch  26 Batch   95/269   train_loss = 3.598\n",
      "Epoch  26 Batch   96/269   train_loss = 3.677\n",
      "Epoch  26 Batch   97/269   train_loss = 3.796\n",
      "Epoch  26 Batch   98/269   train_loss = 3.918\n",
      "Epoch  26 Batch   99/269   train_loss = 3.820\n",
      "Epoch  26 Batch  100/269   train_loss = 3.762\n",
      "Epoch  26 Batch  101/269   train_loss = 3.919\n",
      "Epoch  26 Batch  102/269   train_loss = 3.626\n",
      "Epoch  26 Batch  103/269   train_loss = 3.767\n",
      "Epoch  26 Batch  104/269   train_loss = 3.664\n",
      "Epoch  26 Batch  105/269   train_loss = 3.919\n",
      "Epoch  26 Batch  106/269   train_loss = 4.044\n",
      "Epoch  26 Batch  107/269   train_loss = 4.043\n",
      "Epoch  26 Batch  108/269   train_loss = 3.846\n",
      "Epoch  26 Batch  109/269   train_loss = 3.874\n",
      "Epoch  26 Batch  110/269   train_loss = 3.827\n",
      "Epoch  26 Batch  111/269   train_loss = 3.957\n",
      "Epoch  26 Batch  112/269   train_loss = 3.953\n",
      "Epoch  26 Batch  113/269   train_loss = 3.775\n",
      "Epoch  26 Batch  114/269   train_loss = 3.817\n",
      "Epoch  26 Batch  115/269   train_loss = 3.672\n",
      "Epoch  26 Batch  116/269   train_loss = 3.640\n",
      "Epoch  26 Batch  117/269   train_loss = 3.659\n",
      "Epoch  26 Batch  118/269   train_loss = 3.996\n",
      "Epoch  26 Batch  119/269   train_loss = 3.630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  26 Batch  120/269   train_loss = 3.880\n",
      "Epoch  26 Batch  121/269   train_loss = 3.727\n",
      "Epoch  26 Batch  122/269   train_loss = 3.838\n",
      "Epoch  26 Batch  123/269   train_loss = 3.595\n",
      "Epoch  26 Batch  124/269   train_loss = 3.966\n",
      "Epoch  26 Batch  125/269   train_loss = 3.612\n",
      "Epoch  26 Batch  126/269   train_loss = 3.483\n",
      "Epoch  26 Batch  127/269   train_loss = 3.784\n",
      "Epoch  26 Batch  128/269   train_loss = 3.753\n",
      "Epoch  26 Batch  129/269   train_loss = 3.702\n",
      "Epoch  26 Batch  130/269   train_loss = 3.905\n",
      "Epoch  26 Batch  131/269   train_loss = 3.529\n",
      "Epoch  26 Batch  132/269   train_loss = 3.845\n",
      "Epoch  26 Batch  133/269   train_loss = 3.560\n",
      "Epoch  26 Batch  134/269   train_loss = 3.952\n",
      "Epoch  26 Batch  135/269   train_loss = 3.612\n",
      "Epoch  26 Batch  136/269   train_loss = 3.533\n",
      "Epoch  26 Batch  137/269   train_loss = 3.859\n",
      "Epoch  26 Batch  138/269   train_loss = 3.638\n",
      "Epoch  26 Batch  139/269   train_loss = 3.801\n",
      "Epoch  26 Batch  140/269   train_loss = 3.847\n",
      "Epoch  26 Batch  141/269   train_loss = 4.129\n",
      "Epoch  26 Batch  142/269   train_loss = 3.607\n",
      "Epoch  26 Batch  143/269   train_loss = 3.802\n",
      "Epoch  26 Batch  144/269   train_loss = 3.891\n",
      "Epoch  26 Batch  145/269   train_loss = 3.754\n",
      "Epoch  26 Batch  146/269   train_loss = 3.961\n",
      "Epoch  26 Batch  147/269   train_loss = 3.566\n",
      "Epoch  26 Batch  148/269   train_loss = 3.922\n",
      "Epoch  26 Batch  149/269   train_loss = 3.739\n",
      "Epoch  26 Batch  150/269   train_loss = 3.845\n",
      "Epoch  26 Batch  151/269   train_loss = 3.837\n",
      "Epoch  26 Batch  152/269   train_loss = 3.786\n",
      "Epoch  26 Batch  153/269   train_loss = 3.848\n",
      "Epoch  26 Batch  154/269   train_loss = 3.702\n",
      "Epoch  26 Batch  155/269   train_loss = 3.687\n",
      "Epoch  26 Batch  156/269   train_loss = 4.173\n",
      "Epoch  26 Batch  157/269   train_loss = 3.958\n",
      "Epoch  26 Batch  158/269   train_loss = 3.900\n",
      "Epoch  26 Batch  159/269   train_loss = 3.881\n",
      "Epoch  26 Batch  160/269   train_loss = 3.736\n",
      "Epoch  26 Batch  161/269   train_loss = 3.904\n",
      "Epoch  26 Batch  162/269   train_loss = 3.851\n",
      "Epoch  26 Batch  163/269   train_loss = 3.850\n",
      "Epoch  26 Batch  164/269   train_loss = 3.973\n",
      "Epoch  26 Batch  165/269   train_loss = 3.911\n",
      "Epoch  26 Batch  166/269   train_loss = 3.870\n",
      "Epoch  26 Batch  167/269   train_loss = 3.687\n",
      "Epoch  26 Batch  168/269   train_loss = 3.804\n",
      "Epoch  26 Batch  169/269   train_loss = 4.056\n",
      "Epoch  26 Batch  170/269   train_loss = 3.644\n",
      "Epoch  26 Batch  171/269   train_loss = 3.897\n",
      "Epoch  26 Batch  172/269   train_loss = 3.629\n",
      "Epoch  26 Batch  173/269   train_loss = 3.833\n",
      "Epoch  26 Batch  174/269   train_loss = 3.942\n",
      "Epoch  26 Batch  175/269   train_loss = 3.851\n",
      "Epoch  26 Batch  176/269   train_loss = 3.763\n",
      "Epoch  26 Batch  177/269   train_loss = 3.812\n",
      "Epoch  26 Batch  178/269   train_loss = 4.087\n",
      "Epoch  26 Batch  179/269   train_loss = 3.856\n",
      "Epoch  26 Batch  180/269   train_loss = 3.720\n",
      "Epoch  26 Batch  181/269   train_loss = 3.937\n",
      "Epoch  26 Batch  182/269   train_loss = 3.729\n",
      "Epoch  26 Batch  183/269   train_loss = 3.754\n",
      "Epoch  26 Batch  184/269   train_loss = 3.793\n",
      "Epoch  26 Batch  185/269   train_loss = 3.870\n",
      "Epoch  26 Batch  186/269   train_loss = 3.737\n",
      "Epoch  26 Batch  187/269   train_loss = 3.971\n",
      "Epoch  26 Batch  188/269   train_loss = 3.704\n",
      "Epoch  26 Batch  189/269   train_loss = 3.765\n",
      "Epoch  26 Batch  190/269   train_loss = 4.152\n",
      "Epoch  26 Batch  191/269   train_loss = 3.884\n",
      "Epoch  26 Batch  192/269   train_loss = 3.753\n",
      "Epoch  26 Batch  193/269   train_loss = 3.713\n",
      "Epoch  26 Batch  194/269   train_loss = 3.897\n",
      "Epoch  26 Batch  195/269   train_loss = 3.708\n",
      "Epoch  26 Batch  196/269   train_loss = 3.852\n",
      "Epoch  26 Batch  197/269   train_loss = 4.048\n",
      "Epoch  26 Batch  198/269   train_loss = 3.913\n",
      "Epoch  26 Batch  199/269   train_loss = 3.900\n",
      "Epoch  26 Batch  200/269   train_loss = 3.743\n",
      "Epoch  26 Batch  201/269   train_loss = 3.744\n",
      "Epoch  26 Batch  202/269   train_loss = 3.672\n",
      "Epoch  26 Batch  203/269   train_loss = 3.684\n",
      "Epoch  26 Batch  204/269   train_loss = 3.885\n",
      "Epoch  26 Batch  205/269   train_loss = 3.833\n",
      "Epoch  26 Batch  206/269   train_loss = 3.771\n",
      "Epoch  26 Batch  207/269   train_loss = 3.796\n",
      "Epoch  26 Batch  208/269   train_loss = 3.807\n",
      "Epoch  26 Batch  209/269   train_loss = 3.937\n",
      "Epoch  26 Batch  210/269   train_loss = 3.678\n",
      "Epoch  26 Batch  211/269   train_loss = 3.773\n",
      "Epoch  26 Batch  212/269   train_loss = 4.055\n",
      "Epoch  26 Batch  213/269   train_loss = 3.716\n",
      "Epoch  26 Batch  214/269   train_loss = 3.801\n",
      "Epoch  26 Batch  215/269   train_loss = 3.966\n",
      "Epoch  26 Batch  216/269   train_loss = 4.002\n",
      "Epoch  26 Batch  217/269   train_loss = 3.650\n",
      "Epoch  26 Batch  218/269   train_loss = 3.852\n",
      "Epoch  26 Batch  219/269   train_loss = 3.506\n",
      "Epoch  26 Batch  220/269   train_loss = 3.934\n",
      "Epoch  26 Batch  221/269   train_loss = 3.718\n",
      "Epoch  26 Batch  222/269   train_loss = 3.860\n",
      "Epoch  26 Batch  223/269   train_loss = 3.651\n",
      "Epoch  26 Batch  224/269   train_loss = 3.887\n",
      "Epoch  26 Batch  225/269   train_loss = 3.996\n",
      "Epoch  26 Batch  226/269   train_loss = 3.848\n",
      "Epoch  26 Batch  227/269   train_loss = 3.521\n",
      "Epoch  26 Batch  228/269   train_loss = 3.765\n",
      "Epoch  26 Batch  229/269   train_loss = 3.920\n",
      "Epoch  26 Batch  230/269   train_loss = 3.922\n",
      "Epoch  26 Batch  231/269   train_loss = 3.777\n",
      "Epoch  26 Batch  232/269   train_loss = 3.790\n",
      "Epoch  26 Batch  233/269   train_loss = 3.860\n",
      "Epoch  26 Batch  234/269   train_loss = 3.787\n",
      "Epoch  26 Batch  235/269   train_loss = 4.038\n",
      "Epoch  26 Batch  236/269   train_loss = 3.658\n",
      "Epoch  26 Batch  237/269   train_loss = 3.377\n",
      "Epoch  26 Batch  238/269   train_loss = 3.596\n",
      "Epoch  26 Batch  239/269   train_loss = 4.198\n",
      "Epoch  26 Batch  240/269   train_loss = 3.773\n",
      "Epoch  26 Batch  241/269   train_loss = 4.003\n",
      "Epoch  26 Batch  242/269   train_loss = 3.625\n",
      "Epoch  26 Batch  243/269   train_loss = 3.896\n",
      "Epoch  26 Batch  244/269   train_loss = 3.696\n",
      "Epoch  26 Batch  245/269   train_loss = 3.798\n",
      "Epoch  26 Batch  246/269   train_loss = 3.447\n",
      "Epoch  26 Batch  247/269   train_loss = 3.864\n",
      "Epoch  26 Batch  248/269   train_loss = 3.859\n",
      "Epoch  26 Batch  249/269   train_loss = 3.714\n",
      "Epoch  26 Batch  250/269   train_loss = 3.528\n",
      "Epoch  26 Batch  251/269   train_loss = 3.966\n",
      "Epoch  26 Batch  252/269   train_loss = 3.944\n",
      "Epoch  26 Batch  253/269   train_loss = 3.723\n",
      "Epoch  26 Batch  254/269   train_loss = 3.712\n",
      "Epoch  26 Batch  255/269   train_loss = 3.868\n",
      "Epoch  26 Batch  256/269   train_loss = 3.708\n",
      "Epoch  26 Batch  257/269   train_loss = 3.594\n",
      "Epoch  26 Batch  258/269   train_loss = 3.530\n",
      "Epoch  26 Batch  259/269   train_loss = 3.587\n",
      "Epoch  26 Batch  260/269   train_loss = 3.876\n",
      "Epoch  26 Batch  261/269   train_loss = 3.952\n",
      "Epoch  26 Batch  262/269   train_loss = 3.683\n",
      "Epoch  26 Batch  263/269   train_loss = 3.534\n",
      "Epoch  26 Batch  264/269   train_loss = 4.066\n",
      "Epoch  26 Batch  265/269   train_loss = 3.857\n",
      "Epoch  26 Batch  266/269   train_loss = 3.762\n",
      "Epoch  26 Batch  267/269   train_loss = 3.862\n",
      "Epoch  26 Batch  268/269   train_loss = 3.895\n",
      "Epoch  27 Batch    0/269   train_loss = 3.709\n",
      "Epoch  27 Batch    1/269   train_loss = 3.557\n",
      "Epoch  27 Batch    2/269   train_loss = 3.710\n",
      "Epoch  27 Batch    3/269   train_loss = 3.701\n",
      "Epoch  27 Batch    4/269   train_loss = 4.136\n",
      "Epoch  27 Batch    5/269   train_loss = 3.847\n",
      "Epoch  27 Batch    6/269   train_loss = 3.729\n",
      "Epoch  27 Batch    7/269   train_loss = 3.530\n",
      "Epoch  27 Batch    8/269   train_loss = 3.952\n",
      "Epoch  27 Batch    9/269   train_loss = 3.570\n",
      "Epoch  27 Batch   10/269   train_loss = 3.580\n",
      "Epoch  27 Batch   11/269   train_loss = 3.749\n",
      "Epoch  27 Batch   12/269   train_loss = 3.452\n",
      "Epoch  27 Batch   13/269   train_loss = 3.819\n",
      "Epoch  27 Batch   14/269   train_loss = 3.649\n",
      "Epoch  27 Batch   15/269   train_loss = 4.036\n",
      "Epoch  27 Batch   16/269   train_loss = 3.757\n",
      "Epoch  27 Batch   17/269   train_loss = 3.842\n",
      "Epoch  27 Batch   18/269   train_loss = 3.708\n",
      "Epoch  27 Batch   19/269   train_loss = 3.757\n",
      "Epoch  27 Batch   20/269   train_loss = 4.072\n",
      "Epoch  27 Batch   21/269   train_loss = 3.926\n",
      "Epoch  27 Batch   22/269   train_loss = 3.687\n",
      "Epoch  27 Batch   23/269   train_loss = 3.840\n",
      "Epoch  27 Batch   24/269   train_loss = 3.709\n",
      "Epoch  27 Batch   25/269   train_loss = 4.018\n",
      "Epoch  27 Batch   26/269   train_loss = 3.861\n",
      "Epoch  27 Batch   27/269   train_loss = 3.625\n",
      "Epoch  27 Batch   28/269   train_loss = 3.840\n",
      "Epoch  27 Batch   29/269   train_loss = 3.852\n",
      "Epoch  27 Batch   30/269   train_loss = 4.127\n",
      "Epoch  27 Batch   31/269   train_loss = 3.799\n",
      "Epoch  27 Batch   32/269   train_loss = 3.695\n",
      "Epoch  27 Batch   33/269   train_loss = 3.453\n",
      "Epoch  27 Batch   34/269   train_loss = 3.739\n",
      "Epoch  27 Batch   35/269   train_loss = 3.599\n",
      "Epoch  27 Batch   36/269   train_loss = 3.605\n",
      "Epoch  27 Batch   37/269   train_loss = 3.796\n",
      "Epoch  27 Batch   38/269   train_loss = 3.574\n",
      "Epoch  27 Batch   39/269   train_loss = 3.871\n",
      "Epoch  27 Batch   40/269   train_loss = 3.620\n",
      "Epoch  27 Batch   41/269   train_loss = 3.626\n",
      "Epoch  27 Batch   42/269   train_loss = 4.179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27 Batch   43/269   train_loss = 3.949\n",
      "Epoch  27 Batch   44/269   train_loss = 3.811\n",
      "Epoch  27 Batch   45/269   train_loss = 3.507\n",
      "Epoch  27 Batch   46/269   train_loss = 3.854\n",
      "Epoch  27 Batch   47/269   train_loss = 3.462\n",
      "Epoch  27 Batch   48/269   train_loss = 3.999\n",
      "Epoch  27 Batch   49/269   train_loss = 3.971\n",
      "Epoch  27 Batch   50/269   train_loss = 3.932\n",
      "Epoch  27 Batch   51/269   train_loss = 3.824\n",
      "Epoch  27 Batch   52/269   train_loss = 3.840\n",
      "Epoch  27 Batch   53/269   train_loss = 4.006\n",
      "Epoch  27 Batch   54/269   train_loss = 3.798\n",
      "Epoch  27 Batch   55/269   train_loss = 3.893\n",
      "Epoch  27 Batch   56/269   train_loss = 3.720\n",
      "Epoch  27 Batch   57/269   train_loss = 3.938\n",
      "Epoch  27 Batch   58/269   train_loss = 3.686\n",
      "Epoch  27 Batch   59/269   train_loss = 3.677\n",
      "Epoch  27 Batch   60/269   train_loss = 3.805\n",
      "Epoch  27 Batch   61/269   train_loss = 4.070\n",
      "Epoch  27 Batch   62/269   train_loss = 3.653\n",
      "Epoch  27 Batch   63/269   train_loss = 3.954\n",
      "Epoch  27 Batch   64/269   train_loss = 3.718\n",
      "Epoch  27 Batch   65/269   train_loss = 3.850\n",
      "Epoch  27 Batch   66/269   train_loss = 3.727\n",
      "Epoch  27 Batch   67/269   train_loss = 3.816\n",
      "Epoch  27 Batch   68/269   train_loss = 3.826\n",
      "Epoch  27 Batch   69/269   train_loss = 3.828\n",
      "Epoch  27 Batch   70/269   train_loss = 3.461\n",
      "Epoch  27 Batch   71/269   train_loss = 3.907\n",
      "Epoch  27 Batch   72/269   train_loss = 3.548\n",
      "Epoch  27 Batch   73/269   train_loss = 3.752\n",
      "Epoch  27 Batch   74/269   train_loss = 3.784\n",
      "Epoch  27 Batch   75/269   train_loss = 3.925\n",
      "Epoch  27 Batch   76/269   train_loss = 3.798\n",
      "Epoch  27 Batch   77/269   train_loss = 3.466\n",
      "Epoch  27 Batch   78/269   train_loss = 3.884\n",
      "Epoch  27 Batch   79/269   train_loss = 3.968\n",
      "Epoch  27 Batch   80/269   train_loss = 3.886\n",
      "Epoch  27 Batch   81/269   train_loss = 3.669\n",
      "Epoch  27 Batch   82/269   train_loss = 4.026\n",
      "Epoch  27 Batch   83/269   train_loss = 3.771\n",
      "Epoch  27 Batch   84/269   train_loss = 3.990\n",
      "Epoch  27 Batch   85/269   train_loss = 4.221\n",
      "Epoch  27 Batch   86/269   train_loss = 4.057\n",
      "Epoch  27 Batch   87/269   train_loss = 3.787\n",
      "Epoch  27 Batch   88/269   train_loss = 3.842\n",
      "Epoch  27 Batch   89/269   train_loss = 3.763\n",
      "Epoch  27 Batch   90/269   train_loss = 3.647\n",
      "Epoch  27 Batch   91/269   train_loss = 3.958\n",
      "Epoch  27 Batch   92/269   train_loss = 3.978\n",
      "Epoch  27 Batch   93/269   train_loss = 3.656\n",
      "Epoch  27 Batch   94/269   train_loss = 4.069\n",
      "Epoch  27 Batch   95/269   train_loss = 3.610\n",
      "Epoch  27 Batch   96/269   train_loss = 3.670\n",
      "Epoch  27 Batch   97/269   train_loss = 3.803\n",
      "Epoch  27 Batch   98/269   train_loss = 3.912\n",
      "Epoch  27 Batch   99/269   train_loss = 3.850\n",
      "Epoch  27 Batch  100/269   train_loss = 3.732\n",
      "Epoch  27 Batch  101/269   train_loss = 3.941\n",
      "Epoch  27 Batch  102/269   train_loss = 3.610\n",
      "Epoch  27 Batch  103/269   train_loss = 3.745\n",
      "Epoch  27 Batch  104/269   train_loss = 3.688\n",
      "Epoch  27 Batch  105/269   train_loss = 3.889\n",
      "Epoch  27 Batch  106/269   train_loss = 4.016\n",
      "Epoch  27 Batch  107/269   train_loss = 4.037\n",
      "Epoch  27 Batch  108/269   train_loss = 3.836\n",
      "Epoch  27 Batch  109/269   train_loss = 3.865\n",
      "Epoch  27 Batch  110/269   train_loss = 3.826\n",
      "Epoch  27 Batch  111/269   train_loss = 3.931\n",
      "Epoch  27 Batch  112/269   train_loss = 3.964\n",
      "Epoch  27 Batch  113/269   train_loss = 3.818\n",
      "Epoch  27 Batch  114/269   train_loss = 3.804\n",
      "Epoch  27 Batch  115/269   train_loss = 3.656\n",
      "Epoch  27 Batch  116/269   train_loss = 3.636\n",
      "Epoch  27 Batch  117/269   train_loss = 3.651\n",
      "Epoch  27 Batch  118/269   train_loss = 3.980\n",
      "Epoch  27 Batch  119/269   train_loss = 3.626\n",
      "Epoch  27 Batch  120/269   train_loss = 3.878\n",
      "Epoch  27 Batch  121/269   train_loss = 3.701\n",
      "Epoch  27 Batch  122/269   train_loss = 3.844\n",
      "Epoch  27 Batch  123/269   train_loss = 3.573\n",
      "Epoch  27 Batch  124/269   train_loss = 3.954\n",
      "Epoch  27 Batch  125/269   train_loss = 3.600\n",
      "Epoch  27 Batch  126/269   train_loss = 3.477\n",
      "Epoch  27 Batch  127/269   train_loss = 3.775\n",
      "Epoch  27 Batch  128/269   train_loss = 3.771\n",
      "Epoch  27 Batch  129/269   train_loss = 3.748\n",
      "Epoch  27 Batch  130/269   train_loss = 3.907\n",
      "Epoch  27 Batch  131/269   train_loss = 3.501\n",
      "Epoch  27 Batch  132/269   train_loss = 3.863\n",
      "Epoch  27 Batch  133/269   train_loss = 3.574\n",
      "Epoch  27 Batch  134/269   train_loss = 3.946\n",
      "Epoch  27 Batch  135/269   train_loss = 3.623\n",
      "Epoch  27 Batch  136/269   train_loss = 3.528\n",
      "Epoch  27 Batch  137/269   train_loss = 3.850\n",
      "Epoch  27 Batch  138/269   train_loss = 3.620\n",
      "Epoch  27 Batch  139/269   train_loss = 3.786\n",
      "Epoch  27 Batch  140/269   train_loss = 3.819\n",
      "Epoch  27 Batch  141/269   train_loss = 4.126\n",
      "Epoch  27 Batch  142/269   train_loss = 3.602\n",
      "Epoch  27 Batch  143/269   train_loss = 3.791\n",
      "Epoch  27 Batch  144/269   train_loss = 3.894\n",
      "Epoch  27 Batch  145/269   train_loss = 3.738\n",
      "Epoch  27 Batch  146/269   train_loss = 3.946\n",
      "Epoch  27 Batch  147/269   train_loss = 3.577\n",
      "Epoch  27 Batch  148/269   train_loss = 3.916\n",
      "Epoch  27 Batch  149/269   train_loss = 3.731\n",
      "Epoch  27 Batch  150/269   train_loss = 3.846\n",
      "Epoch  27 Batch  151/269   train_loss = 3.802\n",
      "Epoch  27 Batch  152/269   train_loss = 3.815\n",
      "Epoch  27 Batch  153/269   train_loss = 3.847\n",
      "Epoch  27 Batch  154/269   train_loss = 3.705\n",
      "Epoch  27 Batch  155/269   train_loss = 3.675\n",
      "Epoch  27 Batch  156/269   train_loss = 4.121\n",
      "Epoch  27 Batch  157/269   train_loss = 3.937\n",
      "Epoch  27 Batch  158/269   train_loss = 3.865\n",
      "Epoch  27 Batch  159/269   train_loss = 3.855\n",
      "Epoch  27 Batch  160/269   train_loss = 3.735\n",
      "Epoch  27 Batch  161/269   train_loss = 3.895\n",
      "Epoch  27 Batch  162/269   train_loss = 3.824\n",
      "Epoch  27 Batch  163/269   train_loss = 3.822\n",
      "Epoch  27 Batch  164/269   train_loss = 3.947\n",
      "Epoch  27 Batch  165/269   train_loss = 3.905\n",
      "Epoch  27 Batch  166/269   train_loss = 3.865\n",
      "Epoch  27 Batch  167/269   train_loss = 3.678\n",
      "Epoch  27 Batch  168/269   train_loss = 3.783\n",
      "Epoch  27 Batch  169/269   train_loss = 4.050\n",
      "Epoch  27 Batch  170/269   train_loss = 3.638\n",
      "Epoch  27 Batch  171/269   train_loss = 3.886\n",
      "Epoch  27 Batch  172/269   train_loss = 3.579\n",
      "Epoch  27 Batch  173/269   train_loss = 3.820\n",
      "Epoch  27 Batch  174/269   train_loss = 3.935\n",
      "Epoch  27 Batch  175/269   train_loss = 3.821\n",
      "Epoch  27 Batch  176/269   train_loss = 3.771\n",
      "Epoch  27 Batch  177/269   train_loss = 3.811\n",
      "Epoch  27 Batch  178/269   train_loss = 4.088\n",
      "Epoch  27 Batch  179/269   train_loss = 3.845\n",
      "Epoch  27 Batch  180/269   train_loss = 3.692\n",
      "Epoch  27 Batch  181/269   train_loss = 3.927\n",
      "Epoch  27 Batch  182/269   train_loss = 3.727\n",
      "Epoch  27 Batch  183/269   train_loss = 3.752\n",
      "Epoch  27 Batch  184/269   train_loss = 3.765\n",
      "Epoch  27 Batch  185/269   train_loss = 3.853\n",
      "Epoch  27 Batch  186/269   train_loss = 3.725\n",
      "Epoch  27 Batch  187/269   train_loss = 3.966\n",
      "Epoch  27 Batch  188/269   train_loss = 3.687\n",
      "Epoch  27 Batch  189/269   train_loss = 3.732\n",
      "Epoch  27 Batch  190/269   train_loss = 4.160\n",
      "Epoch  27 Batch  191/269   train_loss = 3.856\n",
      "Epoch  27 Batch  192/269   train_loss = 3.767\n",
      "Epoch  27 Batch  193/269   train_loss = 3.730\n",
      "Epoch  27 Batch  194/269   train_loss = 3.909\n",
      "Epoch  27 Batch  195/269   train_loss = 3.697\n",
      "Epoch  27 Batch  196/269   train_loss = 3.817\n",
      "Epoch  27 Batch  197/269   train_loss = 4.059\n",
      "Epoch  27 Batch  198/269   train_loss = 3.889\n",
      "Epoch  27 Batch  199/269   train_loss = 3.863\n",
      "Epoch  27 Batch  200/269   train_loss = 3.726\n",
      "Epoch  27 Batch  201/269   train_loss = 3.743\n",
      "Epoch  27 Batch  202/269   train_loss = 3.666\n",
      "Epoch  27 Batch  203/269   train_loss = 3.673\n",
      "Epoch  27 Batch  204/269   train_loss = 3.886\n",
      "Epoch  27 Batch  205/269   train_loss = 3.824\n",
      "Epoch  27 Batch  206/269   train_loss = 3.758\n",
      "Epoch  27 Batch  207/269   train_loss = 3.767\n",
      "Epoch  27 Batch  208/269   train_loss = 3.835\n",
      "Epoch  27 Batch  209/269   train_loss = 3.929\n",
      "Epoch  27 Batch  210/269   train_loss = 3.661\n",
      "Epoch  27 Batch  211/269   train_loss = 3.784\n",
      "Epoch  27 Batch  212/269   train_loss = 4.050\n",
      "Epoch  27 Batch  213/269   train_loss = 3.687\n",
      "Epoch  27 Batch  214/269   train_loss = 3.789\n",
      "Epoch  27 Batch  215/269   train_loss = 3.979\n",
      "Epoch  27 Batch  216/269   train_loss = 3.970\n",
      "Epoch  27 Batch  217/269   train_loss = 3.693\n",
      "Epoch  27 Batch  218/269   train_loss = 3.844\n",
      "Epoch  27 Batch  219/269   train_loss = 3.489\n",
      "Epoch  27 Batch  220/269   train_loss = 3.938\n",
      "Epoch  27 Batch  221/269   train_loss = 3.679\n",
      "Epoch  27 Batch  222/269   train_loss = 3.830\n",
      "Epoch  27 Batch  223/269   train_loss = 3.656\n",
      "Epoch  27 Batch  224/269   train_loss = 3.885\n",
      "Epoch  27 Batch  225/269   train_loss = 4.028\n",
      "Epoch  27 Batch  226/269   train_loss = 3.812\n",
      "Epoch  27 Batch  227/269   train_loss = 3.491\n",
      "Epoch  27 Batch  228/269   train_loss = 3.747\n",
      "Epoch  27 Batch  229/269   train_loss = 3.925\n",
      "Epoch  27 Batch  230/269   train_loss = 3.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27 Batch  231/269   train_loss = 3.786\n",
      "Epoch  27 Batch  232/269   train_loss = 3.792\n",
      "Epoch  27 Batch  233/269   train_loss = 3.890\n",
      "Epoch  27 Batch  234/269   train_loss = 3.799\n",
      "Epoch  27 Batch  235/269   train_loss = 4.029\n",
      "Epoch  27 Batch  236/269   train_loss = 3.655\n",
      "Epoch  27 Batch  237/269   train_loss = 3.396\n",
      "Epoch  27 Batch  238/269   train_loss = 3.579\n",
      "Epoch  27 Batch  239/269   train_loss = 4.193\n",
      "Epoch  27 Batch  240/269   train_loss = 3.744\n",
      "Epoch  27 Batch  241/269   train_loss = 3.979\n",
      "Epoch  27 Batch  242/269   train_loss = 3.611\n",
      "Epoch  27 Batch  243/269   train_loss = 3.917\n",
      "Epoch  27 Batch  244/269   train_loss = 3.705\n",
      "Epoch  27 Batch  245/269   train_loss = 3.816\n",
      "Epoch  27 Batch  246/269   train_loss = 3.448\n",
      "Epoch  27 Batch  247/269   train_loss = 3.855\n",
      "Epoch  27 Batch  248/269   train_loss = 3.869\n",
      "Epoch  27 Batch  249/269   train_loss = 3.695\n",
      "Epoch  27 Batch  250/269   train_loss = 3.528\n",
      "Epoch  27 Batch  251/269   train_loss = 3.960\n",
      "Epoch  27 Batch  252/269   train_loss = 3.954\n",
      "Epoch  27 Batch  253/269   train_loss = 3.721\n",
      "Epoch  27 Batch  254/269   train_loss = 3.692\n",
      "Epoch  27 Batch  255/269   train_loss = 3.821\n",
      "Epoch  27 Batch  256/269   train_loss = 3.682\n",
      "Epoch  27 Batch  257/269   train_loss = 3.580\n",
      "Epoch  27 Batch  258/269   train_loss = 3.518\n",
      "Epoch  27 Batch  259/269   train_loss = 3.585\n",
      "Epoch  27 Batch  260/269   train_loss = 3.894\n",
      "Epoch  27 Batch  261/269   train_loss = 3.948\n",
      "Epoch  27 Batch  262/269   train_loss = 3.678\n",
      "Epoch  27 Batch  263/269   train_loss = 3.515\n",
      "Epoch  27 Batch  264/269   train_loss = 4.037\n",
      "Epoch  27 Batch  265/269   train_loss = 3.851\n",
      "Epoch  27 Batch  266/269   train_loss = 3.767\n",
      "Epoch  27 Batch  267/269   train_loss = 3.835\n",
      "Epoch  27 Batch  268/269   train_loss = 3.901\n",
      "Epoch  28 Batch    0/269   train_loss = 3.687\n",
      "Epoch  28 Batch    1/269   train_loss = 3.546\n",
      "Epoch  28 Batch    2/269   train_loss = 3.687\n",
      "Epoch  28 Batch    3/269   train_loss = 3.695\n",
      "Epoch  28 Batch    4/269   train_loss = 4.136\n",
      "Epoch  28 Batch    5/269   train_loss = 3.814\n",
      "Epoch  28 Batch    6/269   train_loss = 3.720\n",
      "Epoch  28 Batch    7/269   train_loss = 3.509\n",
      "Epoch  28 Batch    8/269   train_loss = 3.923\n",
      "Epoch  28 Batch    9/269   train_loss = 3.539\n",
      "Epoch  28 Batch   10/269   train_loss = 3.585\n",
      "Epoch  28 Batch   11/269   train_loss = 3.730\n",
      "Epoch  28 Batch   12/269   train_loss = 3.450\n",
      "Epoch  28 Batch   13/269   train_loss = 3.825\n",
      "Epoch  28 Batch   14/269   train_loss = 3.649\n",
      "Epoch  28 Batch   15/269   train_loss = 4.017\n",
      "Epoch  28 Batch   16/269   train_loss = 3.744\n",
      "Epoch  28 Batch   17/269   train_loss = 3.846\n",
      "Epoch  28 Batch   18/269   train_loss = 3.719\n",
      "Epoch  28 Batch   19/269   train_loss = 3.735\n",
      "Epoch  28 Batch   20/269   train_loss = 4.042\n",
      "Epoch  28 Batch   21/269   train_loss = 3.885\n",
      "Epoch  28 Batch   22/269   train_loss = 3.667\n",
      "Epoch  28 Batch   23/269   train_loss = 3.812\n",
      "Epoch  28 Batch   24/269   train_loss = 3.684\n",
      "Epoch  28 Batch   25/269   train_loss = 3.992\n",
      "Epoch  28 Batch   26/269   train_loss = 3.835\n",
      "Epoch  28 Batch   27/269   train_loss = 3.650\n",
      "Epoch  28 Batch   28/269   train_loss = 3.819\n",
      "Epoch  28 Batch   29/269   train_loss = 3.837\n",
      "Epoch  28 Batch   30/269   train_loss = 4.130\n",
      "Epoch  28 Batch   31/269   train_loss = 3.783\n",
      "Epoch  28 Batch   32/269   train_loss = 3.693\n",
      "Epoch  28 Batch   33/269   train_loss = 3.438\n",
      "Epoch  28 Batch   34/269   train_loss = 3.733\n",
      "Epoch  28 Batch   35/269   train_loss = 3.595\n",
      "Epoch  28 Batch   36/269   train_loss = 3.615\n",
      "Epoch  28 Batch   37/269   train_loss = 3.780\n",
      "Epoch  28 Batch   38/269   train_loss = 3.556\n",
      "Epoch  28 Batch   39/269   train_loss = 3.882\n",
      "Epoch  28 Batch   40/269   train_loss = 3.607\n",
      "Epoch  28 Batch   41/269   train_loss = 3.643\n",
      "Epoch  28 Batch   42/269   train_loss = 4.164\n",
      "Epoch  28 Batch   43/269   train_loss = 3.910\n",
      "Epoch  28 Batch   44/269   train_loss = 3.769\n",
      "Epoch  28 Batch   45/269   train_loss = 3.494\n",
      "Epoch  28 Batch   46/269   train_loss = 3.843\n",
      "Epoch  28 Batch   47/269   train_loss = 3.472\n",
      "Epoch  28 Batch   48/269   train_loss = 3.997\n",
      "Epoch  28 Batch   49/269   train_loss = 3.970\n",
      "Epoch  28 Batch   50/269   train_loss = 3.911\n",
      "Epoch  28 Batch   51/269   train_loss = 3.810\n",
      "Epoch  28 Batch   52/269   train_loss = 3.817\n",
      "Epoch  28 Batch   53/269   train_loss = 3.996\n",
      "Epoch  28 Batch   54/269   train_loss = 3.828\n",
      "Epoch  28 Batch   55/269   train_loss = 3.883\n",
      "Epoch  28 Batch   56/269   train_loss = 3.716\n",
      "Epoch  28 Batch   57/269   train_loss = 3.938\n",
      "Epoch  28 Batch   58/269   train_loss = 3.644\n",
      "Epoch  28 Batch   59/269   train_loss = 3.681\n",
      "Epoch  28 Batch   60/269   train_loss = 3.804\n",
      "Epoch  28 Batch   61/269   train_loss = 4.092\n",
      "Epoch  28 Batch   62/269   train_loss = 3.681\n",
      "Epoch  28 Batch   63/269   train_loss = 3.950\n",
      "Epoch  28 Batch   64/269   train_loss = 3.697\n",
      "Epoch  28 Batch   65/269   train_loss = 3.808\n",
      "Epoch  28 Batch   66/269   train_loss = 3.710\n",
      "Epoch  28 Batch   67/269   train_loss = 3.796\n",
      "Epoch  28 Batch   68/269   train_loss = 3.816\n",
      "Epoch  28 Batch   69/269   train_loss = 3.832\n",
      "Epoch  28 Batch   70/269   train_loss = 3.453\n",
      "Epoch  28 Batch   71/269   train_loss = 3.888\n",
      "Epoch  28 Batch   72/269   train_loss = 3.537\n",
      "Epoch  28 Batch   73/269   train_loss = 3.732\n",
      "Epoch  28 Batch   74/269   train_loss = 3.773\n",
      "Epoch  28 Batch   75/269   train_loss = 3.908\n",
      "Epoch  28 Batch   76/269   train_loss = 3.799\n",
      "Epoch  28 Batch   77/269   train_loss = 3.454\n",
      "Epoch  28 Batch   78/269   train_loss = 3.885\n",
      "Epoch  28 Batch   79/269   train_loss = 3.981\n",
      "Epoch  28 Batch   80/269   train_loss = 3.847\n",
      "Epoch  28 Batch   81/269   train_loss = 3.695\n",
      "Epoch  28 Batch   82/269   train_loss = 4.002\n",
      "Epoch  28 Batch   83/269   train_loss = 3.768\n",
      "Epoch  28 Batch   84/269   train_loss = 3.974\n",
      "Epoch  28 Batch   85/269   train_loss = 4.236\n",
      "Epoch  28 Batch   86/269   train_loss = 4.063\n",
      "Epoch  28 Batch   87/269   train_loss = 3.774\n",
      "Epoch  28 Batch   88/269   train_loss = 3.838\n",
      "Epoch  28 Batch   89/269   train_loss = 3.767\n",
      "Epoch  28 Batch   90/269   train_loss = 3.632\n",
      "Epoch  28 Batch   91/269   train_loss = 3.960\n",
      "Epoch  28 Batch   92/269   train_loss = 4.008\n",
      "Epoch  28 Batch   93/269   train_loss = 3.671\n",
      "Epoch  28 Batch   94/269   train_loss = 4.054\n",
      "Epoch  28 Batch   95/269   train_loss = 3.609\n",
      "Epoch  28 Batch   96/269   train_loss = 3.685\n",
      "Epoch  28 Batch   97/269   train_loss = 3.768\n",
      "Epoch  28 Batch   98/269   train_loss = 3.925\n",
      "Epoch  28 Batch   99/269   train_loss = 3.798\n",
      "Epoch  28 Batch  100/269   train_loss = 3.740\n",
      "Epoch  28 Batch  101/269   train_loss = 3.945\n",
      "Epoch  28 Batch  102/269   train_loss = 3.609\n",
      "Epoch  28 Batch  103/269   train_loss = 3.727\n",
      "Epoch  28 Batch  104/269   train_loss = 3.676\n",
      "Epoch  28 Batch  105/269   train_loss = 3.900\n",
      "Epoch  28 Batch  106/269   train_loss = 4.012\n",
      "Epoch  28 Batch  107/269   train_loss = 4.026\n",
      "Epoch  28 Batch  108/269   train_loss = 3.820\n",
      "Epoch  28 Batch  109/269   train_loss = 3.857\n",
      "Epoch  28 Batch  110/269   train_loss = 3.839\n",
      "Epoch  28 Batch  111/269   train_loss = 3.911\n",
      "Epoch  28 Batch  112/269   train_loss = 3.938\n",
      "Epoch  28 Batch  113/269   train_loss = 3.772\n",
      "Epoch  28 Batch  114/269   train_loss = 3.834\n",
      "Epoch  28 Batch  115/269   train_loss = 3.643\n",
      "Epoch  28 Batch  116/269   train_loss = 3.643\n",
      "Epoch  28 Batch  117/269   train_loss = 3.637\n",
      "Epoch  28 Batch  118/269   train_loss = 3.981\n",
      "Epoch  28 Batch  119/269   train_loss = 3.625\n",
      "Epoch  28 Batch  120/269   train_loss = 3.869\n",
      "Epoch  28 Batch  121/269   train_loss = 3.700\n",
      "Epoch  28 Batch  122/269   train_loss = 3.862\n",
      "Epoch  28 Batch  123/269   train_loss = 3.570\n",
      "Epoch  28 Batch  124/269   train_loss = 3.973\n",
      "Epoch  28 Batch  125/269   train_loss = 3.580\n",
      "Epoch  28 Batch  126/269   train_loss = 3.449\n",
      "Epoch  28 Batch  127/269   train_loss = 3.776\n",
      "Epoch  28 Batch  128/269   train_loss = 3.745\n",
      "Epoch  28 Batch  129/269   train_loss = 3.695\n",
      "Epoch  28 Batch  130/269   train_loss = 3.841\n",
      "Epoch  28 Batch  131/269   train_loss = 3.515\n",
      "Epoch  28 Batch  132/269   train_loss = 3.838\n",
      "Epoch  28 Batch  133/269   train_loss = 3.575\n",
      "Epoch  28 Batch  134/269   train_loss = 3.977\n",
      "Epoch  28 Batch  135/269   train_loss = 3.602\n",
      "Epoch  28 Batch  136/269   train_loss = 3.532\n",
      "Epoch  28 Batch  137/269   train_loss = 3.833\n",
      "Epoch  28 Batch  138/269   train_loss = 3.622\n",
      "Epoch  28 Batch  139/269   train_loss = 3.762\n",
      "Epoch  28 Batch  140/269   train_loss = 3.851\n",
      "Epoch  28 Batch  141/269   train_loss = 4.133\n",
      "Epoch  28 Batch  142/269   train_loss = 3.616\n",
      "Epoch  28 Batch  143/269   train_loss = 3.770\n",
      "Epoch  28 Batch  144/269   train_loss = 3.901\n",
      "Epoch  28 Batch  145/269   train_loss = 3.723\n",
      "Epoch  28 Batch  146/269   train_loss = 3.925\n",
      "Epoch  28 Batch  147/269   train_loss = 3.579\n",
      "Epoch  28 Batch  148/269   train_loss = 3.942\n",
      "Epoch  28 Batch  149/269   train_loss = 3.713\n",
      "Epoch  28 Batch  150/269   train_loss = 3.869\n",
      "Epoch  28 Batch  151/269   train_loss = 3.811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  28 Batch  152/269   train_loss = 3.822\n",
      "Epoch  28 Batch  153/269   train_loss = 3.827\n",
      "Epoch  28 Batch  154/269   train_loss = 3.675\n",
      "Epoch  28 Batch  155/269   train_loss = 3.664\n",
      "Epoch  28 Batch  156/269   train_loss = 4.138\n",
      "Epoch  28 Batch  157/269   train_loss = 3.915\n",
      "Epoch  28 Batch  158/269   train_loss = 3.827\n",
      "Epoch  28 Batch  159/269   train_loss = 3.874\n",
      "Epoch  28 Batch  160/269   train_loss = 3.739\n",
      "Epoch  28 Batch  161/269   train_loss = 3.878\n",
      "Epoch  28 Batch  162/269   train_loss = 3.865\n",
      "Epoch  28 Batch  163/269   train_loss = 3.840\n",
      "Epoch  28 Batch  164/269   train_loss = 3.945\n",
      "Epoch  28 Batch  165/269   train_loss = 3.917\n",
      "Epoch  28 Batch  166/269   train_loss = 3.869\n",
      "Epoch  28 Batch  167/269   train_loss = 3.681\n",
      "Epoch  28 Batch  168/269   train_loss = 3.758\n",
      "Epoch  28 Batch  169/269   train_loss = 4.023\n",
      "Epoch  28 Batch  170/269   train_loss = 3.631\n",
      "Epoch  28 Batch  171/269   train_loss = 3.877\n",
      "Epoch  28 Batch  172/269   train_loss = 3.579\n",
      "Epoch  28 Batch  173/269   train_loss = 3.808\n",
      "Epoch  28 Batch  174/269   train_loss = 3.938\n",
      "Epoch  28 Batch  175/269   train_loss = 3.826\n",
      "Epoch  28 Batch  176/269   train_loss = 3.763\n",
      "Epoch  28 Batch  177/269   train_loss = 3.789\n",
      "Epoch  28 Batch  178/269   train_loss = 4.120\n",
      "Epoch  28 Batch  179/269   train_loss = 3.830\n",
      "Epoch  28 Batch  180/269   train_loss = 3.709\n",
      "Epoch  28 Batch  181/269   train_loss = 3.914\n",
      "Epoch  28 Batch  182/269   train_loss = 3.726\n",
      "Epoch  28 Batch  183/269   train_loss = 3.759\n",
      "Epoch  28 Batch  184/269   train_loss = 3.796\n",
      "Epoch  28 Batch  185/269   train_loss = 3.870\n",
      "Epoch  28 Batch  186/269   train_loss = 3.704\n",
      "Epoch  28 Batch  187/269   train_loss = 3.982\n",
      "Epoch  28 Batch  188/269   train_loss = 3.688\n",
      "Epoch  28 Batch  189/269   train_loss = 3.738\n",
      "Epoch  28 Batch  190/269   train_loss = 4.135\n",
      "Epoch  28 Batch  191/269   train_loss = 3.858\n",
      "Epoch  28 Batch  192/269   train_loss = 3.746\n",
      "Epoch  28 Batch  193/269   train_loss = 3.721\n",
      "Epoch  28 Batch  194/269   train_loss = 3.910\n",
      "Epoch  28 Batch  195/269   train_loss = 3.688\n",
      "Epoch  28 Batch  196/269   train_loss = 3.847\n",
      "Epoch  28 Batch  197/269   train_loss = 4.030\n",
      "Epoch  28 Batch  198/269   train_loss = 3.908\n",
      "Epoch  28 Batch  199/269   train_loss = 3.900\n",
      "Epoch  28 Batch  200/269   train_loss = 3.757\n",
      "Epoch  28 Batch  201/269   train_loss = 3.770\n",
      "Epoch  28 Batch  202/269   train_loss = 3.663\n",
      "Epoch  28 Batch  203/269   train_loss = 3.665\n",
      "Epoch  28 Batch  204/269   train_loss = 3.888\n",
      "Epoch  28 Batch  205/269   train_loss = 3.837\n",
      "Epoch  28 Batch  206/269   train_loss = 3.763\n",
      "Epoch  28 Batch  207/269   train_loss = 3.779\n",
      "Epoch  28 Batch  208/269   train_loss = 3.812\n",
      "Epoch  28 Batch  209/269   train_loss = 3.925\n",
      "Epoch  28 Batch  210/269   train_loss = 3.704\n",
      "Epoch  28 Batch  211/269   train_loss = 3.757\n",
      "Epoch  28 Batch  212/269   train_loss = 4.077\n",
      "Epoch  28 Batch  213/269   train_loss = 3.696\n",
      "Epoch  28 Batch  214/269   train_loss = 3.827\n",
      "Epoch  28 Batch  215/269   train_loss = 3.993\n",
      "Epoch  28 Batch  216/269   train_loss = 4.005\n",
      "Epoch  28 Batch  217/269   train_loss = 3.677\n",
      "Epoch  28 Batch  218/269   train_loss = 3.846\n",
      "Epoch  28 Batch  219/269   train_loss = 3.480\n",
      "Epoch  28 Batch  220/269   train_loss = 3.940\n",
      "Epoch  28 Batch  221/269   train_loss = 3.672\n",
      "Epoch  28 Batch  222/269   train_loss = 3.826\n",
      "Epoch  28 Batch  223/269   train_loss = 3.632\n",
      "Epoch  28 Batch  224/269   train_loss = 3.898\n",
      "Epoch  28 Batch  225/269   train_loss = 4.000\n",
      "Epoch  28 Batch  226/269   train_loss = 3.798\n",
      "Epoch  28 Batch  227/269   train_loss = 3.511\n",
      "Epoch  28 Batch  228/269   train_loss = 3.744\n",
      "Epoch  28 Batch  229/269   train_loss = 3.940\n",
      "Epoch  28 Batch  230/269   train_loss = 3.874\n",
      "Epoch  28 Batch  231/269   train_loss = 3.766\n",
      "Epoch  28 Batch  232/269   train_loss = 3.775\n",
      "Epoch  28 Batch  233/269   train_loss = 3.860\n",
      "Epoch  28 Batch  234/269   train_loss = 3.793\n",
      "Epoch  28 Batch  235/269   train_loss = 4.036\n",
      "Epoch  28 Batch  236/269   train_loss = 3.645\n",
      "Epoch  28 Batch  237/269   train_loss = 3.401\n",
      "Epoch  28 Batch  238/269   train_loss = 3.598\n",
      "Epoch  28 Batch  239/269   train_loss = 4.182\n",
      "Epoch  28 Batch  240/269   train_loss = 3.719\n",
      "Epoch  28 Batch  241/269   train_loss = 3.966\n",
      "Epoch  28 Batch  242/269   train_loss = 3.614\n",
      "Epoch  28 Batch  243/269   train_loss = 3.898\n",
      "Epoch  28 Batch  244/269   train_loss = 3.743\n",
      "Epoch  28 Batch  245/269   train_loss = 3.799\n",
      "Epoch  28 Batch  246/269   train_loss = 3.448\n",
      "Epoch  28 Batch  247/269   train_loss = 3.880\n",
      "Epoch  28 Batch  248/269   train_loss = 3.860\n",
      "Epoch  28 Batch  249/269   train_loss = 3.699\n",
      "Epoch  28 Batch  250/269   train_loss = 3.529\n",
      "Epoch  28 Batch  251/269   train_loss = 3.948\n",
      "Epoch  28 Batch  252/269   train_loss = 3.942\n",
      "Epoch  28 Batch  253/269   train_loss = 3.785\n",
      "Epoch  28 Batch  254/269   train_loss = 3.699\n",
      "Epoch  28 Batch  255/269   train_loss = 3.795\n",
      "Epoch  28 Batch  256/269   train_loss = 3.674\n",
      "Epoch  28 Batch  257/269   train_loss = 3.591\n",
      "Epoch  28 Batch  258/269   train_loss = 3.539\n",
      "Epoch  28 Batch  259/269   train_loss = 3.603\n",
      "Epoch  28 Batch  260/269   train_loss = 3.884\n",
      "Epoch  28 Batch  261/269   train_loss = 3.928\n",
      "Epoch  28 Batch  262/269   train_loss = 3.696\n",
      "Epoch  28 Batch  263/269   train_loss = 3.515\n",
      "Epoch  28 Batch  264/269   train_loss = 4.079\n",
      "Epoch  28 Batch  265/269   train_loss = 3.871\n",
      "Epoch  28 Batch  266/269   train_loss = 3.759\n",
      "Epoch  28 Batch  267/269   train_loss = 3.829\n",
      "Epoch  28 Batch  268/269   train_loss = 3.926\n",
      "Epoch  29 Batch    0/269   train_loss = 3.701\n",
      "Epoch  29 Batch    1/269   train_loss = 3.555\n",
      "Epoch  29 Batch    2/269   train_loss = 3.682\n",
      "Epoch  29 Batch    3/269   train_loss = 3.676\n",
      "Epoch  29 Batch    4/269   train_loss = 4.142\n",
      "Epoch  29 Batch    5/269   train_loss = 3.811\n",
      "Epoch  29 Batch    6/269   train_loss = 3.746\n",
      "Epoch  29 Batch    7/269   train_loss = 3.512\n",
      "Epoch  29 Batch    8/269   train_loss = 3.923\n",
      "Epoch  29 Batch    9/269   train_loss = 3.551\n",
      "Epoch  29 Batch   10/269   train_loss = 3.604\n",
      "Epoch  29 Batch   11/269   train_loss = 3.743\n",
      "Epoch  29 Batch   12/269   train_loss = 3.459\n",
      "Epoch  29 Batch   13/269   train_loss = 3.777\n",
      "Epoch  29 Batch   14/269   train_loss = 3.655\n",
      "Epoch  29 Batch   15/269   train_loss = 4.051\n",
      "Epoch  29 Batch   16/269   train_loss = 3.745\n",
      "Epoch  29 Batch   17/269   train_loss = 3.834\n",
      "Epoch  29 Batch   18/269   train_loss = 3.720\n",
      "Epoch  29 Batch   19/269   train_loss = 3.725\n",
      "Epoch  29 Batch   20/269   train_loss = 4.051\n",
      "Epoch  29 Batch   21/269   train_loss = 3.891\n",
      "Epoch  29 Batch   22/269   train_loss = 3.675\n",
      "Epoch  29 Batch   23/269   train_loss = 3.806\n",
      "Epoch  29 Batch   24/269   train_loss = 3.709\n",
      "Epoch  29 Batch   25/269   train_loss = 3.995\n",
      "Epoch  29 Batch   26/269   train_loss = 3.822\n",
      "Epoch  29 Batch   27/269   train_loss = 3.637\n",
      "Epoch  29 Batch   28/269   train_loss = 3.840\n",
      "Epoch  29 Batch   29/269   train_loss = 3.824\n",
      "Epoch  29 Batch   30/269   train_loss = 4.117\n",
      "Epoch  29 Batch   31/269   train_loss = 3.790\n",
      "Epoch  29 Batch   32/269   train_loss = 3.706\n",
      "Epoch  29 Batch   33/269   train_loss = 3.455\n",
      "Epoch  29 Batch   34/269   train_loss = 3.739\n",
      "Epoch  29 Batch   35/269   train_loss = 3.612\n",
      "Epoch  29 Batch   36/269   train_loss = 3.629\n",
      "Epoch  29 Batch   37/269   train_loss = 3.811\n",
      "Epoch  29 Batch   38/269   train_loss = 3.510\n",
      "Epoch  29 Batch   39/269   train_loss = 3.884\n",
      "Epoch  29 Batch   40/269   train_loss = 3.616\n",
      "Epoch  29 Batch   41/269   train_loss = 3.652\n",
      "Epoch  29 Batch   42/269   train_loss = 4.190\n",
      "Epoch  29 Batch   43/269   train_loss = 3.924\n",
      "Epoch  29 Batch   44/269   train_loss = 3.749\n",
      "Epoch  29 Batch   45/269   train_loss = 3.486\n",
      "Epoch  29 Batch   46/269   train_loss = 3.869\n",
      "Epoch  29 Batch   47/269   train_loss = 3.481\n",
      "Epoch  29 Batch   48/269   train_loss = 4.021\n",
      "Epoch  29 Batch   49/269   train_loss = 3.994\n",
      "Epoch  29 Batch   50/269   train_loss = 3.896\n",
      "Epoch  29 Batch   51/269   train_loss = 3.813\n",
      "Epoch  29 Batch   52/269   train_loss = 3.833\n",
      "Epoch  29 Batch   53/269   train_loss = 4.031\n",
      "Epoch  29 Batch   54/269   train_loss = 3.850\n",
      "Epoch  29 Batch   55/269   train_loss = 3.858\n",
      "Epoch  29 Batch   56/269   train_loss = 3.718\n",
      "Epoch  29 Batch   57/269   train_loss = 3.931\n",
      "Epoch  29 Batch   58/269   train_loss = 3.638\n",
      "Epoch  29 Batch   59/269   train_loss = 3.668\n",
      "Epoch  29 Batch   60/269   train_loss = 3.800\n",
      "Epoch  29 Batch   61/269   train_loss = 4.068\n",
      "Epoch  29 Batch   62/269   train_loss = 3.670\n",
      "Epoch  29 Batch   63/269   train_loss = 3.976\n",
      "Epoch  29 Batch   64/269   train_loss = 3.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  29 Batch   65/269   train_loss = 3.813\n",
      "Epoch  29 Batch   66/269   train_loss = 3.698\n",
      "Epoch  29 Batch   67/269   train_loss = 3.793\n",
      "Epoch  29 Batch   68/269   train_loss = 3.784\n",
      "Epoch  29 Batch   69/269   train_loss = 3.815\n",
      "Epoch  29 Batch   70/269   train_loss = 3.447\n",
      "Epoch  29 Batch   71/269   train_loss = 3.893\n",
      "Epoch  29 Batch   72/269   train_loss = 3.547\n",
      "Epoch  29 Batch   73/269   train_loss = 3.721\n",
      "Epoch  29 Batch   74/269   train_loss = 3.810\n",
      "Epoch  29 Batch   75/269   train_loss = 3.908\n",
      "Epoch  29 Batch   76/269   train_loss = 3.780\n",
      "Epoch  29 Batch   77/269   train_loss = 3.463\n",
      "Epoch  29 Batch   78/269   train_loss = 3.872\n",
      "Epoch  29 Batch   79/269   train_loss = 3.974\n",
      "Epoch  29 Batch   80/269   train_loss = 3.837\n",
      "Epoch  29 Batch   81/269   train_loss = 3.666\n",
      "Epoch  29 Batch   82/269   train_loss = 3.985\n",
      "Epoch  29 Batch   83/269   train_loss = 3.734\n",
      "Epoch  29 Batch   84/269   train_loss = 3.970\n",
      "Epoch  29 Batch   85/269   train_loss = 4.171\n",
      "Epoch  29 Batch   86/269   train_loss = 4.025\n",
      "Epoch  29 Batch   87/269   train_loss = 3.759\n",
      "Epoch  29 Batch   88/269   train_loss = 3.837\n",
      "Epoch  29 Batch   89/269   train_loss = 3.775\n",
      "Epoch  29 Batch   90/269   train_loss = 3.646\n",
      "Epoch  29 Batch   91/269   train_loss = 3.923\n",
      "Epoch  29 Batch   92/269   train_loss = 3.949\n",
      "Epoch  29 Batch   93/269   train_loss = 3.675\n",
      "Epoch  29 Batch   94/269   train_loss = 4.055\n",
      "Epoch  29 Batch   95/269   train_loss = 3.629\n",
      "Epoch  29 Batch   96/269   train_loss = 3.675\n",
      "Epoch  29 Batch   97/269   train_loss = 3.776\n",
      "Epoch  29 Batch   98/269   train_loss = 3.947\n",
      "Epoch  29 Batch   99/269   train_loss = 3.801\n",
      "Epoch  29 Batch  100/269   train_loss = 3.724\n",
      "Epoch  29 Batch  101/269   train_loss = 3.926\n",
      "Epoch  29 Batch  102/269   train_loss = 3.612\n",
      "Epoch  29 Batch  103/269   train_loss = 3.695\n",
      "Epoch  29 Batch  104/269   train_loss = 3.681\n",
      "Epoch  29 Batch  105/269   train_loss = 3.909\n",
      "Epoch  29 Batch  106/269   train_loss = 4.026\n",
      "Epoch  29 Batch  107/269   train_loss = 4.018\n",
      "Epoch  29 Batch  108/269   train_loss = 3.812\n",
      "Epoch  29 Batch  109/269   train_loss = 3.873\n",
      "Epoch  29 Batch  110/269   train_loss = 3.827\n",
      "Epoch  29 Batch  111/269   train_loss = 3.912\n",
      "Epoch  29 Batch  112/269   train_loss = 3.925\n",
      "Epoch  29 Batch  113/269   train_loss = 3.784\n",
      "Epoch  29 Batch  114/269   train_loss = 3.793\n",
      "Epoch  29 Batch  115/269   train_loss = 3.636\n",
      "Epoch  29 Batch  116/269   train_loss = 3.648\n",
      "Epoch  29 Batch  117/269   train_loss = 3.628\n",
      "Epoch  29 Batch  118/269   train_loss = 3.964\n",
      "Epoch  29 Batch  119/269   train_loss = 3.624\n",
      "Epoch  29 Batch  120/269   train_loss = 3.857\n",
      "Epoch  29 Batch  121/269   train_loss = 3.728\n",
      "Epoch  29 Batch  122/269   train_loss = 3.877\n",
      "Epoch  29 Batch  123/269   train_loss = 3.607\n",
      "Epoch  29 Batch  124/269   train_loss = 3.995\n",
      "Epoch  29 Batch  125/269   train_loss = 3.574\n",
      "Epoch  29 Batch  126/269   train_loss = 3.447\n",
      "Epoch  29 Batch  127/269   train_loss = 3.792\n",
      "Epoch  29 Batch  128/269   train_loss = 3.736\n",
      "Epoch  29 Batch  129/269   train_loss = 3.706\n",
      "Epoch  29 Batch  130/269   train_loss = 3.874\n",
      "Epoch  29 Batch  131/269   train_loss = 3.484\n",
      "Epoch  29 Batch  132/269   train_loss = 3.808\n",
      "Epoch  29 Batch  133/269   train_loss = 3.561\n",
      "Epoch  29 Batch  134/269   train_loss = 3.987\n",
      "Epoch  29 Batch  135/269   train_loss = 3.627\n",
      "Epoch  29 Batch  136/269   train_loss = 3.520\n",
      "Epoch  29 Batch  137/269   train_loss = 3.841\n",
      "Epoch  29 Batch  138/269   train_loss = 3.621\n",
      "Epoch  29 Batch  139/269   train_loss = 3.782\n",
      "Epoch  29 Batch  140/269   train_loss = 3.796\n",
      "Epoch  29 Batch  141/269   train_loss = 4.091\n",
      "Epoch  29 Batch  142/269   train_loss = 3.588\n",
      "Epoch  29 Batch  143/269   train_loss = 3.785\n",
      "Epoch  29 Batch  144/269   train_loss = 3.895\n",
      "Epoch  29 Batch  145/269   train_loss = 3.727\n",
      "Epoch  29 Batch  146/269   train_loss = 3.920\n",
      "Epoch  29 Batch  147/269   train_loss = 3.598\n",
      "Epoch  29 Batch  148/269   train_loss = 3.904\n",
      "Epoch  29 Batch  149/269   train_loss = 3.712\n",
      "Epoch  29 Batch  150/269   train_loss = 3.849\n",
      "Epoch  29 Batch  151/269   train_loss = 3.795\n",
      "Epoch  29 Batch  152/269   train_loss = 3.786\n",
      "Epoch  29 Batch  153/269   train_loss = 3.840\n",
      "Epoch  29 Batch  154/269   train_loss = 3.688\n",
      "Epoch  29 Batch  155/269   train_loss = 3.672\n",
      "Epoch  29 Batch  156/269   train_loss = 4.144\n",
      "Epoch  29 Batch  157/269   train_loss = 3.922\n",
      "Epoch  29 Batch  158/269   train_loss = 3.855\n",
      "Epoch  29 Batch  159/269   train_loss = 3.860\n",
      "Epoch  29 Batch  160/269   train_loss = 3.726\n",
      "Epoch  29 Batch  161/269   train_loss = 3.867\n",
      "Epoch  29 Batch  162/269   train_loss = 3.841\n",
      "Epoch  29 Batch  163/269   train_loss = 3.845\n",
      "Epoch  29 Batch  164/269   train_loss = 3.974\n",
      "Epoch  29 Batch  165/269   train_loss = 3.921\n",
      "Epoch  29 Batch  166/269   train_loss = 3.857\n",
      "Epoch  29 Batch  167/269   train_loss = 3.664\n",
      "Epoch  29 Batch  168/269   train_loss = 3.758\n",
      "Epoch  29 Batch  169/269   train_loss = 4.028\n",
      "Epoch  29 Batch  170/269   train_loss = 3.613\n",
      "Epoch  29 Batch  171/269   train_loss = 3.829\n",
      "Epoch  29 Batch  172/269   train_loss = 3.607\n",
      "Epoch  29 Batch  173/269   train_loss = 3.809\n",
      "Epoch  29 Batch  174/269   train_loss = 3.952\n",
      "Epoch  29 Batch  175/269   train_loss = 3.827\n",
      "Epoch  29 Batch  176/269   train_loss = 3.759\n",
      "Epoch  29 Batch  177/269   train_loss = 3.775\n",
      "Epoch  29 Batch  178/269   train_loss = 4.102\n",
      "Epoch  29 Batch  179/269   train_loss = 3.832\n",
      "Epoch  29 Batch  180/269   train_loss = 3.700\n",
      "Epoch  29 Batch  181/269   train_loss = 3.891\n",
      "Epoch  29 Batch  182/269   train_loss = 3.721\n",
      "Epoch  29 Batch  183/269   train_loss = 3.735\n",
      "Epoch  29 Batch  184/269   train_loss = 3.791\n",
      "Epoch  29 Batch  185/269   train_loss = 3.849\n",
      "Epoch  29 Batch  186/269   train_loss = 3.723\n",
      "Epoch  29 Batch  187/269   train_loss = 3.954\n",
      "Epoch  29 Batch  188/269   train_loss = 3.711\n",
      "Epoch  29 Batch  189/269   train_loss = 3.744\n",
      "Epoch  29 Batch  190/269   train_loss = 4.145\n",
      "Epoch  29 Batch  191/269   train_loss = 3.844\n",
      "Epoch  29 Batch  192/269   train_loss = 3.759\n",
      "Epoch  29 Batch  193/269   train_loss = 3.728\n",
      "Epoch  29 Batch  194/269   train_loss = 3.925\n",
      "Epoch  29 Batch  195/269   train_loss = 3.741\n",
      "Epoch  29 Batch  196/269   train_loss = 3.851\n",
      "Epoch  29 Batch  197/269   train_loss = 4.004\n",
      "Epoch  29 Batch  198/269   train_loss = 3.927\n",
      "Epoch  29 Batch  199/269   train_loss = 3.867\n",
      "Epoch  29 Batch  200/269   train_loss = 3.737\n",
      "Epoch  29 Batch  201/269   train_loss = 3.753\n",
      "Epoch  29 Batch  202/269   train_loss = 3.651\n",
      "Epoch  29 Batch  203/269   train_loss = 3.650\n",
      "Epoch  29 Batch  204/269   train_loss = 3.886\n",
      "Epoch  29 Batch  205/269   train_loss = 3.868\n",
      "Epoch  29 Batch  206/269   train_loss = 3.736\n",
      "Epoch  29 Batch  207/269   train_loss = 3.748\n",
      "Epoch  29 Batch  208/269   train_loss = 3.810\n",
      "Epoch  29 Batch  209/269   train_loss = 3.935\n",
      "Epoch  29 Batch  210/269   train_loss = 3.694\n",
      "Epoch  29 Batch  211/269   train_loss = 3.746\n",
      "Epoch  29 Batch  212/269   train_loss = 4.079\n",
      "Epoch  29 Batch  213/269   train_loss = 3.716\n",
      "Epoch  29 Batch  214/269   train_loss = 3.791\n",
      "Epoch  29 Batch  215/269   train_loss = 4.010\n",
      "Epoch  29 Batch  216/269   train_loss = 3.974\n",
      "Epoch  29 Batch  217/269   train_loss = 3.651\n",
      "Epoch  29 Batch  218/269   train_loss = 3.835\n",
      "Epoch  29 Batch  219/269   train_loss = 3.512\n",
      "Epoch  29 Batch  220/269   train_loss = 3.909\n",
      "Epoch  29 Batch  221/269   train_loss = 3.697\n",
      "Epoch  29 Batch  222/269   train_loss = 3.815\n",
      "Epoch  29 Batch  223/269   train_loss = 3.649\n",
      "Epoch  29 Batch  224/269   train_loss = 3.888\n",
      "Epoch  29 Batch  225/269   train_loss = 4.008\n",
      "Epoch  29 Batch  226/269   train_loss = 3.771\n",
      "Epoch  29 Batch  227/269   train_loss = 3.517\n",
      "Epoch  29 Batch  228/269   train_loss = 3.777\n",
      "Epoch  29 Batch  229/269   train_loss = 3.920\n",
      "Epoch  29 Batch  230/269   train_loss = 3.860\n",
      "Epoch  29 Batch  231/269   train_loss = 3.758\n",
      "Epoch  29 Batch  232/269   train_loss = 3.759\n",
      "Epoch  29 Batch  233/269   train_loss = 3.854\n",
      "Epoch  29 Batch  234/269   train_loss = 3.789\n",
      "Epoch  29 Batch  235/269   train_loss = 4.035\n",
      "Epoch  29 Batch  236/269   train_loss = 3.669\n",
      "Epoch  29 Batch  237/269   train_loss = 3.383\n",
      "Epoch  29 Batch  238/269   train_loss = 3.588\n",
      "Epoch  29 Batch  239/269   train_loss = 4.196\n",
      "Epoch  29 Batch  240/269   train_loss = 3.728\n",
      "Epoch  29 Batch  241/269   train_loss = 3.945\n",
      "Epoch  29 Batch  242/269   train_loss = 3.601\n",
      "Epoch  29 Batch  243/269   train_loss = 3.901\n",
      "Epoch  29 Batch  244/269   train_loss = 3.751\n",
      "Epoch  29 Batch  245/269   train_loss = 3.785\n",
      "Epoch  29 Batch  246/269   train_loss = 3.461\n",
      "Epoch  29 Batch  247/269   train_loss = 3.879\n",
      "Epoch  29 Batch  248/269   train_loss = 3.847\n",
      "Epoch  29 Batch  249/269   train_loss = 3.666\n",
      "Epoch  29 Batch  250/269   train_loss = 3.525\n",
      "Epoch  29 Batch  251/269   train_loss = 3.976\n",
      "Epoch  29 Batch  252/269   train_loss = 3.966\n",
      "Epoch  29 Batch  253/269   train_loss = 3.760\n",
      "Epoch  29 Batch  254/269   train_loss = 3.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  29 Batch  255/269   train_loss = 3.777\n",
      "Epoch  29 Batch  256/269   train_loss = 3.669\n",
      "Epoch  29 Batch  257/269   train_loss = 3.597\n",
      "Epoch  29 Batch  258/269   train_loss = 3.512\n",
      "Epoch  29 Batch  259/269   train_loss = 3.606\n",
      "Epoch  29 Batch  260/269   train_loss = 3.859\n",
      "Epoch  29 Batch  261/269   train_loss = 3.939\n",
      "Epoch  29 Batch  262/269   train_loss = 3.665\n",
      "Epoch  29 Batch  263/269   train_loss = 3.511\n",
      "Epoch  29 Batch  264/269   train_loss = 4.053\n",
      "Epoch  29 Batch  265/269   train_loss = 3.883\n",
      "Epoch  29 Batch  266/269   train_loss = 3.751\n",
      "Epoch  29 Batch  267/269   train_loss = 3.832\n",
      "Epoch  29 Batch  268/269   train_loss = 3.917\n",
      "Epoch  30 Batch    0/269   train_loss = 3.671\n",
      "Epoch  30 Batch    1/269   train_loss = 3.556\n",
      "Epoch  30 Batch    2/269   train_loss = 3.691\n",
      "Epoch  30 Batch    3/269   train_loss = 3.706\n",
      "Epoch  30 Batch    4/269   train_loss = 4.141\n",
      "Epoch  30 Batch    5/269   train_loss = 3.792\n",
      "Epoch  30 Batch    6/269   train_loss = 3.711\n",
      "Epoch  30 Batch    7/269   train_loss = 3.513\n",
      "Epoch  30 Batch    8/269   train_loss = 3.952\n",
      "Epoch  30 Batch    9/269   train_loss = 3.541\n",
      "Epoch  30 Batch   10/269   train_loss = 3.573\n",
      "Epoch  30 Batch   11/269   train_loss = 3.726\n",
      "Epoch  30 Batch   12/269   train_loss = 3.476\n",
      "Epoch  30 Batch   13/269   train_loss = 3.777\n",
      "Epoch  30 Batch   14/269   train_loss = 3.621\n",
      "Epoch  30 Batch   15/269   train_loss = 4.036\n",
      "Epoch  30 Batch   16/269   train_loss = 3.732\n",
      "Epoch  30 Batch   17/269   train_loss = 3.807\n",
      "Epoch  30 Batch   18/269   train_loss = 3.718\n",
      "Epoch  30 Batch   19/269   train_loss = 3.714\n",
      "Epoch  30 Batch   20/269   train_loss = 4.036\n",
      "Epoch  30 Batch   21/269   train_loss = 3.899\n",
      "Epoch  30 Batch   22/269   train_loss = 3.653\n",
      "Epoch  30 Batch   23/269   train_loss = 3.816\n",
      "Epoch  30 Batch   24/269   train_loss = 3.696\n",
      "Epoch  30 Batch   25/269   train_loss = 4.007\n",
      "Epoch  30 Batch   26/269   train_loss = 3.835\n",
      "Epoch  30 Batch   27/269   train_loss = 3.650\n",
      "Epoch  30 Batch   28/269   train_loss = 3.838\n",
      "Epoch  30 Batch   29/269   train_loss = 3.813\n",
      "Epoch  30 Batch   30/269   train_loss = 4.131\n",
      "Epoch  30 Batch   31/269   train_loss = 3.780\n",
      "Epoch  30 Batch   32/269   train_loss = 3.703\n",
      "Epoch  30 Batch   33/269   train_loss = 3.426\n",
      "Epoch  30 Batch   34/269   train_loss = 3.746\n",
      "Epoch  30 Batch   35/269   train_loss = 3.593\n",
      "Epoch  30 Batch   36/269   train_loss = 3.605\n",
      "Epoch  30 Batch   37/269   train_loss = 3.814\n",
      "Epoch  30 Batch   38/269   train_loss = 3.544\n",
      "Epoch  30 Batch   39/269   train_loss = 3.890\n",
      "Epoch  30 Batch   40/269   train_loss = 3.637\n",
      "Epoch  30 Batch   41/269   train_loss = 3.627\n",
      "Epoch  30 Batch   42/269   train_loss = 4.160\n",
      "Epoch  30 Batch   43/269   train_loss = 3.932\n",
      "Epoch  30 Batch   44/269   train_loss = 3.740\n",
      "Epoch  30 Batch   45/269   train_loss = 3.473\n",
      "Epoch  30 Batch   46/269   train_loss = 3.849\n",
      "Epoch  30 Batch   47/269   train_loss = 3.511\n",
      "Epoch  30 Batch   48/269   train_loss = 4.053\n",
      "Epoch  30 Batch   49/269   train_loss = 3.997\n",
      "Epoch  30 Batch   50/269   train_loss = 3.868\n",
      "Epoch  30 Batch   51/269   train_loss = 3.810\n",
      "Epoch  30 Batch   52/269   train_loss = 3.843\n",
      "Epoch  30 Batch   53/269   train_loss = 4.061\n",
      "Epoch  30 Batch   54/269   train_loss = 3.863\n",
      "Epoch  30 Batch   55/269   train_loss = 3.854\n",
      "Epoch  30 Batch   56/269   train_loss = 3.703\n",
      "Epoch  30 Batch   57/269   train_loss = 3.942\n",
      "Epoch  30 Batch   58/269   train_loss = 3.631\n",
      "Epoch  30 Batch   59/269   train_loss = 3.649\n",
      "Epoch  30 Batch   60/269   train_loss = 3.787\n",
      "Epoch  30 Batch   61/269   train_loss = 4.046\n",
      "Epoch  30 Batch   62/269   train_loss = 3.651\n",
      "Epoch  30 Batch   63/269   train_loss = 3.967\n",
      "Epoch  30 Batch   64/269   train_loss = 3.718\n",
      "Epoch  30 Batch   65/269   train_loss = 3.821\n",
      "Epoch  30 Batch   66/269   train_loss = 3.704\n",
      "Epoch  30 Batch   67/269   train_loss = 3.789\n",
      "Epoch  30 Batch   68/269   train_loss = 3.790\n",
      "Epoch  30 Batch   69/269   train_loss = 3.816\n",
      "Epoch  30 Batch   70/269   train_loss = 3.413\n",
      "Epoch  30 Batch   71/269   train_loss = 3.903\n",
      "Epoch  30 Batch   72/269   train_loss = 3.546\n",
      "Epoch  30 Batch   73/269   train_loss = 3.718\n",
      "Epoch  30 Batch   74/269   train_loss = 3.755\n",
      "Epoch  30 Batch   75/269   train_loss = 3.910\n",
      "Epoch  30 Batch   76/269   train_loss = 3.796\n",
      "Epoch  30 Batch   77/269   train_loss = 3.450\n",
      "Epoch  30 Batch   78/269   train_loss = 3.868\n",
      "Epoch  30 Batch   79/269   train_loss = 3.968\n",
      "Epoch  30 Batch   80/269   train_loss = 3.847\n",
      "Epoch  30 Batch   81/269   train_loss = 3.660\n",
      "Epoch  30 Batch   82/269   train_loss = 3.993\n",
      "Epoch  30 Batch   83/269   train_loss = 3.760\n",
      "Epoch  30 Batch   84/269   train_loss = 3.980\n",
      "Epoch  30 Batch   85/269   train_loss = 4.191\n",
      "Epoch  30 Batch   86/269   train_loss = 4.037\n",
      "Epoch  30 Batch   87/269   train_loss = 3.753\n",
      "Epoch  30 Batch   88/269   train_loss = 3.838\n",
      "Epoch  30 Batch   89/269   train_loss = 3.787\n",
      "Epoch  30 Batch   90/269   train_loss = 3.633\n",
      "Epoch  30 Batch   91/269   train_loss = 3.925\n",
      "Epoch  30 Batch   92/269   train_loss = 3.974\n",
      "Epoch  30 Batch   93/269   train_loss = 3.661\n",
      "Epoch  30 Batch   94/269   train_loss = 4.044\n",
      "Epoch  30 Batch   95/269   train_loss = 3.622\n",
      "Epoch  30 Batch   96/269   train_loss = 3.673\n",
      "Epoch  30 Batch   97/269   train_loss = 3.773\n",
      "Epoch  30 Batch   98/269   train_loss = 3.929\n",
      "Epoch  30 Batch   99/269   train_loss = 3.762\n",
      "Epoch  30 Batch  100/269   train_loss = 3.736\n",
      "Epoch  30 Batch  101/269   train_loss = 3.952\n",
      "Epoch  30 Batch  102/269   train_loss = 3.597\n",
      "Epoch  30 Batch  103/269   train_loss = 3.671\n",
      "Epoch  30 Batch  104/269   train_loss = 3.681\n",
      "Epoch  30 Batch  105/269   train_loss = 3.897\n",
      "Epoch  30 Batch  106/269   train_loss = 4.028\n",
      "Epoch  30 Batch  107/269   train_loss = 3.988\n",
      "Epoch  30 Batch  108/269   train_loss = 3.834\n",
      "Epoch  30 Batch  109/269   train_loss = 3.843\n",
      "Epoch  30 Batch  110/269   train_loss = 3.836\n",
      "Epoch  30 Batch  111/269   train_loss = 3.931\n",
      "Epoch  30 Batch  112/269   train_loss = 3.925\n",
      "Epoch  30 Batch  113/269   train_loss = 3.784\n",
      "Epoch  30 Batch  114/269   train_loss = 3.757\n",
      "Epoch  30 Batch  115/269   train_loss = 3.649\n",
      "Epoch  30 Batch  116/269   train_loss = 3.639\n",
      "Epoch  30 Batch  117/269   train_loss = 3.624\n",
      "Epoch  30 Batch  118/269   train_loss = 3.997\n",
      "Epoch  30 Batch  119/269   train_loss = 3.612\n",
      "Epoch  30 Batch  120/269   train_loss = 3.857\n",
      "Epoch  30 Batch  121/269   train_loss = 3.716\n",
      "Epoch  30 Batch  122/269   train_loss = 3.848\n",
      "Epoch  30 Batch  123/269   train_loss = 3.625\n",
      "Epoch  30 Batch  124/269   train_loss = 3.981\n",
      "Epoch  30 Batch  125/269   train_loss = 3.560\n",
      "Epoch  30 Batch  126/269   train_loss = 3.456\n",
      "Epoch  30 Batch  127/269   train_loss = 3.773\n",
      "Epoch  30 Batch  128/269   train_loss = 3.736\n",
      "Epoch  30 Batch  129/269   train_loss = 3.684\n",
      "Epoch  30 Batch  130/269   train_loss = 3.855\n",
      "Epoch  30 Batch  131/269   train_loss = 3.466\n",
      "Epoch  30 Batch  132/269   train_loss = 3.804\n",
      "Epoch  30 Batch  133/269   train_loss = 3.572\n",
      "Epoch  30 Batch  134/269   train_loss = 3.984\n",
      "Epoch  30 Batch  135/269   train_loss = 3.601\n",
      "Epoch  30 Batch  136/269   train_loss = 3.504\n",
      "Epoch  30 Batch  137/269   train_loss = 3.864\n",
      "Epoch  30 Batch  138/269   train_loss = 3.599\n",
      "Epoch  30 Batch  139/269   train_loss = 3.764\n",
      "Epoch  30 Batch  140/269   train_loss = 3.813\n",
      "Epoch  30 Batch  141/269   train_loss = 4.106\n",
      "Epoch  30 Batch  142/269   train_loss = 3.585\n",
      "Epoch  30 Batch  143/269   train_loss = 3.776\n",
      "Epoch  30 Batch  144/269   train_loss = 3.893\n",
      "Epoch  30 Batch  145/269   train_loss = 3.724\n",
      "Epoch  30 Batch  146/269   train_loss = 3.924\n",
      "Epoch  30 Batch  147/269   train_loss = 3.574\n",
      "Epoch  30 Batch  148/269   train_loss = 3.881\n",
      "Epoch  30 Batch  149/269   train_loss = 3.737\n",
      "Epoch  30 Batch  150/269   train_loss = 3.854\n",
      "Epoch  30 Batch  151/269   train_loss = 3.767\n",
      "Epoch  30 Batch  152/269   train_loss = 3.763\n",
      "Epoch  30 Batch  153/269   train_loss = 3.858\n",
      "Epoch  30 Batch  154/269   train_loss = 3.686\n",
      "Epoch  30 Batch  155/269   train_loss = 3.651\n",
      "Epoch  30 Batch  156/269   train_loss = 4.143\n",
      "Epoch  30 Batch  157/269   train_loss = 3.908\n",
      "Epoch  30 Batch  158/269   train_loss = 3.848\n",
      "Epoch  30 Batch  159/269   train_loss = 3.831\n",
      "Epoch  30 Batch  160/269   train_loss = 3.734\n",
      "Epoch  30 Batch  161/269   train_loss = 3.872\n",
      "Epoch  30 Batch  162/269   train_loss = 3.827\n",
      "Epoch  30 Batch  163/269   train_loss = 3.803\n",
      "Epoch  30 Batch  164/269   train_loss = 3.944\n",
      "Epoch  30 Batch  165/269   train_loss = 3.911\n",
      "Epoch  30 Batch  166/269   train_loss = 3.831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30 Batch  167/269   train_loss = 3.680\n",
      "Epoch  30 Batch  168/269   train_loss = 3.755\n",
      "Epoch  30 Batch  169/269   train_loss = 4.009\n",
      "Epoch  30 Batch  170/269   train_loss = 3.621\n",
      "Epoch  30 Batch  171/269   train_loss = 3.841\n",
      "Epoch  30 Batch  172/269   train_loss = 3.570\n",
      "Epoch  30 Batch  173/269   train_loss = 3.822\n",
      "Epoch  30 Batch  174/269   train_loss = 3.962\n",
      "Epoch  30 Batch  175/269   train_loss = 3.834\n",
      "Epoch  30 Batch  176/269   train_loss = 3.748\n",
      "Epoch  30 Batch  177/269   train_loss = 3.785\n",
      "Epoch  30 Batch  178/269   train_loss = 4.078\n",
      "Epoch  30 Batch  179/269   train_loss = 3.796\n",
      "Epoch  30 Batch  180/269   train_loss = 3.674\n",
      "Epoch  30 Batch  181/269   train_loss = 3.876\n",
      "Epoch  30 Batch  182/269   train_loss = 3.730\n",
      "Epoch  30 Batch  183/269   train_loss = 3.732\n",
      "Epoch  30 Batch  184/269   train_loss = 3.808\n",
      "Epoch  30 Batch  185/269   train_loss = 3.831\n",
      "Epoch  30 Batch  186/269   train_loss = 3.709\n",
      "Epoch  30 Batch  187/269   train_loss = 3.970\n",
      "Epoch  30 Batch  188/269   train_loss = 3.708\n",
      "Epoch  30 Batch  189/269   train_loss = 3.741\n",
      "Epoch  30 Batch  190/269   train_loss = 4.143\n",
      "Epoch  30 Batch  191/269   train_loss = 3.866\n",
      "Epoch  30 Batch  192/269   train_loss = 3.748\n",
      "Epoch  30 Batch  193/269   train_loss = 3.721\n",
      "Epoch  30 Batch  194/269   train_loss = 3.900\n",
      "Epoch  30 Batch  195/269   train_loss = 3.710\n",
      "Epoch  30 Batch  196/269   train_loss = 3.838\n",
      "Epoch  30 Batch  197/269   train_loss = 3.986\n",
      "Epoch  30 Batch  198/269   train_loss = 3.894\n",
      "Epoch  30 Batch  199/269   train_loss = 3.827\n",
      "Epoch  30 Batch  200/269   train_loss = 3.753\n",
      "Epoch  30 Batch  201/269   train_loss = 3.749\n",
      "Epoch  30 Batch  202/269   train_loss = 3.670\n",
      "Epoch  30 Batch  203/269   train_loss = 3.646\n",
      "Epoch  30 Batch  204/269   train_loss = 3.869\n",
      "Epoch  30 Batch  205/269   train_loss = 3.853\n",
      "Epoch  30 Batch  206/269   train_loss = 3.741\n",
      "Epoch  30 Batch  207/269   train_loss = 3.741\n",
      "Epoch  30 Batch  208/269   train_loss = 3.807\n",
      "Epoch  30 Batch  209/269   train_loss = 3.904\n",
      "Epoch  30 Batch  210/269   train_loss = 3.665\n",
      "Epoch  30 Batch  211/269   train_loss = 3.743\n",
      "Epoch  30 Batch  212/269   train_loss = 4.050\n",
      "Epoch  30 Batch  213/269   train_loss = 3.702\n",
      "Epoch  30 Batch  214/269   train_loss = 3.760\n",
      "Epoch  30 Batch  215/269   train_loss = 4.029\n",
      "Epoch  30 Batch  216/269   train_loss = 3.938\n",
      "Epoch  30 Batch  217/269   train_loss = 3.675\n",
      "Epoch  30 Batch  218/269   train_loss = 3.848\n",
      "Epoch  30 Batch  219/269   train_loss = 3.490\n",
      "Epoch  30 Batch  220/269   train_loss = 3.912\n",
      "Epoch  30 Batch  221/269   train_loss = 3.678\n",
      "Epoch  30 Batch  222/269   train_loss = 3.844\n",
      "Epoch  30 Batch  223/269   train_loss = 3.643\n",
      "Epoch  30 Batch  224/269   train_loss = 3.917\n",
      "Epoch  30 Batch  225/269   train_loss = 3.998\n",
      "Epoch  30 Batch  226/269   train_loss = 3.774\n",
      "Epoch  30 Batch  227/269   train_loss = 3.488\n",
      "Epoch  30 Batch  228/269   train_loss = 3.788\n",
      "Epoch  30 Batch  229/269   train_loss = 3.923\n",
      "Epoch  30 Batch  230/269   train_loss = 3.862\n",
      "Epoch  30 Batch  231/269   train_loss = 3.738\n",
      "Epoch  30 Batch  232/269   train_loss = 3.747\n",
      "Epoch  30 Batch  233/269   train_loss = 3.856\n",
      "Epoch  30 Batch  234/269   train_loss = 3.807\n",
      "Epoch  30 Batch  235/269   train_loss = 4.019\n",
      "Epoch  30 Batch  236/269   train_loss = 3.651\n",
      "Epoch  30 Batch  237/269   train_loss = 3.376\n",
      "Epoch  30 Batch  238/269   train_loss = 3.615\n",
      "Epoch  30 Batch  239/269   train_loss = 4.214\n",
      "Epoch  30 Batch  240/269   train_loss = 3.763\n",
      "Epoch  30 Batch  241/269   train_loss = 3.919\n",
      "Epoch  30 Batch  242/269   train_loss = 3.599\n",
      "Epoch  30 Batch  243/269   train_loss = 3.927\n",
      "Epoch  30 Batch  244/269   train_loss = 3.754\n",
      "Epoch  30 Batch  245/269   train_loss = 3.779\n",
      "Epoch  30 Batch  246/269   train_loss = 3.430\n",
      "Epoch  30 Batch  247/269   train_loss = 3.878\n",
      "Epoch  30 Batch  248/269   train_loss = 3.870\n",
      "Epoch  30 Batch  249/269   train_loss = 3.666\n",
      "Epoch  30 Batch  250/269   train_loss = 3.532\n",
      "Epoch  30 Batch  251/269   train_loss = 3.966\n",
      "Epoch  30 Batch  252/269   train_loss = 3.951\n",
      "Epoch  30 Batch  253/269   train_loss = 3.748\n",
      "Epoch  30 Batch  254/269   train_loss = 3.702\n",
      "Epoch  30 Batch  255/269   train_loss = 3.771\n",
      "Epoch  30 Batch  256/269   train_loss = 3.669\n",
      "Epoch  30 Batch  257/269   train_loss = 3.588\n",
      "Epoch  30 Batch  258/269   train_loss = 3.540\n",
      "Epoch  30 Batch  259/269   train_loss = 3.586\n",
      "Epoch  30 Batch  260/269   train_loss = 3.847\n",
      "Epoch  30 Batch  261/269   train_loss = 3.952\n",
      "Epoch  30 Batch  262/269   train_loss = 3.686\n",
      "Epoch  30 Batch  263/269   train_loss = 3.535\n",
      "Epoch  30 Batch  264/269   train_loss = 4.054\n",
      "Epoch  30 Batch  265/269   train_loss = 3.871\n",
      "Epoch  30 Batch  266/269   train_loss = 3.739\n",
      "Epoch  30 Batch  267/269   train_loss = 3.846\n",
      "Epoch  30 Batch  268/269   train_loss = 3.937\n",
      "Epoch  31 Batch    0/269   train_loss = 3.678\n",
      "Epoch  31 Batch    1/269   train_loss = 3.536\n",
      "Epoch  31 Batch    2/269   train_loss = 3.696\n",
      "Epoch  31 Batch    3/269   train_loss = 3.718\n",
      "Epoch  31 Batch    4/269   train_loss = 4.131\n",
      "Epoch  31 Batch    5/269   train_loss = 3.797\n",
      "Epoch  31 Batch    6/269   train_loss = 3.686\n",
      "Epoch  31 Batch    7/269   train_loss = 3.506\n",
      "Epoch  31 Batch    8/269   train_loss = 3.925\n",
      "Epoch  31 Batch    9/269   train_loss = 3.557\n",
      "Epoch  31 Batch   10/269   train_loss = 3.567\n",
      "Epoch  31 Batch   11/269   train_loss = 3.742\n",
      "Epoch  31 Batch   12/269   train_loss = 3.460\n",
      "Epoch  31 Batch   13/269   train_loss = 3.775\n",
      "Epoch  31 Batch   14/269   train_loss = 3.622\n",
      "Epoch  31 Batch   15/269   train_loss = 4.018\n",
      "Epoch  31 Batch   16/269   train_loss = 3.714\n",
      "Epoch  31 Batch   17/269   train_loss = 3.780\n",
      "Epoch  31 Batch   18/269   train_loss = 3.700\n",
      "Epoch  31 Batch   19/269   train_loss = 3.709\n",
      "Epoch  31 Batch   20/269   train_loss = 4.046\n",
      "Epoch  31 Batch   21/269   train_loss = 3.910\n",
      "Epoch  31 Batch   22/269   train_loss = 3.669\n",
      "Epoch  31 Batch   23/269   train_loss = 3.791\n",
      "Epoch  31 Batch   24/269   train_loss = 3.670\n",
      "Epoch  31 Batch   25/269   train_loss = 4.004\n",
      "Epoch  31 Batch   26/269   train_loss = 3.842\n",
      "Epoch  31 Batch   27/269   train_loss = 3.648\n",
      "Epoch  31 Batch   28/269   train_loss = 3.819\n",
      "Epoch  31 Batch   29/269   train_loss = 3.815\n",
      "Epoch  31 Batch   30/269   train_loss = 4.123\n",
      "Epoch  31 Batch   31/269   train_loss = 3.757\n",
      "Epoch  31 Batch   32/269   train_loss = 3.646\n",
      "Epoch  31 Batch   33/269   train_loss = 3.462\n",
      "Epoch  31 Batch   34/269   train_loss = 3.728\n",
      "Epoch  31 Batch   35/269   train_loss = 3.567\n",
      "Epoch  31 Batch   36/269   train_loss = 3.606\n",
      "Epoch  31 Batch   37/269   train_loss = 3.819\n",
      "Epoch  31 Batch   38/269   train_loss = 3.524\n",
      "Epoch  31 Batch   39/269   train_loss = 3.898\n",
      "Epoch  31 Batch   40/269   train_loss = 3.639\n",
      "Epoch  31 Batch   41/269   train_loss = 3.624\n",
      "Epoch  31 Batch   42/269   train_loss = 4.143\n",
      "Epoch  31 Batch   43/269   train_loss = 3.927\n",
      "Epoch  31 Batch   44/269   train_loss = 3.733\n",
      "Epoch  31 Batch   45/269   train_loss = 3.500\n",
      "Epoch  31 Batch   46/269   train_loss = 3.830\n",
      "Epoch  31 Batch   47/269   train_loss = 3.461\n",
      "Epoch  31 Batch   48/269   train_loss = 4.031\n",
      "Epoch  31 Batch   49/269   train_loss = 4.015\n",
      "Epoch  31 Batch   50/269   train_loss = 3.888\n",
      "Epoch  31 Batch   51/269   train_loss = 3.800\n",
      "Epoch  31 Batch   52/269   train_loss = 3.851\n",
      "Epoch  31 Batch   53/269   train_loss = 4.035\n",
      "Epoch  31 Batch   54/269   train_loss = 3.838\n",
      "Epoch  31 Batch   55/269   train_loss = 3.877\n",
      "Epoch  31 Batch   56/269   train_loss = 3.701\n",
      "Epoch  31 Batch   57/269   train_loss = 3.948\n",
      "Epoch  31 Batch   58/269   train_loss = 3.634\n",
      "Epoch  31 Batch   59/269   train_loss = 3.650\n",
      "Epoch  31 Batch   60/269   train_loss = 3.787\n",
      "Epoch  31 Batch   61/269   train_loss = 4.058\n",
      "Epoch  31 Batch   62/269   train_loss = 3.652\n",
      "Epoch  31 Batch   63/269   train_loss = 3.971\n",
      "Epoch  31 Batch   64/269   train_loss = 3.702\n",
      "Epoch  31 Batch   65/269   train_loss = 3.806\n",
      "Epoch  31 Batch   66/269   train_loss = 3.696\n",
      "Epoch  31 Batch   67/269   train_loss = 3.765\n",
      "Epoch  31 Batch   68/269   train_loss = 3.803\n",
      "Epoch  31 Batch   69/269   train_loss = 3.829\n",
      "Epoch  31 Batch   70/269   train_loss = 3.453\n",
      "Epoch  31 Batch   71/269   train_loss = 3.855\n",
      "Epoch  31 Batch   72/269   train_loss = 3.584\n",
      "Epoch  31 Batch   73/269   train_loss = 3.709\n",
      "Epoch  31 Batch   74/269   train_loss = 3.774\n",
      "Epoch  31 Batch   75/269   train_loss = 3.958\n",
      "Epoch  31 Batch   76/269   train_loss = 3.805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  31 Batch   77/269   train_loss = 3.472\n",
      "Epoch  31 Batch   78/269   train_loss = 3.850\n",
      "Epoch  31 Batch   79/269   train_loss = 3.954\n",
      "Epoch  31 Batch   80/269   train_loss = 3.834\n",
      "Epoch  31 Batch   81/269   train_loss = 3.659\n",
      "Epoch  31 Batch   82/269   train_loss = 4.006\n",
      "Epoch  31 Batch   83/269   train_loss = 3.731\n",
      "Epoch  31 Batch   84/269   train_loss = 3.954\n",
      "Epoch  31 Batch   85/269   train_loss = 4.183\n",
      "Epoch  31 Batch   86/269   train_loss = 4.038\n",
      "Epoch  31 Batch   87/269   train_loss = 3.762\n",
      "Epoch  31 Batch   88/269   train_loss = 3.872\n",
      "Epoch  31 Batch   89/269   train_loss = 3.758\n",
      "Epoch  31 Batch   90/269   train_loss = 3.624\n",
      "Epoch  31 Batch   91/269   train_loss = 3.925\n",
      "Epoch  31 Batch   92/269   train_loss = 3.971\n",
      "Epoch  31 Batch   93/269   train_loss = 3.657\n",
      "Epoch  31 Batch   94/269   train_loss = 4.043\n",
      "Epoch  31 Batch   95/269   train_loss = 3.574\n",
      "Epoch  31 Batch   96/269   train_loss = 3.664\n",
      "Epoch  31 Batch   97/269   train_loss = 3.755\n",
      "Epoch  31 Batch   98/269   train_loss = 3.913\n",
      "Epoch  31 Batch   99/269   train_loss = 3.768\n",
      "Epoch  31 Batch  100/269   train_loss = 3.745\n",
      "Epoch  31 Batch  101/269   train_loss = 3.938\n",
      "Epoch  31 Batch  102/269   train_loss = 3.599\n",
      "Epoch  31 Batch  103/269   train_loss = 3.663\n",
      "Epoch  31 Batch  104/269   train_loss = 3.680\n",
      "Epoch  31 Batch  105/269   train_loss = 3.903\n",
      "Epoch  31 Batch  106/269   train_loss = 4.036\n",
      "Epoch  31 Batch  107/269   train_loss = 3.972\n",
      "Epoch  31 Batch  108/269   train_loss = 3.865\n",
      "Epoch  31 Batch  109/269   train_loss = 3.833\n",
      "Epoch  31 Batch  110/269   train_loss = 3.805\n",
      "Epoch  31 Batch  111/269   train_loss = 3.946\n",
      "Epoch  31 Batch  112/269   train_loss = 3.910\n",
      "Epoch  31 Batch  113/269   train_loss = 3.733\n",
      "Epoch  31 Batch  114/269   train_loss = 3.787\n",
      "Epoch  31 Batch  115/269   train_loss = 3.621\n",
      "Epoch  31 Batch  116/269   train_loss = 3.652\n",
      "Epoch  31 Batch  117/269   train_loss = 3.650\n",
      "Epoch  31 Batch  118/269   train_loss = 3.963\n",
      "Epoch  31 Batch  119/269   train_loss = 3.633\n",
      "Epoch  31 Batch  120/269   train_loss = 3.886\n",
      "Epoch  31 Batch  121/269   train_loss = 3.704\n",
      "Epoch  31 Batch  122/269   train_loss = 3.858\n",
      "Epoch  31 Batch  123/269   train_loss = 3.590\n",
      "Epoch  31 Batch  124/269   train_loss = 3.967\n",
      "Epoch  31 Batch  125/269   train_loss = 3.528\n",
      "Epoch  31 Batch  126/269   train_loss = 3.453\n",
      "Epoch  31 Batch  127/269   train_loss = 3.753\n",
      "Epoch  31 Batch  128/269   train_loss = 3.748\n",
      "Epoch  31 Batch  129/269   train_loss = 3.709\n",
      "Epoch  31 Batch  130/269   train_loss = 3.880\n",
      "Epoch  31 Batch  131/269   train_loss = 3.472\n",
      "Epoch  31 Batch  132/269   train_loss = 3.790\n",
      "Epoch  31 Batch  133/269   train_loss = 3.588\n",
      "Epoch  31 Batch  134/269   train_loss = 3.965\n",
      "Epoch  31 Batch  135/269   train_loss = 3.625\n",
      "Epoch  31 Batch  136/269   train_loss = 3.502\n",
      "Epoch  31 Batch  137/269   train_loss = 3.843\n",
      "Epoch  31 Batch  138/269   train_loss = 3.591\n",
      "Epoch  31 Batch  139/269   train_loss = 3.785\n",
      "Epoch  31 Batch  140/269   train_loss = 3.812\n",
      "Epoch  31 Batch  141/269   train_loss = 4.096\n",
      "Epoch  31 Batch  142/269   train_loss = 3.581\n",
      "Epoch  31 Batch  143/269   train_loss = 3.764\n",
      "Epoch  31 Batch  144/269   train_loss = 3.881\n",
      "Epoch  31 Batch  145/269   train_loss = 3.748\n",
      "Epoch  31 Batch  146/269   train_loss = 3.941\n",
      "Epoch  31 Batch  147/269   train_loss = 3.568\n",
      "Epoch  31 Batch  148/269   train_loss = 3.870\n",
      "Epoch  31 Batch  149/269   train_loss = 3.731\n",
      "Epoch  31 Batch  150/269   train_loss = 3.846\n",
      "Epoch  31 Batch  151/269   train_loss = 3.805\n",
      "Epoch  31 Batch  152/269   train_loss = 3.774\n",
      "Epoch  31 Batch  153/269   train_loss = 3.796\n",
      "Epoch  31 Batch  154/269   train_loss = 3.685\n",
      "Epoch  31 Batch  155/269   train_loss = 3.670\n",
      "Epoch  31 Batch  156/269   train_loss = 4.135\n",
      "Epoch  31 Batch  157/269   train_loss = 3.879\n",
      "Epoch  31 Batch  158/269   train_loss = 3.859\n",
      "Epoch  31 Batch  159/269   train_loss = 3.870\n",
      "Epoch  31 Batch  160/269   train_loss = 3.749\n",
      "Epoch  31 Batch  161/269   train_loss = 3.890\n",
      "Epoch  31 Batch  162/269   train_loss = 3.836\n",
      "Epoch  31 Batch  163/269   train_loss = 3.823\n",
      "Epoch  31 Batch  164/269   train_loss = 3.935\n",
      "Epoch  31 Batch  165/269   train_loss = 3.917\n",
      "Epoch  31 Batch  166/269   train_loss = 3.818\n",
      "Epoch  31 Batch  167/269   train_loss = 3.639\n",
      "Epoch  31 Batch  168/269   train_loss = 3.738\n",
      "Epoch  31 Batch  169/269   train_loss = 3.989\n",
      "Epoch  31 Batch  170/269   train_loss = 3.585\n",
      "Epoch  31 Batch  171/269   train_loss = 3.851\n",
      "Epoch  31 Batch  172/269   train_loss = 3.554\n",
      "Epoch  31 Batch  173/269   train_loss = 3.778\n",
      "Epoch  31 Batch  174/269   train_loss = 3.949\n",
      "Epoch  31 Batch  175/269   train_loss = 3.848\n",
      "Epoch  31 Batch  176/269   train_loss = 3.767\n",
      "Epoch  31 Batch  177/269   train_loss = 3.783\n",
      "Epoch  31 Batch  178/269   train_loss = 4.055\n",
      "Epoch  31 Batch  179/269   train_loss = 3.800\n",
      "Epoch  31 Batch  180/269   train_loss = 3.635\n",
      "Epoch  31 Batch  181/269   train_loss = 3.891\n",
      "Epoch  31 Batch  182/269   train_loss = 3.745\n",
      "Epoch  31 Batch  183/269   train_loss = 3.722\n",
      "Epoch  31 Batch  184/269   train_loss = 3.768\n",
      "Epoch  31 Batch  185/269   train_loss = 3.837\n",
      "Epoch  31 Batch  186/269   train_loss = 3.681\n",
      "Epoch  31 Batch  187/269   train_loss = 3.955\n",
      "Epoch  31 Batch  188/269   train_loss = 3.704\n",
      "Epoch  31 Batch  189/269   train_loss = 3.742\n",
      "Epoch  31 Batch  190/269   train_loss = 4.165\n",
      "Epoch  31 Batch  191/269   train_loss = 3.858\n",
      "Epoch  31 Batch  192/269   train_loss = 3.731\n",
      "Epoch  31 Batch  193/269   train_loss = 3.725\n",
      "Epoch  31 Batch  194/269   train_loss = 3.880\n",
      "Epoch  31 Batch  195/269   train_loss = 3.718\n",
      "Epoch  31 Batch  196/269   train_loss = 3.846\n",
      "Epoch  31 Batch  197/269   train_loss = 3.969\n",
      "Epoch  31 Batch  198/269   train_loss = 3.921\n",
      "Epoch  31 Batch  199/269   train_loss = 3.816\n",
      "Epoch  31 Batch  200/269   train_loss = 3.758\n",
      "Epoch  31 Batch  201/269   train_loss = 3.756\n",
      "Epoch  31 Batch  202/269   train_loss = 3.657\n",
      "Epoch  31 Batch  203/269   train_loss = 3.658\n",
      "Epoch  31 Batch  204/269   train_loss = 3.871\n",
      "Epoch  31 Batch  205/269   train_loss = 3.871\n",
      "Epoch  31 Batch  206/269   train_loss = 3.718\n",
      "Epoch  31 Batch  207/269   train_loss = 3.753\n",
      "Epoch  31 Batch  208/269   train_loss = 3.817\n",
      "Epoch  31 Batch  209/269   train_loss = 3.896\n",
      "Epoch  31 Batch  210/269   train_loss = 3.649\n",
      "Epoch  31 Batch  211/269   train_loss = 3.734\n",
      "Epoch  31 Batch  212/269   train_loss = 4.078\n",
      "Epoch  31 Batch  213/269   train_loss = 3.700\n",
      "Epoch  31 Batch  214/269   train_loss = 3.754\n",
      "Epoch  31 Batch  215/269   train_loss = 4.028\n",
      "Epoch  31 Batch  216/269   train_loss = 3.954\n",
      "Epoch  31 Batch  217/269   train_loss = 3.664\n",
      "Epoch  31 Batch  218/269   train_loss = 3.832\n",
      "Epoch  31 Batch  219/269   train_loss = 3.514\n",
      "Epoch  31 Batch  220/269   train_loss = 3.920\n",
      "Epoch  31 Batch  221/269   train_loss = 3.669\n",
      "Epoch  31 Batch  222/269   train_loss = 3.874\n",
      "Epoch  31 Batch  223/269   train_loss = 3.601\n",
      "Epoch  31 Batch  224/269   train_loss = 3.901\n",
      "Epoch  31 Batch  225/269   train_loss = 4.002\n",
      "Epoch  31 Batch  226/269   train_loss = 3.747\n",
      "Epoch  31 Batch  227/269   train_loss = 3.512\n",
      "Epoch  31 Batch  228/269   train_loss = 3.760\n",
      "Epoch  31 Batch  229/269   train_loss = 3.909\n",
      "Epoch  31 Batch  230/269   train_loss = 3.830\n",
      "Epoch  31 Batch  231/269   train_loss = 3.741\n",
      "Epoch  31 Batch  232/269   train_loss = 3.745\n",
      "Epoch  31 Batch  233/269   train_loss = 3.827\n",
      "Epoch  31 Batch  234/269   train_loss = 3.825\n",
      "Epoch  31 Batch  235/269   train_loss = 4.013\n",
      "Epoch  31 Batch  236/269   train_loss = 3.645\n",
      "Epoch  31 Batch  237/269   train_loss = 3.352\n",
      "Epoch  31 Batch  238/269   train_loss = 3.597\n",
      "Epoch  31 Batch  239/269   train_loss = 4.178\n",
      "Epoch  31 Batch  240/269   train_loss = 3.729\n",
      "Epoch  31 Batch  241/269   train_loss = 3.919\n",
      "Epoch  31 Batch  242/269   train_loss = 3.569\n",
      "Epoch  31 Batch  243/269   train_loss = 3.941\n",
      "Epoch  31 Batch  244/269   train_loss = 3.751\n",
      "Epoch  31 Batch  245/269   train_loss = 3.752\n",
      "Epoch  31 Batch  246/269   train_loss = 3.448\n",
      "Epoch  31 Batch  247/269   train_loss = 3.869\n",
      "Epoch  31 Batch  248/269   train_loss = 3.867\n",
      "Epoch  31 Batch  249/269   train_loss = 3.682\n",
      "Epoch  31 Batch  250/269   train_loss = 3.533\n",
      "Epoch  31 Batch  251/269   train_loss = 3.956\n",
      "Epoch  31 Batch  252/269   train_loss = 3.920\n",
      "Epoch  31 Batch  253/269   train_loss = 3.743\n",
      "Epoch  31 Batch  254/269   train_loss = 3.669\n",
      "Epoch  31 Batch  255/269   train_loss = 3.780\n",
      "Epoch  31 Batch  256/269   train_loss = 3.638\n",
      "Epoch  31 Batch  257/269   train_loss = 3.585\n",
      "Epoch  31 Batch  258/269   train_loss = 3.504\n",
      "Epoch  31 Batch  259/269   train_loss = 3.618\n",
      "Epoch  31 Batch  260/269   train_loss = 3.848\n",
      "Epoch  31 Batch  261/269   train_loss = 3.939\n",
      "Epoch  31 Batch  262/269   train_loss = 3.672\n",
      "Epoch  31 Batch  263/269   train_loss = 3.502\n",
      "Epoch  31 Batch  264/269   train_loss = 4.077\n",
      "Epoch  31 Batch  265/269   train_loss = 3.872\n",
      "Epoch  31 Batch  266/269   train_loss = 3.751\n",
      "Epoch  31 Batch  267/269   train_loss = 3.810\n",
      "Epoch  31 Batch  268/269   train_loss = 3.915\n",
      "Epoch  32 Batch    0/269   train_loss = 3.681\n",
      "Epoch  32 Batch    1/269   train_loss = 3.517\n",
      "Epoch  32 Batch    2/269   train_loss = 3.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  32 Batch    3/269   train_loss = 3.706\n",
      "Epoch  32 Batch    4/269   train_loss = 4.102\n",
      "Epoch  32 Batch    5/269   train_loss = 3.814\n",
      "Epoch  32 Batch    6/269   train_loss = 3.694\n",
      "Epoch  32 Batch    7/269   train_loss = 3.530\n",
      "Epoch  32 Batch    8/269   train_loss = 3.935\n",
      "Epoch  32 Batch    9/269   train_loss = 3.531\n",
      "Epoch  32 Batch   10/269   train_loss = 3.557\n",
      "Epoch  32 Batch   11/269   train_loss = 3.724\n",
      "Epoch  32 Batch   12/269   train_loss = 3.431\n",
      "Epoch  32 Batch   13/269   train_loss = 3.773\n",
      "Epoch  32 Batch   14/269   train_loss = 3.615\n",
      "Epoch  32 Batch   15/269   train_loss = 3.991\n",
      "Epoch  32 Batch   16/269   train_loss = 3.696\n",
      "Epoch  32 Batch   17/269   train_loss = 3.776\n",
      "Epoch  32 Batch   18/269   train_loss = 3.681\n",
      "Epoch  32 Batch   19/269   train_loss = 3.700\n",
      "Epoch  32 Batch   20/269   train_loss = 4.037\n",
      "Epoch  32 Batch   21/269   train_loss = 3.877\n",
      "Epoch  32 Batch   22/269   train_loss = 3.662\n",
      "Epoch  32 Batch   23/269   train_loss = 3.849\n",
      "Epoch  32 Batch   24/269   train_loss = 3.679\n",
      "Epoch  32 Batch   25/269   train_loss = 3.955\n",
      "Epoch  32 Batch   26/269   train_loss = 3.805\n",
      "Epoch  32 Batch   27/269   train_loss = 3.621\n",
      "Epoch  32 Batch   28/269   train_loss = 3.800\n",
      "Epoch  32 Batch   29/269   train_loss = 3.803\n",
      "Epoch  32 Batch   30/269   train_loss = 4.126\n",
      "Epoch  32 Batch   31/269   train_loss = 3.746\n",
      "Epoch  32 Batch   32/269   train_loss = 3.657\n",
      "Epoch  32 Batch   33/269   train_loss = 3.438\n",
      "Epoch  32 Batch   34/269   train_loss = 3.741\n",
      "Epoch  32 Batch   35/269   train_loss = 3.550\n",
      "Epoch  32 Batch   36/269   train_loss = 3.610\n",
      "Epoch  32 Batch   37/269   train_loss = 3.819\n",
      "Epoch  32 Batch   38/269   train_loss = 3.510\n",
      "Epoch  32 Batch   39/269   train_loss = 3.860\n",
      "Epoch  32 Batch   40/269   train_loss = 3.635\n",
      "Epoch  32 Batch   41/269   train_loss = 3.619\n",
      "Epoch  32 Batch   42/269   train_loss = 4.120\n",
      "Epoch  32 Batch   43/269   train_loss = 3.898\n",
      "Epoch  32 Batch   44/269   train_loss = 3.753\n",
      "Epoch  32 Batch   45/269   train_loss = 3.496\n",
      "Epoch  32 Batch   46/269   train_loss = 3.861\n",
      "Epoch  32 Batch   47/269   train_loss = 3.448\n",
      "Epoch  32 Batch   48/269   train_loss = 3.999\n",
      "Epoch  32 Batch   49/269   train_loss = 3.971\n",
      "Epoch  32 Batch   50/269   train_loss = 3.878\n",
      "Epoch  32 Batch   51/269   train_loss = 3.806\n",
      "Epoch  32 Batch   52/269   train_loss = 3.839\n",
      "Epoch  32 Batch   53/269   train_loss = 4.027\n",
      "Epoch  32 Batch   54/269   train_loss = 3.837\n",
      "Epoch  32 Batch   55/269   train_loss = 3.861\n",
      "Epoch  32 Batch   56/269   train_loss = 3.700\n",
      "Epoch  32 Batch   57/269   train_loss = 3.917\n",
      "Epoch  32 Batch   58/269   train_loss = 3.627\n",
      "Epoch  32 Batch   59/269   train_loss = 3.664\n",
      "Epoch  32 Batch   60/269   train_loss = 3.769\n",
      "Epoch  32 Batch   61/269   train_loss = 4.039\n",
      "Epoch  32 Batch   62/269   train_loss = 3.638\n",
      "Epoch  32 Batch   63/269   train_loss = 3.980\n",
      "Epoch  32 Batch   64/269   train_loss = 3.716\n",
      "Epoch  32 Batch   65/269   train_loss = 3.814\n",
      "Epoch  32 Batch   66/269   train_loss = 3.672\n",
      "Epoch  32 Batch   67/269   train_loss = 3.766\n",
      "Epoch  32 Batch   68/269   train_loss = 3.797\n",
      "Epoch  32 Batch   69/269   train_loss = 3.802\n",
      "Epoch  32 Batch   70/269   train_loss = 3.470\n",
      "Epoch  32 Batch   71/269   train_loss = 3.828\n",
      "Epoch  32 Batch   72/269   train_loss = 3.543\n",
      "Epoch  32 Batch   73/269   train_loss = 3.682\n",
      "Epoch  32 Batch   74/269   train_loss = 3.758\n",
      "Epoch  32 Batch   75/269   train_loss = 3.930\n",
      "Epoch  32 Batch   76/269   train_loss = 3.791\n",
      "Epoch  32 Batch   77/269   train_loss = 3.472\n",
      "Epoch  32 Batch   78/269   train_loss = 3.834\n",
      "Epoch  32 Batch   79/269   train_loss = 3.937\n",
      "Epoch  32 Batch   80/269   train_loss = 3.829\n",
      "Epoch  32 Batch   81/269   train_loss = 3.647\n",
      "Epoch  32 Batch   82/269   train_loss = 3.971\n",
      "Epoch  32 Batch   83/269   train_loss = 3.728\n",
      "Epoch  32 Batch   84/269   train_loss = 3.921\n",
      "Epoch  32 Batch   85/269   train_loss = 4.147\n",
      "Epoch  32 Batch   86/269   train_loss = 4.031\n",
      "Epoch  32 Batch   87/269   train_loss = 3.788\n",
      "Epoch  32 Batch   88/269   train_loss = 3.823\n",
      "Epoch  32 Batch   89/269   train_loss = 3.752\n",
      "Epoch  32 Batch   90/269   train_loss = 3.637\n",
      "Epoch  32 Batch   91/269   train_loss = 3.927\n",
      "Epoch  32 Batch   92/269   train_loss = 3.952\n",
      "Epoch  32 Batch   93/269   train_loss = 3.660\n",
      "Epoch  32 Batch   94/269   train_loss = 4.058\n",
      "Epoch  32 Batch   95/269   train_loss = 3.592\n",
      "Epoch  32 Batch   96/269   train_loss = 3.643\n",
      "Epoch  32 Batch   97/269   train_loss = 3.742\n",
      "Epoch  32 Batch   98/269   train_loss = 3.909\n",
      "Epoch  32 Batch   99/269   train_loss = 3.768\n",
      "Epoch  32 Batch  100/269   train_loss = 3.722\n",
      "Epoch  32 Batch  101/269   train_loss = 3.918\n",
      "Epoch  32 Batch  102/269   train_loss = 3.603\n",
      "Epoch  32 Batch  103/269   train_loss = 3.659\n",
      "Epoch  32 Batch  104/269   train_loss = 3.670\n",
      "Epoch  32 Batch  105/269   train_loss = 3.916\n",
      "Epoch  32 Batch  106/269   train_loss = 4.038\n",
      "Epoch  32 Batch  107/269   train_loss = 3.968\n",
      "Epoch  32 Batch  108/269   train_loss = 3.824\n",
      "Epoch  32 Batch  109/269   train_loss = 3.823\n",
      "Epoch  32 Batch  110/269   train_loss = 3.795\n",
      "Epoch  32 Batch  111/269   train_loss = 3.905\n",
      "Epoch  32 Batch  112/269   train_loss = 3.916\n",
      "Epoch  32 Batch  113/269   train_loss = 3.729\n",
      "Epoch  32 Batch  114/269   train_loss = 3.799\n",
      "Epoch  32 Batch  115/269   train_loss = 3.617\n",
      "Epoch  32 Batch  116/269   train_loss = 3.597\n",
      "Epoch  32 Batch  117/269   train_loss = 3.629\n",
      "Epoch  32 Batch  118/269   train_loss = 3.975\n",
      "Epoch  32 Batch  119/269   train_loss = 3.617\n",
      "Epoch  32 Batch  120/269   train_loss = 3.847\n",
      "Epoch  32 Batch  121/269   train_loss = 3.706\n",
      "Epoch  32 Batch  122/269   train_loss = 3.828\n",
      "Epoch  32 Batch  123/269   train_loss = 3.569\n",
      "Epoch  32 Batch  124/269   train_loss = 3.946\n",
      "Epoch  32 Batch  125/269   train_loss = 3.526\n",
      "Epoch  32 Batch  126/269   train_loss = 3.447\n",
      "Epoch  32 Batch  127/269   train_loss = 3.736\n",
      "Epoch  32 Batch  128/269   train_loss = 3.754\n",
      "Epoch  32 Batch  129/269   train_loss = 3.718\n",
      "Epoch  32 Batch  130/269   train_loss = 3.856\n",
      "Epoch  32 Batch  131/269   train_loss = 3.440\n",
      "Epoch  32 Batch  132/269   train_loss = 3.789\n",
      "Epoch  32 Batch  133/269   train_loss = 3.537\n",
      "Epoch  32 Batch  134/269   train_loss = 3.960\n",
      "Epoch  32 Batch  135/269   train_loss = 3.565\n",
      "Epoch  32 Batch  136/269   train_loss = 3.490\n",
      "Epoch  32 Batch  137/269   train_loss = 3.820\n",
      "Epoch  32 Batch  138/269   train_loss = 3.581\n",
      "Epoch  32 Batch  139/269   train_loss = 3.811\n",
      "Epoch  32 Batch  140/269   train_loss = 3.840\n",
      "Epoch  32 Batch  141/269   train_loss = 4.088\n",
      "Epoch  32 Batch  142/269   train_loss = 3.566\n",
      "Epoch  32 Batch  143/269   train_loss = 3.760\n",
      "Epoch  32 Batch  144/269   train_loss = 3.884\n",
      "Epoch  32 Batch  145/269   train_loss = 3.674\n",
      "Epoch  32 Batch  146/269   train_loss = 3.895\n",
      "Epoch  32 Batch  147/269   train_loss = 3.545\n",
      "Epoch  32 Batch  148/269   train_loss = 3.852\n",
      "Epoch  32 Batch  149/269   train_loss = 3.738\n",
      "Epoch  32 Batch  150/269   train_loss = 3.845\n",
      "Epoch  32 Batch  151/269   train_loss = 3.801\n",
      "Epoch  32 Batch  152/269   train_loss = 3.778\n",
      "Epoch  32 Batch  153/269   train_loss = 3.780\n",
      "Epoch  32 Batch  154/269   train_loss = 3.653\n",
      "Epoch  32 Batch  155/269   train_loss = 3.674\n",
      "Epoch  32 Batch  156/269   train_loss = 4.119\n",
      "Epoch  32 Batch  157/269   train_loss = 3.898\n",
      "Epoch  32 Batch  158/269   train_loss = 3.870\n",
      "Epoch  32 Batch  159/269   train_loss = 3.843\n",
      "Epoch  32 Batch  160/269   train_loss = 3.687\n",
      "Epoch  32 Batch  161/269   train_loss = 3.858\n",
      "Epoch  32 Batch  162/269   train_loss = 3.808\n",
      "Epoch  32 Batch  163/269   train_loss = 3.802\n",
      "Epoch  32 Batch  164/269   train_loss = 3.909\n",
      "Epoch  32 Batch  165/269   train_loss = 3.893\n",
      "Epoch  32 Batch  166/269   train_loss = 3.823\n",
      "Epoch  32 Batch  167/269   train_loss = 3.641\n",
      "Epoch  32 Batch  168/269   train_loss = 3.733\n",
      "Epoch  32 Batch  169/269   train_loss = 3.990\n",
      "Epoch  32 Batch  170/269   train_loss = 3.591\n",
      "Epoch  32 Batch  171/269   train_loss = 3.817\n",
      "Epoch  32 Batch  172/269   train_loss = 3.554\n",
      "Epoch  32 Batch  173/269   train_loss = 3.778\n",
      "Epoch  32 Batch  174/269   train_loss = 3.942\n",
      "Epoch  32 Batch  175/269   train_loss = 3.849\n",
      "Epoch  32 Batch  176/269   train_loss = 3.738\n",
      "Epoch  32 Batch  177/269   train_loss = 3.781\n",
      "Epoch  32 Batch  178/269   train_loss = 4.077\n",
      "Epoch  32 Batch  179/269   train_loss = 3.779\n",
      "Epoch  32 Batch  180/269   train_loss = 3.609\n",
      "Epoch  32 Batch  181/269   train_loss = 3.888\n",
      "Epoch  32 Batch  182/269   train_loss = 3.746\n",
      "Epoch  32 Batch  183/269   train_loss = 3.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  32 Batch  184/269   train_loss = 3.742\n",
      "Epoch  32 Batch  185/269   train_loss = 3.814\n",
      "Epoch  32 Batch  186/269   train_loss = 3.699\n",
      "Epoch  32 Batch  187/269   train_loss = 3.962\n",
      "Epoch  32 Batch  188/269   train_loss = 3.712\n",
      "Epoch  32 Batch  189/269   train_loss = 3.713\n",
      "Epoch  32 Batch  190/269   train_loss = 4.150\n",
      "Epoch  32 Batch  191/269   train_loss = 3.859\n",
      "Epoch  32 Batch  192/269   train_loss = 3.704\n",
      "Epoch  32 Batch  193/269   train_loss = 3.716\n",
      "Epoch  32 Batch  194/269   train_loss = 3.872\n",
      "Epoch  32 Batch  195/269   train_loss = 3.702\n",
      "Epoch  32 Batch  196/269   train_loss = 3.833\n",
      "Epoch  32 Batch  197/269   train_loss = 3.959\n",
      "Epoch  32 Batch  198/269   train_loss = 3.892\n",
      "Epoch  32 Batch  199/269   train_loss = 3.839\n",
      "Epoch  32 Batch  200/269   train_loss = 3.730\n",
      "Epoch  32 Batch  201/269   train_loss = 3.737\n",
      "Epoch  32 Batch  202/269   train_loss = 3.628\n",
      "Epoch  32 Batch  203/269   train_loss = 3.635\n",
      "Epoch  32 Batch  204/269   train_loss = 3.867\n",
      "Epoch  32 Batch  205/269   train_loss = 3.877\n",
      "Epoch  32 Batch  206/269   train_loss = 3.709\n",
      "Epoch  32 Batch  207/269   train_loss = 3.705\n",
      "Epoch  32 Batch  208/269   train_loss = 3.826\n",
      "Epoch  32 Batch  209/269   train_loss = 3.877\n",
      "Epoch  32 Batch  210/269   train_loss = 3.629\n",
      "Epoch  32 Batch  211/269   train_loss = 3.727\n",
      "Epoch  32 Batch  212/269   train_loss = 4.052\n",
      "Epoch  32 Batch  213/269   train_loss = 3.648\n",
      "Epoch  32 Batch  214/269   train_loss = 3.721\n",
      "Epoch  32 Batch  215/269   train_loss = 3.997\n",
      "Epoch  32 Batch  216/269   train_loss = 3.950\n",
      "Epoch  32 Batch  217/269   train_loss = 3.642\n",
      "Epoch  32 Batch  218/269   train_loss = 3.824\n",
      "Epoch  32 Batch  219/269   train_loss = 3.494\n",
      "Epoch  32 Batch  220/269   train_loss = 3.915\n",
      "Epoch  32 Batch  221/269   train_loss = 3.627\n",
      "Epoch  32 Batch  222/269   train_loss = 3.841\n",
      "Epoch  32 Batch  223/269   train_loss = 3.610\n",
      "Epoch  32 Batch  224/269   train_loss = 3.908\n",
      "Epoch  32 Batch  225/269   train_loss = 4.025\n",
      "Epoch  32 Batch  226/269   train_loss = 3.747\n",
      "Epoch  32 Batch  227/269   train_loss = 3.509\n",
      "Epoch  32 Batch  228/269   train_loss = 3.769\n",
      "Epoch  32 Batch  229/269   train_loss = 3.915\n",
      "Epoch  32 Batch  230/269   train_loss = 3.809\n",
      "Epoch  32 Batch  231/269   train_loss = 3.735\n",
      "Epoch  32 Batch  232/269   train_loss = 3.732\n",
      "Epoch  32 Batch  233/269   train_loss = 3.819\n",
      "Epoch  32 Batch  234/269   train_loss = 3.793\n",
      "Epoch  32 Batch  235/269   train_loss = 4.051\n",
      "Epoch  32 Batch  236/269   train_loss = 3.630\n",
      "Epoch  32 Batch  237/269   train_loss = 3.352\n",
      "Epoch  32 Batch  238/269   train_loss = 3.554\n",
      "Epoch  32 Batch  239/269   train_loss = 4.150\n",
      "Epoch  32 Batch  240/269   train_loss = 3.746\n",
      "Epoch  32 Batch  241/269   train_loss = 3.895\n",
      "Epoch  32 Batch  242/269   train_loss = 3.551\n",
      "Epoch  32 Batch  243/269   train_loss = 3.940\n",
      "Epoch  32 Batch  244/269   train_loss = 3.749\n",
      "Epoch  32 Batch  245/269   train_loss = 3.756\n",
      "Epoch  32 Batch  246/269   train_loss = 3.450\n",
      "Epoch  32 Batch  247/269   train_loss = 3.846\n",
      "Epoch  32 Batch  248/269   train_loss = 3.868\n",
      "Epoch  32 Batch  249/269   train_loss = 3.663\n",
      "Epoch  32 Batch  250/269   train_loss = 3.545\n",
      "Epoch  32 Batch  251/269   train_loss = 3.945\n",
      "Epoch  32 Batch  252/269   train_loss = 3.879\n",
      "Epoch  32 Batch  253/269   train_loss = 3.702\n",
      "Epoch  32 Batch  254/269   train_loss = 3.667\n",
      "Epoch  32 Batch  255/269   train_loss = 3.756\n",
      "Epoch  32 Batch  256/269   train_loss = 3.640\n",
      "Epoch  32 Batch  257/269   train_loss = 3.571\n",
      "Epoch  32 Batch  258/269   train_loss = 3.472\n",
      "Epoch  32 Batch  259/269   train_loss = 3.592\n",
      "Epoch  32 Batch  260/269   train_loss = 3.848\n",
      "Epoch  32 Batch  261/269   train_loss = 3.883\n",
      "Epoch  32 Batch  262/269   train_loss = 3.681\n",
      "Epoch  32 Batch  263/269   train_loss = 3.491\n",
      "Epoch  32 Batch  264/269   train_loss = 4.023\n",
      "Epoch  32 Batch  265/269   train_loss = 3.833\n",
      "Epoch  32 Batch  266/269   train_loss = 3.735\n",
      "Epoch  32 Batch  267/269   train_loss = 3.790\n",
      "Epoch  32 Batch  268/269   train_loss = 3.879\n",
      "Epoch  33 Batch    0/269   train_loss = 3.652\n",
      "Epoch  33 Batch    1/269   train_loss = 3.518\n",
      "Epoch  33 Batch    2/269   train_loss = 3.655\n",
      "Epoch  33 Batch    3/269   train_loss = 3.674\n",
      "Epoch  33 Batch    4/269   train_loss = 4.071\n",
      "Epoch  33 Batch    5/269   train_loss = 3.768\n",
      "Epoch  33 Batch    6/269   train_loss = 3.679\n",
      "Epoch  33 Batch    7/269   train_loss = 3.525\n",
      "Epoch  33 Batch    8/269   train_loss = 3.905\n",
      "Epoch  33 Batch    9/269   train_loss = 3.520\n",
      "Epoch  33 Batch   10/269   train_loss = 3.549\n",
      "Epoch  33 Batch   11/269   train_loss = 3.723\n",
      "Epoch  33 Batch   12/269   train_loss = 3.403\n",
      "Epoch  33 Batch   13/269   train_loss = 3.764\n",
      "Epoch  33 Batch   14/269   train_loss = 3.592\n",
      "Epoch  33 Batch   15/269   train_loss = 3.971\n",
      "Epoch  33 Batch   16/269   train_loss = 3.699\n",
      "Epoch  33 Batch   17/269   train_loss = 3.749\n",
      "Epoch  33 Batch   18/269   train_loss = 3.680\n",
      "Epoch  33 Batch   19/269   train_loss = 3.663\n",
      "Epoch  33 Batch   20/269   train_loss = 4.027\n",
      "Epoch  33 Batch   21/269   train_loss = 3.862\n",
      "Epoch  33 Batch   22/269   train_loss = 3.636\n",
      "Epoch  33 Batch   23/269   train_loss = 3.796\n",
      "Epoch  33 Batch   24/269   train_loss = 3.661\n",
      "Epoch  33 Batch   25/269   train_loss = 3.965\n",
      "Epoch  33 Batch   26/269   train_loss = 3.784\n",
      "Epoch  33 Batch   27/269   train_loss = 3.629\n",
      "Epoch  33 Batch   28/269   train_loss = 3.788\n",
      "Epoch  33 Batch   29/269   train_loss = 3.821\n",
      "Epoch  33 Batch   30/269   train_loss = 4.110\n",
      "Epoch  33 Batch   31/269   train_loss = 3.769\n",
      "Epoch  33 Batch   32/269   train_loss = 3.628\n",
      "Epoch  33 Batch   33/269   train_loss = 3.450\n",
      "Epoch  33 Batch   34/269   train_loss = 3.764\n",
      "Epoch  33 Batch   35/269   train_loss = 3.606\n",
      "Epoch  33 Batch   36/269   train_loss = 3.638\n",
      "Epoch  33 Batch   37/269   train_loss = 3.842\n",
      "Epoch  33 Batch   38/269   train_loss = 3.538\n",
      "Epoch  33 Batch   39/269   train_loss = 3.858\n",
      "Epoch  33 Batch   40/269   train_loss = 3.640\n",
      "Epoch  33 Batch   41/269   train_loss = 3.602\n",
      "Epoch  33 Batch   42/269   train_loss = 4.108\n",
      "Epoch  33 Batch   43/269   train_loss = 3.881\n",
      "Epoch  33 Batch   44/269   train_loss = 3.737\n",
      "Epoch  33 Batch   45/269   train_loss = 3.474\n",
      "Epoch  33 Batch   46/269   train_loss = 3.856\n",
      "Epoch  33 Batch   47/269   train_loss = 3.437\n",
      "Epoch  33 Batch   48/269   train_loss = 3.995\n",
      "Epoch  33 Batch   49/269   train_loss = 3.985\n",
      "Epoch  33 Batch   50/269   train_loss = 3.858\n",
      "Epoch  33 Batch   51/269   train_loss = 3.802\n",
      "Epoch  33 Batch   52/269   train_loss = 3.813\n",
      "Epoch  33 Batch   53/269   train_loss = 4.022\n",
      "Epoch  33 Batch   54/269   train_loss = 3.807\n",
      "Epoch  33 Batch   55/269   train_loss = 3.852\n",
      "Epoch  33 Batch   56/269   train_loss = 3.693\n",
      "Epoch  33 Batch   57/269   train_loss = 3.903\n",
      "Epoch  33 Batch   58/269   train_loss = 3.607\n",
      "Epoch  33 Batch   59/269   train_loss = 3.662\n",
      "Epoch  33 Batch   60/269   train_loss = 3.779\n",
      "Epoch  33 Batch   61/269   train_loss = 4.028\n",
      "Epoch  33 Batch   62/269   train_loss = 3.651\n",
      "Epoch  33 Batch   63/269   train_loss = 3.937\n",
      "Epoch  33 Batch   64/269   train_loss = 3.678\n",
      "Epoch  33 Batch   65/269   train_loss = 3.788\n",
      "Epoch  33 Batch   66/269   train_loss = 3.641\n",
      "Epoch  33 Batch   67/269   train_loss = 3.759\n",
      "Epoch  33 Batch   68/269   train_loss = 3.802\n",
      "Epoch  33 Batch   69/269   train_loss = 3.820\n",
      "Epoch  33 Batch   70/269   train_loss = 3.458\n",
      "Epoch  33 Batch   71/269   train_loss = 3.828\n",
      "Epoch  33 Batch   72/269   train_loss = 3.527\n",
      "Epoch  33 Batch   73/269   train_loss = 3.719\n",
      "Epoch  33 Batch   74/269   train_loss = 3.760\n",
      "Epoch  33 Batch   75/269   train_loss = 3.914\n",
      "Epoch  33 Batch   76/269   train_loss = 3.799\n",
      "Epoch  33 Batch   77/269   train_loss = 3.502\n",
      "Epoch  33 Batch   78/269   train_loss = 3.846\n",
      "Epoch  33 Batch   79/269   train_loss = 3.938\n",
      "Epoch  33 Batch   80/269   train_loss = 3.836\n",
      "Epoch  33 Batch   81/269   train_loss = 3.664\n",
      "Epoch  33 Batch   82/269   train_loss = 3.942\n",
      "Epoch  33 Batch   83/269   train_loss = 3.721\n",
      "Epoch  33 Batch   84/269   train_loss = 3.981\n",
      "Epoch  33 Batch   85/269   train_loss = 4.148\n",
      "Epoch  33 Batch   86/269   train_loss = 4.032\n",
      "Epoch  33 Batch   87/269   train_loss = 3.777\n",
      "Epoch  33 Batch   88/269   train_loss = 3.827\n",
      "Epoch  33 Batch   89/269   train_loss = 3.724\n",
      "Epoch  33 Batch   90/269   train_loss = 3.643\n",
      "Epoch  33 Batch   91/269   train_loss = 3.910\n",
      "Epoch  33 Batch   92/269   train_loss = 3.949\n",
      "Epoch  33 Batch   93/269   train_loss = 3.645\n",
      "Epoch  33 Batch   94/269   train_loss = 4.062\n",
      "Epoch  33 Batch   95/269   train_loss = 3.547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  33 Batch   96/269   train_loss = 3.652\n",
      "Epoch  33 Batch   97/269   train_loss = 3.745\n",
      "Epoch  33 Batch   98/269   train_loss = 3.882\n",
      "Epoch  33 Batch   99/269   train_loss = 3.758\n",
      "Epoch  33 Batch  100/269   train_loss = 3.727\n",
      "Epoch  33 Batch  101/269   train_loss = 3.907\n",
      "Epoch  33 Batch  102/269   train_loss = 3.603\n",
      "Epoch  33 Batch  103/269   train_loss = 3.642\n",
      "Epoch  33 Batch  104/269   train_loss = 3.608\n",
      "Epoch  33 Batch  105/269   train_loss = 3.924\n",
      "Epoch  33 Batch  106/269   train_loss = 4.014\n",
      "Epoch  33 Batch  107/269   train_loss = 3.967\n",
      "Epoch  33 Batch  108/269   train_loss = 3.803\n",
      "Epoch  33 Batch  109/269   train_loss = 3.832\n",
      "Epoch  33 Batch  110/269   train_loss = 3.820\n",
      "Epoch  33 Batch  111/269   train_loss = 3.918\n",
      "Epoch  33 Batch  112/269   train_loss = 3.891\n",
      "Epoch  33 Batch  113/269   train_loss = 3.736\n",
      "Epoch  33 Batch  114/269   train_loss = 3.789\n",
      "Epoch  33 Batch  115/269   train_loss = 3.603\n",
      "Epoch  33 Batch  116/269   train_loss = 3.567\n",
      "Epoch  33 Batch  117/269   train_loss = 3.601\n",
      "Epoch  33 Batch  118/269   train_loss = 3.964\n",
      "Epoch  33 Batch  119/269   train_loss = 3.598\n",
      "Epoch  33 Batch  120/269   train_loss = 3.845\n",
      "Epoch  33 Batch  121/269   train_loss = 3.697\n",
      "Epoch  33 Batch  122/269   train_loss = 3.817\n",
      "Epoch  33 Batch  123/269   train_loss = 3.571\n",
      "Epoch  33 Batch  124/269   train_loss = 3.915\n",
      "Epoch  33 Batch  125/269   train_loss = 3.513\n",
      "Epoch  33 Batch  126/269   train_loss = 3.440\n",
      "Epoch  33 Batch  127/269   train_loss = 3.724\n",
      "Epoch  33 Batch  128/269   train_loss = 3.730\n",
      "Epoch  33 Batch  129/269   train_loss = 3.693\n",
      "Epoch  33 Batch  130/269   train_loss = 3.855\n",
      "Epoch  33 Batch  131/269   train_loss = 3.400\n",
      "Epoch  33 Batch  132/269   train_loss = 3.762\n",
      "Epoch  33 Batch  133/269   train_loss = 3.523\n",
      "Epoch  33 Batch  134/269   train_loss = 3.944\n",
      "Epoch  33 Batch  135/269   train_loss = 3.577\n",
      "Epoch  33 Batch  136/269   train_loss = 3.484\n",
      "Epoch  33 Batch  137/269   train_loss = 3.828\n",
      "Epoch  33 Batch  138/269   train_loss = 3.602\n",
      "Epoch  33 Batch  139/269   train_loss = 3.776\n",
      "Epoch  33 Batch  140/269   train_loss = 3.815\n",
      "Epoch  33 Batch  141/269   train_loss = 4.066\n",
      "Epoch  33 Batch  142/269   train_loss = 3.557\n",
      "Epoch  33 Batch  143/269   train_loss = 3.771\n",
      "Epoch  33 Batch  144/269   train_loss = 3.879\n",
      "Epoch  33 Batch  145/269   train_loss = 3.670\n",
      "Epoch  33 Batch  146/269   train_loss = 3.903\n",
      "Epoch  33 Batch  147/269   train_loss = 3.566\n",
      "Epoch  33 Batch  148/269   train_loss = 3.867\n",
      "Epoch  33 Batch  149/269   train_loss = 3.711\n",
      "Epoch  33 Batch  150/269   train_loss = 3.847\n",
      "Epoch  33 Batch  151/269   train_loss = 3.797\n",
      "Epoch  33 Batch  152/269   train_loss = 3.782\n",
      "Epoch  33 Batch  153/269   train_loss = 3.795\n",
      "Epoch  33 Batch  154/269   train_loss = 3.653\n",
      "Epoch  33 Batch  155/269   train_loss = 3.657\n",
      "Epoch  33 Batch  156/269   train_loss = 4.133\n",
      "Epoch  33 Batch  157/269   train_loss = 3.880\n",
      "Epoch  33 Batch  158/269   train_loss = 3.847\n",
      "Epoch  33 Batch  159/269   train_loss = 3.820\n",
      "Epoch  33 Batch  160/269   train_loss = 3.670\n",
      "Epoch  33 Batch  161/269   train_loss = 3.866\n",
      "Epoch  33 Batch  162/269   train_loss = 3.798\n",
      "Epoch  33 Batch  163/269   train_loss = 3.784\n",
      "Epoch  33 Batch  164/269   train_loss = 3.934\n",
      "Epoch  33 Batch  165/269   train_loss = 3.899\n",
      "Epoch  33 Batch  166/269   train_loss = 3.820\n",
      "Epoch  33 Batch  167/269   train_loss = 3.630\n",
      "Epoch  33 Batch  168/269   train_loss = 3.740\n",
      "Epoch  33 Batch  169/269   train_loss = 3.986\n",
      "Epoch  33 Batch  170/269   train_loss = 3.565\n",
      "Epoch  33 Batch  171/269   train_loss = 3.791\n",
      "Epoch  33 Batch  172/269   train_loss = 3.560\n",
      "Epoch  33 Batch  173/269   train_loss = 3.780\n",
      "Epoch  33 Batch  174/269   train_loss = 3.934\n",
      "Epoch  33 Batch  175/269   train_loss = 3.842\n",
      "Epoch  33 Batch  176/269   train_loss = 3.726\n",
      "Epoch  33 Batch  177/269   train_loss = 3.792\n",
      "Epoch  33 Batch  178/269   train_loss = 4.057\n",
      "Epoch  33 Batch  179/269   train_loss = 3.802\n",
      "Epoch  33 Batch  180/269   train_loss = 3.623\n",
      "Epoch  33 Batch  181/269   train_loss = 3.885\n",
      "Epoch  33 Batch  182/269   train_loss = 3.729\n",
      "Epoch  33 Batch  183/269   train_loss = 3.692\n",
      "Epoch  33 Batch  184/269   train_loss = 3.735\n",
      "Epoch  33 Batch  185/269   train_loss = 3.826\n",
      "Epoch  33 Batch  186/269   train_loss = 3.701\n",
      "Epoch  33 Batch  187/269   train_loss = 3.975\n",
      "Epoch  33 Batch  188/269   train_loss = 3.723\n",
      "Epoch  33 Batch  189/269   train_loss = 3.704\n",
      "Epoch  33 Batch  190/269   train_loss = 4.159\n",
      "Epoch  33 Batch  191/269   train_loss = 3.854\n",
      "Epoch  33 Batch  192/269   train_loss = 3.682\n",
      "Epoch  33 Batch  193/269   train_loss = 3.713\n",
      "Epoch  33 Batch  194/269   train_loss = 3.884\n",
      "Epoch  33 Batch  195/269   train_loss = 3.690\n",
      "Epoch  33 Batch  196/269   train_loss = 3.884\n",
      "Epoch  33 Batch  197/269   train_loss = 3.948\n",
      "Epoch  33 Batch  198/269   train_loss = 3.857\n",
      "Epoch  33 Batch  199/269   train_loss = 3.819\n",
      "Epoch  33 Batch  200/269   train_loss = 3.680\n",
      "Epoch  33 Batch  201/269   train_loss = 3.741\n",
      "Epoch  33 Batch  202/269   train_loss = 3.621\n",
      "Epoch  33 Batch  203/269   train_loss = 3.642\n",
      "Epoch  33 Batch  204/269   train_loss = 3.891\n",
      "Epoch  33 Batch  205/269   train_loss = 3.848\n",
      "Epoch  33 Batch  206/269   train_loss = 3.718\n",
      "Epoch  33 Batch  207/269   train_loss = 3.698\n",
      "Epoch  33 Batch  208/269   train_loss = 3.792\n",
      "Epoch  33 Batch  209/269   train_loss = 3.856\n",
      "Epoch  33 Batch  210/269   train_loss = 3.628\n",
      "Epoch  33 Batch  211/269   train_loss = 3.739\n",
      "Epoch  33 Batch  212/269   train_loss = 4.019\n",
      "Epoch  33 Batch  213/269   train_loss = 3.611\n",
      "Epoch  33 Batch  214/269   train_loss = 3.743\n",
      "Epoch  33 Batch  215/269   train_loss = 3.987\n",
      "Epoch  33 Batch  216/269   train_loss = 3.911\n",
      "Epoch  33 Batch  217/269   train_loss = 3.667\n",
      "Epoch  33 Batch  218/269   train_loss = 3.815\n",
      "Epoch  33 Batch  219/269   train_loss = 3.518\n",
      "Epoch  33 Batch  220/269   train_loss = 3.901\n",
      "Epoch  33 Batch  221/269   train_loss = 3.612\n",
      "Epoch  33 Batch  222/269   train_loss = 3.817\n",
      "Epoch  33 Batch  223/269   train_loss = 3.587\n",
      "Epoch  33 Batch  224/269   train_loss = 3.870\n",
      "Epoch  33 Batch  225/269   train_loss = 3.997\n",
      "Epoch  33 Batch  226/269   train_loss = 3.746\n",
      "Epoch  33 Batch  227/269   train_loss = 3.508\n",
      "Epoch  33 Batch  228/269   train_loss = 3.741\n",
      "Epoch  33 Batch  229/269   train_loss = 3.907\n",
      "Epoch  33 Batch  230/269   train_loss = 3.806\n",
      "Epoch  33 Batch  231/269   train_loss = 3.738\n",
      "Epoch  33 Batch  232/269   train_loss = 3.741\n",
      "Epoch  33 Batch  233/269   train_loss = 3.829\n",
      "Epoch  33 Batch  234/269   train_loss = 3.791\n",
      "Epoch  33 Batch  235/269   train_loss = 4.011\n",
      "Epoch  33 Batch  236/269   train_loss = 3.642\n",
      "Epoch  33 Batch  237/269   train_loss = 3.379\n",
      "Epoch  33 Batch  238/269   train_loss = 3.563\n",
      "Epoch  33 Batch  239/269   train_loss = 4.153\n",
      "Epoch  33 Batch  240/269   train_loss = 3.734\n",
      "Epoch  33 Batch  241/269   train_loss = 3.931\n",
      "Epoch  33 Batch  242/269   train_loss = 3.580\n",
      "Epoch  33 Batch  243/269   train_loss = 3.955\n",
      "Epoch  33 Batch  244/269   train_loss = 3.755\n",
      "Epoch  33 Batch  245/269   train_loss = 3.739\n",
      "Epoch  33 Batch  246/269   train_loss = 3.434\n",
      "Epoch  33 Batch  247/269   train_loss = 3.853\n",
      "Epoch  33 Batch  248/269   train_loss = 3.830\n",
      "Epoch  33 Batch  249/269   train_loss = 3.647\n",
      "Epoch  33 Batch  250/269   train_loss = 3.519\n",
      "Epoch  33 Batch  251/269   train_loss = 3.945\n",
      "Epoch  33 Batch  252/269   train_loss = 3.858\n",
      "Epoch  33 Batch  253/269   train_loss = 3.704\n",
      "Epoch  33 Batch  254/269   train_loss = 3.690\n",
      "Epoch  33 Batch  255/269   train_loss = 3.750\n",
      "Epoch  33 Batch  256/269   train_loss = 3.628\n",
      "Epoch  33 Batch  257/269   train_loss = 3.576\n",
      "Epoch  33 Batch  258/269   train_loss = 3.472\n",
      "Epoch  33 Batch  259/269   train_loss = 3.600\n",
      "Epoch  33 Batch  260/269   train_loss = 3.827\n",
      "Epoch  33 Batch  261/269   train_loss = 3.889\n",
      "Epoch  33 Batch  262/269   train_loss = 3.648\n",
      "Epoch  33 Batch  263/269   train_loss = 3.490\n",
      "Epoch  33 Batch  264/269   train_loss = 4.024\n",
      "Epoch  33 Batch  265/269   train_loss = 3.822\n",
      "Epoch  33 Batch  266/269   train_loss = 3.722\n",
      "Epoch  33 Batch  267/269   train_loss = 3.774\n",
      "Epoch  33 Batch  268/269   train_loss = 3.863\n",
      "Epoch  34 Batch    0/269   train_loss = 3.664\n",
      "Epoch  34 Batch    1/269   train_loss = 3.526\n",
      "Epoch  34 Batch    2/269   train_loss = 3.653\n",
      "Epoch  34 Batch    3/269   train_loss = 3.706\n",
      "Epoch  34 Batch    4/269   train_loss = 4.073\n",
      "Epoch  34 Batch    5/269   train_loss = 3.763\n",
      "Epoch  34 Batch    6/269   train_loss = 3.702\n",
      "Epoch  34 Batch    7/269   train_loss = 3.510\n",
      "Epoch  34 Batch    8/269   train_loss = 3.881\n",
      "Epoch  34 Batch    9/269   train_loss = 3.537\n",
      "Epoch  34 Batch   10/269   train_loss = 3.543\n",
      "Epoch  34 Batch   11/269   train_loss = 3.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  34 Batch   12/269   train_loss = 3.430\n",
      "Epoch  34 Batch   13/269   train_loss = 3.756\n",
      "Epoch  34 Batch   14/269   train_loss = 3.606\n",
      "Epoch  34 Batch   15/269   train_loss = 3.963\n",
      "Epoch  34 Batch   16/269   train_loss = 3.706\n",
      "Epoch  34 Batch   17/269   train_loss = 3.761\n",
      "Epoch  34 Batch   18/269   train_loss = 3.667\n",
      "Epoch  34 Batch   19/269   train_loss = 3.658\n",
      "Epoch  34 Batch   20/269   train_loss = 4.062\n",
      "Epoch  34 Batch   21/269   train_loss = 3.856\n",
      "Epoch  34 Batch   22/269   train_loss = 3.599\n",
      "Epoch  34 Batch   23/269   train_loss = 3.804\n",
      "Epoch  34 Batch   24/269   train_loss = 3.671\n",
      "Epoch  34 Batch   25/269   train_loss = 3.981\n",
      "Epoch  34 Batch   26/269   train_loss = 3.800\n",
      "Epoch  34 Batch   27/269   train_loss = 3.627\n",
      "Epoch  34 Batch   28/269   train_loss = 3.801\n",
      "Epoch  34 Batch   29/269   train_loss = 3.786\n",
      "Epoch  34 Batch   30/269   train_loss = 4.109\n",
      "Epoch  34 Batch   31/269   train_loss = 3.763\n",
      "Epoch  34 Batch   32/269   train_loss = 3.662\n",
      "Epoch  34 Batch   33/269   train_loss = 3.412\n",
      "Epoch  34 Batch   34/269   train_loss = 3.705\n",
      "Epoch  34 Batch   35/269   train_loss = 3.553\n",
      "Epoch  34 Batch   36/269   train_loss = 3.569\n",
      "Epoch  34 Batch   37/269   train_loss = 3.811\n",
      "Epoch  34 Batch   38/269   train_loss = 3.545\n",
      "Epoch  34 Batch   39/269   train_loss = 3.851\n",
      "Epoch  34 Batch   40/269   train_loss = 3.603\n",
      "Epoch  34 Batch   41/269   train_loss = 3.600\n",
      "Epoch  34 Batch   42/269   train_loss = 4.084\n",
      "Epoch  34 Batch   43/269   train_loss = 3.883\n",
      "Epoch  34 Batch   44/269   train_loss = 3.715\n",
      "Epoch  34 Batch   45/269   train_loss = 3.447\n",
      "Epoch  34 Batch   46/269   train_loss = 3.853\n",
      "Epoch  34 Batch   47/269   train_loss = 3.472\n",
      "Epoch  34 Batch   48/269   train_loss = 4.002\n",
      "Epoch  34 Batch   49/269   train_loss = 3.938\n",
      "Epoch  34 Batch   50/269   train_loss = 3.866\n",
      "Epoch  34 Batch   51/269   train_loss = 3.774\n",
      "Epoch  34 Batch   52/269   train_loss = 3.798\n",
      "Epoch  34 Batch   53/269   train_loss = 3.997\n",
      "Epoch  34 Batch   54/269   train_loss = 3.831\n",
      "Epoch  34 Batch   55/269   train_loss = 3.817\n",
      "Epoch  34 Batch   56/269   train_loss = 3.690\n",
      "Epoch  34 Batch   57/269   train_loss = 3.882\n",
      "Epoch  34 Batch   58/269   train_loss = 3.597\n",
      "Epoch  34 Batch   59/269   train_loss = 3.641\n",
      "Epoch  34 Batch   60/269   train_loss = 3.767\n",
      "Epoch  34 Batch   61/269   train_loss = 4.003\n",
      "Epoch  34 Batch   62/269   train_loss = 3.652\n",
      "Epoch  34 Batch   63/269   train_loss = 3.941\n",
      "Epoch  34 Batch   64/269   train_loss = 3.676\n",
      "Epoch  34 Batch   65/269   train_loss = 3.777\n",
      "Epoch  34 Batch   66/269   train_loss = 3.640\n",
      "Epoch  34 Batch   67/269   train_loss = 3.744\n",
      "Epoch  34 Batch   68/269   train_loss = 3.797\n",
      "Epoch  34 Batch   69/269   train_loss = 3.783\n",
      "Epoch  34 Batch   70/269   train_loss = 3.444\n",
      "Epoch  34 Batch   71/269   train_loss = 3.830\n",
      "Epoch  34 Batch   72/269   train_loss = 3.521\n",
      "Epoch  34 Batch   73/269   train_loss = 3.709\n",
      "Epoch  34 Batch   74/269   train_loss = 3.743\n",
      "Epoch  34 Batch   75/269   train_loss = 3.914\n",
      "Epoch  34 Batch   76/269   train_loss = 3.795\n",
      "Epoch  34 Batch   77/269   train_loss = 3.488\n",
      "Epoch  34 Batch   78/269   train_loss = 3.858\n",
      "Epoch  34 Batch   79/269   train_loss = 3.933\n",
      "Epoch  34 Batch   80/269   train_loss = 3.845\n",
      "Epoch  34 Batch   81/269   train_loss = 3.673\n",
      "Epoch  34 Batch   82/269   train_loss = 3.918\n",
      "Epoch  34 Batch   83/269   train_loss = 3.698\n",
      "Epoch  34 Batch   84/269   train_loss = 3.960\n",
      "Epoch  34 Batch   85/269   train_loss = 4.121\n",
      "Epoch  34 Batch   86/269   train_loss = 4.040\n",
      "Epoch  34 Batch   87/269   train_loss = 3.773\n",
      "Epoch  34 Batch   88/269   train_loss = 3.809\n",
      "Epoch  34 Batch   89/269   train_loss = 3.728\n",
      "Epoch  34 Batch   90/269   train_loss = 3.640\n",
      "Epoch  34 Batch   91/269   train_loss = 3.937\n",
      "Epoch  34 Batch   92/269   train_loss = 3.919\n",
      "Epoch  34 Batch   93/269   train_loss = 3.637\n",
      "Epoch  34 Batch   94/269   train_loss = 4.070\n",
      "Epoch  34 Batch   95/269   train_loss = 3.542\n",
      "Epoch  34 Batch   96/269   train_loss = 3.648\n",
      "Epoch  34 Batch   97/269   train_loss = 3.749\n",
      "Epoch  34 Batch   98/269   train_loss = 3.869\n",
      "Epoch  34 Batch   99/269   train_loss = 3.747\n",
      "Epoch  34 Batch  100/269   train_loss = 3.713\n",
      "Epoch  34 Batch  101/269   train_loss = 3.890\n",
      "Epoch  34 Batch  102/269   train_loss = 3.595\n",
      "Epoch  34 Batch  103/269   train_loss = 3.673\n",
      "Epoch  34 Batch  104/269   train_loss = 3.624\n",
      "Epoch  34 Batch  105/269   train_loss = 3.897\n",
      "Epoch  34 Batch  106/269   train_loss = 4.010\n",
      "Epoch  34 Batch  107/269   train_loss = 3.974\n",
      "Epoch  34 Batch  108/269   train_loss = 3.793\n",
      "Epoch  34 Batch  109/269   train_loss = 3.813\n",
      "Epoch  34 Batch  110/269   train_loss = 3.816\n",
      "Epoch  34 Batch  111/269   train_loss = 3.898\n",
      "Epoch  34 Batch  112/269   train_loss = 3.917\n",
      "Epoch  34 Batch  113/269   train_loss = 3.730\n",
      "Epoch  34 Batch  114/269   train_loss = 3.771\n",
      "Epoch  34 Batch  115/269   train_loss = 3.591\n",
      "Epoch  34 Batch  116/269   train_loss = 3.585\n",
      "Epoch  34 Batch  117/269   train_loss = 3.598\n",
      "Epoch  34 Batch  118/269   train_loss = 3.980\n",
      "Epoch  34 Batch  119/269   train_loss = 3.590\n",
      "Epoch  34 Batch  120/269   train_loss = 3.837\n",
      "Epoch  34 Batch  121/269   train_loss = 3.705\n",
      "Epoch  34 Batch  122/269   train_loss = 3.796\n",
      "Epoch  34 Batch  123/269   train_loss = 3.571\n",
      "Epoch  34 Batch  124/269   train_loss = 3.951\n",
      "Epoch  34 Batch  125/269   train_loss = 3.531\n",
      "Epoch  34 Batch  126/269   train_loss = 3.441\n",
      "Epoch  34 Batch  127/269   train_loss = 3.717\n",
      "Epoch  34 Batch  128/269   train_loss = 3.704\n",
      "Epoch  34 Batch  129/269   train_loss = 3.715\n",
      "Epoch  34 Batch  130/269   train_loss = 3.836\n",
      "Epoch  34 Batch  131/269   train_loss = 3.407\n",
      "Epoch  34 Batch  132/269   train_loss = 3.765\n",
      "Epoch  34 Batch  133/269   train_loss = 3.528\n",
      "Epoch  34 Batch  134/269   train_loss = 3.913\n",
      "Epoch  34 Batch  135/269   train_loss = 3.594\n",
      "Epoch  34 Batch  136/269   train_loss = 3.475\n",
      "Epoch  34 Batch  137/269   train_loss = 3.795\n",
      "Epoch  34 Batch  138/269   train_loss = 3.594\n",
      "Epoch  34 Batch  139/269   train_loss = 3.768\n",
      "Epoch  34 Batch  140/269   train_loss = 3.811\n",
      "Epoch  34 Batch  141/269   train_loss = 4.045\n",
      "Epoch  34 Batch  142/269   train_loss = 3.578\n",
      "Epoch  34 Batch  143/269   train_loss = 3.750\n",
      "Epoch  34 Batch  144/269   train_loss = 3.876\n",
      "Epoch  34 Batch  145/269   train_loss = 3.647\n",
      "Epoch  34 Batch  146/269   train_loss = 3.900\n",
      "Epoch  34 Batch  147/269   train_loss = 3.553\n",
      "Epoch  34 Batch  148/269   train_loss = 3.848\n",
      "Epoch  34 Batch  149/269   train_loss = 3.714\n",
      "Epoch  34 Batch  150/269   train_loss = 3.827\n",
      "Epoch  34 Batch  151/269   train_loss = 3.788\n",
      "Epoch  34 Batch  152/269   train_loss = 3.776\n",
      "Epoch  34 Batch  153/269   train_loss = 3.775\n",
      "Epoch  34 Batch  154/269   train_loss = 3.649\n",
      "Epoch  34 Batch  155/269   train_loss = 3.621\n",
      "Epoch  34 Batch  156/269   train_loss = 4.110\n",
      "Epoch  34 Batch  157/269   train_loss = 3.880\n",
      "Epoch  34 Batch  158/269   train_loss = 3.839\n",
      "Epoch  34 Batch  159/269   train_loss = 3.832\n",
      "Epoch  34 Batch  160/269   train_loss = 3.678\n",
      "Epoch  34 Batch  161/269   train_loss = 3.862\n",
      "Epoch  34 Batch  162/269   train_loss = 3.808\n",
      "Epoch  34 Batch  163/269   train_loss = 3.762\n",
      "Epoch  34 Batch  164/269   train_loss = 3.913\n",
      "Epoch  34 Batch  165/269   train_loss = 3.897\n",
      "Epoch  34 Batch  166/269   train_loss = 3.821\n",
      "Epoch  34 Batch  167/269   train_loss = 3.640\n",
      "Epoch  34 Batch  168/269   train_loss = 3.719\n",
      "Epoch  34 Batch  169/269   train_loss = 3.997\n",
      "Epoch  34 Batch  170/269   train_loss = 3.583\n",
      "Epoch  34 Batch  171/269   train_loss = 3.772\n",
      "Epoch  34 Batch  172/269   train_loss = 3.547\n",
      "Epoch  34 Batch  173/269   train_loss = 3.738\n",
      "Epoch  34 Batch  174/269   train_loss = 3.921\n",
      "Epoch  34 Batch  175/269   train_loss = 3.838\n",
      "Epoch  34 Batch  176/269   train_loss = 3.705\n",
      "Epoch  34 Batch  177/269   train_loss = 3.802\n",
      "Epoch  34 Batch  178/269   train_loss = 4.061\n",
      "Epoch  34 Batch  179/269   train_loss = 3.786\n",
      "Epoch  34 Batch  180/269   train_loss = 3.606\n",
      "Epoch  34 Batch  181/269   train_loss = 3.873\n",
      "Epoch  34 Batch  182/269   train_loss = 3.734\n",
      "Epoch  34 Batch  183/269   train_loss = 3.681\n",
      "Epoch  34 Batch  184/269   train_loss = 3.733\n",
      "Epoch  34 Batch  185/269   train_loss = 3.796\n",
      "Epoch  34 Batch  186/269   train_loss = 3.685\n",
      "Epoch  34 Batch  187/269   train_loss = 3.959\n",
      "Epoch  34 Batch  188/269   train_loss = 3.749\n",
      "Epoch  34 Batch  189/269   train_loss = 3.700\n",
      "Epoch  34 Batch  190/269   train_loss = 4.131\n",
      "Epoch  34 Batch  191/269   train_loss = 3.837\n",
      "Epoch  34 Batch  192/269   train_loss = 3.730\n",
      "Epoch  34 Batch  193/269   train_loss = 3.714\n",
      "Epoch  34 Batch  194/269   train_loss = 3.853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  34 Batch  195/269   train_loss = 3.674\n",
      "Epoch  34 Batch  196/269   train_loss = 3.841\n",
      "Epoch  34 Batch  197/269   train_loss = 3.976\n",
      "Epoch  34 Batch  198/269   train_loss = 3.878\n",
      "Epoch  34 Batch  199/269   train_loss = 3.868\n",
      "Epoch  34 Batch  200/269   train_loss = 3.680\n",
      "Epoch  34 Batch  201/269   train_loss = 3.758\n",
      "Epoch  34 Batch  202/269   train_loss = 3.643\n",
      "Epoch  34 Batch  203/269   train_loss = 3.606\n",
      "Epoch  34 Batch  204/269   train_loss = 3.868\n",
      "Epoch  34 Batch  205/269   train_loss = 3.814\n",
      "Epoch  34 Batch  206/269   train_loss = 3.701\n",
      "Epoch  34 Batch  207/269   train_loss = 3.664\n",
      "Epoch  34 Batch  208/269   train_loss = 3.789\n",
      "Epoch  34 Batch  209/269   train_loss = 3.866\n",
      "Epoch  34 Batch  210/269   train_loss = 3.628\n",
      "Epoch  34 Batch  211/269   train_loss = 3.748\n",
      "Epoch  34 Batch  212/269   train_loss = 4.047\n",
      "Epoch  34 Batch  213/269   train_loss = 3.624\n",
      "Epoch  34 Batch  214/269   train_loss = 3.733\n",
      "Epoch  34 Batch  215/269   train_loss = 3.989\n",
      "Epoch  34 Batch  216/269   train_loss = 3.904\n",
      "Epoch  34 Batch  217/269   train_loss = 3.674\n",
      "Epoch  34 Batch  218/269   train_loss = 3.812\n",
      "Epoch  34 Batch  219/269   train_loss = 3.484\n",
      "Epoch  34 Batch  220/269   train_loss = 3.895\n",
      "Epoch  34 Batch  221/269   train_loss = 3.601\n",
      "Epoch  34 Batch  222/269   train_loss = 3.801\n",
      "Epoch  34 Batch  223/269   train_loss = 3.611\n",
      "Epoch  34 Batch  224/269   train_loss = 3.868\n",
      "Epoch  34 Batch  225/269   train_loss = 3.982\n",
      "Epoch  34 Batch  226/269   train_loss = 3.754\n",
      "Epoch  34 Batch  227/269   train_loss = 3.477\n",
      "Epoch  34 Batch  228/269   train_loss = 3.772\n",
      "Epoch  34 Batch  229/269   train_loss = 3.910\n",
      "Epoch  34 Batch  230/269   train_loss = 3.801\n",
      "Epoch  34 Batch  231/269   train_loss = 3.697\n",
      "Epoch  34 Batch  232/269   train_loss = 3.739\n",
      "Epoch  34 Batch  233/269   train_loss = 3.811\n",
      "Epoch  34 Batch  234/269   train_loss = 3.800\n",
      "Epoch  34 Batch  235/269   train_loss = 4.006\n",
      "Epoch  34 Batch  236/269   train_loss = 3.635\n",
      "Epoch  34 Batch  237/269   train_loss = 3.383\n",
      "Epoch  34 Batch  238/269   train_loss = 3.562\n",
      "Epoch  34 Batch  239/269   train_loss = 4.150\n",
      "Epoch  34 Batch  240/269   train_loss = 3.727\n",
      "Epoch  34 Batch  241/269   train_loss = 3.953\n",
      "Epoch  34 Batch  242/269   train_loss = 3.533\n",
      "Epoch  34 Batch  243/269   train_loss = 3.943\n",
      "Epoch  34 Batch  244/269   train_loss = 3.743\n",
      "Epoch  34 Batch  245/269   train_loss = 3.736\n",
      "Epoch  34 Batch  246/269   train_loss = 3.454\n",
      "Epoch  34 Batch  247/269   train_loss = 3.842\n",
      "Epoch  34 Batch  248/269   train_loss = 3.811\n",
      "Epoch  34 Batch  249/269   train_loss = 3.635\n",
      "Epoch  34 Batch  250/269   train_loss = 3.499\n",
      "Epoch  34 Batch  251/269   train_loss = 3.937\n",
      "Epoch  34 Batch  252/269   train_loss = 3.841\n",
      "Epoch  34 Batch  253/269   train_loss = 3.691\n",
      "Epoch  34 Batch  254/269   train_loss = 3.678\n",
      "Epoch  34 Batch  255/269   train_loss = 3.733\n",
      "Epoch  34 Batch  256/269   train_loss = 3.617\n",
      "Epoch  34 Batch  257/269   train_loss = 3.577\n",
      "Epoch  34 Batch  258/269   train_loss = 3.490\n",
      "Epoch  34 Batch  259/269   train_loss = 3.623\n",
      "Epoch  34 Batch  260/269   train_loss = 3.826\n",
      "Epoch  34 Batch  261/269   train_loss = 3.856\n",
      "Epoch  34 Batch  262/269   train_loss = 3.646\n",
      "Epoch  34 Batch  263/269   train_loss = 3.481\n",
      "Epoch  34 Batch  264/269   train_loss = 4.019\n",
      "Epoch  34 Batch  265/269   train_loss = 3.826\n",
      "Epoch  34 Batch  266/269   train_loss = 3.712\n",
      "Epoch  34 Batch  267/269   train_loss = 3.806\n",
      "Epoch  34 Batch  268/269   train_loss = 3.865\n",
      "Epoch  35 Batch    0/269   train_loss = 3.680\n",
      "Epoch  35 Batch    1/269   train_loss = 3.502\n",
      "Epoch  35 Batch    2/269   train_loss = 3.645\n",
      "Epoch  35 Batch    3/269   train_loss = 3.690\n",
      "Epoch  35 Batch    4/269   train_loss = 4.049\n",
      "Epoch  35 Batch    5/269   train_loss = 3.807\n",
      "Epoch  35 Batch    6/269   train_loss = 3.711\n",
      "Epoch  35 Batch    7/269   train_loss = 3.561\n",
      "Epoch  35 Batch    8/269   train_loss = 3.874\n",
      "Epoch  35 Batch    9/269   train_loss = 3.543\n",
      "Epoch  35 Batch   10/269   train_loss = 3.541\n",
      "Epoch  35 Batch   11/269   train_loss = 3.700\n",
      "Epoch  35 Batch   12/269   train_loss = 3.415\n",
      "Epoch  35 Batch   13/269   train_loss = 3.738\n",
      "Epoch  35 Batch   14/269   train_loss = 3.613\n",
      "Epoch  35 Batch   15/269   train_loss = 3.981\n",
      "Epoch  35 Batch   16/269   train_loss = 3.684\n",
      "Epoch  35 Batch   17/269   train_loss = 3.777\n",
      "Epoch  35 Batch   18/269   train_loss = 3.692\n",
      "Epoch  35 Batch   19/269   train_loss = 3.658\n",
      "Epoch  35 Batch   20/269   train_loss = 4.040\n",
      "Epoch  35 Batch   21/269   train_loss = 3.873\n",
      "Epoch  35 Batch   22/269   train_loss = 3.618\n",
      "Epoch  35 Batch   23/269   train_loss = 3.829\n",
      "Epoch  35 Batch   24/269   train_loss = 3.673\n",
      "Epoch  35 Batch   25/269   train_loss = 3.997\n",
      "Epoch  35 Batch   26/269   train_loss = 3.781\n",
      "Epoch  35 Batch   27/269   train_loss = 3.602\n",
      "Epoch  35 Batch   28/269   train_loss = 3.794\n",
      "Epoch  35 Batch   29/269   train_loss = 3.794\n",
      "Epoch  35 Batch   30/269   train_loss = 4.107\n",
      "Epoch  35 Batch   31/269   train_loss = 3.765\n",
      "Epoch  35 Batch   32/269   train_loss = 3.640\n",
      "Epoch  35 Batch   33/269   train_loss = 3.400\n",
      "Epoch  35 Batch   34/269   train_loss = 3.718\n",
      "Epoch  35 Batch   35/269   train_loss = 3.570\n",
      "Epoch  35 Batch   36/269   train_loss = 3.584\n",
      "Epoch  35 Batch   37/269   train_loss = 3.797\n",
      "Epoch  35 Batch   38/269   train_loss = 3.565\n",
      "Epoch  35 Batch   39/269   train_loss = 3.855\n",
      "Epoch  35 Batch   40/269   train_loss = 3.594\n",
      "Epoch  35 Batch   41/269   train_loss = 3.617\n",
      "Epoch  35 Batch   42/269   train_loss = 4.097\n",
      "Epoch  35 Batch   43/269   train_loss = 3.896\n",
      "Epoch  35 Batch   44/269   train_loss = 3.749\n",
      "Epoch  35 Batch   45/269   train_loss = 3.454\n",
      "Epoch  35 Batch   46/269   train_loss = 3.846\n",
      "Epoch  35 Batch   47/269   train_loss = 3.463\n",
      "Epoch  35 Batch   48/269   train_loss = 3.995\n",
      "Epoch  35 Batch   49/269   train_loss = 3.940\n",
      "Epoch  35 Batch   50/269   train_loss = 3.899\n",
      "Epoch  35 Batch   51/269   train_loss = 3.793\n",
      "Epoch  35 Batch   52/269   train_loss = 3.799\n",
      "Epoch  35 Batch   53/269   train_loss = 4.029\n",
      "Epoch  35 Batch   54/269   train_loss = 3.806\n",
      "Epoch  35 Batch   55/269   train_loss = 3.852\n",
      "Epoch  35 Batch   56/269   train_loss = 3.718\n",
      "Epoch  35 Batch   57/269   train_loss = 3.889\n",
      "Epoch  35 Batch   58/269   train_loss = 3.618\n",
      "Epoch  35 Batch   59/269   train_loss = 3.628\n",
      "Epoch  35 Batch   60/269   train_loss = 3.783\n",
      "Epoch  35 Batch   61/269   train_loss = 4.009\n",
      "Epoch  35 Batch   62/269   train_loss = 3.639\n",
      "Epoch  35 Batch   63/269   train_loss = 3.934\n",
      "Epoch  35 Batch   64/269   train_loss = 3.699\n",
      "Epoch  35 Batch   65/269   train_loss = 3.793\n",
      "Epoch  35 Batch   66/269   train_loss = 3.654\n",
      "Epoch  35 Batch   67/269   train_loss = 3.748\n",
      "Epoch  35 Batch   68/269   train_loss = 3.814\n",
      "Epoch  35 Batch   69/269   train_loss = 3.809\n",
      "Epoch  35 Batch   70/269   train_loss = 3.459\n",
      "Epoch  35 Batch   71/269   train_loss = 3.864\n",
      "Epoch  35 Batch   72/269   train_loss = 3.502\n",
      "Epoch  35 Batch   73/269   train_loss = 3.734\n",
      "Epoch  35 Batch   74/269   train_loss = 3.775\n",
      "Epoch  35 Batch   75/269   train_loss = 3.913\n",
      "Epoch  35 Batch   76/269   train_loss = 3.801\n",
      "Epoch  35 Batch   77/269   train_loss = 3.498\n",
      "Epoch  35 Batch   78/269   train_loss = 3.867\n",
      "Epoch  35 Batch   79/269   train_loss = 3.941\n",
      "Epoch  35 Batch   80/269   train_loss = 3.835\n",
      "Epoch  35 Batch   81/269   train_loss = 3.724\n",
      "Epoch  35 Batch   82/269   train_loss = 3.941\n",
      "Epoch  35 Batch   83/269   train_loss = 3.681\n",
      "Epoch  35 Batch   84/269   train_loss = 3.970\n",
      "Epoch  35 Batch   85/269   train_loss = 4.160\n",
      "Epoch  35 Batch   86/269   train_loss = 4.043\n",
      "Epoch  35 Batch   87/269   train_loss = 3.757\n",
      "Epoch  35 Batch   88/269   train_loss = 3.819\n",
      "Epoch  35 Batch   89/269   train_loss = 3.747\n",
      "Epoch  35 Batch   90/269   train_loss = 3.632\n",
      "Epoch  35 Batch   91/269   train_loss = 3.915\n",
      "Epoch  35 Batch   92/269   train_loss = 3.912\n",
      "Epoch  35 Batch   93/269   train_loss = 3.631\n",
      "Epoch  35 Batch   94/269   train_loss = 4.065\n",
      "Epoch  35 Batch   95/269   train_loss = 3.532\n",
      "Epoch  35 Batch   96/269   train_loss = 3.643\n",
      "Epoch  35 Batch   97/269   train_loss = 3.751\n",
      "Epoch  35 Batch   98/269   train_loss = 3.874\n",
      "Epoch  35 Batch   99/269   train_loss = 3.743\n",
      "Epoch  35 Batch  100/269   train_loss = 3.726\n",
      "Epoch  35 Batch  101/269   train_loss = 3.892\n",
      "Epoch  35 Batch  102/269   train_loss = 3.585\n",
      "Epoch  35 Batch  103/269   train_loss = 3.661\n",
      "Epoch  35 Batch  104/269   train_loss = 3.593\n",
      "Epoch  35 Batch  105/269   train_loss = 3.934\n",
      "Epoch  35 Batch  106/269   train_loss = 4.034\n",
      "Epoch  35 Batch  107/269   train_loss = 4.016\n",
      "Epoch  35 Batch  108/269   train_loss = 3.796\n",
      "Epoch  35 Batch  109/269   train_loss = 3.844\n",
      "Epoch  35 Batch  110/269   train_loss = 3.814\n",
      "Epoch  35 Batch  111/269   train_loss = 3.896\n",
      "Epoch  35 Batch  112/269   train_loss = 3.934\n",
      "Epoch  35 Batch  113/269   train_loss = 3.757\n",
      "Epoch  35 Batch  114/269   train_loss = 3.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35 Batch  115/269   train_loss = 3.588\n",
      "Epoch  35 Batch  116/269   train_loss = 3.575\n",
      "Epoch  35 Batch  117/269   train_loss = 3.616\n",
      "Epoch  35 Batch  118/269   train_loss = 3.986\n",
      "Epoch  35 Batch  119/269   train_loss = 3.605\n",
      "Epoch  35 Batch  120/269   train_loss = 3.838\n",
      "Epoch  35 Batch  121/269   train_loss = 3.708\n",
      "Epoch  35 Batch  122/269   train_loss = 3.764\n",
      "Epoch  35 Batch  123/269   train_loss = 3.580\n",
      "Epoch  35 Batch  124/269   train_loss = 3.946\n",
      "Epoch  35 Batch  125/269   train_loss = 3.517\n",
      "Epoch  35 Batch  126/269   train_loss = 3.438\n",
      "Epoch  35 Batch  127/269   train_loss = 3.728\n",
      "Epoch  35 Batch  128/269   train_loss = 3.708\n",
      "Epoch  35 Batch  129/269   train_loss = 3.702\n",
      "Epoch  35 Batch  130/269   train_loss = 3.859\n",
      "Epoch  35 Batch  131/269   train_loss = 3.427\n",
      "Epoch  35 Batch  132/269   train_loss = 3.778\n",
      "Epoch  35 Batch  133/269   train_loss = 3.531\n",
      "Epoch  35 Batch  134/269   train_loss = 3.921\n",
      "Epoch  35 Batch  135/269   train_loss = 3.616\n",
      "Epoch  35 Batch  136/269   train_loss = 3.493\n",
      "Epoch  35 Batch  137/269   train_loss = 3.822\n",
      "Epoch  35 Batch  138/269   train_loss = 3.582\n",
      "Epoch  35 Batch  139/269   train_loss = 3.755\n",
      "Epoch  35 Batch  140/269   train_loss = 3.803\n",
      "Epoch  35 Batch  141/269   train_loss = 4.054\n",
      "Epoch  35 Batch  142/269   train_loss = 3.558\n",
      "Epoch  35 Batch  143/269   train_loss = 3.763\n",
      "Epoch  35 Batch  144/269   train_loss = 3.888\n",
      "Epoch  35 Batch  145/269   train_loss = 3.658\n",
      "Epoch  35 Batch  146/269   train_loss = 3.931\n",
      "Epoch  35 Batch  147/269   train_loss = 3.539\n",
      "Epoch  35 Batch  148/269   train_loss = 3.830\n",
      "Epoch  35 Batch  149/269   train_loss = 3.708\n",
      "Epoch  35 Batch  150/269   train_loss = 3.857\n",
      "Epoch  35 Batch  151/269   train_loss = 3.799\n",
      "Epoch  35 Batch  152/269   train_loss = 3.773\n",
      "Epoch  35 Batch  153/269   train_loss = 3.790\n",
      "Epoch  35 Batch  154/269   train_loss = 3.663\n",
      "Epoch  35 Batch  155/269   train_loss = 3.656\n",
      "Epoch  35 Batch  156/269   train_loss = 4.105\n",
      "Epoch  35 Batch  157/269   train_loss = 3.902\n",
      "Epoch  35 Batch  158/269   train_loss = 3.829\n",
      "Epoch  35 Batch  159/269   train_loss = 3.824\n",
      "Epoch  35 Batch  160/269   train_loss = 3.701\n",
      "Epoch  35 Batch  161/269   train_loss = 3.845\n",
      "Epoch  35 Batch  162/269   train_loss = 3.810\n",
      "Epoch  35 Batch  163/269   train_loss = 3.773\n",
      "Epoch  35 Batch  164/269   train_loss = 3.939\n",
      "Epoch  35 Batch  165/269   train_loss = 3.881\n",
      "Epoch  35 Batch  166/269   train_loss = 3.829\n",
      "Epoch  35 Batch  167/269   train_loss = 3.679\n",
      "Epoch  35 Batch  168/269   train_loss = 3.740\n",
      "Epoch  35 Batch  169/269   train_loss = 3.990\n",
      "Epoch  35 Batch  170/269   train_loss = 3.581\n",
      "Epoch  35 Batch  171/269   train_loss = 3.819\n",
      "Epoch  35 Batch  172/269   train_loss = 3.541\n",
      "Epoch  35 Batch  173/269   train_loss = 3.783\n",
      "Epoch  35 Batch  174/269   train_loss = 3.926\n",
      "Epoch  35 Batch  175/269   train_loss = 3.838\n",
      "Epoch  35 Batch  176/269   train_loss = 3.715\n",
      "Epoch  35 Batch  177/269   train_loss = 3.797\n",
      "Epoch  35 Batch  178/269   train_loss = 4.089\n",
      "Epoch  35 Batch  179/269   train_loss = 3.769\n",
      "Epoch  35 Batch  180/269   train_loss = 3.606\n",
      "Epoch  35 Batch  181/269   train_loss = 3.886\n",
      "Epoch  35 Batch  182/269   train_loss = 3.739\n",
      "Epoch  35 Batch  183/269   train_loss = 3.675\n",
      "Epoch  35 Batch  184/269   train_loss = 3.749\n",
      "Epoch  35 Batch  185/269   train_loss = 3.811\n",
      "Epoch  35 Batch  186/269   train_loss = 3.685\n",
      "Epoch  35 Batch  187/269   train_loss = 3.966\n",
      "Epoch  35 Batch  188/269   train_loss = 3.738\n",
      "Epoch  35 Batch  189/269   train_loss = 3.692\n",
      "Epoch  35 Batch  190/269   train_loss = 4.141\n",
      "Epoch  35 Batch  191/269   train_loss = 3.842\n",
      "Epoch  35 Batch  192/269   train_loss = 3.709\n",
      "Epoch  35 Batch  193/269   train_loss = 3.719\n",
      "Epoch  35 Batch  194/269   train_loss = 3.817\n",
      "Epoch  35 Batch  195/269   train_loss = 3.700\n",
      "Epoch  35 Batch  196/269   train_loss = 3.844\n",
      "Epoch  35 Batch  197/269   train_loss = 3.984\n",
      "Epoch  35 Batch  198/269   train_loss = 3.847\n",
      "Epoch  35 Batch  199/269   train_loss = 3.845\n",
      "Epoch  35 Batch  200/269   train_loss = 3.671\n",
      "Epoch  35 Batch  201/269   train_loss = 3.753\n",
      "Epoch  35 Batch  202/269   train_loss = 3.632\n",
      "Epoch  35 Batch  203/269   train_loss = 3.605\n",
      "Epoch  35 Batch  204/269   train_loss = 3.861\n",
      "Epoch  35 Batch  205/269   train_loss = 3.817\n",
      "Epoch  35 Batch  206/269   train_loss = 3.670\n",
      "Epoch  35 Batch  207/269   train_loss = 3.682\n",
      "Epoch  35 Batch  208/269   train_loss = 3.782\n",
      "Epoch  35 Batch  209/269   train_loss = 3.887\n",
      "Epoch  35 Batch  210/269   train_loss = 3.594\n",
      "Epoch  35 Batch  211/269   train_loss = 3.767\n",
      "Epoch  35 Batch  212/269   train_loss = 4.040\n",
      "Epoch  35 Batch  213/269   train_loss = 3.596\n",
      "Epoch  35 Batch  214/269   train_loss = 3.727\n",
      "Epoch  35 Batch  215/269   train_loss = 3.966\n",
      "Epoch  35 Batch  216/269   train_loss = 3.908\n",
      "Epoch  35 Batch  217/269   train_loss = 3.666\n",
      "Epoch  35 Batch  218/269   train_loss = 3.865\n",
      "Epoch  35 Batch  219/269   train_loss = 3.485\n",
      "Epoch  35 Batch  220/269   train_loss = 3.897\n",
      "Epoch  35 Batch  221/269   train_loss = 3.605\n",
      "Epoch  35 Batch  222/269   train_loss = 3.783\n",
      "Epoch  35 Batch  223/269   train_loss = 3.623\n",
      "Epoch  35 Batch  224/269   train_loss = 3.869\n",
      "Epoch  35 Batch  225/269   train_loss = 3.972\n",
      "Epoch  35 Batch  226/269   train_loss = 3.724\n",
      "Epoch  35 Batch  227/269   train_loss = 3.495\n",
      "Epoch  35 Batch  228/269   train_loss = 3.756\n",
      "Epoch  35 Batch  229/269   train_loss = 3.891\n",
      "Epoch  35 Batch  230/269   train_loss = 3.838\n",
      "Epoch  35 Batch  231/269   train_loss = 3.699\n",
      "Epoch  35 Batch  232/269   train_loss = 3.723\n",
      "Epoch  35 Batch  233/269   train_loss = 3.805\n",
      "Epoch  35 Batch  234/269   train_loss = 3.744\n",
      "Epoch  35 Batch  235/269   train_loss = 4.038\n",
      "Epoch  35 Batch  236/269   train_loss = 3.612\n",
      "Epoch  35 Batch  237/269   train_loss = 3.379\n",
      "Epoch  35 Batch  238/269   train_loss = 3.559\n",
      "Epoch  35 Batch  239/269   train_loss = 4.188\n",
      "Epoch  35 Batch  240/269   train_loss = 3.704\n",
      "Epoch  35 Batch  241/269   train_loss = 3.957\n",
      "Epoch  35 Batch  242/269   train_loss = 3.523\n",
      "Epoch  35 Batch  243/269   train_loss = 3.958\n",
      "Epoch  35 Batch  244/269   train_loss = 3.763\n",
      "Epoch  35 Batch  245/269   train_loss = 3.741\n",
      "Epoch  35 Batch  246/269   train_loss = 3.442\n",
      "Epoch  35 Batch  247/269   train_loss = 3.805\n",
      "Epoch  35 Batch  248/269   train_loss = 3.806\n",
      "Epoch  35 Batch  249/269   train_loss = 3.643\n",
      "Epoch  35 Batch  250/269   train_loss = 3.497\n",
      "Epoch  35 Batch  251/269   train_loss = 3.947\n",
      "Epoch  35 Batch  252/269   train_loss = 3.831\n",
      "Epoch  35 Batch  253/269   train_loss = 3.672\n",
      "Epoch  35 Batch  254/269   train_loss = 3.659\n",
      "Epoch  35 Batch  255/269   train_loss = 3.722\n",
      "Epoch  35 Batch  256/269   train_loss = 3.596\n",
      "Epoch  35 Batch  257/269   train_loss = 3.583\n",
      "Epoch  35 Batch  258/269   train_loss = 3.477\n",
      "Epoch  35 Batch  259/269   train_loss = 3.639\n",
      "Epoch  35 Batch  260/269   train_loss = 3.834\n",
      "Epoch  35 Batch  261/269   train_loss = 3.855\n",
      "Epoch  35 Batch  262/269   train_loss = 3.653\n",
      "Epoch  35 Batch  263/269   train_loss = 3.469\n",
      "Epoch  35 Batch  264/269   train_loss = 4.014\n",
      "Epoch  35 Batch  265/269   train_loss = 3.824\n",
      "Epoch  35 Batch  266/269   train_loss = 3.703\n",
      "Epoch  35 Batch  267/269   train_loss = 3.796\n",
      "Epoch  35 Batch  268/269   train_loss = 3.912\n",
      "Epoch  36 Batch    0/269   train_loss = 3.644\n",
      "Epoch  36 Batch    1/269   train_loss = 3.509\n",
      "Epoch  36 Batch    2/269   train_loss = 3.609\n",
      "Epoch  36 Batch    3/269   train_loss = 3.675\n",
      "Epoch  36 Batch    4/269   train_loss = 4.051\n",
      "Epoch  36 Batch    5/269   train_loss = 3.794\n",
      "Epoch  36 Batch    6/269   train_loss = 3.682\n",
      "Epoch  36 Batch    7/269   train_loss = 3.548\n",
      "Epoch  36 Batch    8/269   train_loss = 3.889\n",
      "Epoch  36 Batch    9/269   train_loss = 3.540\n",
      "Epoch  36 Batch   10/269   train_loss = 3.544\n",
      "Epoch  36 Batch   11/269   train_loss = 3.673\n",
      "Epoch  36 Batch   12/269   train_loss = 3.375\n",
      "Epoch  36 Batch   13/269   train_loss = 3.748\n",
      "Epoch  36 Batch   14/269   train_loss = 3.595\n",
      "Epoch  36 Batch   15/269   train_loss = 3.988\n",
      "Epoch  36 Batch   16/269   train_loss = 3.688\n",
      "Epoch  36 Batch   17/269   train_loss = 3.750\n",
      "Epoch  36 Batch   18/269   train_loss = 3.684\n",
      "Epoch  36 Batch   19/269   train_loss = 3.655\n",
      "Epoch  36 Batch   20/269   train_loss = 4.029\n",
      "Epoch  36 Batch   21/269   train_loss = 3.876\n",
      "Epoch  36 Batch   22/269   train_loss = 3.608\n",
      "Epoch  36 Batch   23/269   train_loss = 3.792\n",
      "Epoch  36 Batch   24/269   train_loss = 3.661\n",
      "Epoch  36 Batch   25/269   train_loss = 3.970\n",
      "Epoch  36 Batch   26/269   train_loss = 3.760\n",
      "Epoch  36 Batch   27/269   train_loss = 3.602\n",
      "Epoch  36 Batch   28/269   train_loss = 3.816\n",
      "Epoch  36 Batch   29/269   train_loss = 3.776\n",
      "Epoch  36 Batch   30/269   train_loss = 4.084\n",
      "Epoch  36 Batch   31/269   train_loss = 3.703\n",
      "Epoch  36 Batch   32/269   train_loss = 3.648\n",
      "Epoch  36 Batch   33/269   train_loss = 3.416\n",
      "Epoch  36 Batch   34/269   train_loss = 3.683\n",
      "Epoch  36 Batch   35/269   train_loss = 3.542\n",
      "Epoch  36 Batch   36/269   train_loss = 3.586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  36 Batch   37/269   train_loss = 3.782\n",
      "Epoch  36 Batch   38/269   train_loss = 3.557\n",
      "Epoch  36 Batch   39/269   train_loss = 3.838\n",
      "Epoch  36 Batch   40/269   train_loss = 3.590\n",
      "Epoch  36 Batch   41/269   train_loss = 3.577\n",
      "Epoch  36 Batch   42/269   train_loss = 4.087\n",
      "Epoch  36 Batch   43/269   train_loss = 3.900\n",
      "Epoch  36 Batch   44/269   train_loss = 3.764\n",
      "Epoch  36 Batch   45/269   train_loss = 3.450\n",
      "Epoch  36 Batch   46/269   train_loss = 3.827\n",
      "Epoch  36 Batch   47/269   train_loss = 3.452\n",
      "Epoch  36 Batch   48/269   train_loss = 4.008\n",
      "Epoch  36 Batch   49/269   train_loss = 3.947\n",
      "Epoch  36 Batch   50/269   train_loss = 3.877\n",
      "Epoch  36 Batch   51/269   train_loss = 3.797\n",
      "Epoch  36 Batch   52/269   train_loss = 3.796\n",
      "Epoch  36 Batch   53/269   train_loss = 4.015\n",
      "Epoch  36 Batch   54/269   train_loss = 3.815\n",
      "Epoch  36 Batch   55/269   train_loss = 3.842\n",
      "Epoch  36 Batch   56/269   train_loss = 3.695\n",
      "Epoch  36 Batch   57/269   train_loss = 3.901\n",
      "Epoch  36 Batch   58/269   train_loss = 3.597\n",
      "Epoch  36 Batch   59/269   train_loss = 3.651\n",
      "Epoch  36 Batch   60/269   train_loss = 3.782\n",
      "Epoch  36 Batch   61/269   train_loss = 4.035\n",
      "Epoch  36 Batch   62/269   train_loss = 3.631\n",
      "Epoch  36 Batch   63/269   train_loss = 3.911\n",
      "Epoch  36 Batch   64/269   train_loss = 3.673\n",
      "Epoch  36 Batch   65/269   train_loss = 3.786\n",
      "Epoch  36 Batch   66/269   train_loss = 3.631\n",
      "Epoch  36 Batch   67/269   train_loss = 3.728\n",
      "Epoch  36 Batch   68/269   train_loss = 3.784\n",
      "Epoch  36 Batch   69/269   train_loss = 3.828\n",
      "Epoch  36 Batch   70/269   train_loss = 3.468\n",
      "Epoch  36 Batch   71/269   train_loss = 3.849\n",
      "Epoch  36 Batch   72/269   train_loss = 3.490\n",
      "Epoch  36 Batch   73/269   train_loss = 3.711\n",
      "Epoch  36 Batch   74/269   train_loss = 3.746\n",
      "Epoch  36 Batch   75/269   train_loss = 3.913\n",
      "Epoch  36 Batch   76/269   train_loss = 3.783\n",
      "Epoch  36 Batch   77/269   train_loss = 3.489\n",
      "Epoch  36 Batch   78/269   train_loss = 3.916\n",
      "Epoch  36 Batch   79/269   train_loss = 3.907\n",
      "Epoch  36 Batch   80/269   train_loss = 3.859\n",
      "Epoch  36 Batch   81/269   train_loss = 3.745\n",
      "Epoch  36 Batch   82/269   train_loss = 3.938\n",
      "Epoch  36 Batch   83/269   train_loss = 3.670\n",
      "Epoch  36 Batch   84/269   train_loss = 3.924\n",
      "Epoch  36 Batch   85/269   train_loss = 4.165\n",
      "Epoch  36 Batch   86/269   train_loss = 4.034\n",
      "Epoch  36 Batch   87/269   train_loss = 3.768\n",
      "Epoch  36 Batch   88/269   train_loss = 3.823\n",
      "Epoch  36 Batch   89/269   train_loss = 3.741\n",
      "Epoch  36 Batch   90/269   train_loss = 3.609\n",
      "Epoch  36 Batch   91/269   train_loss = 3.927\n",
      "Epoch  36 Batch   92/269   train_loss = 3.916\n",
      "Epoch  36 Batch   93/269   train_loss = 3.651\n",
      "Epoch  36 Batch   94/269   train_loss = 4.048\n",
      "Epoch  36 Batch   95/269   train_loss = 3.540\n",
      "Epoch  36 Batch   96/269   train_loss = 3.652\n",
      "Epoch  36 Batch   97/269   train_loss = 3.756\n",
      "Epoch  36 Batch   98/269   train_loss = 3.865\n",
      "Epoch  36 Batch   99/269   train_loss = 3.734\n",
      "Epoch  36 Batch  100/269   train_loss = 3.720\n",
      "Epoch  36 Batch  101/269   train_loss = 3.917\n",
      "Epoch  36 Batch  102/269   train_loss = 3.607\n",
      "Epoch  36 Batch  103/269   train_loss = 3.661\n",
      "Epoch  36 Batch  104/269   train_loss = 3.608\n",
      "Epoch  36 Batch  105/269   train_loss = 3.895\n",
      "Epoch  36 Batch  106/269   train_loss = 4.011\n",
      "Epoch  36 Batch  107/269   train_loss = 4.013\n",
      "Epoch  36 Batch  108/269   train_loss = 3.773\n",
      "Epoch  36 Batch  109/269   train_loss = 3.838\n",
      "Epoch  36 Batch  110/269   train_loss = 3.810\n",
      "Epoch  36 Batch  111/269   train_loss = 3.877\n",
      "Epoch  36 Batch  112/269   train_loss = 3.917\n",
      "Epoch  36 Batch  113/269   train_loss = 3.750\n",
      "Epoch  36 Batch  114/269   train_loss = 3.761\n",
      "Epoch  36 Batch  115/269   train_loss = 3.645\n",
      "Epoch  36 Batch  116/269   train_loss = 3.565\n",
      "Epoch  36 Batch  117/269   train_loss = 3.613\n",
      "Epoch  36 Batch  118/269   train_loss = 3.992\n",
      "Epoch  36 Batch  119/269   train_loss = 3.610\n",
      "Epoch  36 Batch  120/269   train_loss = 3.839\n",
      "Epoch  36 Batch  121/269   train_loss = 3.660\n",
      "Epoch  36 Batch  122/269   train_loss = 3.830\n",
      "Epoch  36 Batch  123/269   train_loss = 3.554\n",
      "Epoch  36 Batch  124/269   train_loss = 3.943\n",
      "Epoch  36 Batch  125/269   train_loss = 3.510\n",
      "Epoch  36 Batch  126/269   train_loss = 3.436\n",
      "Epoch  36 Batch  127/269   train_loss = 3.752\n",
      "Epoch  36 Batch  128/269   train_loss = 3.679\n",
      "Epoch  36 Batch  129/269   train_loss = 3.688\n",
      "Epoch  36 Batch  130/269   train_loss = 3.865\n",
      "Epoch  36 Batch  131/269   train_loss = 3.396\n",
      "Epoch  36 Batch  132/269   train_loss = 3.742\n",
      "Epoch  36 Batch  133/269   train_loss = 3.540\n",
      "Epoch  36 Batch  134/269   train_loss = 3.923\n",
      "Epoch  36 Batch  135/269   train_loss = 3.606\n",
      "Epoch  36 Batch  136/269   train_loss = 3.493\n",
      "Epoch  36 Batch  137/269   train_loss = 3.818\n",
      "Epoch  36 Batch  138/269   train_loss = 3.589\n",
      "Epoch  36 Batch  139/269   train_loss = 3.760\n",
      "Epoch  36 Batch  140/269   train_loss = 3.809\n",
      "Epoch  36 Batch  141/269   train_loss = 4.097\n",
      "Epoch  36 Batch  142/269   train_loss = 3.573\n",
      "Epoch  36 Batch  143/269   train_loss = 3.763\n",
      "Epoch  36 Batch  144/269   train_loss = 3.912\n",
      "Epoch  36 Batch  145/269   train_loss = 3.693\n",
      "Epoch  36 Batch  146/269   train_loss = 3.907\n",
      "Epoch  36 Batch  147/269   train_loss = 3.559\n",
      "Epoch  36 Batch  148/269   train_loss = 3.830\n",
      "Epoch  36 Batch  149/269   train_loss = 3.707\n",
      "Epoch  36 Batch  150/269   train_loss = 3.837\n",
      "Epoch  36 Batch  151/269   train_loss = 3.777\n",
      "Epoch  36 Batch  152/269   train_loss = 3.755\n",
      "Epoch  36 Batch  153/269   train_loss = 3.756\n",
      "Epoch  36 Batch  154/269   train_loss = 3.681\n",
      "Epoch  36 Batch  155/269   train_loss = 3.641\n",
      "Epoch  36 Batch  156/269   train_loss = 4.168\n",
      "Epoch  36 Batch  157/269   train_loss = 3.857\n",
      "Epoch  36 Batch  158/269   train_loss = 3.830\n",
      "Epoch  36 Batch  159/269   train_loss = 3.841\n",
      "Epoch  36 Batch  160/269   train_loss = 3.697\n",
      "Epoch  36 Batch  161/269   train_loss = 3.860\n",
      "Epoch  36 Batch  162/269   train_loss = 3.838\n",
      "Epoch  36 Batch  163/269   train_loss = 3.774\n",
      "Epoch  36 Batch  164/269   train_loss = 3.928\n",
      "Epoch  36 Batch  165/269   train_loss = 3.877\n",
      "Epoch  36 Batch  166/269   train_loss = 3.823\n",
      "Epoch  36 Batch  167/269   train_loss = 3.658\n",
      "Epoch  36 Batch  168/269   train_loss = 3.752\n",
      "Epoch  36 Batch  169/269   train_loss = 4.042\n",
      "Epoch  36 Batch  170/269   train_loss = 3.565\n",
      "Epoch  36 Batch  171/269   train_loss = 3.786\n",
      "Epoch  36 Batch  172/269   train_loss = 3.566\n",
      "Epoch  36 Batch  173/269   train_loss = 3.785\n",
      "Epoch  36 Batch  174/269   train_loss = 3.930\n",
      "Epoch  36 Batch  175/269   train_loss = 3.841\n",
      "Epoch  36 Batch  176/269   train_loss = 3.741\n",
      "Epoch  36 Batch  177/269   train_loss = 3.772\n",
      "Epoch  36 Batch  178/269   train_loss = 4.045\n",
      "Epoch  36 Batch  179/269   train_loss = 3.775\n",
      "Epoch  36 Batch  180/269   train_loss = 3.586\n",
      "Epoch  36 Batch  181/269   train_loss = 3.883\n",
      "Epoch  36 Batch  182/269   train_loss = 3.719\n",
      "Epoch  36 Batch  183/269   train_loss = 3.668\n",
      "Epoch  36 Batch  184/269   train_loss = 3.718\n",
      "Epoch  36 Batch  185/269   train_loss = 3.813\n",
      "Epoch  36 Batch  186/269   train_loss = 3.714\n",
      "Epoch  36 Batch  187/269   train_loss = 3.946\n",
      "Epoch  36 Batch  188/269   train_loss = 3.700\n",
      "Epoch  36 Batch  189/269   train_loss = 3.675\n",
      "Epoch  36 Batch  190/269   train_loss = 4.115\n",
      "Epoch  36 Batch  191/269   train_loss = 3.835\n",
      "Epoch  36 Batch  192/269   train_loss = 3.683\n",
      "Epoch  36 Batch  193/269   train_loss = 3.740\n",
      "Epoch  36 Batch  194/269   train_loss = 3.813\n",
      "Epoch  36 Batch  195/269   train_loss = 3.702\n",
      "Epoch  36 Batch  196/269   train_loss = 3.825\n",
      "Epoch  36 Batch  197/269   train_loss = 3.978\n",
      "Epoch  36 Batch  198/269   train_loss = 3.852\n",
      "Epoch  36 Batch  199/269   train_loss = 3.834\n",
      "Epoch  36 Batch  200/269   train_loss = 3.696\n",
      "Epoch  36 Batch  201/269   train_loss = 3.724\n",
      "Epoch  36 Batch  202/269   train_loss = 3.614\n",
      "Epoch  36 Batch  203/269   train_loss = 3.625\n",
      "Epoch  36 Batch  204/269   train_loss = 3.868\n",
      "Epoch  36 Batch  205/269   train_loss = 3.823\n",
      "Epoch  36 Batch  206/269   train_loss = 3.661\n",
      "Epoch  36 Batch  207/269   train_loss = 3.719\n",
      "Epoch  36 Batch  208/269   train_loss = 3.749\n",
      "Epoch  36 Batch  209/269   train_loss = 3.870\n",
      "Epoch  36 Batch  210/269   train_loss = 3.571\n",
      "Epoch  36 Batch  211/269   train_loss = 3.756\n",
      "Epoch  36 Batch  212/269   train_loss = 4.075\n",
      "Epoch  36 Batch  213/269   train_loss = 3.636\n",
      "Epoch  36 Batch  214/269   train_loss = 3.727\n",
      "Epoch  36 Batch  215/269   train_loss = 3.967\n",
      "Epoch  36 Batch  216/269   train_loss = 3.899\n",
      "Epoch  36 Batch  217/269   train_loss = 3.651\n",
      "Epoch  36 Batch  218/269   train_loss = 3.810\n",
      "Epoch  36 Batch  219/269   train_loss = 3.466\n",
      "Epoch  36 Batch  220/269   train_loss = 3.907\n",
      "Epoch  36 Batch  221/269   train_loss = 3.587\n",
      "Epoch  36 Batch  222/269   train_loss = 3.805\n",
      "Epoch  36 Batch  223/269   train_loss = 3.592\n",
      "Epoch  36 Batch  224/269   train_loss = 3.873\n",
      "Epoch  36 Batch  225/269   train_loss = 3.977\n",
      "Epoch  36 Batch  226/269   train_loss = 3.736\n",
      "Epoch  36 Batch  227/269   train_loss = 3.485\n",
      "Epoch  36 Batch  228/269   train_loss = 3.762\n",
      "Epoch  36 Batch  229/269   train_loss = 3.899\n",
      "Epoch  36 Batch  230/269   train_loss = 3.811\n",
      "Epoch  36 Batch  231/269   train_loss = 3.733\n",
      "Epoch  36 Batch  232/269   train_loss = 3.719\n",
      "Epoch  36 Batch  233/269   train_loss = 3.788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  36 Batch  234/269   train_loss = 3.781\n",
      "Epoch  36 Batch  235/269   train_loss = 4.021\n",
      "Epoch  36 Batch  236/269   train_loss = 3.634\n",
      "Epoch  36 Batch  237/269   train_loss = 3.407\n",
      "Epoch  36 Batch  238/269   train_loss = 3.523\n",
      "Epoch  36 Batch  239/269   train_loss = 4.152\n",
      "Epoch  36 Batch  240/269   train_loss = 3.719\n",
      "Epoch  36 Batch  241/269   train_loss = 3.943\n",
      "Epoch  36 Batch  242/269   train_loss = 3.530\n",
      "Epoch  36 Batch  243/269   train_loss = 3.946\n",
      "Epoch  36 Batch  244/269   train_loss = 3.758\n",
      "Epoch  36 Batch  245/269   train_loss = 3.740\n",
      "Epoch  36 Batch  246/269   train_loss = 3.464\n",
      "Epoch  36 Batch  247/269   train_loss = 3.803\n",
      "Epoch  36 Batch  248/269   train_loss = 3.796\n",
      "Epoch  36 Batch  249/269   train_loss = 3.622\n",
      "Epoch  36 Batch  250/269   train_loss = 3.525\n",
      "Epoch  36 Batch  251/269   train_loss = 3.945\n",
      "Epoch  36 Batch  252/269   train_loss = 3.840\n",
      "Epoch  36 Batch  253/269   train_loss = 3.674\n",
      "Epoch  36 Batch  254/269   train_loss = 3.661\n",
      "Epoch  36 Batch  255/269   train_loss = 3.736\n",
      "Epoch  36 Batch  256/269   train_loss = 3.622\n",
      "Epoch  36 Batch  257/269   train_loss = 3.550\n",
      "Epoch  36 Batch  258/269   train_loss = 3.523\n",
      "Epoch  36 Batch  259/269   train_loss = 3.602\n",
      "Epoch  36 Batch  260/269   train_loss = 3.836\n",
      "Epoch  36 Batch  261/269   train_loss = 3.867\n",
      "Epoch  36 Batch  262/269   train_loss = 3.662\n",
      "Epoch  36 Batch  263/269   train_loss = 3.481\n",
      "Epoch  36 Batch  264/269   train_loss = 3.999\n",
      "Epoch  36 Batch  265/269   train_loss = 3.841\n",
      "Epoch  36 Batch  266/269   train_loss = 3.711\n",
      "Epoch  36 Batch  267/269   train_loss = 3.795\n",
      "Epoch  36 Batch  268/269   train_loss = 3.862\n",
      "Epoch  37 Batch    0/269   train_loss = 3.657\n",
      "Epoch  37 Batch    1/269   train_loss = 3.495\n",
      "Epoch  37 Batch    2/269   train_loss = 3.633\n",
      "Epoch  37 Batch    3/269   train_loss = 3.687\n",
      "Epoch  37 Batch    4/269   train_loss = 4.046\n",
      "Epoch  37 Batch    5/269   train_loss = 3.767\n",
      "Epoch  37 Batch    6/269   train_loss = 3.678\n",
      "Epoch  37 Batch    7/269   train_loss = 3.581\n",
      "Epoch  37 Batch    8/269   train_loss = 3.881\n",
      "Epoch  37 Batch    9/269   train_loss = 3.545\n",
      "Epoch  37 Batch   10/269   train_loss = 3.545\n",
      "Epoch  37 Batch   11/269   train_loss = 3.681\n",
      "Epoch  37 Batch   12/269   train_loss = 3.405\n",
      "Epoch  37 Batch   13/269   train_loss = 3.723\n",
      "Epoch  37 Batch   14/269   train_loss = 3.568\n",
      "Epoch  37 Batch   15/269   train_loss = 4.005\n",
      "Epoch  37 Batch   16/269   train_loss = 3.709\n",
      "Epoch  37 Batch   17/269   train_loss = 3.738\n",
      "Epoch  37 Batch   18/269   train_loss = 3.684\n",
      "Epoch  37 Batch   19/269   train_loss = 3.647\n",
      "Epoch  37 Batch   20/269   train_loss = 4.044\n",
      "Epoch  37 Batch   21/269   train_loss = 3.876\n",
      "Epoch  37 Batch   22/269   train_loss = 3.629\n",
      "Epoch  37 Batch   23/269   train_loss = 3.800\n",
      "Epoch  37 Batch   24/269   train_loss = 3.665\n",
      "Epoch  37 Batch   25/269   train_loss = 3.973\n",
      "Epoch  37 Batch   26/269   train_loss = 3.781\n",
      "Epoch  37 Batch   27/269   train_loss = 3.609\n",
      "Epoch  37 Batch   28/269   train_loss = 3.802\n",
      "Epoch  37 Batch   29/269   train_loss = 3.777\n",
      "Epoch  37 Batch   30/269   train_loss = 4.054\n",
      "Epoch  37 Batch   31/269   train_loss = 3.708\n",
      "Epoch  37 Batch   32/269   train_loss = 3.638\n",
      "Epoch  37 Batch   33/269   train_loss = 3.396\n",
      "Epoch  37 Batch   34/269   train_loss = 3.686\n",
      "Epoch  37 Batch   35/269   train_loss = 3.545\n",
      "Epoch  37 Batch   36/269   train_loss = 3.581\n",
      "Epoch  37 Batch   37/269   train_loss = 3.760\n",
      "Epoch  37 Batch   38/269   train_loss = 3.549\n",
      "Epoch  37 Batch   39/269   train_loss = 3.877\n",
      "Epoch  37 Batch   40/269   train_loss = 3.566\n",
      "Epoch  37 Batch   41/269   train_loss = 3.577\n",
      "Epoch  37 Batch   42/269   train_loss = 4.083\n",
      "Epoch  37 Batch   43/269   train_loss = 3.898\n",
      "Epoch  37 Batch   44/269   train_loss = 3.751\n",
      "Epoch  37 Batch   45/269   train_loss = 3.447\n",
      "Epoch  37 Batch   46/269   train_loss = 3.830\n",
      "Epoch  37 Batch   47/269   train_loss = 3.480\n",
      "Epoch  37 Batch   48/269   train_loss = 3.949\n",
      "Epoch  37 Batch   49/269   train_loss = 3.933\n",
      "Epoch  37 Batch   50/269   train_loss = 3.857\n",
      "Epoch  37 Batch   51/269   train_loss = 3.770\n",
      "Epoch  37 Batch   52/269   train_loss = 3.778\n",
      "Epoch  37 Batch   53/269   train_loss = 4.043\n",
      "Epoch  37 Batch   54/269   train_loss = 3.836\n",
      "Epoch  37 Batch   55/269   train_loss = 3.863\n",
      "Epoch  37 Batch   56/269   train_loss = 3.708\n",
      "Epoch  37 Batch   57/269   train_loss = 3.886\n",
      "Epoch  37 Batch   58/269   train_loss = 3.607\n",
      "Epoch  37 Batch   59/269   train_loss = 3.629\n",
      "Epoch  37 Batch   60/269   train_loss = 3.780\n",
      "Epoch  37 Batch   61/269   train_loss = 4.012\n",
      "Epoch  37 Batch   62/269   train_loss = 3.600\n",
      "Epoch  37 Batch   63/269   train_loss = 3.913\n",
      "Epoch  37 Batch   64/269   train_loss = 3.652\n",
      "Epoch  37 Batch   65/269   train_loss = 3.819\n",
      "Epoch  37 Batch   66/269   train_loss = 3.626\n",
      "Epoch  37 Batch   67/269   train_loss = 3.723\n",
      "Epoch  37 Batch   68/269   train_loss = 3.810\n",
      "Epoch  37 Batch   69/269   train_loss = 3.787\n",
      "Epoch  37 Batch   70/269   train_loss = 3.444\n",
      "Epoch  37 Batch   71/269   train_loss = 3.872\n",
      "Epoch  37 Batch   72/269   train_loss = 3.506\n",
      "Epoch  37 Batch   73/269   train_loss = 3.728\n",
      "Epoch  37 Batch   74/269   train_loss = 3.733\n",
      "Epoch  37 Batch   75/269   train_loss = 3.899\n",
      "Epoch  37 Batch   76/269   train_loss = 3.810\n",
      "Epoch  37 Batch   77/269   train_loss = 3.483\n",
      "Epoch  37 Batch   78/269   train_loss = 3.883\n",
      "Epoch  37 Batch   79/269   train_loss = 3.898\n",
      "Epoch  37 Batch   80/269   train_loss = 3.874\n",
      "Epoch  37 Batch   81/269   train_loss = 3.727\n",
      "Epoch  37 Batch   82/269   train_loss = 3.926\n",
      "Epoch  37 Batch   83/269   train_loss = 3.720\n",
      "Epoch  37 Batch   84/269   train_loss = 3.937\n",
      "Epoch  37 Batch   85/269   train_loss = 4.174\n",
      "Epoch  37 Batch   86/269   train_loss = 4.044\n",
      "Epoch  37 Batch   87/269   train_loss = 3.756\n",
      "Epoch  37 Batch   88/269   train_loss = 3.845\n",
      "Epoch  37 Batch   89/269   train_loss = 3.743\n",
      "Epoch  37 Batch   90/269   train_loss = 3.608\n",
      "Epoch  37 Batch   91/269   train_loss = 3.907\n",
      "Epoch  37 Batch   92/269   train_loss = 3.960\n",
      "Epoch  37 Batch   93/269   train_loss = 3.610\n",
      "Epoch  37 Batch   94/269   train_loss = 4.041\n",
      "Epoch  37 Batch   95/269   train_loss = 3.536\n",
      "Epoch  37 Batch   96/269   train_loss = 3.634\n",
      "Epoch  37 Batch   97/269   train_loss = 3.760\n",
      "Epoch  37 Batch   98/269   train_loss = 3.865\n",
      "Epoch  37 Batch   99/269   train_loss = 3.761\n",
      "Epoch  37 Batch  100/269   train_loss = 3.671\n",
      "Epoch  37 Batch  101/269   train_loss = 3.918\n",
      "Epoch  37 Batch  102/269   train_loss = 3.564\n",
      "Epoch  37 Batch  103/269   train_loss = 3.674\n",
      "Epoch  37 Batch  104/269   train_loss = 3.628\n",
      "Epoch  37 Batch  105/269   train_loss = 3.877\n",
      "Epoch  37 Batch  106/269   train_loss = 4.014\n",
      "Epoch  37 Batch  107/269   train_loss = 3.991\n",
      "Epoch  37 Batch  108/269   train_loss = 3.768\n",
      "Epoch  37 Batch  109/269   train_loss = 3.841\n",
      "Epoch  37 Batch  110/269   train_loss = 3.783\n",
      "Epoch  37 Batch  111/269   train_loss = 3.891\n",
      "Epoch  37 Batch  112/269   train_loss = 3.904\n",
      "Epoch  37 Batch  113/269   train_loss = 3.744\n",
      "Epoch  37 Batch  114/269   train_loss = 3.760\n",
      "Epoch  37 Batch  115/269   train_loss = 3.586\n",
      "Epoch  37 Batch  116/269   train_loss = 3.556\n",
      "Epoch  37 Batch  117/269   train_loss = 3.614\n",
      "Epoch  37 Batch  118/269   train_loss = 4.017\n",
      "Epoch  37 Batch  119/269   train_loss = 3.569\n",
      "Epoch  37 Batch  120/269   train_loss = 3.849\n",
      "Epoch  37 Batch  121/269   train_loss = 3.670\n",
      "Epoch  37 Batch  122/269   train_loss = 3.791\n",
      "Epoch  37 Batch  123/269   train_loss = 3.572\n",
      "Epoch  37 Batch  124/269   train_loss = 3.941\n",
      "Epoch  37 Batch  125/269   train_loss = 3.508\n",
      "Epoch  37 Batch  126/269   train_loss = 3.443\n",
      "Epoch  37 Batch  127/269   train_loss = 3.754\n",
      "Epoch  37 Batch  128/269   train_loss = 3.674\n",
      "Epoch  37 Batch  129/269   train_loss = 3.677\n",
      "Epoch  37 Batch  130/269   train_loss = 3.840\n",
      "Epoch  37 Batch  131/269   train_loss = 3.394\n",
      "Epoch  37 Batch  132/269   train_loss = 3.723\n",
      "Epoch  37 Batch  133/269   train_loss = 3.526\n",
      "Epoch  37 Batch  134/269   train_loss = 3.917\n",
      "Epoch  37 Batch  135/269   train_loss = 3.596\n",
      "Epoch  37 Batch  136/269   train_loss = 3.487\n",
      "Epoch  37 Batch  137/269   train_loss = 3.802\n",
      "Epoch  37 Batch  138/269   train_loss = 3.583\n",
      "Epoch  37 Batch  139/269   train_loss = 3.770\n",
      "Epoch  37 Batch  140/269   train_loss = 3.823\n",
      "Epoch  37 Batch  141/269   train_loss = 4.052\n",
      "Epoch  37 Batch  142/269   train_loss = 3.569\n",
      "Epoch  37 Batch  143/269   train_loss = 3.774\n",
      "Epoch  37 Batch  144/269   train_loss = 3.907\n",
      "Epoch  37 Batch  145/269   train_loss = 3.667\n",
      "Epoch  37 Batch  146/269   train_loss = 3.898\n",
      "Epoch  37 Batch  147/269   train_loss = 3.564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  37 Batch  148/269   train_loss = 3.844\n",
      "Epoch  37 Batch  149/269   train_loss = 3.685\n",
      "Epoch  37 Batch  150/269   train_loss = 3.834\n",
      "Epoch  37 Batch  151/269   train_loss = 3.777\n",
      "Epoch  37 Batch  152/269   train_loss = 3.745\n",
      "Epoch  37 Batch  153/269   train_loss = 3.814\n",
      "Epoch  37 Batch  154/269   train_loss = 3.671\n",
      "Epoch  37 Batch  155/269   train_loss = 3.653\n",
      "Epoch  37 Batch  156/269   train_loss = 4.093\n",
      "Epoch  37 Batch  157/269   train_loss = 3.850\n",
      "Epoch  37 Batch  158/269   train_loss = 3.806\n",
      "Epoch  37 Batch  159/269   train_loss = 3.835\n",
      "Epoch  37 Batch  160/269   train_loss = 3.696\n",
      "Epoch  37 Batch  161/269   train_loss = 3.871\n",
      "Epoch  37 Batch  162/269   train_loss = 3.817\n",
      "Epoch  37 Batch  163/269   train_loss = 3.762\n",
      "Epoch  37 Batch  164/269   train_loss = 3.947\n",
      "Epoch  37 Batch  165/269   train_loss = 3.905\n",
      "Epoch  37 Batch  166/269   train_loss = 3.814\n",
      "Epoch  37 Batch  167/269   train_loss = 3.647\n",
      "Epoch  37 Batch  168/269   train_loss = 3.723\n",
      "Epoch  37 Batch  169/269   train_loss = 4.006\n",
      "Epoch  37 Batch  170/269   train_loss = 3.592\n",
      "Epoch  37 Batch  171/269   train_loss = 3.794\n",
      "Epoch  37 Batch  172/269   train_loss = 3.585\n",
      "Epoch  37 Batch  173/269   train_loss = 3.740\n",
      "Epoch  37 Batch  174/269   train_loss = 3.918\n",
      "Epoch  37 Batch  175/269   train_loss = 3.803\n",
      "Epoch  37 Batch  176/269   train_loss = 3.749\n",
      "Epoch  37 Batch  177/269   train_loss = 3.753\n",
      "Epoch  37 Batch  178/269   train_loss = 4.042\n",
      "Epoch  37 Batch  179/269   train_loss = 3.773\n",
      "Epoch  37 Batch  180/269   train_loss = 3.605\n",
      "Epoch  37 Batch  181/269   train_loss = 3.860\n",
      "Epoch  37 Batch  182/269   train_loss = 3.700\n",
      "Epoch  37 Batch  183/269   train_loss = 3.647\n",
      "Epoch  37 Batch  184/269   train_loss = 3.713\n",
      "Epoch  37 Batch  185/269   train_loss = 3.800\n",
      "Epoch  37 Batch  186/269   train_loss = 3.743\n",
      "Epoch  37 Batch  187/269   train_loss = 3.958\n",
      "Epoch  37 Batch  188/269   train_loss = 3.736\n",
      "Epoch  37 Batch  189/269   train_loss = 3.711\n",
      "Epoch  37 Batch  190/269   train_loss = 4.091\n",
      "Epoch  37 Batch  191/269   train_loss = 3.834\n",
      "Epoch  37 Batch  192/269   train_loss = 3.701\n",
      "Epoch  37 Batch  193/269   train_loss = 3.720\n",
      "Epoch  37 Batch  194/269   train_loss = 3.826\n",
      "Epoch  37 Batch  195/269   train_loss = 3.694\n",
      "Epoch  37 Batch  196/269   train_loss = 3.868\n",
      "Epoch  37 Batch  197/269   train_loss = 3.972\n",
      "Epoch  37 Batch  198/269   train_loss = 3.836\n",
      "Epoch  37 Batch  199/269   train_loss = 3.850\n",
      "Epoch  37 Batch  200/269   train_loss = 3.693\n",
      "Epoch  37 Batch  201/269   train_loss = 3.737\n",
      "Epoch  37 Batch  202/269   train_loss = 3.609\n",
      "Epoch  37 Batch  203/269   train_loss = 3.588\n",
      "Epoch  37 Batch  204/269   train_loss = 3.862\n",
      "Epoch  37 Batch  205/269   train_loss = 3.806\n",
      "Epoch  37 Batch  206/269   train_loss = 3.720\n",
      "Epoch  37 Batch  207/269   train_loss = 3.705\n",
      "Epoch  37 Batch  208/269   train_loss = 3.755\n",
      "Epoch  37 Batch  209/269   train_loss = 3.888\n",
      "Epoch  37 Batch  210/269   train_loss = 3.567\n",
      "Epoch  37 Batch  211/269   train_loss = 3.773\n",
      "Epoch  37 Batch  212/269   train_loss = 4.080\n",
      "Epoch  37 Batch  213/269   train_loss = 3.665\n",
      "Epoch  37 Batch  214/269   train_loss = 3.738\n",
      "Epoch  37 Batch  215/269   train_loss = 3.917\n",
      "Epoch  37 Batch  216/269   train_loss = 3.891\n",
      "Epoch  37 Batch  217/269   train_loss = 3.612\n",
      "Epoch  37 Batch  218/269   train_loss = 3.815\n",
      "Epoch  37 Batch  219/269   train_loss = 3.484\n",
      "Epoch  37 Batch  220/269   train_loss = 3.894\n",
      "Epoch  37 Batch  221/269   train_loss = 3.583\n",
      "Epoch  37 Batch  222/269   train_loss = 3.793\n",
      "Epoch  37 Batch  223/269   train_loss = 3.579\n",
      "Epoch  37 Batch  224/269   train_loss = 3.868\n",
      "Epoch  37 Batch  225/269   train_loss = 3.963\n",
      "Epoch  37 Batch  226/269   train_loss = 3.736\n",
      "Epoch  37 Batch  227/269   train_loss = 3.482\n",
      "Epoch  37 Batch  228/269   train_loss = 3.741\n",
      "Epoch  37 Batch  229/269   train_loss = 3.880\n",
      "Epoch  37 Batch  230/269   train_loss = 3.829\n",
      "Epoch  37 Batch  231/269   train_loss = 3.722\n",
      "Epoch  37 Batch  232/269   train_loss = 3.712\n",
      "Epoch  37 Batch  233/269   train_loss = 3.786\n",
      "Epoch  37 Batch  234/269   train_loss = 3.760\n",
      "Epoch  37 Batch  235/269   train_loss = 4.022\n",
      "Epoch  37 Batch  236/269   train_loss = 3.635\n",
      "Epoch  37 Batch  237/269   train_loss = 3.400\n",
      "Epoch  37 Batch  238/269   train_loss = 3.512\n",
      "Epoch  37 Batch  239/269   train_loss = 4.168\n",
      "Epoch  37 Batch  240/269   train_loss = 3.685\n",
      "Epoch  37 Batch  241/269   train_loss = 3.926\n",
      "Epoch  37 Batch  242/269   train_loss = 3.531\n",
      "Epoch  37 Batch  243/269   train_loss = 3.900\n",
      "Epoch  37 Batch  244/269   train_loss = 3.752\n",
      "Epoch  37 Batch  245/269   train_loss = 3.739\n",
      "Epoch  37 Batch  246/269   train_loss = 3.443\n",
      "Epoch  37 Batch  247/269   train_loss = 3.809\n",
      "Epoch  37 Batch  248/269   train_loss = 3.773\n",
      "Epoch  37 Batch  249/269   train_loss = 3.603\n",
      "Epoch  37 Batch  250/269   train_loss = 3.511\n",
      "Epoch  37 Batch  251/269   train_loss = 3.958\n",
      "Epoch  37 Batch  252/269   train_loss = 3.834\n",
      "Epoch  37 Batch  253/269   train_loss = 3.658\n",
      "Epoch  37 Batch  254/269   train_loss = 3.677\n",
      "Epoch  37 Batch  255/269   train_loss = 3.708\n",
      "Epoch  37 Batch  256/269   train_loss = 3.605\n",
      "Epoch  37 Batch  257/269   train_loss = 3.544\n",
      "Epoch  37 Batch  258/269   train_loss = 3.532\n",
      "Epoch  37 Batch  259/269   train_loss = 3.619\n",
      "Epoch  37 Batch  260/269   train_loss = 3.843\n",
      "Epoch  37 Batch  261/269   train_loss = 3.848\n",
      "Epoch  37 Batch  262/269   train_loss = 3.670\n",
      "Epoch  37 Batch  263/269   train_loss = 3.485\n",
      "Epoch  37 Batch  264/269   train_loss = 4.004\n",
      "Epoch  37 Batch  265/269   train_loss = 3.803\n",
      "Epoch  37 Batch  266/269   train_loss = 3.685\n",
      "Epoch  37 Batch  267/269   train_loss = 3.777\n",
      "Epoch  37 Batch  268/269   train_loss = 3.836\n",
      "Epoch  38 Batch    0/269   train_loss = 3.654\n",
      "Epoch  38 Batch    1/269   train_loss = 3.482\n",
      "Epoch  38 Batch    2/269   train_loss = 3.618\n",
      "Epoch  38 Batch    3/269   train_loss = 3.698\n",
      "Epoch  38 Batch    4/269   train_loss = 4.037\n",
      "Epoch  38 Batch    5/269   train_loss = 3.757\n",
      "Epoch  38 Batch    6/269   train_loss = 3.661\n",
      "Epoch  38 Batch    7/269   train_loss = 3.533\n",
      "Epoch  38 Batch    8/269   train_loss = 3.869\n",
      "Epoch  38 Batch    9/269   train_loss = 3.534\n",
      "Epoch  38 Batch   10/269   train_loss = 3.550\n",
      "Epoch  38 Batch   11/269   train_loss = 3.660\n",
      "Epoch  38 Batch   12/269   train_loss = 3.410\n",
      "Epoch  38 Batch   13/269   train_loss = 3.723\n",
      "Epoch  38 Batch   14/269   train_loss = 3.597\n",
      "Epoch  38 Batch   15/269   train_loss = 3.984\n",
      "Epoch  38 Batch   16/269   train_loss = 3.689\n",
      "Epoch  38 Batch   17/269   train_loss = 3.750\n",
      "Epoch  38 Batch   18/269   train_loss = 3.687\n",
      "Epoch  38 Batch   19/269   train_loss = 3.647\n",
      "Epoch  38 Batch   20/269   train_loss = 4.021\n",
      "Epoch  38 Batch   21/269   train_loss = 3.871\n",
      "Epoch  38 Batch   22/269   train_loss = 3.611\n",
      "Epoch  38 Batch   23/269   train_loss = 3.803\n",
      "Epoch  38 Batch   24/269   train_loss = 3.643\n",
      "Epoch  38 Batch   25/269   train_loss = 3.974\n",
      "Epoch  38 Batch   26/269   train_loss = 3.774\n",
      "Epoch  38 Batch   27/269   train_loss = 3.577\n",
      "Epoch  38 Batch   28/269   train_loss = 3.795\n",
      "Epoch  38 Batch   29/269   train_loss = 3.793\n",
      "Epoch  38 Batch   30/269   train_loss = 4.063\n",
      "Epoch  38 Batch   31/269   train_loss = 3.707\n",
      "Epoch  38 Batch   32/269   train_loss = 3.646\n",
      "Epoch  38 Batch   33/269   train_loss = 3.411\n",
      "Epoch  38 Batch   34/269   train_loss = 3.691\n",
      "Epoch  38 Batch   35/269   train_loss = 3.521\n",
      "Epoch  38 Batch   36/269   train_loss = 3.569\n",
      "Epoch  38 Batch   37/269   train_loss = 3.748\n",
      "Epoch  38 Batch   38/269   train_loss = 3.562\n",
      "Epoch  38 Batch   39/269   train_loss = 3.840\n",
      "Epoch  38 Batch   40/269   train_loss = 3.568\n",
      "Epoch  38 Batch   41/269   train_loss = 3.597\n",
      "Epoch  38 Batch   42/269   train_loss = 4.070\n",
      "Epoch  38 Batch   43/269   train_loss = 3.882\n",
      "Epoch  38 Batch   44/269   train_loss = 3.721\n",
      "Epoch  38 Batch   45/269   train_loss = 3.445\n",
      "Epoch  38 Batch   46/269   train_loss = 3.838\n",
      "Epoch  38 Batch   47/269   train_loss = 3.466\n",
      "Epoch  38 Batch   48/269   train_loss = 3.973\n",
      "Epoch  38 Batch   49/269   train_loss = 3.889\n",
      "Epoch  38 Batch   50/269   train_loss = 3.865\n",
      "Epoch  38 Batch   51/269   train_loss = 3.758\n",
      "Epoch  38 Batch   52/269   train_loss = 3.768\n",
      "Epoch  38 Batch   53/269   train_loss = 3.985\n",
      "Epoch  38 Batch   54/269   train_loss = 3.793\n",
      "Epoch  38 Batch   55/269   train_loss = 3.885\n",
      "Epoch  38 Batch   56/269   train_loss = 3.689\n",
      "Epoch  38 Batch   57/269   train_loss = 3.883\n",
      "Epoch  38 Batch   58/269   train_loss = 3.571\n",
      "Epoch  38 Batch   59/269   train_loss = 3.647\n",
      "Epoch  38 Batch   60/269   train_loss = 3.765\n",
      "Epoch  38 Batch   61/269   train_loss = 4.008\n",
      "Epoch  38 Batch   62/269   train_loss = 3.644\n",
      "Epoch  38 Batch   63/269   train_loss = 3.914\n",
      "Epoch  38 Batch   64/269   train_loss = 3.676\n",
      "Epoch  38 Batch   65/269   train_loss = 3.800\n",
      "Epoch  38 Batch   66/269   train_loss = 3.617\n",
      "Epoch  38 Batch   67/269   train_loss = 3.722\n",
      "Epoch  38 Batch   68/269   train_loss = 3.771\n",
      "Epoch  38 Batch   69/269   train_loss = 3.802\n",
      "Epoch  38 Batch   70/269   train_loss = 3.432\n",
      "Epoch  38 Batch   71/269   train_loss = 3.853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  38 Batch   72/269   train_loss = 3.497\n",
      "Epoch  38 Batch   73/269   train_loss = 3.745\n",
      "Epoch  38 Batch   74/269   train_loss = 3.726\n",
      "Epoch  38 Batch   75/269   train_loss = 3.862\n",
      "Epoch  38 Batch   76/269   train_loss = 3.820\n",
      "Epoch  38 Batch   77/269   train_loss = 3.469\n",
      "Epoch  38 Batch   78/269   train_loss = 3.859\n",
      "Epoch  38 Batch   79/269   train_loss = 3.948\n",
      "Epoch  38 Batch   80/269   train_loss = 3.869\n",
      "Epoch  38 Batch   81/269   train_loss = 3.704\n",
      "Epoch  38 Batch   82/269   train_loss = 3.900\n",
      "Epoch  38 Batch   83/269   train_loss = 3.663\n",
      "Epoch  38 Batch   84/269   train_loss = 3.893\n",
      "Epoch  38 Batch   85/269   train_loss = 4.149\n",
      "Epoch  38 Batch   86/269   train_loss = 4.042\n",
      "Epoch  38 Batch   87/269   train_loss = 3.764\n",
      "Epoch  38 Batch   88/269   train_loss = 3.823\n",
      "Epoch  38 Batch   89/269   train_loss = 3.737\n",
      "Epoch  38 Batch   90/269   train_loss = 3.561\n",
      "Epoch  38 Batch   91/269   train_loss = 3.906\n",
      "Epoch  38 Batch   92/269   train_loss = 3.903\n",
      "Epoch  38 Batch   93/269   train_loss = 3.605\n",
      "Epoch  38 Batch   94/269   train_loss = 4.029\n",
      "Epoch  38 Batch   95/269   train_loss = 3.546\n",
      "Epoch  38 Batch   96/269   train_loss = 3.655\n",
      "Epoch  38 Batch   97/269   train_loss = 3.725\n",
      "Epoch  38 Batch   98/269   train_loss = 3.848\n",
      "Epoch  38 Batch   99/269   train_loss = 3.800\n",
      "Epoch  38 Batch  100/269   train_loss = 3.758\n",
      "Epoch  38 Batch  101/269   train_loss = 3.932\n",
      "Epoch  38 Batch  102/269   train_loss = 3.585\n",
      "Epoch  38 Batch  103/269   train_loss = 3.698\n",
      "Epoch  38 Batch  104/269   train_loss = 3.595\n",
      "Epoch  38 Batch  105/269   train_loss = 3.883\n",
      "Epoch  38 Batch  106/269   train_loss = 3.984\n",
      "Epoch  38 Batch  107/269   train_loss = 3.992\n",
      "Epoch  38 Batch  108/269   train_loss = 3.775\n",
      "Epoch  38 Batch  109/269   train_loss = 3.835\n",
      "Epoch  38 Batch  110/269   train_loss = 3.777\n",
      "Epoch  38 Batch  111/269   train_loss = 3.887\n",
      "Epoch  38 Batch  112/269   train_loss = 3.900\n",
      "Epoch  38 Batch  113/269   train_loss = 3.753\n",
      "Epoch  38 Batch  114/269   train_loss = 3.748\n",
      "Epoch  38 Batch  115/269   train_loss = 3.581\n",
      "Epoch  38 Batch  116/269   train_loss = 3.584\n",
      "Epoch  38 Batch  117/269   train_loss = 3.629\n",
      "Epoch  38 Batch  118/269   train_loss = 3.950\n",
      "Epoch  38 Batch  119/269   train_loss = 3.534\n",
      "Epoch  38 Batch  120/269   train_loss = 3.858\n",
      "Epoch  38 Batch  121/269   train_loss = 3.667\n",
      "Epoch  38 Batch  122/269   train_loss = 3.794\n",
      "Epoch  38 Batch  123/269   train_loss = 3.538\n",
      "Epoch  38 Batch  124/269   train_loss = 3.942\n",
      "Epoch  38 Batch  125/269   train_loss = 3.475\n",
      "Epoch  38 Batch  126/269   train_loss = 3.439\n",
      "Epoch  38 Batch  127/269   train_loss = 3.712\n",
      "Epoch  38 Batch  128/269   train_loss = 3.676\n",
      "Epoch  38 Batch  129/269   train_loss = 3.643\n",
      "Epoch  38 Batch  130/269   train_loss = 3.839\n",
      "Epoch  38 Batch  131/269   train_loss = 3.377\n",
      "Epoch  38 Batch  132/269   train_loss = 3.757\n",
      "Epoch  38 Batch  133/269   train_loss = 3.527\n",
      "Epoch  38 Batch  134/269   train_loss = 3.926\n",
      "Epoch  38 Batch  135/269   train_loss = 3.599\n",
      "Epoch  38 Batch  136/269   train_loss = 3.484\n",
      "Epoch  38 Batch  137/269   train_loss = 3.811\n",
      "Epoch  38 Batch  138/269   train_loss = 3.608\n",
      "Epoch  38 Batch  139/269   train_loss = 3.749\n",
      "Epoch  38 Batch  140/269   train_loss = 3.819\n",
      "Epoch  38 Batch  141/269   train_loss = 4.091\n",
      "Epoch  38 Batch  142/269   train_loss = 3.567\n",
      "Epoch  38 Batch  143/269   train_loss = 3.765\n",
      "Epoch  38 Batch  144/269   train_loss = 3.903\n",
      "Epoch  38 Batch  145/269   train_loss = 3.644\n",
      "Epoch  38 Batch  146/269   train_loss = 3.912\n",
      "Epoch  38 Batch  147/269   train_loss = 3.536\n",
      "Epoch  38 Batch  148/269   train_loss = 3.845\n",
      "Epoch  38 Batch  149/269   train_loss = 3.666\n",
      "Epoch  38 Batch  150/269   train_loss = 3.830\n",
      "Epoch  38 Batch  151/269   train_loss = 3.766\n",
      "Epoch  38 Batch  152/269   train_loss = 3.736\n",
      "Epoch  38 Batch  153/269   train_loss = 3.751\n",
      "Epoch  38 Batch  154/269   train_loss = 3.665\n",
      "Epoch  38 Batch  155/269   train_loss = 3.654\n",
      "Epoch  38 Batch  156/269   train_loss = 4.124\n",
      "Epoch  38 Batch  157/269   train_loss = 3.845\n",
      "Epoch  38 Batch  158/269   train_loss = 3.811\n",
      "Epoch  38 Batch  159/269   train_loss = 3.822\n",
      "Epoch  38 Batch  160/269   train_loss = 3.716\n",
      "Epoch  38 Batch  161/269   train_loss = 3.858\n",
      "Epoch  38 Batch  162/269   train_loss = 3.828\n",
      "Epoch  38 Batch  163/269   train_loss = 3.748\n",
      "Epoch  38 Batch  164/269   train_loss = 3.950\n",
      "Epoch  38 Batch  165/269   train_loss = 3.895\n",
      "Epoch  38 Batch  166/269   train_loss = 3.814\n",
      "Epoch  38 Batch  167/269   train_loss = 3.636\n",
      "Epoch  38 Batch  168/269   train_loss = 3.721\n",
      "Epoch  38 Batch  169/269   train_loss = 4.002\n",
      "Epoch  38 Batch  170/269   train_loss = 3.618\n",
      "Epoch  38 Batch  171/269   train_loss = 3.758\n",
      "Epoch  38 Batch  172/269   train_loss = 3.565\n",
      "Epoch  38 Batch  173/269   train_loss = 3.745\n",
      "Epoch  38 Batch  174/269   train_loss = 3.945\n",
      "Epoch  38 Batch  175/269   train_loss = 3.835\n",
      "Epoch  38 Batch  176/269   train_loss = 3.739\n",
      "Epoch  38 Batch  177/269   train_loss = 3.768\n",
      "Epoch  38 Batch  178/269   train_loss = 4.045\n",
      "Epoch  38 Batch  179/269   train_loss = 3.766\n",
      "Epoch  38 Batch  180/269   train_loss = 3.592\n",
      "Epoch  38 Batch  181/269   train_loss = 3.865\n",
      "Epoch  38 Batch  182/269   train_loss = 3.697\n",
      "Epoch  38 Batch  183/269   train_loss = 3.668\n",
      "Epoch  38 Batch  184/269   train_loss = 3.743\n",
      "Epoch  38 Batch  185/269   train_loss = 3.787\n",
      "Epoch  38 Batch  186/269   train_loss = 3.696\n",
      "Epoch  38 Batch  187/269   train_loss = 3.982\n",
      "Epoch  38 Batch  188/269   train_loss = 3.683\n",
      "Epoch  38 Batch  189/269   train_loss = 3.677\n",
      "Epoch  38 Batch  190/269   train_loss = 4.124\n",
      "Epoch  38 Batch  191/269   train_loss = 3.823\n",
      "Epoch  38 Batch  192/269   train_loss = 3.697\n",
      "Epoch  38 Batch  193/269   train_loss = 3.714\n",
      "Epoch  38 Batch  194/269   train_loss = 3.827\n",
      "Epoch  38 Batch  195/269   train_loss = 3.694\n",
      "Epoch  38 Batch  196/269   train_loss = 3.830\n",
      "Epoch  38 Batch  197/269   train_loss = 3.939\n",
      "Epoch  38 Batch  198/269   train_loss = 3.846\n",
      "Epoch  38 Batch  199/269   train_loss = 3.831\n",
      "Epoch  38 Batch  200/269   train_loss = 3.679\n",
      "Epoch  38 Batch  201/269   train_loss = 3.696\n",
      "Epoch  38 Batch  202/269   train_loss = 3.609\n",
      "Epoch  38 Batch  203/269   train_loss = 3.613\n",
      "Epoch  38 Batch  204/269   train_loss = 3.847\n",
      "Epoch  38 Batch  205/269   train_loss = 3.816\n",
      "Epoch  38 Batch  206/269   train_loss = 3.696\n",
      "Epoch  38 Batch  207/269   train_loss = 3.677\n",
      "Epoch  38 Batch  208/269   train_loss = 3.735\n",
      "Epoch  38 Batch  209/269   train_loss = 3.927\n",
      "Epoch  38 Batch  210/269   train_loss = 3.566\n",
      "Epoch  38 Batch  211/269   train_loss = 3.757\n",
      "Epoch  38 Batch  212/269   train_loss = 4.087\n",
      "Epoch  38 Batch  213/269   train_loss = 3.633\n",
      "Epoch  38 Batch  214/269   train_loss = 3.703\n",
      "Epoch  38 Batch  215/269   train_loss = 3.918\n",
      "Epoch  38 Batch  216/269   train_loss = 3.877\n",
      "Epoch  38 Batch  217/269   train_loss = 3.611\n",
      "Epoch  38 Batch  218/269   train_loss = 3.844\n",
      "Epoch  38 Batch  219/269   train_loss = 3.479\n",
      "Epoch  38 Batch  220/269   train_loss = 3.907\n",
      "Epoch  38 Batch  221/269   train_loss = 3.599\n",
      "Epoch  38 Batch  222/269   train_loss = 3.775\n",
      "Epoch  38 Batch  223/269   train_loss = 3.585\n",
      "Epoch  38 Batch  224/269   train_loss = 3.874\n",
      "Epoch  38 Batch  225/269   train_loss = 3.975\n",
      "Epoch  38 Batch  226/269   train_loss = 3.725\n",
      "Epoch  38 Batch  227/269   train_loss = 3.449\n",
      "Epoch  38 Batch  228/269   train_loss = 3.747\n",
      "Epoch  38 Batch  229/269   train_loss = 3.896\n",
      "Epoch  38 Batch  230/269   train_loss = 3.803\n",
      "Epoch  38 Batch  231/269   train_loss = 3.703\n",
      "Epoch  38 Batch  232/269   train_loss = 3.723\n",
      "Epoch  38 Batch  233/269   train_loss = 3.849\n",
      "Epoch  38 Batch  234/269   train_loss = 3.740\n",
      "Epoch  38 Batch  235/269   train_loss = 3.991\n",
      "Epoch  38 Batch  236/269   train_loss = 3.641\n",
      "Epoch  38 Batch  237/269   train_loss = 3.386\n",
      "Epoch  38 Batch  238/269   train_loss = 3.510\n",
      "Epoch  38 Batch  239/269   train_loss = 4.133\n",
      "Epoch  38 Batch  240/269   train_loss = 3.690\n",
      "Epoch  38 Batch  241/269   train_loss = 3.926\n",
      "Epoch  38 Batch  242/269   train_loss = 3.538\n",
      "Epoch  38 Batch  243/269   train_loss = 3.894\n",
      "Epoch  38 Batch  244/269   train_loss = 3.739\n",
      "Epoch  38 Batch  245/269   train_loss = 3.729\n",
      "Epoch  38 Batch  246/269   train_loss = 3.423\n",
      "Epoch  38 Batch  247/269   train_loss = 3.794\n",
      "Epoch  38 Batch  248/269   train_loss = 3.762\n",
      "Epoch  38 Batch  249/269   train_loss = 3.610\n",
      "Epoch  38 Batch  250/269   train_loss = 3.496\n",
      "Epoch  38 Batch  251/269   train_loss = 3.947\n",
      "Epoch  38 Batch  252/269   train_loss = 3.835\n",
      "Epoch  38 Batch  253/269   train_loss = 3.649\n",
      "Epoch  38 Batch  254/269   train_loss = 3.704\n",
      "Epoch  38 Batch  255/269   train_loss = 3.716\n",
      "Epoch  38 Batch  256/269   train_loss = 3.604\n",
      "Epoch  38 Batch  257/269   train_loss = 3.540\n",
      "Epoch  38 Batch  258/269   train_loss = 3.478\n",
      "Epoch  38 Batch  259/269   train_loss = 3.636\n",
      "Epoch  38 Batch  260/269   train_loss = 3.825\n",
      "Epoch  38 Batch  261/269   train_loss = 3.842\n",
      "Epoch  38 Batch  262/269   train_loss = 3.655\n",
      "Epoch  38 Batch  263/269   train_loss = 3.471\n",
      "Epoch  38 Batch  264/269   train_loss = 4.005\n",
      "Epoch  38 Batch  265/269   train_loss = 3.828\n",
      "Epoch  38 Batch  266/269   train_loss = 3.680\n",
      "Epoch  38 Batch  267/269   train_loss = 3.764\n",
      "Epoch  38 Batch  268/269   train_loss = 3.834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  39 Batch    0/269   train_loss = 3.659\n",
      "Epoch  39 Batch    1/269   train_loss = 3.486\n",
      "Epoch  39 Batch    2/269   train_loss = 3.631\n",
      "Epoch  39 Batch    3/269   train_loss = 3.687\n",
      "Epoch  39 Batch    4/269   train_loss = 4.032\n",
      "Epoch  39 Batch    5/269   train_loss = 3.748\n",
      "Epoch  39 Batch    6/269   train_loss = 3.726\n",
      "Epoch  39 Batch    7/269   train_loss = 3.546\n",
      "Epoch  39 Batch    8/269   train_loss = 3.883\n",
      "Epoch  39 Batch    9/269   train_loss = 3.527\n",
      "Epoch  39 Batch   10/269   train_loss = 3.559\n",
      "Epoch  39 Batch   11/269   train_loss = 3.672\n",
      "Epoch  39 Batch   12/269   train_loss = 3.381\n",
      "Epoch  39 Batch   13/269   train_loss = 3.699\n",
      "Epoch  39 Batch   14/269   train_loss = 3.565\n",
      "Epoch  39 Batch   15/269   train_loss = 3.966\n",
      "Epoch  39 Batch   16/269   train_loss = 3.679\n",
      "Epoch  39 Batch   17/269   train_loss = 3.723\n",
      "Epoch  39 Batch   18/269   train_loss = 3.667\n",
      "Epoch  39 Batch   19/269   train_loss = 3.647\n",
      "Epoch  39 Batch   20/269   train_loss = 4.044\n",
      "Epoch  39 Batch   21/269   train_loss = 3.852\n",
      "Epoch  39 Batch   22/269   train_loss = 3.625\n",
      "Epoch  39 Batch   23/269   train_loss = 3.818\n",
      "Epoch  39 Batch   24/269   train_loss = 3.632\n",
      "Epoch  39 Batch   25/269   train_loss = 3.965\n",
      "Epoch  39 Batch   26/269   train_loss = 3.797\n",
      "Epoch  39 Batch   27/269   train_loss = 3.619\n",
      "Epoch  39 Batch   28/269   train_loss = 3.784\n",
      "Epoch  39 Batch   29/269   train_loss = 3.794\n",
      "Epoch  39 Batch   30/269   train_loss = 4.065\n",
      "Epoch  39 Batch   31/269   train_loss = 3.687\n",
      "Epoch  39 Batch   32/269   train_loss = 3.657\n",
      "Epoch  39 Batch   33/269   train_loss = 3.426\n",
      "Epoch  39 Batch   34/269   train_loss = 3.665\n",
      "Epoch  39 Batch   35/269   train_loss = 3.512\n",
      "Epoch  39 Batch   36/269   train_loss = 3.581\n",
      "Epoch  39 Batch   37/269   train_loss = 3.760\n",
      "Epoch  39 Batch   38/269   train_loss = 3.547\n",
      "Epoch  39 Batch   39/269   train_loss = 3.860\n",
      "Epoch  39 Batch   40/269   train_loss = 3.572\n",
      "Epoch  39 Batch   41/269   train_loss = 3.580\n",
      "Epoch  39 Batch   42/269   train_loss = 4.078\n",
      "Epoch  39 Batch   43/269   train_loss = 3.896\n",
      "Epoch  39 Batch   44/269   train_loss = 3.758\n",
      "Epoch  39 Batch   45/269   train_loss = 3.452\n",
      "Epoch  39 Batch   46/269   train_loss = 3.821\n",
      "Epoch  39 Batch   47/269   train_loss = 3.451\n",
      "Epoch  39 Batch   48/269   train_loss = 3.968\n",
      "Epoch  39 Batch   49/269   train_loss = 3.910\n",
      "Epoch  39 Batch   50/269   train_loss = 3.857\n",
      "Epoch  39 Batch   51/269   train_loss = 3.753\n",
      "Epoch  39 Batch   52/269   train_loss = 3.774\n",
      "Epoch  39 Batch   53/269   train_loss = 3.993\n",
      "Epoch  39 Batch   54/269   train_loss = 3.758\n",
      "Epoch  39 Batch   55/269   train_loss = 3.822\n",
      "Epoch  39 Batch   56/269   train_loss = 3.715\n",
      "Epoch  39 Batch   57/269   train_loss = 3.891\n",
      "Epoch  39 Batch   58/269   train_loss = 3.582\n",
      "Epoch  39 Batch   59/269   train_loss = 3.620\n",
      "Epoch  39 Batch   60/269   train_loss = 3.782\n",
      "Epoch  39 Batch   61/269   train_loss = 4.012\n",
      "Epoch  39 Batch   62/269   train_loss = 3.602\n",
      "Epoch  39 Batch   63/269   train_loss = 3.919\n",
      "Epoch  39 Batch   64/269   train_loss = 3.635\n",
      "Epoch  39 Batch   65/269   train_loss = 3.801\n",
      "Epoch  39 Batch   66/269   train_loss = 3.622\n",
      "Epoch  39 Batch   67/269   train_loss = 3.733\n",
      "Epoch  39 Batch   68/269   train_loss = 3.803\n",
      "Epoch  39 Batch   69/269   train_loss = 3.790\n",
      "Epoch  39 Batch   70/269   train_loss = 3.408\n",
      "Epoch  39 Batch   71/269   train_loss = 3.857\n",
      "Epoch  39 Batch   72/269   train_loss = 3.499\n",
      "Epoch  39 Batch   73/269   train_loss = 3.713\n",
      "Epoch  39 Batch   74/269   train_loss = 3.712\n",
      "Epoch  39 Batch   75/269   train_loss = 3.837\n",
      "Epoch  39 Batch   76/269   train_loss = 3.831\n",
      "Epoch  39 Batch   77/269   train_loss = 3.448\n",
      "Epoch  39 Batch   78/269   train_loss = 3.861\n",
      "Epoch  39 Batch   79/269   train_loss = 3.923\n",
      "Epoch  39 Batch   80/269   train_loss = 3.857\n",
      "Epoch  39 Batch   81/269   train_loss = 3.686\n",
      "Epoch  39 Batch   82/269   train_loss = 3.901\n",
      "Epoch  39 Batch   83/269   train_loss = 3.666\n",
      "Epoch  39 Batch   84/269   train_loss = 3.890\n",
      "Epoch  39 Batch   85/269   train_loss = 4.171\n",
      "Epoch  39 Batch   86/269   train_loss = 4.036\n",
      "Epoch  39 Batch   87/269   train_loss = 3.744\n",
      "Epoch  39 Batch   88/269   train_loss = 3.836\n",
      "Epoch  39 Batch   89/269   train_loss = 3.739\n",
      "Epoch  39 Batch   90/269   train_loss = 3.621\n",
      "Epoch  39 Batch   91/269   train_loss = 3.878\n",
      "Epoch  39 Batch   92/269   train_loss = 3.912\n",
      "Epoch  39 Batch   93/269   train_loss = 3.634\n",
      "Epoch  39 Batch   94/269   train_loss = 4.043\n",
      "Epoch  39 Batch   95/269   train_loss = 3.534\n",
      "Epoch  39 Batch   96/269   train_loss = 3.620\n",
      "Epoch  39 Batch   97/269   train_loss = 3.698\n",
      "Epoch  39 Batch   98/269   train_loss = 3.884\n",
      "Epoch  39 Batch   99/269   train_loss = 3.771\n",
      "Epoch  39 Batch  100/269   train_loss = 3.713\n",
      "Epoch  39 Batch  101/269   train_loss = 3.927\n",
      "Epoch  39 Batch  102/269   train_loss = 3.564\n",
      "Epoch  39 Batch  103/269   train_loss = 3.680\n",
      "Epoch  39 Batch  104/269   train_loss = 3.610\n",
      "Epoch  39 Batch  105/269   train_loss = 3.892\n",
      "Epoch  39 Batch  106/269   train_loss = 3.958\n",
      "Epoch  39 Batch  107/269   train_loss = 3.970\n",
      "Epoch  39 Batch  108/269   train_loss = 3.782\n",
      "Epoch  39 Batch  109/269   train_loss = 3.822\n",
      "Epoch  39 Batch  110/269   train_loss = 3.792\n",
      "Epoch  39 Batch  111/269   train_loss = 3.885\n",
      "Epoch  39 Batch  112/269   train_loss = 3.895\n",
      "Epoch  39 Batch  113/269   train_loss = 3.746\n",
      "Epoch  39 Batch  114/269   train_loss = 3.723\n",
      "Epoch  39 Batch  115/269   train_loss = 3.561\n",
      "Epoch  39 Batch  116/269   train_loss = 3.551\n",
      "Epoch  39 Batch  117/269   train_loss = 3.586\n",
      "Epoch  39 Batch  118/269   train_loss = 3.947\n",
      "Epoch  39 Batch  119/269   train_loss = 3.527\n",
      "Epoch  39 Batch  120/269   train_loss = 3.845\n",
      "Epoch  39 Batch  121/269   train_loss = 3.665\n",
      "Epoch  39 Batch  122/269   train_loss = 3.823\n",
      "Epoch  39 Batch  123/269   train_loss = 3.574\n",
      "Epoch  39 Batch  124/269   train_loss = 3.946\n",
      "Epoch  39 Batch  125/269   train_loss = 3.497\n",
      "Epoch  39 Batch  126/269   train_loss = 3.443\n",
      "Epoch  39 Batch  127/269   train_loss = 3.729\n",
      "Epoch  39 Batch  128/269   train_loss = 3.679\n",
      "Epoch  39 Batch  129/269   train_loss = 3.660\n",
      "Epoch  39 Batch  130/269   train_loss = 3.825\n",
      "Epoch  39 Batch  131/269   train_loss = 3.385\n",
      "Epoch  39 Batch  132/269   train_loss = 3.748\n",
      "Epoch  39 Batch  133/269   train_loss = 3.501\n",
      "Epoch  39 Batch  134/269   train_loss = 3.911\n",
      "Epoch  39 Batch  135/269   train_loss = 3.582\n",
      "Epoch  39 Batch  136/269   train_loss = 3.489\n",
      "Epoch  39 Batch  137/269   train_loss = 3.793\n",
      "Epoch  39 Batch  138/269   train_loss = 3.595\n",
      "Epoch  39 Batch  139/269   train_loss = 3.762\n",
      "Epoch  39 Batch  140/269   train_loss = 3.801\n",
      "Epoch  39 Batch  141/269   train_loss = 4.068\n",
      "Epoch  39 Batch  142/269   train_loss = 3.602\n",
      "Epoch  39 Batch  143/269   train_loss = 3.770\n",
      "Epoch  39 Batch  144/269   train_loss = 3.919\n",
      "Epoch  39 Batch  145/269   train_loss = 3.658\n",
      "Epoch  39 Batch  146/269   train_loss = 3.907\n",
      "Epoch  39 Batch  147/269   train_loss = 3.538\n",
      "Epoch  39 Batch  148/269   train_loss = 3.843\n",
      "Epoch  39 Batch  149/269   train_loss = 3.667\n",
      "Epoch  39 Batch  150/269   train_loss = 3.863\n",
      "Epoch  39 Batch  151/269   train_loss = 3.801\n",
      "Epoch  39 Batch  152/269   train_loss = 3.734\n",
      "Epoch  39 Batch  153/269   train_loss = 3.717\n",
      "Epoch  39 Batch  154/269   train_loss = 3.634\n",
      "Epoch  39 Batch  155/269   train_loss = 3.672\n",
      "Epoch  39 Batch  156/269   train_loss = 4.111\n",
      "Epoch  39 Batch  157/269   train_loss = 3.843\n",
      "Epoch  39 Batch  158/269   train_loss = 3.854\n",
      "Epoch  39 Batch  159/269   train_loss = 3.818\n",
      "Epoch  39 Batch  160/269   train_loss = 3.702\n",
      "Epoch  39 Batch  161/269   train_loss = 3.839\n",
      "Epoch  39 Batch  162/269   train_loss = 3.827\n",
      "Epoch  39 Batch  163/269   train_loss = 3.737\n",
      "Epoch  39 Batch  164/269   train_loss = 3.929\n",
      "Epoch  39 Batch  165/269   train_loss = 3.881\n",
      "Epoch  39 Batch  166/269   train_loss = 3.799\n",
      "Epoch  39 Batch  167/269   train_loss = 3.644\n",
      "Epoch  39 Batch  168/269   train_loss = 3.713\n",
      "Epoch  39 Batch  169/269   train_loss = 4.021\n",
      "Epoch  39 Batch  170/269   train_loss = 3.632\n",
      "Epoch  39 Batch  171/269   train_loss = 3.764\n",
      "Epoch  39 Batch  172/269   train_loss = 3.575\n",
      "Epoch  39 Batch  173/269   train_loss = 3.770\n",
      "Epoch  39 Batch  174/269   train_loss = 3.908\n",
      "Epoch  39 Batch  175/269   train_loss = 3.806\n",
      "Epoch  39 Batch  176/269   train_loss = 3.732\n",
      "Epoch  39 Batch  177/269   train_loss = 3.773\n",
      "Epoch  39 Batch  178/269   train_loss = 4.038\n",
      "Epoch  39 Batch  179/269   train_loss = 3.774\n",
      "Epoch  39 Batch  180/269   train_loss = 3.579\n",
      "Epoch  39 Batch  181/269   train_loss = 3.878\n",
      "Epoch  39 Batch  182/269   train_loss = 3.707\n",
      "Epoch  39 Batch  183/269   train_loss = 3.640\n",
      "Epoch  39 Batch  184/269   train_loss = 3.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  39 Batch  185/269   train_loss = 3.790\n",
      "Epoch  39 Batch  186/269   train_loss = 3.724\n",
      "Epoch  39 Batch  187/269   train_loss = 3.930\n",
      "Epoch  39 Batch  188/269   train_loss = 3.681\n",
      "Epoch  39 Batch  189/269   train_loss = 3.654\n",
      "Epoch  39 Batch  190/269   train_loss = 4.113\n",
      "Epoch  39 Batch  191/269   train_loss = 3.823\n",
      "Epoch  39 Batch  192/269   train_loss = 3.721\n",
      "Epoch  39 Batch  193/269   train_loss = 3.712\n",
      "Epoch  39 Batch  194/269   train_loss = 3.847\n",
      "Epoch  39 Batch  195/269   train_loss = 3.695\n",
      "Epoch  39 Batch  196/269   train_loss = 3.795\n",
      "Epoch  39 Batch  197/269   train_loss = 3.938\n",
      "Epoch  39 Batch  198/269   train_loss = 3.850\n",
      "Epoch  39 Batch  199/269   train_loss = 3.823\n",
      "Epoch  39 Batch  200/269   train_loss = 3.684\n",
      "Epoch  39 Batch  201/269   train_loss = 3.727\n",
      "Epoch  39 Batch  202/269   train_loss = 3.610\n",
      "Epoch  39 Batch  203/269   train_loss = 3.596\n",
      "Epoch  39 Batch  204/269   train_loss = 3.822\n",
      "Epoch  39 Batch  205/269   train_loss = 3.775\n",
      "Epoch  39 Batch  206/269   train_loss = 3.707\n",
      "Epoch  39 Batch  207/269   train_loss = 3.716\n",
      "Epoch  39 Batch  208/269   train_loss = 3.752\n",
      "Epoch  39 Batch  209/269   train_loss = 3.900\n",
      "Epoch  39 Batch  210/269   train_loss = 3.575\n",
      "Epoch  39 Batch  211/269   train_loss = 3.759\n",
      "Epoch  39 Batch  212/269   train_loss = 4.073\n",
      "Epoch  39 Batch  213/269   train_loss = 3.642\n",
      "Epoch  39 Batch  214/269   train_loss = 3.716\n",
      "Epoch  39 Batch  215/269   train_loss = 3.965\n",
      "Epoch  39 Batch  216/269   train_loss = 3.904\n",
      "Epoch  39 Batch  217/269   train_loss = 3.618\n",
      "Epoch  39 Batch  218/269   train_loss = 3.862\n",
      "Epoch  39 Batch  219/269   train_loss = 3.463\n",
      "Epoch  39 Batch  220/269   train_loss = 3.914\n",
      "Epoch  39 Batch  221/269   train_loss = 3.595\n",
      "Epoch  39 Batch  222/269   train_loss = 3.808\n",
      "Epoch  39 Batch  223/269   train_loss = 3.554\n",
      "Epoch  39 Batch  224/269   train_loss = 3.897\n",
      "Epoch  39 Batch  225/269   train_loss = 3.997\n",
      "Epoch  39 Batch  226/269   train_loss = 3.738\n",
      "Epoch  39 Batch  227/269   train_loss = 3.422\n",
      "Epoch  39 Batch  228/269   train_loss = 3.739\n",
      "Epoch  39 Batch  229/269   train_loss = 3.895\n",
      "Epoch  39 Batch  230/269   train_loss = 3.812\n",
      "Epoch  39 Batch  231/269   train_loss = 3.703\n",
      "Epoch  39 Batch  232/269   train_loss = 3.731\n",
      "Epoch  39 Batch  233/269   train_loss = 3.856\n",
      "Epoch  39 Batch  234/269   train_loss = 3.717\n",
      "Epoch  39 Batch  235/269   train_loss = 3.991\n",
      "Epoch  39 Batch  236/269   train_loss = 3.634\n",
      "Epoch  39 Batch  237/269   train_loss = 3.374\n",
      "Epoch  39 Batch  238/269   train_loss = 3.529\n",
      "Epoch  39 Batch  239/269   train_loss = 4.089\n",
      "Epoch  39 Batch  240/269   train_loss = 3.686\n",
      "Epoch  39 Batch  241/269   train_loss = 3.940\n",
      "Epoch  39 Batch  242/269   train_loss = 3.560\n",
      "Epoch  39 Batch  243/269   train_loss = 3.923\n",
      "Epoch  39 Batch  244/269   train_loss = 3.729\n",
      "Epoch  39 Batch  245/269   train_loss = 3.744\n",
      "Epoch  39 Batch  246/269   train_loss = 3.437\n",
      "Epoch  39 Batch  247/269   train_loss = 3.799\n",
      "Epoch  39 Batch  248/269   train_loss = 3.800\n",
      "Epoch  39 Batch  249/269   train_loss = 3.627\n",
      "Epoch  39 Batch  250/269   train_loss = 3.522\n",
      "Epoch  39 Batch  251/269   train_loss = 3.947\n",
      "Epoch  39 Batch  252/269   train_loss = 3.870\n",
      "Epoch  39 Batch  253/269   train_loss = 3.672\n",
      "Epoch  39 Batch  254/269   train_loss = 3.691\n",
      "Epoch  39 Batch  255/269   train_loss = 3.712\n",
      "Epoch  39 Batch  256/269   train_loss = 3.599\n",
      "Epoch  39 Batch  257/269   train_loss = 3.535\n",
      "Epoch  39 Batch  258/269   train_loss = 3.468\n",
      "Epoch  39 Batch  259/269   train_loss = 3.619\n",
      "Epoch  39 Batch  260/269   train_loss = 3.804\n",
      "Epoch  39 Batch  261/269   train_loss = 3.855\n",
      "Epoch  39 Batch  262/269   train_loss = 3.650\n",
      "Epoch  39 Batch  263/269   train_loss = 3.512\n",
      "Epoch  39 Batch  264/269   train_loss = 4.014\n",
      "Epoch  39 Batch  265/269   train_loss = 3.823\n",
      "Epoch  39 Batch  266/269   train_loss = 3.660\n",
      "Epoch  39 Batch  267/269   train_loss = 3.788\n",
      "Epoch  39 Batch  268/269   train_loss = 3.856\n",
      "Epoch  40 Batch    0/269   train_loss = 3.666\n",
      "Epoch  40 Batch    1/269   train_loss = 3.517\n",
      "Epoch  40 Batch    2/269   train_loss = 3.601\n",
      "Epoch  40 Batch    3/269   train_loss = 3.662\n",
      "Epoch  40 Batch    4/269   train_loss = 4.114\n",
      "Epoch  40 Batch    5/269   train_loss = 3.761\n",
      "Epoch  40 Batch    6/269   train_loss = 3.619\n",
      "Epoch  40 Batch    7/269   train_loss = 3.518\n",
      "Epoch  40 Batch    8/269   train_loss = 3.876\n",
      "Epoch  40 Batch    9/269   train_loss = 3.509\n",
      "Epoch  40 Batch   10/269   train_loss = 3.559\n",
      "Epoch  40 Batch   11/269   train_loss = 3.692\n",
      "Epoch  40 Batch   12/269   train_loss = 3.378\n",
      "Epoch  40 Batch   13/269   train_loss = 3.700\n",
      "Epoch  40 Batch   14/269   train_loss = 3.585\n",
      "Epoch  40 Batch   15/269   train_loss = 3.961\n",
      "Epoch  40 Batch   16/269   train_loss = 3.695\n",
      "Epoch  40 Batch   17/269   train_loss = 3.724\n",
      "Epoch  40 Batch   18/269   train_loss = 3.669\n",
      "Epoch  40 Batch   19/269   train_loss = 3.627\n",
      "Epoch  40 Batch   20/269   train_loss = 4.045\n",
      "Epoch  40 Batch   21/269   train_loss = 3.869\n",
      "Epoch  40 Batch   22/269   train_loss = 3.611\n",
      "Epoch  40 Batch   23/269   train_loss = 3.779\n",
      "Epoch  40 Batch   24/269   train_loss = 3.622\n",
      "Epoch  40 Batch   25/269   train_loss = 3.972\n",
      "Epoch  40 Batch   26/269   train_loss = 3.792\n",
      "Epoch  40 Batch   27/269   train_loss = 3.583\n",
      "Epoch  40 Batch   28/269   train_loss = 3.773\n",
      "Epoch  40 Batch   29/269   train_loss = 3.785\n",
      "Epoch  40 Batch   30/269   train_loss = 4.073\n",
      "Epoch  40 Batch   31/269   train_loss = 3.714\n",
      "Epoch  40 Batch   32/269   train_loss = 3.639\n",
      "Epoch  40 Batch   33/269   train_loss = 3.421\n",
      "Epoch  40 Batch   34/269   train_loss = 3.662\n",
      "Epoch  40 Batch   35/269   train_loss = 3.497\n",
      "Epoch  40 Batch   36/269   train_loss = 3.567\n",
      "Epoch  40 Batch   37/269   train_loss = 3.772\n",
      "Epoch  40 Batch   38/269   train_loss = 3.517\n",
      "Epoch  40 Batch   39/269   train_loss = 3.823\n",
      "Epoch  40 Batch   40/269   train_loss = 3.584\n",
      "Epoch  40 Batch   41/269   train_loss = 3.593\n",
      "Epoch  40 Batch   42/269   train_loss = 4.038\n",
      "Epoch  40 Batch   43/269   train_loss = 3.901\n",
      "Epoch  40 Batch   44/269   train_loss = 3.729\n",
      "Epoch  40 Batch   45/269   train_loss = 3.429\n",
      "Epoch  40 Batch   46/269   train_loss = 3.810\n",
      "Epoch  40 Batch   47/269   train_loss = 3.446\n",
      "Epoch  40 Batch   48/269   train_loss = 4.002\n",
      "Epoch  40 Batch   49/269   train_loss = 3.958\n",
      "Epoch  40 Batch   50/269   train_loss = 3.848\n",
      "Epoch  40 Batch   51/269   train_loss = 3.766\n",
      "Epoch  40 Batch   52/269   train_loss = 3.787\n",
      "Epoch  40 Batch   53/269   train_loss = 3.958\n",
      "Epoch  40 Batch   54/269   train_loss = 3.761\n",
      "Epoch  40 Batch   55/269   train_loss = 3.821\n",
      "Epoch  40 Batch   56/269   train_loss = 3.713\n",
      "Epoch  40 Batch   57/269   train_loss = 3.896\n",
      "Epoch  40 Batch   58/269   train_loss = 3.619\n",
      "Epoch  40 Batch   59/269   train_loss = 3.618\n",
      "Epoch  40 Batch   60/269   train_loss = 3.803\n",
      "Epoch  40 Batch   61/269   train_loss = 4.041\n",
      "Epoch  40 Batch   62/269   train_loss = 3.625\n",
      "Epoch  40 Batch   63/269   train_loss = 3.925\n",
      "Epoch  40 Batch   64/269   train_loss = 3.688\n",
      "Epoch  40 Batch   65/269   train_loss = 3.825\n",
      "Epoch  40 Batch   66/269   train_loss = 3.663\n",
      "Epoch  40 Batch   67/269   train_loss = 3.732\n",
      "Epoch  40 Batch   68/269   train_loss = 3.808\n",
      "Epoch  40 Batch   69/269   train_loss = 3.771\n",
      "Epoch  40 Batch   70/269   train_loss = 3.411\n",
      "Epoch  40 Batch   71/269   train_loss = 3.845\n",
      "Epoch  40 Batch   72/269   train_loss = 3.496\n",
      "Epoch  40 Batch   73/269   train_loss = 3.698\n",
      "Epoch  40 Batch   74/269   train_loss = 3.715\n",
      "Epoch  40 Batch   75/269   train_loss = 3.881\n",
      "Epoch  40 Batch   76/269   train_loss = 3.846\n",
      "Epoch  40 Batch   77/269   train_loss = 3.463\n",
      "Epoch  40 Batch   78/269   train_loss = 3.893\n",
      "Epoch  40 Batch   79/269   train_loss = 3.908\n",
      "Epoch  40 Batch   80/269   train_loss = 3.846\n",
      "Epoch  40 Batch   81/269   train_loss = 3.702\n",
      "Epoch  40 Batch   82/269   train_loss = 3.914\n",
      "Epoch  40 Batch   83/269   train_loss = 3.639\n",
      "Epoch  40 Batch   84/269   train_loss = 3.912\n",
      "Epoch  40 Batch   85/269   train_loss = 4.143\n",
      "Epoch  40 Batch   86/269   train_loss = 4.014\n",
      "Epoch  40 Batch   87/269   train_loss = 3.720\n",
      "Epoch  40 Batch   88/269   train_loss = 3.817\n",
      "Epoch  40 Batch   89/269   train_loss = 3.745\n",
      "Epoch  40 Batch   90/269   train_loss = 3.648\n",
      "Epoch  40 Batch   91/269   train_loss = 3.890\n",
      "Epoch  40 Batch   92/269   train_loss = 3.906\n",
      "Epoch  40 Batch   93/269   train_loss = 3.605\n",
      "Epoch  40 Batch   94/269   train_loss = 4.045\n",
      "Epoch  40 Batch   95/269   train_loss = 3.537\n",
      "Epoch  40 Batch   96/269   train_loss = 3.607\n",
      "Epoch  40 Batch   97/269   train_loss = 3.786\n",
      "Epoch  40 Batch   98/269   train_loss = 3.897\n",
      "Epoch  40 Batch   99/269   train_loss = 3.716\n",
      "Epoch  40 Batch  100/269   train_loss = 3.722\n",
      "Epoch  40 Batch  101/269   train_loss = 3.919\n",
      "Epoch  40 Batch  102/269   train_loss = 3.570\n",
      "Epoch  40 Batch  103/269   train_loss = 3.680\n",
      "Epoch  40 Batch  104/269   train_loss = 3.579\n",
      "Epoch  40 Batch  105/269   train_loss = 3.878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40 Batch  106/269   train_loss = 3.944\n",
      "Epoch  40 Batch  107/269   train_loss = 3.982\n",
      "Epoch  40 Batch  108/269   train_loss = 3.786\n",
      "Epoch  40 Batch  109/269   train_loss = 3.804\n",
      "Epoch  40 Batch  110/269   train_loss = 3.768\n",
      "Epoch  40 Batch  111/269   train_loss = 3.875\n",
      "Epoch  40 Batch  112/269   train_loss = 3.928\n",
      "Epoch  40 Batch  113/269   train_loss = 3.765\n",
      "Epoch  40 Batch  114/269   train_loss = 3.739\n",
      "Epoch  40 Batch  115/269   train_loss = 3.576\n",
      "Epoch  40 Batch  116/269   train_loss = 3.546\n",
      "Epoch  40 Batch  117/269   train_loss = 3.608\n",
      "Epoch  40 Batch  118/269   train_loss = 3.963\n",
      "Epoch  40 Batch  119/269   train_loss = 3.518\n",
      "Epoch  40 Batch  120/269   train_loss = 3.863\n",
      "Epoch  40 Batch  121/269   train_loss = 3.649\n",
      "Epoch  40 Batch  122/269   train_loss = 3.820\n",
      "Epoch  40 Batch  123/269   train_loss = 3.566\n",
      "Epoch  40 Batch  124/269   train_loss = 3.937\n",
      "Epoch  40 Batch  125/269   train_loss = 3.521\n",
      "Epoch  40 Batch  126/269   train_loss = 3.428\n",
      "Epoch  40 Batch  127/269   train_loss = 3.704\n",
      "Epoch  40 Batch  128/269   train_loss = 3.694\n",
      "Epoch  40 Batch  129/269   train_loss = 3.666\n",
      "Epoch  40 Batch  130/269   train_loss = 3.861\n",
      "Epoch  40 Batch  131/269   train_loss = 3.394\n",
      "Epoch  40 Batch  132/269   train_loss = 3.707\n",
      "Epoch  40 Batch  133/269   train_loss = 3.502\n",
      "Epoch  40 Batch  134/269   train_loss = 3.921\n",
      "Epoch  40 Batch  135/269   train_loss = 3.561\n",
      "Epoch  40 Batch  136/269   train_loss = 3.503\n",
      "Epoch  40 Batch  137/269   train_loss = 3.816\n",
      "Epoch  40 Batch  138/269   train_loss = 3.597\n",
      "Epoch  40 Batch  139/269   train_loss = 3.737\n",
      "Epoch  40 Batch  140/269   train_loss = 3.783\n",
      "Epoch  40 Batch  141/269   train_loss = 4.052\n",
      "Epoch  40 Batch  142/269   train_loss = 3.543\n",
      "Epoch  40 Batch  143/269   train_loss = 3.776\n",
      "Epoch  40 Batch  144/269   train_loss = 3.920\n",
      "Epoch  40 Batch  145/269   train_loss = 3.633\n",
      "Epoch  40 Batch  146/269   train_loss = 3.923\n",
      "Epoch  40 Batch  147/269   train_loss = 3.542\n",
      "Epoch  40 Batch  148/269   train_loss = 3.840\n",
      "Epoch  40 Batch  149/269   train_loss = 3.666\n",
      "Epoch  40 Batch  150/269   train_loss = 3.838\n",
      "Epoch  40 Batch  151/269   train_loss = 3.785\n",
      "Epoch  40 Batch  152/269   train_loss = 3.726\n",
      "Epoch  40 Batch  153/269   train_loss = 3.757\n",
      "Epoch  40 Batch  154/269   train_loss = 3.676\n",
      "Epoch  40 Batch  155/269   train_loss = 3.701\n",
      "Epoch  40 Batch  156/269   train_loss = 4.155\n",
      "Epoch  40 Batch  157/269   train_loss = 3.880\n",
      "Epoch  40 Batch  158/269   train_loss = 3.886\n",
      "Epoch  40 Batch  159/269   train_loss = 3.832\n",
      "Epoch  40 Batch  160/269   train_loss = 3.706\n",
      "Epoch  40 Batch  161/269   train_loss = 3.857\n",
      "Epoch  40 Batch  162/269   train_loss = 3.831\n",
      "Epoch  40 Batch  163/269   train_loss = 3.746\n",
      "Epoch  40 Batch  164/269   train_loss = 3.909\n",
      "Epoch  40 Batch  165/269   train_loss = 3.883\n",
      "Epoch  40 Batch  166/269   train_loss = 3.787\n",
      "Epoch  40 Batch  167/269   train_loss = 3.659\n",
      "Epoch  40 Batch  168/269   train_loss = 3.703\n",
      "Epoch  40 Batch  169/269   train_loss = 4.006\n",
      "Epoch  40 Batch  170/269   train_loss = 3.577\n",
      "Epoch  40 Batch  171/269   train_loss = 3.755\n",
      "Epoch  40 Batch  172/269   train_loss = 3.555\n",
      "Epoch  40 Batch  173/269   train_loss = 3.764\n",
      "Epoch  40 Batch  174/269   train_loss = 3.890\n",
      "Epoch  40 Batch  175/269   train_loss = 3.832\n",
      "Epoch  40 Batch  176/269   train_loss = 3.804\n",
      "Epoch  40 Batch  177/269   train_loss = 3.756\n",
      "Epoch  40 Batch  178/269   train_loss = 4.024\n",
      "Epoch  40 Batch  179/269   train_loss = 3.744\n",
      "Epoch  40 Batch  180/269   train_loss = 3.631\n",
      "Epoch  40 Batch  181/269   train_loss = 3.874\n",
      "Epoch  40 Batch  182/269   train_loss = 3.684\n",
      "Epoch  40 Batch  183/269   train_loss = 3.646\n",
      "Epoch  40 Batch  184/269   train_loss = 3.745\n",
      "Epoch  40 Batch  185/269   train_loss = 3.781\n",
      "Epoch  40 Batch  186/269   train_loss = 3.727\n",
      "Epoch  40 Batch  187/269   train_loss = 3.968\n",
      "Epoch  40 Batch  188/269   train_loss = 3.677\n",
      "Epoch  40 Batch  189/269   train_loss = 3.689\n",
      "Epoch  40 Batch  190/269   train_loss = 4.112\n",
      "Epoch  40 Batch  191/269   train_loss = 3.811\n",
      "Epoch  40 Batch  192/269   train_loss = 3.713\n",
      "Epoch  40 Batch  193/269   train_loss = 3.737\n",
      "Epoch  40 Batch  194/269   train_loss = 3.807\n",
      "Epoch  40 Batch  195/269   train_loss = 3.696\n",
      "Epoch  40 Batch  196/269   train_loss = 3.780\n",
      "Epoch  40 Batch  197/269   train_loss = 3.968\n",
      "Epoch  40 Batch  198/269   train_loss = 3.856\n",
      "Epoch  40 Batch  199/269   train_loss = 3.877\n",
      "Epoch  40 Batch  200/269   train_loss = 3.682\n",
      "Epoch  40 Batch  201/269   train_loss = 3.700\n",
      "Epoch  40 Batch  202/269   train_loss = 3.598\n",
      "Epoch  40 Batch  203/269   train_loss = 3.592\n",
      "Epoch  40 Batch  204/269   train_loss = 3.831\n",
      "Epoch  40 Batch  205/269   train_loss = 3.790\n",
      "Epoch  40 Batch  206/269   train_loss = 3.709\n",
      "Epoch  40 Batch  207/269   train_loss = 3.695\n",
      "Epoch  40 Batch  208/269   train_loss = 3.771\n",
      "Epoch  40 Batch  209/269   train_loss = 3.910\n",
      "Epoch  40 Batch  210/269   train_loss = 3.601\n",
      "Epoch  40 Batch  211/269   train_loss = 3.753\n",
      "Epoch  40 Batch  212/269   train_loss = 4.022\n",
      "Epoch  40 Batch  213/269   train_loss = 3.648\n",
      "Epoch  40 Batch  214/269   train_loss = 3.718\n",
      "Epoch  40 Batch  215/269   train_loss = 3.982\n",
      "Epoch  40 Batch  216/269   train_loss = 3.926\n",
      "Epoch  40 Batch  217/269   train_loss = 3.621\n",
      "Epoch  40 Batch  218/269   train_loss = 3.872\n",
      "Epoch  40 Batch  219/269   train_loss = 3.500\n",
      "Epoch  40 Batch  220/269   train_loss = 3.915\n",
      "Epoch  40 Batch  221/269   train_loss = 3.617\n",
      "Epoch  40 Batch  222/269   train_loss = 3.786\n",
      "Epoch  40 Batch  223/269   train_loss = 3.613\n",
      "Epoch  40 Batch  224/269   train_loss = 3.868\n",
      "Epoch  40 Batch  225/269   train_loss = 3.994\n",
      "Epoch  40 Batch  226/269   train_loss = 3.750\n",
      "Epoch  40 Batch  227/269   train_loss = 3.460\n",
      "Epoch  40 Batch  228/269   train_loss = 3.730\n",
      "Epoch  40 Batch  229/269   train_loss = 3.904\n",
      "Epoch  40 Batch  230/269   train_loss = 3.819\n",
      "Epoch  40 Batch  231/269   train_loss = 3.726\n",
      "Epoch  40 Batch  232/269   train_loss = 3.746\n",
      "Epoch  40 Batch  233/269   train_loss = 3.828\n",
      "Epoch  40 Batch  234/269   train_loss = 3.728\n",
      "Epoch  40 Batch  235/269   train_loss = 4.002\n",
      "Epoch  40 Batch  236/269   train_loss = 3.623\n",
      "Epoch  40 Batch  237/269   train_loss = 3.356\n",
      "Epoch  40 Batch  238/269   train_loss = 3.525\n",
      "Epoch  40 Batch  239/269   train_loss = 4.122\n",
      "Epoch  40 Batch  240/269   train_loss = 3.689\n",
      "Epoch  40 Batch  241/269   train_loss = 3.984\n",
      "Epoch  40 Batch  242/269   train_loss = 3.533\n",
      "Epoch  40 Batch  243/269   train_loss = 3.894\n",
      "Epoch  40 Batch  244/269   train_loss = 3.737\n",
      "Epoch  40 Batch  245/269   train_loss = 3.811\n",
      "Epoch  40 Batch  246/269   train_loss = 3.414\n",
      "Epoch  40 Batch  247/269   train_loss = 3.803\n",
      "Epoch  40 Batch  248/269   train_loss = 3.836\n",
      "Epoch  40 Batch  249/269   train_loss = 3.598\n",
      "Epoch  40 Batch  250/269   train_loss = 3.519\n",
      "Epoch  40 Batch  251/269   train_loss = 3.926\n",
      "Epoch  40 Batch  252/269   train_loss = 3.847\n",
      "Epoch  40 Batch  253/269   train_loss = 3.667\n",
      "Epoch  40 Batch  254/269   train_loss = 3.707\n",
      "Epoch  40 Batch  255/269   train_loss = 3.722\n",
      "Epoch  40 Batch  256/269   train_loss = 3.586\n",
      "Epoch  40 Batch  257/269   train_loss = 3.560\n",
      "Epoch  40 Batch  258/269   train_loss = 3.490\n",
      "Epoch  40 Batch  259/269   train_loss = 3.626\n",
      "Epoch  40 Batch  260/269   train_loss = 3.816\n",
      "Epoch  40 Batch  261/269   train_loss = 3.916\n",
      "Epoch  40 Batch  262/269   train_loss = 3.670\n",
      "Epoch  40 Batch  263/269   train_loss = 3.472\n",
      "Epoch  40 Batch  264/269   train_loss = 3.978\n",
      "Epoch  40 Batch  265/269   train_loss = 3.817\n",
      "Epoch  40 Batch  266/269   train_loss = 3.668\n",
      "Epoch  40 Batch  267/269   train_loss = 3.787\n",
      "Epoch  40 Batch  268/269   train_loss = 3.902\n",
      "Epoch  41 Batch    0/269   train_loss = 3.737\n",
      "Epoch  41 Batch    1/269   train_loss = 3.496\n",
      "Epoch  41 Batch    2/269   train_loss = 3.617\n",
      "Epoch  41 Batch    3/269   train_loss = 3.704\n",
      "Epoch  41 Batch    4/269   train_loss = 4.051\n",
      "Epoch  41 Batch    5/269   train_loss = 3.738\n",
      "Epoch  41 Batch    6/269   train_loss = 3.627\n",
      "Epoch  41 Batch    7/269   train_loss = 3.510\n",
      "Epoch  41 Batch    8/269   train_loss = 3.852\n",
      "Epoch  41 Batch    9/269   train_loss = 3.500\n",
      "Epoch  41 Batch   10/269   train_loss = 3.560\n",
      "Epoch  41 Batch   11/269   train_loss = 3.691\n",
      "Epoch  41 Batch   12/269   train_loss = 3.399\n",
      "Epoch  41 Batch   13/269   train_loss = 3.701\n",
      "Epoch  41 Batch   14/269   train_loss = 3.554\n",
      "Epoch  41 Batch   15/269   train_loss = 3.976\n",
      "Epoch  41 Batch   16/269   train_loss = 3.698\n",
      "Epoch  41 Batch   17/269   train_loss = 3.713\n",
      "Epoch  41 Batch   18/269   train_loss = 3.675\n",
      "Epoch  41 Batch   19/269   train_loss = 3.647\n",
      "Epoch  41 Batch   20/269   train_loss = 4.012\n",
      "Epoch  41 Batch   21/269   train_loss = 3.851\n",
      "Epoch  41 Batch   22/269   train_loss = 3.582\n",
      "Epoch  41 Batch   23/269   train_loss = 3.780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  41 Batch   24/269   train_loss = 3.623\n",
      "Epoch  41 Batch   25/269   train_loss = 3.950\n",
      "Epoch  41 Batch   26/269   train_loss = 3.787\n",
      "Epoch  41 Batch   27/269   train_loss = 3.589\n",
      "Epoch  41 Batch   28/269   train_loss = 3.759\n",
      "Epoch  41 Batch   29/269   train_loss = 3.735\n",
      "Epoch  41 Batch   30/269   train_loss = 4.050\n",
      "Epoch  41 Batch   31/269   train_loss = 3.727\n",
      "Epoch  41 Batch   32/269   train_loss = 3.633\n",
      "Epoch  41 Batch   33/269   train_loss = 3.424\n",
      "Epoch  41 Batch   34/269   train_loss = 3.714\n",
      "Epoch  41 Batch   35/269   train_loss = 3.523\n",
      "Epoch  41 Batch   36/269   train_loss = 3.565\n",
      "Epoch  41 Batch   37/269   train_loss = 3.739\n",
      "Epoch  41 Batch   38/269   train_loss = 3.545\n",
      "Epoch  41 Batch   39/269   train_loss = 3.818\n",
      "Epoch  41 Batch   40/269   train_loss = 3.557\n",
      "Epoch  41 Batch   41/269   train_loss = 3.571\n",
      "Epoch  41 Batch   42/269   train_loss = 4.038\n",
      "Epoch  41 Batch   43/269   train_loss = 3.890\n",
      "Epoch  41 Batch   44/269   train_loss = 3.722\n",
      "Epoch  41 Batch   45/269   train_loss = 3.427\n",
      "Epoch  41 Batch   46/269   train_loss = 3.817\n",
      "Epoch  41 Batch   47/269   train_loss = 3.445\n",
      "Epoch  41 Batch   48/269   train_loss = 3.959\n",
      "Epoch  41 Batch   49/269   train_loss = 3.937\n",
      "Epoch  41 Batch   50/269   train_loss = 3.837\n",
      "Epoch  41 Batch   51/269   train_loss = 3.742\n",
      "Epoch  41 Batch   52/269   train_loss = 3.750\n",
      "Epoch  41 Batch   53/269   train_loss = 3.956\n",
      "Epoch  41 Batch   54/269   train_loss = 3.748\n",
      "Epoch  41 Batch   55/269   train_loss = 3.807\n",
      "Epoch  41 Batch   56/269   train_loss = 3.697\n",
      "Epoch  41 Batch   57/269   train_loss = 3.913\n",
      "Epoch  41 Batch   58/269   train_loss = 3.614\n",
      "Epoch  41 Batch   59/269   train_loss = 3.663\n",
      "Epoch  41 Batch   60/269   train_loss = 3.785\n",
      "Epoch  41 Batch   61/269   train_loss = 4.033\n",
      "Epoch  41 Batch   62/269   train_loss = 3.626\n",
      "Epoch  41 Batch   63/269   train_loss = 3.940\n",
      "Epoch  41 Batch   64/269   train_loss = 3.689\n",
      "Epoch  41 Batch   65/269   train_loss = 3.804\n",
      "Epoch  41 Batch   66/269   train_loss = 3.661\n",
      "Epoch  41 Batch   67/269   train_loss = 3.723\n",
      "Epoch  41 Batch   68/269   train_loss = 3.781\n",
      "Epoch  41 Batch   69/269   train_loss = 3.802\n",
      "Epoch  41 Batch   70/269   train_loss = 3.395\n",
      "Epoch  41 Batch   71/269   train_loss = 3.844\n",
      "Epoch  41 Batch   72/269   train_loss = 3.492\n",
      "Epoch  41 Batch   73/269   train_loss = 3.706\n",
      "Epoch  41 Batch   74/269   train_loss = 3.722\n",
      "Epoch  41 Batch   75/269   train_loss = 3.845\n",
      "Epoch  41 Batch   76/269   train_loss = 3.817\n",
      "Epoch  41 Batch   77/269   train_loss = 3.442\n",
      "Epoch  41 Batch   78/269   train_loss = 3.866\n",
      "Epoch  41 Batch   79/269   train_loss = 3.908\n",
      "Epoch  41 Batch   80/269   train_loss = 3.865\n",
      "Epoch  41 Batch   81/269   train_loss = 3.704\n",
      "Epoch  41 Batch   82/269   train_loss = 3.930\n",
      "Epoch  41 Batch   83/269   train_loss = 3.642\n",
      "Epoch  41 Batch   84/269   train_loss = 3.908\n",
      "Epoch  41 Batch   85/269   train_loss = 4.118\n",
      "Epoch  41 Batch   86/269   train_loss = 4.036\n",
      "Epoch  41 Batch   87/269   train_loss = 3.686\n",
      "Epoch  41 Batch   88/269   train_loss = 3.828\n",
      "Epoch  41 Batch   89/269   train_loss = 3.744\n",
      "Epoch  41 Batch   90/269   train_loss = 3.605\n",
      "Epoch  41 Batch   91/269   train_loss = 3.890\n",
      "Epoch  41 Batch   92/269   train_loss = 3.899\n",
      "Epoch  41 Batch   93/269   train_loss = 3.616\n",
      "Epoch  41 Batch   94/269   train_loss = 4.014\n",
      "Epoch  41 Batch   95/269   train_loss = 3.527\n",
      "Epoch  41 Batch   96/269   train_loss = 3.620\n",
      "Epoch  41 Batch   97/269   train_loss = 3.784\n",
      "Epoch  41 Batch   98/269   train_loss = 3.874\n",
      "Epoch  41 Batch   99/269   train_loss = 3.725\n",
      "Epoch  41 Batch  100/269   train_loss = 3.723\n",
      "Epoch  41 Batch  101/269   train_loss = 3.902\n",
      "Epoch  41 Batch  102/269   train_loss = 3.553\n",
      "Epoch  41 Batch  103/269   train_loss = 3.689\n",
      "Epoch  41 Batch  104/269   train_loss = 3.603\n",
      "Epoch  41 Batch  105/269   train_loss = 3.865\n",
      "Epoch  41 Batch  106/269   train_loss = 3.956\n",
      "Epoch  41 Batch  107/269   train_loss = 3.983\n",
      "Epoch  41 Batch  108/269   train_loss = 3.756\n",
      "Epoch  41 Batch  109/269   train_loss = 3.808\n",
      "Epoch  41 Batch  110/269   train_loss = 3.771\n",
      "Epoch  41 Batch  111/269   train_loss = 3.873\n",
      "Epoch  41 Batch  112/269   train_loss = 3.902\n",
      "Epoch  41 Batch  113/269   train_loss = 3.752\n",
      "Epoch  41 Batch  114/269   train_loss = 3.760\n",
      "Epoch  41 Batch  115/269   train_loss = 3.566\n",
      "Epoch  41 Batch  116/269   train_loss = 3.551\n",
      "Epoch  41 Batch  117/269   train_loss = 3.619\n",
      "Epoch  41 Batch  118/269   train_loss = 3.988\n",
      "Epoch  41 Batch  119/269   train_loss = 3.505\n",
      "Epoch  41 Batch  120/269   train_loss = 3.850\n",
      "Epoch  41 Batch  121/269   train_loss = 3.652\n",
      "Epoch  41 Batch  122/269   train_loss = 3.850\n",
      "Epoch  41 Batch  123/269   train_loss = 3.546\n",
      "Epoch  41 Batch  124/269   train_loss = 3.912\n",
      "Epoch  41 Batch  125/269   train_loss = 3.507\n",
      "Epoch  41 Batch  126/269   train_loss = 3.422\n",
      "Epoch  41 Batch  127/269   train_loss = 3.703\n",
      "Epoch  41 Batch  128/269   train_loss = 3.686\n",
      "Epoch  41 Batch  129/269   train_loss = 3.677\n",
      "Epoch  41 Batch  130/269   train_loss = 3.852\n",
      "Epoch  41 Batch  131/269   train_loss = 3.404\n",
      "Epoch  41 Batch  132/269   train_loss = 3.725\n",
      "Epoch  41 Batch  133/269   train_loss = 3.508\n",
      "Epoch  41 Batch  134/269   train_loss = 3.893\n",
      "Epoch  41 Batch  135/269   train_loss = 3.575\n",
      "Epoch  41 Batch  136/269   train_loss = 3.498\n",
      "Epoch  41 Batch  137/269   train_loss = 3.808\n",
      "Epoch  41 Batch  138/269   train_loss = 3.619\n",
      "Epoch  41 Batch  139/269   train_loss = 3.756\n",
      "Epoch  41 Batch  140/269   train_loss = 3.799\n",
      "Epoch  41 Batch  141/269   train_loss = 4.056\n",
      "Epoch  41 Batch  142/269   train_loss = 3.531\n",
      "Epoch  41 Batch  143/269   train_loss = 3.758\n",
      "Epoch  41 Batch  144/269   train_loss = 3.935\n",
      "Epoch  41 Batch  145/269   train_loss = 3.648\n",
      "Epoch  41 Batch  146/269   train_loss = 3.903\n",
      "Epoch  41 Batch  147/269   train_loss = 3.530\n",
      "Epoch  41 Batch  148/269   train_loss = 3.848\n",
      "Epoch  41 Batch  149/269   train_loss = 3.689\n",
      "Epoch  41 Batch  150/269   train_loss = 3.830\n",
      "Epoch  41 Batch  151/269   train_loss = 3.785\n",
      "Epoch  41 Batch  152/269   train_loss = 3.751\n",
      "Epoch  41 Batch  153/269   train_loss = 3.781\n",
      "Epoch  41 Batch  154/269   train_loss = 3.661\n",
      "Epoch  41 Batch  155/269   train_loss = 3.693\n",
      "Epoch  41 Batch  156/269   train_loss = 4.158\n",
      "Epoch  41 Batch  157/269   train_loss = 3.852\n",
      "Epoch  41 Batch  158/269   train_loss = 3.889\n",
      "Epoch  41 Batch  159/269   train_loss = 3.802\n",
      "Epoch  41 Batch  160/269   train_loss = 3.685\n",
      "Epoch  41 Batch  161/269   train_loss = 3.861\n",
      "Epoch  41 Batch  162/269   train_loss = 3.842\n",
      "Epoch  41 Batch  163/269   train_loss = 3.730\n",
      "Epoch  41 Batch  164/269   train_loss = 3.922\n",
      "Epoch  41 Batch  165/269   train_loss = 3.890\n",
      "Epoch  41 Batch  166/269   train_loss = 3.774\n",
      "Epoch  41 Batch  167/269   train_loss = 3.665\n",
      "Epoch  41 Batch  168/269   train_loss = 3.714\n",
      "Epoch  41 Batch  169/269   train_loss = 4.049\n",
      "Epoch  41 Batch  170/269   train_loss = 3.612\n",
      "Epoch  41 Batch  171/269   train_loss = 3.728\n",
      "Epoch  41 Batch  172/269   train_loss = 3.545\n",
      "Epoch  41 Batch  173/269   train_loss = 3.779\n",
      "Epoch  41 Batch  174/269   train_loss = 3.910\n",
      "Epoch  41 Batch  175/269   train_loss = 3.822\n",
      "Epoch  41 Batch  176/269   train_loss = 3.801\n",
      "Epoch  41 Batch  177/269   train_loss = 3.758\n",
      "Epoch  41 Batch  178/269   train_loss = 4.006\n",
      "Epoch  41 Batch  179/269   train_loss = 3.768\n",
      "Epoch  41 Batch  180/269   train_loss = 3.633\n",
      "Epoch  41 Batch  181/269   train_loss = 3.840\n",
      "Epoch  41 Batch  182/269   train_loss = 3.658\n",
      "Epoch  41 Batch  183/269   train_loss = 3.620\n",
      "Epoch  41 Batch  184/269   train_loss = 3.780\n",
      "Epoch  41 Batch  185/269   train_loss = 3.789\n",
      "Epoch  41 Batch  186/269   train_loss = 3.709\n",
      "Epoch  41 Batch  187/269   train_loss = 3.947\n",
      "Epoch  41 Batch  188/269   train_loss = 3.672\n",
      "Epoch  41 Batch  189/269   train_loss = 3.666\n",
      "Epoch  41 Batch  190/269   train_loss = 4.107\n",
      "Epoch  41 Batch  191/269   train_loss = 3.810\n",
      "Epoch  41 Batch  192/269   train_loss = 3.681\n",
      "Epoch  41 Batch  193/269   train_loss = 3.702\n",
      "Epoch  41 Batch  194/269   train_loss = 3.806\n",
      "Epoch  41 Batch  195/269   train_loss = 3.692\n",
      "Epoch  41 Batch  196/269   train_loss = 3.790\n",
      "Epoch  41 Batch  197/269   train_loss = 3.918\n",
      "Epoch  41 Batch  198/269   train_loss = 3.825\n",
      "Epoch  41 Batch  199/269   train_loss = 3.845\n",
      "Epoch  41 Batch  200/269   train_loss = 3.695\n",
      "Epoch  41 Batch  201/269   train_loss = 3.700\n",
      "Epoch  41 Batch  202/269   train_loss = 3.599\n",
      "Epoch  41 Batch  203/269   train_loss = 3.615\n",
      "Epoch  41 Batch  204/269   train_loss = 3.835\n",
      "Epoch  41 Batch  205/269   train_loss = 3.773\n",
      "Epoch  41 Batch  206/269   train_loss = 3.657\n",
      "Epoch  41 Batch  207/269   train_loss = 3.672\n",
      "Epoch  41 Batch  208/269   train_loss = 3.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  41 Batch  209/269   train_loss = 3.903\n",
      "Epoch  41 Batch  210/269   train_loss = 3.576\n",
      "Epoch  41 Batch  211/269   train_loss = 3.751\n",
      "Epoch  41 Batch  212/269   train_loss = 4.027\n",
      "Epoch  41 Batch  213/269   train_loss = 3.627\n",
      "Epoch  41 Batch  214/269   train_loss = 3.698\n",
      "Epoch  41 Batch  215/269   train_loss = 3.964\n",
      "Epoch  41 Batch  216/269   train_loss = 3.919\n",
      "Epoch  41 Batch  217/269   train_loss = 3.623\n",
      "Epoch  41 Batch  218/269   train_loss = 3.874\n",
      "Epoch  41 Batch  219/269   train_loss = 3.492\n",
      "Epoch  41 Batch  220/269   train_loss = 3.877\n",
      "Epoch  41 Batch  221/269   train_loss = 3.597\n",
      "Epoch  41 Batch  222/269   train_loss = 3.797\n",
      "Epoch  41 Batch  223/269   train_loss = 3.606\n",
      "Epoch  41 Batch  224/269   train_loss = 3.856\n",
      "Epoch  41 Batch  225/269   train_loss = 3.975\n",
      "Epoch  41 Batch  226/269   train_loss = 3.757\n",
      "Epoch  41 Batch  227/269   train_loss = 3.486\n",
      "Epoch  41 Batch  228/269   train_loss = 3.700\n",
      "Epoch  41 Batch  229/269   train_loss = 3.926\n",
      "Epoch  41 Batch  230/269   train_loss = 3.804\n",
      "Epoch  41 Batch  231/269   train_loss = 3.714\n",
      "Epoch  41 Batch  232/269   train_loss = 3.784\n",
      "Epoch  41 Batch  233/269   train_loss = 3.833\n",
      "Epoch  41 Batch  234/269   train_loss = 3.731\n",
      "Epoch  41 Batch  235/269   train_loss = 4.017\n",
      "Epoch  41 Batch  236/269   train_loss = 3.630\n",
      "Epoch  41 Batch  237/269   train_loss = 3.340\n",
      "Epoch  41 Batch  238/269   train_loss = 3.498\n",
      "Epoch  41 Batch  239/269   train_loss = 4.119\n",
      "Epoch  41 Batch  240/269   train_loss = 3.696\n",
      "Epoch  41 Batch  241/269   train_loss = 3.947\n",
      "Epoch  41 Batch  242/269   train_loss = 3.526\n",
      "Epoch  41 Batch  243/269   train_loss = 3.912\n",
      "Epoch  41 Batch  244/269   train_loss = 3.737\n",
      "Epoch  41 Batch  245/269   train_loss = 3.753\n",
      "Epoch  41 Batch  246/269   train_loss = 3.400\n",
      "Epoch  41 Batch  247/269   train_loss = 3.799\n",
      "Epoch  41 Batch  248/269   train_loss = 3.792\n",
      "Epoch  41 Batch  249/269   train_loss = 3.586\n",
      "Epoch  41 Batch  250/269   train_loss = 3.511\n",
      "Epoch  41 Batch  251/269   train_loss = 3.913\n",
      "Epoch  41 Batch  252/269   train_loss = 3.851\n",
      "Epoch  41 Batch  253/269   train_loss = 3.675\n",
      "Epoch  41 Batch  254/269   train_loss = 3.680\n",
      "Epoch  41 Batch  255/269   train_loss = 3.748\n",
      "Epoch  41 Batch  256/269   train_loss = 3.604\n",
      "Epoch  41 Batch  257/269   train_loss = 3.544\n",
      "Epoch  41 Batch  258/269   train_loss = 3.483\n",
      "Epoch  41 Batch  259/269   train_loss = 3.628\n",
      "Epoch  41 Batch  260/269   train_loss = 3.815\n",
      "Epoch  41 Batch  261/269   train_loss = 3.882\n",
      "Epoch  41 Batch  262/269   train_loss = 3.676\n",
      "Epoch  41 Batch  263/269   train_loss = 3.481\n",
      "Epoch  41 Batch  264/269   train_loss = 3.976\n",
      "Epoch  41 Batch  265/269   train_loss = 3.871\n",
      "Epoch  41 Batch  266/269   train_loss = 3.631\n",
      "Epoch  41 Batch  267/269   train_loss = 3.770\n",
      "Epoch  41 Batch  268/269   train_loss = 3.858\n",
      "Epoch  42 Batch    0/269   train_loss = 3.716\n",
      "Epoch  42 Batch    1/269   train_loss = 3.495\n",
      "Epoch  42 Batch    2/269   train_loss = 3.639\n",
      "Epoch  42 Batch    3/269   train_loss = 3.669\n",
      "Epoch  42 Batch    4/269   train_loss = 4.062\n",
      "Epoch  42 Batch    5/269   train_loss = 3.751\n",
      "Epoch  42 Batch    6/269   train_loss = 3.639\n",
      "Epoch  42 Batch    7/269   train_loss = 3.500\n",
      "Epoch  42 Batch    8/269   train_loss = 3.865\n",
      "Epoch  42 Batch    9/269   train_loss = 3.494\n",
      "Epoch  42 Batch   10/269   train_loss = 3.559\n",
      "Epoch  42 Batch   11/269   train_loss = 3.673\n",
      "Epoch  42 Batch   12/269   train_loss = 3.384\n",
      "Epoch  42 Batch   13/269   train_loss = 3.688\n",
      "Epoch  42 Batch   14/269   train_loss = 3.570\n",
      "Epoch  42 Batch   15/269   train_loss = 3.950\n",
      "Epoch  42 Batch   16/269   train_loss = 3.669\n",
      "Epoch  42 Batch   17/269   train_loss = 3.733\n",
      "Epoch  42 Batch   18/269   train_loss = 3.651\n",
      "Epoch  42 Batch   19/269   train_loss = 3.625\n",
      "Epoch  42 Batch   20/269   train_loss = 4.026\n",
      "Epoch  42 Batch   21/269   train_loss = 3.851\n",
      "Epoch  42 Batch   22/269   train_loss = 3.595\n",
      "Epoch  42 Batch   23/269   train_loss = 3.764\n",
      "Epoch  42 Batch   24/269   train_loss = 3.626\n",
      "Epoch  42 Batch   25/269   train_loss = 3.954\n",
      "Epoch  42 Batch   26/269   train_loss = 3.791\n",
      "Epoch  42 Batch   27/269   train_loss = 3.588\n",
      "Epoch  42 Batch   28/269   train_loss = 3.758\n",
      "Epoch  42 Batch   29/269   train_loss = 3.720\n",
      "Epoch  42 Batch   30/269   train_loss = 4.020\n",
      "Epoch  42 Batch   31/269   train_loss = 3.729\n",
      "Epoch  42 Batch   32/269   train_loss = 3.625\n",
      "Epoch  42 Batch   33/269   train_loss = 3.412\n",
      "Epoch  42 Batch   34/269   train_loss = 3.691\n",
      "Epoch  42 Batch   35/269   train_loss = 3.530\n",
      "Epoch  42 Batch   36/269   train_loss = 3.569\n",
      "Epoch  42 Batch   37/269   train_loss = 3.752\n",
      "Epoch  42 Batch   38/269   train_loss = 3.523\n",
      "Epoch  42 Batch   39/269   train_loss = 3.866\n",
      "Epoch  42 Batch   40/269   train_loss = 3.554\n",
      "Epoch  42 Batch   41/269   train_loss = 3.560\n",
      "Epoch  42 Batch   42/269   train_loss = 4.045\n",
      "Epoch  42 Batch   43/269   train_loss = 3.891\n",
      "Epoch  42 Batch   44/269   train_loss = 3.697\n",
      "Epoch  42 Batch   45/269   train_loss = 3.430\n",
      "Epoch  42 Batch   46/269   train_loss = 3.791\n",
      "Epoch  42 Batch   47/269   train_loss = 3.446\n",
      "Epoch  42 Batch   48/269   train_loss = 3.945\n",
      "Epoch  42 Batch   49/269   train_loss = 3.973\n",
      "Epoch  42 Batch   50/269   train_loss = 3.831\n",
      "Epoch  42 Batch   51/269   train_loss = 3.725\n",
      "Epoch  42 Batch   52/269   train_loss = 3.741\n",
      "Epoch  42 Batch   53/269   train_loss = 3.958\n",
      "Epoch  42 Batch   54/269   train_loss = 3.787\n",
      "Epoch  42 Batch   55/269   train_loss = 3.815\n",
      "Epoch  42 Batch   56/269   train_loss = 3.669\n",
      "Epoch  42 Batch   57/269   train_loss = 3.903\n",
      "Epoch  42 Batch   58/269   train_loss = 3.591\n",
      "Epoch  42 Batch   59/269   train_loss = 3.639\n",
      "Epoch  42 Batch   60/269   train_loss = 3.787\n",
      "Epoch  42 Batch   61/269   train_loss = 4.019\n",
      "Epoch  42 Batch   62/269   train_loss = 3.608\n",
      "Epoch  42 Batch   63/269   train_loss = 3.911\n",
      "Epoch  42 Batch   64/269   train_loss = 3.694\n",
      "Epoch  42 Batch   65/269   train_loss = 3.822\n",
      "Epoch  42 Batch   66/269   train_loss = 3.669\n",
      "Epoch  42 Batch   67/269   train_loss = 3.684\n",
      "Epoch  42 Batch   68/269   train_loss = 3.792\n",
      "Epoch  42 Batch   69/269   train_loss = 3.777\n",
      "Epoch  42 Batch   70/269   train_loss = 3.397\n",
      "Epoch  42 Batch   71/269   train_loss = 3.819\n",
      "Epoch  42 Batch   72/269   train_loss = 3.479\n",
      "Epoch  42 Batch   73/269   train_loss = 3.700\n",
      "Epoch  42 Batch   74/269   train_loss = 3.711\n",
      "Epoch  42 Batch   75/269   train_loss = 3.839\n",
      "Epoch  42 Batch   76/269   train_loss = 3.829\n",
      "Epoch  42 Batch   77/269   train_loss = 3.424\n",
      "Epoch  42 Batch   78/269   train_loss = 3.847\n",
      "Epoch  42 Batch   79/269   train_loss = 3.899\n",
      "Epoch  42 Batch   80/269   train_loss = 3.845\n",
      "Epoch  42 Batch   81/269   train_loss = 3.678\n",
      "Epoch  42 Batch   82/269   train_loss = 3.907\n",
      "Epoch  42 Batch   83/269   train_loss = 3.670\n",
      "Epoch  42 Batch   84/269   train_loss = 3.880\n",
      "Epoch  42 Batch   85/269   train_loss = 4.116\n",
      "Epoch  42 Batch   86/269   train_loss = 4.029\n",
      "Epoch  42 Batch   87/269   train_loss = 3.716\n",
      "Epoch  42 Batch   88/269   train_loss = 3.837\n",
      "Epoch  42 Batch   89/269   train_loss = 3.741\n",
      "Epoch  42 Batch   90/269   train_loss = 3.606\n",
      "Epoch  42 Batch   91/269   train_loss = 3.911\n",
      "Epoch  42 Batch   92/269   train_loss = 3.890\n",
      "Epoch  42 Batch   93/269   train_loss = 3.630\n",
      "Epoch  42 Batch   94/269   train_loss = 4.022\n",
      "Epoch  42 Batch   95/269   train_loss = 3.513\n",
      "Epoch  42 Batch   96/269   train_loss = 3.630\n",
      "Epoch  42 Batch   97/269   train_loss = 3.766\n",
      "Epoch  42 Batch   98/269   train_loss = 3.889\n",
      "Epoch  42 Batch   99/269   train_loss = 3.711\n",
      "Epoch  42 Batch  100/269   train_loss = 3.724\n",
      "Epoch  42 Batch  101/269   train_loss = 3.904\n",
      "Epoch  42 Batch  102/269   train_loss = 3.553\n",
      "Epoch  42 Batch  103/269   train_loss = 3.636\n",
      "Epoch  42 Batch  104/269   train_loss = 3.602\n",
      "Epoch  42 Batch  105/269   train_loss = 3.892\n",
      "Epoch  42 Batch  106/269   train_loss = 3.948\n",
      "Epoch  42 Batch  107/269   train_loss = 3.987\n",
      "Epoch  42 Batch  108/269   train_loss = 3.762\n",
      "Epoch  42 Batch  109/269   train_loss = 3.813\n",
      "Epoch  42 Batch  110/269   train_loss = 3.776\n",
      "Epoch  42 Batch  111/269   train_loss = 3.872\n",
      "Epoch  42 Batch  112/269   train_loss = 3.888\n",
      "Epoch  42 Batch  113/269   train_loss = 3.772\n",
      "Epoch  42 Batch  114/269   train_loss = 3.745\n",
      "Epoch  42 Batch  115/269   train_loss = 3.566\n",
      "Epoch  42 Batch  116/269   train_loss = 3.599\n",
      "Epoch  42 Batch  117/269   train_loss = 3.620\n",
      "Epoch  42 Batch  118/269   train_loss = 3.957\n",
      "Epoch  42 Batch  119/269   train_loss = 3.501\n",
      "Epoch  42 Batch  120/269   train_loss = 3.846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  42 Batch  121/269   train_loss = 3.659\n",
      "Epoch  42 Batch  122/269   train_loss = 3.839\n",
      "Epoch  42 Batch  123/269   train_loss = 3.538\n",
      "Epoch  42 Batch  124/269   train_loss = 3.896\n",
      "Epoch  42 Batch  125/269   train_loss = 3.508\n",
      "Epoch  42 Batch  126/269   train_loss = 3.427\n",
      "Epoch  42 Batch  127/269   train_loss = 3.686\n",
      "Epoch  42 Batch  128/269   train_loss = 3.696\n",
      "Epoch  42 Batch  129/269   train_loss = 3.677\n",
      "Epoch  42 Batch  130/269   train_loss = 3.849\n",
      "Epoch  42 Batch  131/269   train_loss = 3.396\n",
      "Epoch  42 Batch  132/269   train_loss = 3.730\n",
      "Epoch  42 Batch  133/269   train_loss = 3.485\n",
      "Epoch  42 Batch  134/269   train_loss = 3.886\n",
      "Epoch  42 Batch  135/269   train_loss = 3.590\n",
      "Epoch  42 Batch  136/269   train_loss = 3.485\n",
      "Epoch  42 Batch  137/269   train_loss = 3.814\n",
      "Epoch  42 Batch  138/269   train_loss = 3.599\n",
      "Epoch  42 Batch  139/269   train_loss = 3.754\n",
      "Epoch  42 Batch  140/269   train_loss = 3.781\n",
      "Epoch  42 Batch  141/269   train_loss = 4.032\n",
      "Epoch  42 Batch  142/269   train_loss = 3.526\n",
      "Epoch  42 Batch  143/269   train_loss = 3.755\n",
      "Epoch  42 Batch  144/269   train_loss = 3.897\n",
      "Epoch  42 Batch  145/269   train_loss = 3.647\n",
      "Epoch  42 Batch  146/269   train_loss = 3.886\n",
      "Epoch  42 Batch  147/269   train_loss = 3.519\n",
      "Epoch  42 Batch  148/269   train_loss = 3.824\n",
      "Epoch  42 Batch  149/269   train_loss = 3.671\n",
      "Epoch  42 Batch  150/269   train_loss = 3.844\n",
      "Epoch  42 Batch  151/269   train_loss = 3.779\n",
      "Epoch  42 Batch  152/269   train_loss = 3.714\n",
      "Epoch  42 Batch  153/269   train_loss = 3.748\n",
      "Epoch  42 Batch  154/269   train_loss = 3.637\n",
      "Epoch  42 Batch  155/269   train_loss = 3.671\n",
      "Epoch  42 Batch  156/269   train_loss = 4.151\n",
      "Epoch  42 Batch  157/269   train_loss = 3.842\n",
      "Epoch  42 Batch  158/269   train_loss = 3.926\n",
      "Epoch  42 Batch  159/269   train_loss = 3.791\n",
      "Epoch  42 Batch  160/269   train_loss = 3.677\n",
      "Epoch  42 Batch  161/269   train_loss = 3.852\n",
      "Epoch  42 Batch  162/269   train_loss = 3.821\n",
      "Epoch  42 Batch  163/269   train_loss = 3.736\n",
      "Epoch  42 Batch  164/269   train_loss = 3.928\n",
      "Epoch  42 Batch  165/269   train_loss = 3.897\n",
      "Epoch  42 Batch  166/269   train_loss = 3.754\n",
      "Epoch  42 Batch  167/269   train_loss = 3.660\n",
      "Epoch  42 Batch  168/269   train_loss = 3.704\n",
      "Epoch  42 Batch  169/269   train_loss = 4.021\n",
      "Epoch  42 Batch  170/269   train_loss = 3.593\n",
      "Epoch  42 Batch  171/269   train_loss = 3.721\n",
      "Epoch  42 Batch  172/269   train_loss = 3.557\n",
      "Epoch  42 Batch  173/269   train_loss = 3.759\n",
      "Epoch  42 Batch  174/269   train_loss = 3.921\n",
      "Epoch  42 Batch  175/269   train_loss = 3.813\n",
      "Epoch  42 Batch  176/269   train_loss = 3.782\n",
      "Epoch  42 Batch  177/269   train_loss = 3.732\n",
      "Epoch  42 Batch  178/269   train_loss = 4.025\n",
      "Epoch  42 Batch  179/269   train_loss = 3.768\n",
      "Epoch  42 Batch  180/269   train_loss = 3.593\n",
      "Epoch  42 Batch  181/269   train_loss = 3.827\n",
      "Epoch  42 Batch  182/269   train_loss = 3.655\n",
      "Epoch  42 Batch  183/269   train_loss = 3.644\n",
      "Epoch  42 Batch  184/269   train_loss = 3.745\n",
      "Epoch  42 Batch  185/269   train_loss = 3.798\n",
      "Epoch  42 Batch  186/269   train_loss = 3.697\n",
      "Epoch  42 Batch  187/269   train_loss = 3.932\n",
      "Epoch  42 Batch  188/269   train_loss = 3.654\n",
      "Epoch  42 Batch  189/269   train_loss = 3.658\n",
      "Epoch  42 Batch  190/269   train_loss = 4.107\n",
      "Epoch  42 Batch  191/269   train_loss = 3.798\n",
      "Epoch  42 Batch  192/269   train_loss = 3.696\n",
      "Epoch  42 Batch  193/269   train_loss = 3.734\n",
      "Epoch  42 Batch  194/269   train_loss = 3.815\n",
      "Epoch  42 Batch  195/269   train_loss = 3.703\n",
      "Epoch  42 Batch  196/269   train_loss = 3.776\n",
      "Epoch  42 Batch  197/269   train_loss = 3.894\n",
      "Epoch  42 Batch  198/269   train_loss = 3.841\n",
      "Epoch  42 Batch  199/269   train_loss = 3.819\n",
      "Epoch  42 Batch  200/269   train_loss = 3.639\n",
      "Epoch  42 Batch  201/269   train_loss = 3.700\n",
      "Epoch  42 Batch  202/269   train_loss = 3.580\n",
      "Epoch  42 Batch  203/269   train_loss = 3.602\n",
      "Epoch  42 Batch  204/269   train_loss = 3.811\n",
      "Epoch  42 Batch  205/269   train_loss = 3.790\n",
      "Epoch  42 Batch  206/269   train_loss = 3.640\n",
      "Epoch  42 Batch  207/269   train_loss = 3.666\n",
      "Epoch  42 Batch  208/269   train_loss = 3.773\n",
      "Epoch  42 Batch  209/269   train_loss = 3.896\n",
      "Epoch  42 Batch  210/269   train_loss = 3.558\n",
      "Epoch  42 Batch  211/269   train_loss = 3.753\n",
      "Epoch  42 Batch  212/269   train_loss = 4.033\n",
      "Epoch  42 Batch  213/269   train_loss = 3.601\n",
      "Epoch  42 Batch  214/269   train_loss = 3.681\n",
      "Epoch  42 Batch  215/269   train_loss = 3.938\n",
      "Epoch  42 Batch  216/269   train_loss = 3.885\n",
      "Epoch  42 Batch  217/269   train_loss = 3.619\n",
      "Epoch  42 Batch  218/269   train_loss = 3.855\n",
      "Epoch  42 Batch  219/269   train_loss = 3.474\n",
      "Epoch  42 Batch  220/269   train_loss = 3.904\n",
      "Epoch  42 Batch  221/269   train_loss = 3.606\n",
      "Epoch  42 Batch  222/269   train_loss = 3.782\n",
      "Epoch  42 Batch  223/269   train_loss = 3.586\n",
      "Epoch  42 Batch  224/269   train_loss = 3.857\n",
      "Epoch  42 Batch  225/269   train_loss = 3.973\n",
      "Epoch  42 Batch  226/269   train_loss = 3.746\n",
      "Epoch  42 Batch  227/269   train_loss = 3.454\n",
      "Epoch  42 Batch  228/269   train_loss = 3.715\n",
      "Epoch  42 Batch  229/269   train_loss = 3.937\n",
      "Epoch  42 Batch  230/269   train_loss = 3.798\n",
      "Epoch  42 Batch  231/269   train_loss = 3.693\n",
      "Epoch  42 Batch  232/269   train_loss = 3.759\n",
      "Epoch  42 Batch  233/269   train_loss = 3.814\n",
      "Epoch  42 Batch  234/269   train_loss = 3.721\n",
      "Epoch  42 Batch  235/269   train_loss = 4.011\n",
      "Epoch  42 Batch  236/269   train_loss = 3.633\n",
      "Epoch  42 Batch  237/269   train_loss = 3.328\n",
      "Epoch  42 Batch  238/269   train_loss = 3.516\n",
      "Epoch  42 Batch  239/269   train_loss = 4.106\n",
      "Epoch  42 Batch  240/269   train_loss = 3.702\n",
      "Epoch  42 Batch  241/269   train_loss = 3.967\n",
      "Epoch  42 Batch  242/269   train_loss = 3.531\n",
      "Epoch  42 Batch  243/269   train_loss = 3.904\n",
      "Epoch  42 Batch  244/269   train_loss = 3.742\n",
      "Epoch  42 Batch  245/269   train_loss = 3.777\n",
      "Epoch  42 Batch  246/269   train_loss = 3.415\n",
      "Epoch  42 Batch  247/269   train_loss = 3.794\n",
      "Epoch  42 Batch  248/269   train_loss = 3.804\n",
      "Epoch  42 Batch  249/269   train_loss = 3.590\n",
      "Epoch  42 Batch  250/269   train_loss = 3.522\n",
      "Epoch  42 Batch  251/269   train_loss = 3.910\n",
      "Epoch  42 Batch  252/269   train_loss = 3.854\n",
      "Epoch  42 Batch  253/269   train_loss = 3.689\n",
      "Epoch  42 Batch  254/269   train_loss = 3.671\n",
      "Epoch  42 Batch  255/269   train_loss = 3.730\n",
      "Epoch  42 Batch  256/269   train_loss = 3.602\n",
      "Epoch  42 Batch  257/269   train_loss = 3.542\n",
      "Epoch  42 Batch  258/269   train_loss = 3.466\n",
      "Epoch  42 Batch  259/269   train_loss = 3.614\n",
      "Epoch  42 Batch  260/269   train_loss = 3.801\n",
      "Epoch  42 Batch  261/269   train_loss = 3.857\n",
      "Epoch  42 Batch  262/269   train_loss = 3.675\n",
      "Epoch  42 Batch  263/269   train_loss = 3.460\n",
      "Epoch  42 Batch  264/269   train_loss = 3.974\n",
      "Epoch  42 Batch  265/269   train_loss = 3.872\n",
      "Epoch  42 Batch  266/269   train_loss = 3.621\n",
      "Epoch  42 Batch  267/269   train_loss = 3.763\n",
      "Epoch  42 Batch  268/269   train_loss = 3.857\n",
      "Epoch  43 Batch    0/269   train_loss = 3.712\n",
      "Epoch  43 Batch    1/269   train_loss = 3.485\n",
      "Epoch  43 Batch    2/269   train_loss = 3.637\n",
      "Epoch  43 Batch    3/269   train_loss = 3.680\n",
      "Epoch  43 Batch    4/269   train_loss = 4.036\n",
      "Epoch  43 Batch    5/269   train_loss = 3.761\n",
      "Epoch  43 Batch    6/269   train_loss = 3.666\n",
      "Epoch  43 Batch    7/269   train_loss = 3.482\n",
      "Epoch  43 Batch    8/269   train_loss = 3.825\n",
      "Epoch  43 Batch    9/269   train_loss = 3.463\n",
      "Epoch  43 Batch   10/269   train_loss = 3.545\n",
      "Epoch  43 Batch   11/269   train_loss = 3.666\n",
      "Epoch  43 Batch   12/269   train_loss = 3.371\n",
      "Epoch  43 Batch   13/269   train_loss = 3.707\n",
      "Epoch  43 Batch   14/269   train_loss = 3.587\n",
      "Epoch  43 Batch   15/269   train_loss = 3.940\n",
      "Epoch  43 Batch   16/269   train_loss = 3.669\n",
      "Epoch  43 Batch   17/269   train_loss = 3.725\n",
      "Epoch  43 Batch   18/269   train_loss = 3.638\n",
      "Epoch  43 Batch   19/269   train_loss = 3.649\n",
      "Epoch  43 Batch   20/269   train_loss = 4.048\n",
      "Epoch  43 Batch   21/269   train_loss = 3.855\n",
      "Epoch  43 Batch   22/269   train_loss = 3.587\n",
      "Epoch  43 Batch   23/269   train_loss = 3.812\n",
      "Epoch  43 Batch   24/269   train_loss = 3.634\n",
      "Epoch  43 Batch   25/269   train_loss = 3.946\n",
      "Epoch  43 Batch   26/269   train_loss = 3.790\n",
      "Epoch  43 Batch   27/269   train_loss = 3.579\n",
      "Epoch  43 Batch   28/269   train_loss = 3.758\n",
      "Epoch  43 Batch   29/269   train_loss = 3.701\n",
      "Epoch  43 Batch   30/269   train_loss = 4.022\n",
      "Epoch  43 Batch   31/269   train_loss = 3.722\n",
      "Epoch  43 Batch   32/269   train_loss = 3.608\n",
      "Epoch  43 Batch   33/269   train_loss = 3.424\n",
      "Epoch  43 Batch   34/269   train_loss = 3.699\n",
      "Epoch  43 Batch   35/269   train_loss = 3.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  43 Batch   36/269   train_loss = 3.526\n",
      "Epoch  43 Batch   37/269   train_loss = 3.748\n",
      "Epoch  43 Batch   38/269   train_loss = 3.524\n",
      "Epoch  43 Batch   39/269   train_loss = 3.871\n",
      "Epoch  43 Batch   40/269   train_loss = 3.569\n",
      "Epoch  43 Batch   41/269   train_loss = 3.551\n",
      "Epoch  43 Batch   42/269   train_loss = 4.061\n",
      "Epoch  43 Batch   43/269   train_loss = 3.884\n",
      "Epoch  43 Batch   44/269   train_loss = 3.666\n",
      "Epoch  43 Batch   45/269   train_loss = 3.420\n",
      "Epoch  43 Batch   46/269   train_loss = 3.816\n",
      "Epoch  43 Batch   47/269   train_loss = 3.453\n",
      "Epoch  43 Batch   48/269   train_loss = 3.936\n",
      "Epoch  43 Batch   49/269   train_loss = 3.970\n",
      "Epoch  43 Batch   50/269   train_loss = 3.835\n",
      "Epoch  43 Batch   51/269   train_loss = 3.727\n",
      "Epoch  43 Batch   52/269   train_loss = 3.771\n",
      "Epoch  43 Batch   53/269   train_loss = 3.968\n",
      "Epoch  43 Batch   54/269   train_loss = 3.773\n",
      "Epoch  43 Batch   55/269   train_loss = 3.821\n",
      "Epoch  43 Batch   56/269   train_loss = 3.678\n",
      "Epoch  43 Batch   57/269   train_loss = 3.887\n",
      "Epoch  43 Batch   58/269   train_loss = 3.576\n",
      "Epoch  43 Batch   59/269   train_loss = 3.651\n",
      "Epoch  43 Batch   60/269   train_loss = 3.777\n",
      "Epoch  43 Batch   61/269   train_loss = 3.997\n",
      "Epoch  43 Batch   62/269   train_loss = 3.599\n",
      "Epoch  43 Batch   63/269   train_loss = 3.899\n",
      "Epoch  43 Batch   64/269   train_loss = 3.671\n",
      "Epoch  43 Batch   65/269   train_loss = 3.838\n",
      "Epoch  43 Batch   66/269   train_loss = 3.683\n",
      "Epoch  43 Batch   67/269   train_loss = 3.677\n",
      "Epoch  43 Batch   68/269   train_loss = 3.788\n",
      "Epoch  43 Batch   69/269   train_loss = 3.768\n",
      "Epoch  43 Batch   70/269   train_loss = 3.413\n",
      "Epoch  43 Batch   71/269   train_loss = 3.836\n",
      "Epoch  43 Batch   72/269   train_loss = 3.485\n",
      "Epoch  43 Batch   73/269   train_loss = 3.681\n",
      "Epoch  43 Batch   74/269   train_loss = 3.686\n",
      "Epoch  43 Batch   75/269   train_loss = 3.829\n",
      "Epoch  43 Batch   76/269   train_loss = 3.811\n",
      "Epoch  43 Batch   77/269   train_loss = 3.448\n",
      "Epoch  43 Batch   78/269   train_loss = 3.834\n",
      "Epoch  43 Batch   79/269   train_loss = 3.914\n",
      "Epoch  43 Batch   80/269   train_loss = 3.835\n",
      "Epoch  43 Batch   81/269   train_loss = 3.684\n",
      "Epoch  43 Batch   82/269   train_loss = 3.894\n",
      "Epoch  43 Batch   83/269   train_loss = 3.642\n",
      "Epoch  43 Batch   84/269   train_loss = 3.924\n",
      "Epoch  43 Batch   85/269   train_loss = 4.103\n",
      "Epoch  43 Batch   86/269   train_loss = 4.040\n",
      "Epoch  43 Batch   87/269   train_loss = 3.693\n",
      "Epoch  43 Batch   88/269   train_loss = 3.802\n",
      "Epoch  43 Batch   89/269   train_loss = 3.738\n",
      "Epoch  43 Batch   90/269   train_loss = 3.572\n",
      "Epoch  43 Batch   91/269   train_loss = 3.920\n",
      "Epoch  43 Batch   92/269   train_loss = 3.905\n",
      "Epoch  43 Batch   93/269   train_loss = 3.610\n",
      "Epoch  43 Batch   94/269   train_loss = 4.030\n",
      "Epoch  43 Batch   95/269   train_loss = 3.503\n",
      "Epoch  43 Batch   96/269   train_loss = 3.616\n",
      "Epoch  43 Batch   97/269   train_loss = 3.718\n",
      "Epoch  43 Batch   98/269   train_loss = 3.884\n",
      "Epoch  43 Batch   99/269   train_loss = 3.705\n",
      "Epoch  43 Batch  100/269   train_loss = 3.715\n",
      "Epoch  43 Batch  101/269   train_loss = 3.913\n",
      "Epoch  43 Batch  102/269   train_loss = 3.565\n",
      "Epoch  43 Batch  103/269   train_loss = 3.653\n",
      "Epoch  43 Batch  104/269   train_loss = 3.622\n",
      "Epoch  43 Batch  105/269   train_loss = 3.883\n",
      "Epoch  43 Batch  106/269   train_loss = 3.954\n",
      "Epoch  43 Batch  107/269   train_loss = 4.031\n",
      "Epoch  43 Batch  108/269   train_loss = 3.758\n",
      "Epoch  43 Batch  109/269   train_loss = 3.779\n",
      "Epoch  43 Batch  110/269   train_loss = 3.786\n",
      "Epoch  43 Batch  111/269   train_loss = 3.898\n",
      "Epoch  43 Batch  112/269   train_loss = 3.868\n",
      "Epoch  43 Batch  113/269   train_loss = 3.758\n",
      "Epoch  43 Batch  114/269   train_loss = 3.719\n",
      "Epoch  43 Batch  115/269   train_loss = 3.561\n",
      "Epoch  43 Batch  116/269   train_loss = 3.611\n",
      "Epoch  43 Batch  117/269   train_loss = 3.612\n",
      "Epoch  43 Batch  118/269   train_loss = 3.958\n",
      "Epoch  43 Batch  119/269   train_loss = 3.508\n",
      "Epoch  43 Batch  120/269   train_loss = 3.825\n",
      "Epoch  43 Batch  121/269   train_loss = 3.668\n",
      "Epoch  43 Batch  122/269   train_loss = 3.819\n",
      "Epoch  43 Batch  123/269   train_loss = 3.529\n",
      "Epoch  43 Batch  124/269   train_loss = 3.889\n",
      "Epoch  43 Batch  125/269   train_loss = 3.487\n",
      "Epoch  43 Batch  126/269   train_loss = 3.421\n",
      "Epoch  43 Batch  127/269   train_loss = 3.707\n",
      "Epoch  43 Batch  128/269   train_loss = 3.682\n",
      "Epoch  43 Batch  129/269   train_loss = 3.669\n",
      "Epoch  43 Batch  130/269   train_loss = 3.838\n",
      "Epoch  43 Batch  131/269   train_loss = 3.396\n",
      "Epoch  43 Batch  132/269   train_loss = 3.713\n",
      "Epoch  43 Batch  133/269   train_loss = 3.469\n",
      "Epoch  43 Batch  134/269   train_loss = 3.884\n",
      "Epoch  43 Batch  135/269   train_loss = 3.594\n",
      "Epoch  43 Batch  136/269   train_loss = 3.497\n",
      "Epoch  43 Batch  137/269   train_loss = 3.810\n",
      "Epoch  43 Batch  138/269   train_loss = 3.594\n",
      "Epoch  43 Batch  139/269   train_loss = 3.754\n",
      "Epoch  43 Batch  140/269   train_loss = 3.774\n",
      "Epoch  43 Batch  141/269   train_loss = 4.073\n",
      "Epoch  43 Batch  142/269   train_loss = 3.515\n",
      "Epoch  43 Batch  143/269   train_loss = 3.757\n",
      "Epoch  43 Batch  144/269   train_loss = 3.910\n",
      "Epoch  43 Batch  145/269   train_loss = 3.643\n",
      "Epoch  43 Batch  146/269   train_loss = 3.938\n",
      "Epoch  43 Batch  147/269   train_loss = 3.513\n",
      "Epoch  43 Batch  148/269   train_loss = 3.831\n",
      "Epoch  43 Batch  149/269   train_loss = 3.653\n",
      "Epoch  43 Batch  150/269   train_loss = 3.813\n",
      "Epoch  43 Batch  151/269   train_loss = 3.775\n",
      "Epoch  43 Batch  152/269   train_loss = 3.748\n",
      "Epoch  43 Batch  153/269   train_loss = 3.726\n",
      "Epoch  43 Batch  154/269   train_loss = 3.639\n",
      "Epoch  43 Batch  155/269   train_loss = 3.687\n",
      "Epoch  43 Batch  156/269   train_loss = 4.161\n",
      "Epoch  43 Batch  157/269   train_loss = 3.826\n",
      "Epoch  43 Batch  158/269   train_loss = 3.893\n",
      "Epoch  43 Batch  159/269   train_loss = 3.802\n",
      "Epoch  43 Batch  160/269   train_loss = 3.677\n",
      "Epoch  43 Batch  161/269   train_loss = 3.855\n",
      "Epoch  43 Batch  162/269   train_loss = 3.831\n",
      "Epoch  43 Batch  163/269   train_loss = 3.721\n",
      "Epoch  43 Batch  164/269   train_loss = 3.926\n",
      "Epoch  43 Batch  165/269   train_loss = 3.897\n",
      "Epoch  43 Batch  166/269   train_loss = 3.753\n",
      "Epoch  43 Batch  167/269   train_loss = 3.651\n",
      "Epoch  43 Batch  168/269   train_loss = 3.698\n",
      "Epoch  43 Batch  169/269   train_loss = 4.046\n",
      "Epoch  43 Batch  170/269   train_loss = 3.590\n",
      "Epoch  43 Batch  171/269   train_loss = 3.720\n",
      "Epoch  43 Batch  172/269   train_loss = 3.555\n",
      "Epoch  43 Batch  173/269   train_loss = 3.767\n",
      "Epoch  43 Batch  174/269   train_loss = 3.947\n",
      "Epoch  43 Batch  175/269   train_loss = 3.844\n",
      "Epoch  43 Batch  176/269   train_loss = 3.784\n",
      "Epoch  43 Batch  177/269   train_loss = 3.733\n",
      "Epoch  43 Batch  178/269   train_loss = 4.006\n",
      "Epoch  43 Batch  179/269   train_loss = 3.762\n",
      "Epoch  43 Batch  180/269   train_loss = 3.589\n",
      "Epoch  43 Batch  181/269   train_loss = 3.834\n",
      "Epoch  43 Batch  182/269   train_loss = 3.641\n",
      "Epoch  43 Batch  183/269   train_loss = 3.615\n",
      "Epoch  43 Batch  184/269   train_loss = 3.735\n",
      "Epoch  43 Batch  185/269   train_loss = 3.783\n",
      "Epoch  43 Batch  186/269   train_loss = 3.698\n",
      "Epoch  43 Batch  187/269   train_loss = 3.924\n",
      "Epoch  43 Batch  188/269   train_loss = 3.632\n",
      "Epoch  43 Batch  189/269   train_loss = 3.649\n",
      "Epoch  43 Batch  190/269   train_loss = 4.093\n",
      "Epoch  43 Batch  191/269   train_loss = 3.778\n",
      "Epoch  43 Batch  192/269   train_loss = 3.696\n",
      "Epoch  43 Batch  193/269   train_loss = 3.696\n",
      "Epoch  43 Batch  194/269   train_loss = 3.822\n",
      "Epoch  43 Batch  195/269   train_loss = 3.704\n",
      "Epoch  43 Batch  196/269   train_loss = 3.756\n",
      "Epoch  43 Batch  197/269   train_loss = 3.905\n",
      "Epoch  43 Batch  198/269   train_loss = 3.829\n",
      "Epoch  43 Batch  199/269   train_loss = 3.802\n",
      "Epoch  43 Batch  200/269   train_loss = 3.641\n",
      "Epoch  43 Batch  201/269   train_loss = 3.718\n",
      "Epoch  43 Batch  202/269   train_loss = 3.590\n",
      "Epoch  43 Batch  203/269   train_loss = 3.613\n",
      "Epoch  43 Batch  204/269   train_loss = 3.815\n",
      "Epoch  43 Batch  205/269   train_loss = 3.793\n",
      "Epoch  43 Batch  206/269   train_loss = 3.634\n",
      "Epoch  43 Batch  207/269   train_loss = 3.669\n",
      "Epoch  43 Batch  208/269   train_loss = 3.762\n",
      "Epoch  43 Batch  209/269   train_loss = 3.871\n",
      "Epoch  43 Batch  210/269   train_loss = 3.569\n",
      "Epoch  43 Batch  211/269   train_loss = 3.744\n",
      "Epoch  43 Batch  212/269   train_loss = 4.025\n",
      "Epoch  43 Batch  213/269   train_loss = 3.649\n",
      "Epoch  43 Batch  214/269   train_loss = 3.713\n",
      "Epoch  43 Batch  215/269   train_loss = 3.959\n",
      "Epoch  43 Batch  216/269   train_loss = 3.885\n",
      "Epoch  43 Batch  217/269   train_loss = 3.611\n",
      "Epoch  43 Batch  218/269   train_loss = 3.839\n",
      "Epoch  43 Batch  219/269   train_loss = 3.467\n",
      "Epoch  43 Batch  220/269   train_loss = 3.892\n",
      "Epoch  43 Batch  221/269   train_loss = 3.622\n",
      "Epoch  43 Batch  222/269   train_loss = 3.780\n",
      "Epoch  43 Batch  223/269   train_loss = 3.574\n",
      "Epoch  43 Batch  224/269   train_loss = 3.865\n",
      "Epoch  43 Batch  225/269   train_loss = 3.952\n",
      "Epoch  43 Batch  226/269   train_loss = 3.748\n",
      "Epoch  43 Batch  227/269   train_loss = 3.455\n",
      "Epoch  43 Batch  228/269   train_loss = 3.730\n",
      "Epoch  43 Batch  229/269   train_loss = 3.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  43 Batch  230/269   train_loss = 3.812\n",
      "Epoch  43 Batch  231/269   train_loss = 3.679\n",
      "Epoch  43 Batch  232/269   train_loss = 3.765\n",
      "Epoch  43 Batch  233/269   train_loss = 3.795\n",
      "Epoch  43 Batch  234/269   train_loss = 3.723\n",
      "Epoch  43 Batch  235/269   train_loss = 3.982\n",
      "Epoch  43 Batch  236/269   train_loss = 3.619\n",
      "Epoch  43 Batch  237/269   train_loss = 3.298\n",
      "Epoch  43 Batch  238/269   train_loss = 3.500\n",
      "Epoch  43 Batch  239/269   train_loss = 4.086\n",
      "Epoch  43 Batch  240/269   train_loss = 3.698\n",
      "Epoch  43 Batch  241/269   train_loss = 3.971\n",
      "Epoch  43 Batch  242/269   train_loss = 3.565\n",
      "Epoch  43 Batch  243/269   train_loss = 3.911\n",
      "Epoch  43 Batch  244/269   train_loss = 3.743\n",
      "Epoch  43 Batch  245/269   train_loss = 3.753\n",
      "Epoch  43 Batch  246/269   train_loss = 3.403\n",
      "Epoch  43 Batch  247/269   train_loss = 3.785\n",
      "Epoch  43 Batch  248/269   train_loss = 3.760\n",
      "Epoch  43 Batch  249/269   train_loss = 3.563\n",
      "Epoch  43 Batch  250/269   train_loss = 3.486\n",
      "Epoch  43 Batch  251/269   train_loss = 3.906\n",
      "Epoch  43 Batch  252/269   train_loss = 3.832\n",
      "Epoch  43 Batch  253/269   train_loss = 3.701\n",
      "Epoch  43 Batch  254/269   train_loss = 3.664\n",
      "Epoch  43 Batch  255/269   train_loss = 3.705\n",
      "Epoch  43 Batch  256/269   train_loss = 3.621\n",
      "Epoch  43 Batch  257/269   train_loss = 3.545\n",
      "Epoch  43 Batch  258/269   train_loss = 3.470\n",
      "Epoch  43 Batch  259/269   train_loss = 3.607\n",
      "Epoch  43 Batch  260/269   train_loss = 3.814\n",
      "Epoch  43 Batch  261/269   train_loss = 3.835\n",
      "Epoch  43 Batch  262/269   train_loss = 3.641\n",
      "Epoch  43 Batch  263/269   train_loss = 3.461\n",
      "Epoch  43 Batch  264/269   train_loss = 3.971\n",
      "Epoch  43 Batch  265/269   train_loss = 3.838\n",
      "Epoch  43 Batch  266/269   train_loss = 3.625\n",
      "Epoch  43 Batch  267/269   train_loss = 3.734\n",
      "Epoch  43 Batch  268/269   train_loss = 3.882\n",
      "Epoch  44 Batch    0/269   train_loss = 3.697\n",
      "Epoch  44 Batch    1/269   train_loss = 3.484\n",
      "Epoch  44 Batch    2/269   train_loss = 3.625\n",
      "Epoch  44 Batch    3/269   train_loss = 3.678\n",
      "Epoch  44 Batch    4/269   train_loss = 4.039\n",
      "Epoch  44 Batch    5/269   train_loss = 3.788\n",
      "Epoch  44 Batch    6/269   train_loss = 3.617\n",
      "Epoch  44 Batch    7/269   train_loss = 3.493\n",
      "Epoch  44 Batch    8/269   train_loss = 3.832\n",
      "Epoch  44 Batch    9/269   train_loss = 3.477\n",
      "Epoch  44 Batch   10/269   train_loss = 3.550\n",
      "Epoch  44 Batch   11/269   train_loss = 3.680\n",
      "Epoch  44 Batch   12/269   train_loss = 3.427\n",
      "Epoch  44 Batch   13/269   train_loss = 3.704\n",
      "Epoch  44 Batch   14/269   train_loss = 3.582\n",
      "Epoch  44 Batch   15/269   train_loss = 3.949\n",
      "Epoch  44 Batch   16/269   train_loss = 3.654\n",
      "Epoch  44 Batch   17/269   train_loss = 3.718\n",
      "Epoch  44 Batch   18/269   train_loss = 3.649\n",
      "Epoch  44 Batch   19/269   train_loss = 3.644\n",
      "Epoch  44 Batch   20/269   train_loss = 4.036\n",
      "Epoch  44 Batch   21/269   train_loss = 3.852\n",
      "Epoch  44 Batch   22/269   train_loss = 3.577\n",
      "Epoch  44 Batch   23/269   train_loss = 3.794\n",
      "Epoch  44 Batch   24/269   train_loss = 3.623\n",
      "Epoch  44 Batch   25/269   train_loss = 3.947\n",
      "Epoch  44 Batch   26/269   train_loss = 3.767\n",
      "Epoch  44 Batch   27/269   train_loss = 3.574\n",
      "Epoch  44 Batch   28/269   train_loss = 3.752\n",
      "Epoch  44 Batch   29/269   train_loss = 3.724\n",
      "Epoch  44 Batch   30/269   train_loss = 4.009\n",
      "Epoch  44 Batch   31/269   train_loss = 3.691\n",
      "Epoch  44 Batch   32/269   train_loss = 3.608\n",
      "Epoch  44 Batch   33/269   train_loss = 3.394\n",
      "Epoch  44 Batch   34/269   train_loss = 3.674\n",
      "Epoch  44 Batch   35/269   train_loss = 3.528\n",
      "Epoch  44 Batch   36/269   train_loss = 3.518\n",
      "Epoch  44 Batch   37/269   train_loss = 3.743\n",
      "Epoch  44 Batch   38/269   train_loss = 3.513\n",
      "Epoch  44 Batch   39/269   train_loss = 3.850\n",
      "Epoch  44 Batch   40/269   train_loss = 3.551\n",
      "Epoch  44 Batch   41/269   train_loss = 3.574\n",
      "Epoch  44 Batch   42/269   train_loss = 4.060\n",
      "Epoch  44 Batch   43/269   train_loss = 3.918\n",
      "Epoch  44 Batch   44/269   train_loss = 3.682\n",
      "Epoch  44 Batch   45/269   train_loss = 3.388\n",
      "Epoch  44 Batch   46/269   train_loss = 3.805\n",
      "Epoch  44 Batch   47/269   train_loss = 3.439\n",
      "Epoch  44 Batch   48/269   train_loss = 3.912\n",
      "Epoch  44 Batch   49/269   train_loss = 3.966\n",
      "Epoch  44 Batch   50/269   train_loss = 3.841\n",
      "Epoch  44 Batch   51/269   train_loss = 3.723\n",
      "Epoch  44 Batch   52/269   train_loss = 3.753\n",
      "Epoch  44 Batch   53/269   train_loss = 3.941\n",
      "Epoch  44 Batch   54/269   train_loss = 3.757\n",
      "Epoch  44 Batch   55/269   train_loss = 3.820\n",
      "Epoch  44 Batch   56/269   train_loss = 3.676\n",
      "Epoch  44 Batch   57/269   train_loss = 3.890\n",
      "Epoch  44 Batch   58/269   train_loss = 3.592\n",
      "Epoch  44 Batch   59/269   train_loss = 3.642\n",
      "Epoch  44 Batch   60/269   train_loss = 3.792\n",
      "Epoch  44 Batch   61/269   train_loss = 3.993\n",
      "Epoch  44 Batch   62/269   train_loss = 3.590\n",
      "Epoch  44 Batch   63/269   train_loss = 3.886\n",
      "Epoch  44 Batch   64/269   train_loss = 3.642\n",
      "Epoch  44 Batch   65/269   train_loss = 3.815\n",
      "Epoch  44 Batch   66/269   train_loss = 3.674\n",
      "Epoch  44 Batch   67/269   train_loss = 3.686\n",
      "Epoch  44 Batch   68/269   train_loss = 3.806\n",
      "Epoch  44 Batch   69/269   train_loss = 3.782\n",
      "Epoch  44 Batch   70/269   train_loss = 3.426\n",
      "Epoch  44 Batch   71/269   train_loss = 3.838\n",
      "Epoch  44 Batch   72/269   train_loss = 3.514\n",
      "Epoch  44 Batch   73/269   train_loss = 3.697\n",
      "Epoch  44 Batch   74/269   train_loss = 3.703\n",
      "Epoch  44 Batch   75/269   train_loss = 3.843\n",
      "Epoch  44 Batch   76/269   train_loss = 3.839\n",
      "Epoch  44 Batch   77/269   train_loss = 3.450\n",
      "Epoch  44 Batch   78/269   train_loss = 3.870\n",
      "Epoch  44 Batch   79/269   train_loss = 3.915\n",
      "Epoch  44 Batch   80/269   train_loss = 3.857\n",
      "Epoch  44 Batch   81/269   train_loss = 3.667\n",
      "Epoch  44 Batch   82/269   train_loss = 3.879\n",
      "Epoch  44 Batch   83/269   train_loss = 3.632\n",
      "Epoch  44 Batch   84/269   train_loss = 3.885\n",
      "Epoch  44 Batch   85/269   train_loss = 4.082\n",
      "Epoch  44 Batch   86/269   train_loss = 4.032\n",
      "Epoch  44 Batch   87/269   train_loss = 3.672\n",
      "Epoch  44 Batch   88/269   train_loss = 3.822\n",
      "Epoch  44 Batch   89/269   train_loss = 3.731\n",
      "Epoch  44 Batch   90/269   train_loss = 3.557\n",
      "Epoch  44 Batch   91/269   train_loss = 3.867\n",
      "Epoch  44 Batch   92/269   train_loss = 3.886\n",
      "Epoch  44 Batch   93/269   train_loss = 3.580\n",
      "Epoch  44 Batch   94/269   train_loss = 4.034\n",
      "Epoch  44 Batch   95/269   train_loss = 3.519\n",
      "Epoch  44 Batch   96/269   train_loss = 3.612\n",
      "Epoch  44 Batch   97/269   train_loss = 3.685\n",
      "Epoch  44 Batch   98/269   train_loss = 3.892\n",
      "Epoch  44 Batch   99/269   train_loss = 3.690\n",
      "Epoch  44 Batch  100/269   train_loss = 3.719\n",
      "Epoch  44 Batch  101/269   train_loss = 3.890\n",
      "Epoch  44 Batch  102/269   train_loss = 3.541\n",
      "Epoch  44 Batch  103/269   train_loss = 3.635\n",
      "Epoch  44 Batch  104/269   train_loss = 3.577\n",
      "Epoch  44 Batch  105/269   train_loss = 3.873\n",
      "Epoch  44 Batch  106/269   train_loss = 3.990\n",
      "Epoch  44 Batch  107/269   train_loss = 3.997\n",
      "Epoch  44 Batch  108/269   train_loss = 3.753\n",
      "Epoch  44 Batch  109/269   train_loss = 3.794\n",
      "Epoch  44 Batch  110/269   train_loss = 3.759\n",
      "Epoch  44 Batch  111/269   train_loss = 3.905\n",
      "Epoch  44 Batch  112/269   train_loss = 3.891\n",
      "Epoch  44 Batch  113/269   train_loss = 3.744\n",
      "Epoch  44 Batch  114/269   train_loss = 3.742\n",
      "Epoch  44 Batch  115/269   train_loss = 3.569\n",
      "Epoch  44 Batch  116/269   train_loss = 3.571\n",
      "Epoch  44 Batch  117/269   train_loss = 3.573\n",
      "Epoch  44 Batch  118/269   train_loss = 3.943\n",
      "Epoch  44 Batch  119/269   train_loss = 3.511\n",
      "Epoch  44 Batch  120/269   train_loss = 3.844\n",
      "Epoch  44 Batch  121/269   train_loss = 3.664\n",
      "Epoch  44 Batch  122/269   train_loss = 3.785\n",
      "Epoch  44 Batch  123/269   train_loss = 3.535\n",
      "Epoch  44 Batch  124/269   train_loss = 3.885\n",
      "Epoch  44 Batch  125/269   train_loss = 3.467\n",
      "Epoch  44 Batch  126/269   train_loss = 3.442\n",
      "Epoch  44 Batch  127/269   train_loss = 3.690\n",
      "Epoch  44 Batch  128/269   train_loss = 3.664\n",
      "Epoch  44 Batch  129/269   train_loss = 3.675\n",
      "Epoch  44 Batch  130/269   train_loss = 3.809\n",
      "Epoch  44 Batch  131/269   train_loss = 3.379\n",
      "Epoch  44 Batch  132/269   train_loss = 3.705\n",
      "Epoch  44 Batch  133/269   train_loss = 3.498\n",
      "Epoch  44 Batch  134/269   train_loss = 3.908\n",
      "Epoch  44 Batch  135/269   train_loss = 3.583\n",
      "Epoch  44 Batch  136/269   train_loss = 3.483\n",
      "Epoch  44 Batch  137/269   train_loss = 3.856\n",
      "Epoch  44 Batch  138/269   train_loss = 3.588\n",
      "Epoch  44 Batch  139/269   train_loss = 3.742\n",
      "Epoch  44 Batch  140/269   train_loss = 3.792\n",
      "Epoch  44 Batch  141/269   train_loss = 4.049\n",
      "Epoch  44 Batch  142/269   train_loss = 3.501\n",
      "Epoch  44 Batch  143/269   train_loss = 3.720\n",
      "Epoch  44 Batch  144/269   train_loss = 3.874\n",
      "Epoch  44 Batch  145/269   train_loss = 3.646\n",
      "Epoch  44 Batch  146/269   train_loss = 3.922\n",
      "Epoch  44 Batch  147/269   train_loss = 3.527\n",
      "Epoch  44 Batch  148/269   train_loss = 3.834\n",
      "Epoch  44 Batch  149/269   train_loss = 3.658\n",
      "Epoch  44 Batch  150/269   train_loss = 3.862\n",
      "Epoch  44 Batch  151/269   train_loss = 3.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  44 Batch  152/269   train_loss = 3.710\n",
      "Epoch  44 Batch  153/269   train_loss = 3.737\n",
      "Epoch  44 Batch  154/269   train_loss = 3.617\n",
      "Epoch  44 Batch  155/269   train_loss = 3.663\n",
      "Epoch  44 Batch  156/269   train_loss = 4.149\n",
      "Epoch  44 Batch  157/269   train_loss = 3.786\n",
      "Epoch  44 Batch  158/269   train_loss = 3.872\n",
      "Epoch  44 Batch  159/269   train_loss = 3.773\n",
      "Epoch  44 Batch  160/269   train_loss = 3.709\n",
      "Epoch  44 Batch  161/269   train_loss = 3.851\n",
      "Epoch  44 Batch  162/269   train_loss = 3.794\n",
      "Epoch  44 Batch  163/269   train_loss = 3.691\n",
      "Epoch  44 Batch  164/269   train_loss = 3.901\n",
      "Epoch  44 Batch  165/269   train_loss = 3.865\n",
      "Epoch  44 Batch  166/269   train_loss = 3.740\n",
      "Epoch  44 Batch  167/269   train_loss = 3.656\n",
      "Epoch  44 Batch  168/269   train_loss = 3.721\n",
      "Epoch  44 Batch  169/269   train_loss = 4.053\n",
      "Epoch  44 Batch  170/269   train_loss = 3.557\n",
      "Epoch  44 Batch  171/269   train_loss = 3.712\n",
      "Epoch  44 Batch  172/269   train_loss = 3.559\n",
      "Epoch  44 Batch  173/269   train_loss = 3.729\n",
      "Epoch  44 Batch  174/269   train_loss = 3.928\n",
      "Epoch  44 Batch  175/269   train_loss = 3.797\n",
      "Epoch  44 Batch  176/269   train_loss = 3.776\n",
      "Epoch  44 Batch  177/269   train_loss = 3.727\n",
      "Epoch  44 Batch  178/269   train_loss = 3.994\n",
      "Epoch  44 Batch  179/269   train_loss = 3.766\n",
      "Epoch  44 Batch  180/269   train_loss = 3.554\n",
      "Epoch  44 Batch  181/269   train_loss = 3.839\n",
      "Epoch  44 Batch  182/269   train_loss = 3.660\n",
      "Epoch  44 Batch  183/269   train_loss = 3.606\n",
      "Epoch  44 Batch  184/269   train_loss = 3.743\n",
      "Epoch  44 Batch  185/269   train_loss = 3.803\n",
      "Epoch  44 Batch  186/269   train_loss = 3.705\n",
      "Epoch  44 Batch  187/269   train_loss = 3.933\n",
      "Epoch  44 Batch  188/269   train_loss = 3.630\n",
      "Epoch  44 Batch  189/269   train_loss = 3.646\n",
      "Epoch  44 Batch  190/269   train_loss = 4.103\n",
      "Epoch  44 Batch  191/269   train_loss = 3.768\n",
      "Epoch  44 Batch  192/269   train_loss = 3.694\n",
      "Epoch  44 Batch  193/269   train_loss = 3.723\n",
      "Epoch  44 Batch  194/269   train_loss = 3.807\n",
      "Epoch  44 Batch  195/269   train_loss = 3.699\n",
      "Epoch  44 Batch  196/269   train_loss = 3.774\n",
      "Epoch  44 Batch  197/269   train_loss = 3.918\n",
      "Epoch  44 Batch  198/269   train_loss = 3.836\n",
      "Epoch  44 Batch  199/269   train_loss = 3.766\n",
      "Epoch  44 Batch  200/269   train_loss = 3.642\n",
      "Epoch  44 Batch  201/269   train_loss = 3.701\n",
      "Epoch  44 Batch  202/269   train_loss = 3.563\n",
      "Epoch  44 Batch  203/269   train_loss = 3.599\n",
      "Epoch  44 Batch  204/269   train_loss = 3.828\n",
      "Epoch  44 Batch  205/269   train_loss = 3.797\n",
      "Epoch  44 Batch  206/269   train_loss = 3.620\n",
      "Epoch  44 Batch  207/269   train_loss = 3.676\n",
      "Epoch  44 Batch  208/269   train_loss = 3.773\n",
      "Epoch  44 Batch  209/269   train_loss = 3.858\n",
      "Epoch  44 Batch  210/269   train_loss = 3.572\n",
      "Epoch  44 Batch  211/269   train_loss = 3.726\n",
      "Epoch  44 Batch  212/269   train_loss = 4.011\n",
      "Epoch  44 Batch  213/269   train_loss = 3.604\n",
      "Epoch  44 Batch  214/269   train_loss = 3.691\n",
      "Epoch  44 Batch  215/269   train_loss = 3.947\n",
      "Epoch  44 Batch  216/269   train_loss = 3.905\n",
      "Epoch  44 Batch  217/269   train_loss = 3.621\n",
      "Epoch  44 Batch  218/269   train_loss = 3.836\n",
      "Epoch  44 Batch  219/269   train_loss = 3.483\n",
      "Epoch  44 Batch  220/269   train_loss = 3.907\n",
      "Epoch  44 Batch  221/269   train_loss = 3.590\n",
      "Epoch  44 Batch  222/269   train_loss = 3.771\n",
      "Epoch  44 Batch  223/269   train_loss = 3.586\n",
      "Epoch  44 Batch  224/269   train_loss = 3.857\n",
      "Epoch  44 Batch  225/269   train_loss = 3.969\n",
      "Epoch  44 Batch  226/269   train_loss = 3.773\n",
      "Epoch  44 Batch  227/269   train_loss = 3.424\n",
      "Epoch  44 Batch  228/269   train_loss = 3.731\n",
      "Epoch  44 Batch  229/269   train_loss = 3.912\n",
      "Epoch  44 Batch  230/269   train_loss = 3.851\n",
      "Epoch  44 Batch  231/269   train_loss = 3.688\n",
      "Epoch  44 Batch  232/269   train_loss = 3.748\n",
      "Epoch  44 Batch  233/269   train_loss = 3.777\n",
      "Epoch  44 Batch  234/269   train_loss = 3.733\n",
      "Epoch  44 Batch  235/269   train_loss = 4.029\n",
      "Epoch  44 Batch  236/269   train_loss = 3.617\n",
      "Epoch  44 Batch  237/269   train_loss = 3.299\n",
      "Epoch  44 Batch  238/269   train_loss = 3.532\n",
      "Epoch  44 Batch  239/269   train_loss = 4.067\n",
      "Epoch  44 Batch  240/269   train_loss = 3.685\n",
      "Epoch  44 Batch  241/269   train_loss = 3.942\n",
      "Epoch  44 Batch  242/269   train_loss = 3.519\n",
      "Epoch  44 Batch  243/269   train_loss = 3.917\n",
      "Epoch  44 Batch  244/269   train_loss = 3.718\n",
      "Epoch  44 Batch  245/269   train_loss = 3.762\n",
      "Epoch  44 Batch  246/269   train_loss = 3.425\n",
      "Epoch  44 Batch  247/269   train_loss = 3.799\n",
      "Epoch  44 Batch  248/269   train_loss = 3.750\n",
      "Epoch  44 Batch  249/269   train_loss = 3.589\n",
      "Epoch  44 Batch  250/269   train_loss = 3.498\n",
      "Epoch  44 Batch  251/269   train_loss = 3.908\n",
      "Epoch  44 Batch  252/269   train_loss = 3.863\n",
      "Epoch  44 Batch  253/269   train_loss = 3.674\n",
      "Epoch  44 Batch  254/269   train_loss = 3.660\n",
      "Epoch  44 Batch  255/269   train_loss = 3.725\n",
      "Epoch  44 Batch  256/269   train_loss = 3.617\n",
      "Epoch  44 Batch  257/269   train_loss = 3.550\n",
      "Epoch  44 Batch  258/269   train_loss = 3.474\n",
      "Epoch  44 Batch  259/269   train_loss = 3.593\n",
      "Epoch  44 Batch  260/269   train_loss = 3.816\n",
      "Epoch  44 Batch  261/269   train_loss = 3.836\n",
      "Epoch  44 Batch  262/269   train_loss = 3.669\n",
      "Epoch  44 Batch  263/269   train_loss = 3.464\n",
      "Epoch  44 Batch  264/269   train_loss = 3.980\n",
      "Epoch  44 Batch  265/269   train_loss = 3.835\n",
      "Epoch  44 Batch  266/269   train_loss = 3.642\n",
      "Epoch  44 Batch  267/269   train_loss = 3.723\n",
      "Epoch  44 Batch  268/269   train_loss = 3.876\n",
      "Epoch  45 Batch    0/269   train_loss = 3.683\n",
      "Epoch  45 Batch    1/269   train_loss = 3.480\n",
      "Epoch  45 Batch    2/269   train_loss = 3.623\n",
      "Epoch  45 Batch    3/269   train_loss = 3.679\n",
      "Epoch  45 Batch    4/269   train_loss = 4.031\n",
      "Epoch  45 Batch    5/269   train_loss = 3.808\n",
      "Epoch  45 Batch    6/269   train_loss = 3.658\n",
      "Epoch  45 Batch    7/269   train_loss = 3.490\n",
      "Epoch  45 Batch    8/269   train_loss = 3.808\n",
      "Epoch  45 Batch    9/269   train_loss = 3.467\n",
      "Epoch  45 Batch   10/269   train_loss = 3.561\n",
      "Epoch  45 Batch   11/269   train_loss = 3.706\n",
      "Epoch  45 Batch   12/269   train_loss = 3.384\n",
      "Epoch  45 Batch   13/269   train_loss = 3.689\n",
      "Epoch  45 Batch   14/269   train_loss = 3.591\n",
      "Epoch  45 Batch   15/269   train_loss = 3.942\n",
      "Epoch  45 Batch   16/269   train_loss = 3.698\n",
      "Epoch  45 Batch   17/269   train_loss = 3.728\n",
      "Epoch  45 Batch   18/269   train_loss = 3.676\n",
      "Epoch  45 Batch   19/269   train_loss = 3.659\n",
      "Epoch  45 Batch   20/269   train_loss = 4.057\n",
      "Epoch  45 Batch   21/269   train_loss = 3.846\n",
      "Epoch  45 Batch   22/269   train_loss = 3.594\n",
      "Epoch  45 Batch   23/269   train_loss = 3.800\n",
      "Epoch  45 Batch   24/269   train_loss = 3.647\n",
      "Epoch  45 Batch   25/269   train_loss = 3.948\n",
      "Epoch  45 Batch   26/269   train_loss = 3.786\n",
      "Epoch  45 Batch   27/269   train_loss = 3.597\n",
      "Epoch  45 Batch   28/269   train_loss = 3.782\n",
      "Epoch  45 Batch   29/269   train_loss = 3.745\n",
      "Epoch  45 Batch   30/269   train_loss = 4.020\n",
      "Epoch  45 Batch   31/269   train_loss = 3.674\n",
      "Epoch  45 Batch   32/269   train_loss = 3.606\n",
      "Epoch  45 Batch   33/269   train_loss = 3.410\n",
      "Epoch  45 Batch   34/269   train_loss = 3.666\n",
      "Epoch  45 Batch   35/269   train_loss = 3.548\n",
      "Epoch  45 Batch   36/269   train_loss = 3.520\n",
      "Epoch  45 Batch   37/269   train_loss = 3.765\n",
      "Epoch  45 Batch   38/269   train_loss = 3.523\n",
      "Epoch  45 Batch   39/269   train_loss = 3.844\n",
      "Epoch  45 Batch   40/269   train_loss = 3.560\n",
      "Epoch  45 Batch   41/269   train_loss = 3.579\n",
      "Epoch  45 Batch   42/269   train_loss = 4.064\n",
      "Epoch  45 Batch   43/269   train_loss = 3.915\n",
      "Epoch  45 Batch   44/269   train_loss = 3.690\n",
      "Epoch  45 Batch   45/269   train_loss = 3.414\n",
      "Epoch  45 Batch   46/269   train_loss = 3.799\n",
      "Epoch  45 Batch   47/269   train_loss = 3.454\n",
      "Epoch  45 Batch   48/269   train_loss = 3.938\n",
      "Epoch  45 Batch   49/269   train_loss = 3.942\n",
      "Epoch  45 Batch   50/269   train_loss = 3.840\n",
      "Epoch  45 Batch   51/269   train_loss = 3.770\n",
      "Epoch  45 Batch   52/269   train_loss = 3.785\n",
      "Epoch  45 Batch   53/269   train_loss = 3.938\n",
      "Epoch  45 Batch   54/269   train_loss = 3.758\n",
      "Epoch  45 Batch   55/269   train_loss = 3.859\n",
      "Epoch  45 Batch   56/269   train_loss = 3.647\n",
      "Epoch  45 Batch   57/269   train_loss = 3.881\n",
      "Epoch  45 Batch   58/269   train_loss = 3.570\n",
      "Epoch  45 Batch   59/269   train_loss = 3.639\n",
      "Epoch  45 Batch   60/269   train_loss = 3.758\n",
      "Epoch  45 Batch   61/269   train_loss = 4.015\n",
      "Epoch  45 Batch   62/269   train_loss = 3.588\n",
      "Epoch  45 Batch   63/269   train_loss = 3.904\n",
      "Epoch  45 Batch   64/269   train_loss = 3.633\n",
      "Epoch  45 Batch   65/269   train_loss = 3.829\n",
      "Epoch  45 Batch   66/269   train_loss = 3.684\n",
      "Epoch  45 Batch   67/269   train_loss = 3.679\n",
      "Epoch  45 Batch   68/269   train_loss = 3.783\n",
      "Epoch  45 Batch   69/269   train_loss = 3.811\n",
      "Epoch  45 Batch   70/269   train_loss = 3.437\n",
      "Epoch  45 Batch   71/269   train_loss = 3.808\n",
      "Epoch  45 Batch   72/269   train_loss = 3.491\n",
      "Epoch  45 Batch   73/269   train_loss = 3.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45 Batch   74/269   train_loss = 3.662\n",
      "Epoch  45 Batch   75/269   train_loss = 3.833\n",
      "Epoch  45 Batch   76/269   train_loss = 3.839\n",
      "Epoch  45 Batch   77/269   train_loss = 3.440\n",
      "Epoch  45 Batch   78/269   train_loss = 3.846\n",
      "Epoch  45 Batch   79/269   train_loss = 3.940\n",
      "Epoch  45 Batch   80/269   train_loss = 3.852\n",
      "Epoch  45 Batch   81/269   train_loss = 3.662\n",
      "Epoch  45 Batch   82/269   train_loss = 3.895\n",
      "Epoch  45 Batch   83/269   train_loss = 3.628\n",
      "Epoch  45 Batch   84/269   train_loss = 3.941\n",
      "Epoch  45 Batch   85/269   train_loss = 4.102\n",
      "Epoch  45 Batch   86/269   train_loss = 4.070\n",
      "Epoch  45 Batch   87/269   train_loss = 3.704\n",
      "Epoch  45 Batch   88/269   train_loss = 3.817\n",
      "Epoch  45 Batch   89/269   train_loss = 3.762\n",
      "Epoch  45 Batch   90/269   train_loss = 3.565\n",
      "Epoch  45 Batch   91/269   train_loss = 3.867\n",
      "Epoch  45 Batch   92/269   train_loss = 3.886\n",
      "Epoch  45 Batch   93/269   train_loss = 3.577\n",
      "Epoch  45 Batch   94/269   train_loss = 4.052\n",
      "Epoch  45 Batch   95/269   train_loss = 3.512\n",
      "Epoch  45 Batch   96/269   train_loss = 3.675\n",
      "Epoch  45 Batch   97/269   train_loss = 3.698\n",
      "Epoch  45 Batch   98/269   train_loss = 3.919\n",
      "Epoch  45 Batch   99/269   train_loss = 3.736\n",
      "Epoch  45 Batch  100/269   train_loss = 3.728\n",
      "Epoch  45 Batch  101/269   train_loss = 3.910\n",
      "Epoch  45 Batch  102/269   train_loss = 3.565\n",
      "Epoch  45 Batch  103/269   train_loss = 3.633\n",
      "Epoch  45 Batch  104/269   train_loss = 3.597\n",
      "Epoch  45 Batch  105/269   train_loss = 3.909\n",
      "Epoch  45 Batch  106/269   train_loss = 3.958\n",
      "Epoch  45 Batch  107/269   train_loss = 4.002\n",
      "Epoch  45 Batch  108/269   train_loss = 3.741\n",
      "Epoch  45 Batch  109/269   train_loss = 3.807\n",
      "Epoch  45 Batch  110/269   train_loss = 3.741\n",
      "Epoch  45 Batch  111/269   train_loss = 3.926\n",
      "Epoch  45 Batch  112/269   train_loss = 3.910\n",
      "Epoch  45 Batch  113/269   train_loss = 3.738\n",
      "Epoch  45 Batch  114/269   train_loss = 3.759\n",
      "Epoch  45 Batch  115/269   train_loss = 3.568\n",
      "Epoch  45 Batch  116/269   train_loss = 3.603\n",
      "Epoch  45 Batch  117/269   train_loss = 3.561\n",
      "Epoch  45 Batch  118/269   train_loss = 3.932\n",
      "Epoch  45 Batch  119/269   train_loss = 3.522\n",
      "Epoch  45 Batch  120/269   train_loss = 3.830\n",
      "Epoch  45 Batch  121/269   train_loss = 3.675\n",
      "Epoch  45 Batch  122/269   train_loss = 3.792\n",
      "Epoch  45 Batch  123/269   train_loss = 3.578\n",
      "Epoch  45 Batch  124/269   train_loss = 3.914\n",
      "Epoch  45 Batch  125/269   train_loss = 3.502\n",
      "Epoch  45 Batch  126/269   train_loss = 3.445\n",
      "Epoch  45 Batch  127/269   train_loss = 3.670\n",
      "Epoch  45 Batch  128/269   train_loss = 3.685\n",
      "Epoch  45 Batch  129/269   train_loss = 3.703\n",
      "Epoch  45 Batch  130/269   train_loss = 3.833\n",
      "Epoch  45 Batch  131/269   train_loss = 3.384\n",
      "Epoch  45 Batch  132/269   train_loss = 3.736\n",
      "Epoch  45 Batch  133/269   train_loss = 3.525\n",
      "Epoch  45 Batch  134/269   train_loss = 3.921\n",
      "Epoch  45 Batch  135/269   train_loss = 3.583\n",
      "Epoch  45 Batch  136/269   train_loss = 3.494\n",
      "Epoch  45 Batch  137/269   train_loss = 3.843\n",
      "Epoch  45 Batch  138/269   train_loss = 3.608\n",
      "Epoch  45 Batch  139/269   train_loss = 3.785\n",
      "Epoch  45 Batch  140/269   train_loss = 3.796\n",
      "Epoch  45 Batch  141/269   train_loss = 4.056\n",
      "Epoch  45 Batch  142/269   train_loss = 3.520\n",
      "Epoch  45 Batch  143/269   train_loss = 3.726\n",
      "Epoch  45 Batch  144/269   train_loss = 3.864\n",
      "Epoch  45 Batch  145/269   train_loss = 3.647\n",
      "Epoch  45 Batch  146/269   train_loss = 3.952\n",
      "Epoch  45 Batch  147/269   train_loss = 3.515\n",
      "Epoch  45 Batch  148/269   train_loss = 3.831\n",
      "Epoch  45 Batch  149/269   train_loss = 3.693\n",
      "Epoch  45 Batch  150/269   train_loss = 3.840\n",
      "Epoch  45 Batch  151/269   train_loss = 3.759\n",
      "Epoch  45 Batch  152/269   train_loss = 3.695\n",
      "Epoch  45 Batch  153/269   train_loss = 3.760\n",
      "Epoch  45 Batch  154/269   train_loss = 3.608\n",
      "Epoch  45 Batch  155/269   train_loss = 3.638\n",
      "Epoch  45 Batch  156/269   train_loss = 4.151\n",
      "Epoch  45 Batch  157/269   train_loss = 3.808\n",
      "Epoch  45 Batch  158/269   train_loss = 3.855\n",
      "Epoch  45 Batch  159/269   train_loss = 3.772\n",
      "Epoch  45 Batch  160/269   train_loss = 3.689\n",
      "Epoch  45 Batch  161/269   train_loss = 3.853\n",
      "Epoch  45 Batch  162/269   train_loss = 3.805\n",
      "Epoch  45 Batch  163/269   train_loss = 3.685\n",
      "Epoch  45 Batch  164/269   train_loss = 3.947\n",
      "Epoch  45 Batch  165/269   train_loss = 3.895\n",
      "Epoch  45 Batch  166/269   train_loss = 3.748\n",
      "Epoch  45 Batch  167/269   train_loss = 3.641\n",
      "Epoch  45 Batch  168/269   train_loss = 3.705\n",
      "Epoch  45 Batch  169/269   train_loss = 4.048\n",
      "Epoch  45 Batch  170/269   train_loss = 3.578\n",
      "Epoch  45 Batch  171/269   train_loss = 3.739\n",
      "Epoch  45 Batch  172/269   train_loss = 3.558\n",
      "Epoch  45 Batch  173/269   train_loss = 3.748\n",
      "Epoch  45 Batch  174/269   train_loss = 3.926\n",
      "Epoch  45 Batch  175/269   train_loss = 3.824\n",
      "Epoch  45 Batch  176/269   train_loss = 3.758\n",
      "Epoch  45 Batch  177/269   train_loss = 3.705\n",
      "Epoch  45 Batch  178/269   train_loss = 4.004\n",
      "Epoch  45 Batch  179/269   train_loss = 3.803\n",
      "Epoch  45 Batch  180/269   train_loss = 3.567\n",
      "Epoch  45 Batch  181/269   train_loss = 3.861\n",
      "Epoch  45 Batch  182/269   train_loss = 3.668\n",
      "Epoch  45 Batch  183/269   train_loss = 3.601\n",
      "Epoch  45 Batch  184/269   train_loss = 3.753\n",
      "Epoch  45 Batch  185/269   train_loss = 3.792\n",
      "Epoch  45 Batch  186/269   train_loss = 3.690\n",
      "Epoch  45 Batch  187/269   train_loss = 3.906\n",
      "Epoch  45 Batch  188/269   train_loss = 3.651\n",
      "Epoch  45 Batch  189/269   train_loss = 3.655\n",
      "Epoch  45 Batch  190/269   train_loss = 4.122\n",
      "Epoch  45 Batch  191/269   train_loss = 3.784\n",
      "Epoch  45 Batch  192/269   train_loss = 3.698\n",
      "Epoch  45 Batch  193/269   train_loss = 3.710\n",
      "Epoch  45 Batch  194/269   train_loss = 3.789\n",
      "Epoch  45 Batch  195/269   train_loss = 3.697\n",
      "Epoch  45 Batch  196/269   train_loss = 3.785\n",
      "Epoch  45 Batch  197/269   train_loss = 3.890\n",
      "Epoch  45 Batch  198/269   train_loss = 3.795\n",
      "Epoch  45 Batch  199/269   train_loss = 3.786\n",
      "Epoch  45 Batch  200/269   train_loss = 3.617\n",
      "Epoch  45 Batch  201/269   train_loss = 3.701\n",
      "Epoch  45 Batch  202/269   train_loss = 3.556\n",
      "Epoch  45 Batch  203/269   train_loss = 3.584\n",
      "Epoch  45 Batch  204/269   train_loss = 3.818\n",
      "Epoch  45 Batch  205/269   train_loss = 3.781\n",
      "Epoch  45 Batch  206/269   train_loss = 3.621\n",
      "Epoch  45 Batch  207/269   train_loss = 3.647\n",
      "Epoch  45 Batch  208/269   train_loss = 3.732\n",
      "Epoch  45 Batch  209/269   train_loss = 3.882\n",
      "Epoch  45 Batch  210/269   train_loss = 3.572\n",
      "Epoch  45 Batch  211/269   train_loss = 3.729\n",
      "Epoch  45 Batch  212/269   train_loss = 4.026\n",
      "Epoch  45 Batch  213/269   train_loss = 3.581\n",
      "Epoch  45 Batch  214/269   train_loss = 3.695\n",
      "Epoch  45 Batch  215/269   train_loss = 3.935\n",
      "Epoch  45 Batch  216/269   train_loss = 3.891\n",
      "Epoch  45 Batch  217/269   train_loss = 3.618\n",
      "Epoch  45 Batch  218/269   train_loss = 3.852\n",
      "Epoch  45 Batch  219/269   train_loss = 3.467\n",
      "Epoch  45 Batch  220/269   train_loss = 3.984\n",
      "Epoch  45 Batch  221/269   train_loss = 3.598\n",
      "Epoch  45 Batch  222/269   train_loss = 3.828\n",
      "Epoch  45 Batch  223/269   train_loss = 3.615\n",
      "Epoch  45 Batch  224/269   train_loss = 3.859\n",
      "Epoch  45 Batch  225/269   train_loss = 3.999\n",
      "Epoch  45 Batch  226/269   train_loss = 3.788\n",
      "Epoch  45 Batch  227/269   train_loss = 3.440\n",
      "Epoch  45 Batch  228/269   train_loss = 3.699\n",
      "Epoch  45 Batch  229/269   train_loss = 3.921\n",
      "Epoch  45 Batch  230/269   train_loss = 3.842\n",
      "Epoch  45 Batch  231/269   train_loss = 3.671\n",
      "Epoch  45 Batch  232/269   train_loss = 3.737\n",
      "Epoch  45 Batch  233/269   train_loss = 3.782\n",
      "Epoch  45 Batch  234/269   train_loss = 3.738\n",
      "Epoch  45 Batch  235/269   train_loss = 3.987\n",
      "Epoch  45 Batch  236/269   train_loss = 3.593\n",
      "Epoch  45 Batch  237/269   train_loss = 3.302\n",
      "Epoch  45 Batch  238/269   train_loss = 3.525\n",
      "Epoch  45 Batch  239/269   train_loss = 4.069\n",
      "Epoch  45 Batch  240/269   train_loss = 3.657\n",
      "Epoch  45 Batch  241/269   train_loss = 3.960\n",
      "Epoch  45 Batch  242/269   train_loss = 3.510\n",
      "Epoch  45 Batch  243/269   train_loss = 3.916\n",
      "Epoch  45 Batch  244/269   train_loss = 3.728\n",
      "Epoch  45 Batch  245/269   train_loss = 3.788\n",
      "Epoch  45 Batch  246/269   train_loss = 3.438\n",
      "Epoch  45 Batch  247/269   train_loss = 3.799\n",
      "Epoch  45 Batch  248/269   train_loss = 3.759\n",
      "Epoch  45 Batch  249/269   train_loss = 3.605\n",
      "Epoch  45 Batch  250/269   train_loss = 3.493\n",
      "Epoch  45 Batch  251/269   train_loss = 3.951\n",
      "Epoch  45 Batch  252/269   train_loss = 3.867\n",
      "Epoch  45 Batch  253/269   train_loss = 3.667\n",
      "Epoch  45 Batch  254/269   train_loss = 3.662\n",
      "Epoch  45 Batch  255/269   train_loss = 3.715\n",
      "Epoch  45 Batch  256/269   train_loss = 3.642\n",
      "Epoch  45 Batch  257/269   train_loss = 3.582\n",
      "Epoch  45 Batch  258/269   train_loss = 3.457\n",
      "Epoch  45 Batch  259/269   train_loss = 3.597\n",
      "Epoch  45 Batch  260/269   train_loss = 3.801\n",
      "Epoch  45 Batch  261/269   train_loss = 3.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45 Batch  262/269   train_loss = 3.641\n",
      "Epoch  45 Batch  263/269   train_loss = 3.475\n",
      "Epoch  45 Batch  264/269   train_loss = 3.960\n",
      "Epoch  45 Batch  265/269   train_loss = 3.817\n",
      "Epoch  45 Batch  266/269   train_loss = 3.613\n",
      "Epoch  45 Batch  267/269   train_loss = 3.734\n",
      "Epoch  45 Batch  268/269   train_loss = 3.877\n",
      "Epoch  46 Batch    0/269   train_loss = 3.705\n",
      "Epoch  46 Batch    1/269   train_loss = 3.492\n",
      "Epoch  46 Batch    2/269   train_loss = 3.645\n",
      "Epoch  46 Batch    3/269   train_loss = 3.667\n",
      "Epoch  46 Batch    4/269   train_loss = 4.040\n",
      "Epoch  46 Batch    5/269   train_loss = 3.779\n",
      "Epoch  46 Batch    6/269   train_loss = 3.637\n",
      "Epoch  46 Batch    7/269   train_loss = 3.477\n",
      "Epoch  46 Batch    8/269   train_loss = 3.809\n",
      "Epoch  46 Batch    9/269   train_loss = 3.460\n",
      "Epoch  46 Batch   10/269   train_loss = 3.550\n",
      "Epoch  46 Batch   11/269   train_loss = 3.684\n",
      "Epoch  46 Batch   12/269   train_loss = 3.369\n",
      "Epoch  46 Batch   13/269   train_loss = 3.696\n",
      "Epoch  46 Batch   14/269   train_loss = 3.564\n",
      "Epoch  46 Batch   15/269   train_loss = 3.959\n",
      "Epoch  46 Batch   16/269   train_loss = 3.678\n",
      "Epoch  46 Batch   17/269   train_loss = 3.743\n",
      "Epoch  46 Batch   18/269   train_loss = 3.645\n",
      "Epoch  46 Batch   19/269   train_loss = 3.633\n",
      "Epoch  46 Batch   20/269   train_loss = 4.026\n",
      "Epoch  46 Batch   21/269   train_loss = 3.880\n",
      "Epoch  46 Batch   22/269   train_loss = 3.581\n",
      "Epoch  46 Batch   23/269   train_loss = 3.798\n",
      "Epoch  46 Batch   24/269   train_loss = 3.598\n",
      "Epoch  46 Batch   25/269   train_loss = 3.907\n",
      "Epoch  46 Batch   26/269   train_loss = 3.813\n",
      "Epoch  46 Batch   27/269   train_loss = 3.603\n",
      "Epoch  46 Batch   28/269   train_loss = 3.766\n",
      "Epoch  46 Batch   29/269   train_loss = 3.747\n",
      "Epoch  46 Batch   30/269   train_loss = 4.040\n",
      "Epoch  46 Batch   31/269   train_loss = 3.725\n",
      "Epoch  46 Batch   32/269   train_loss = 3.617\n",
      "Epoch  46 Batch   33/269   train_loss = 3.420\n",
      "Epoch  46 Batch   34/269   train_loss = 3.652\n",
      "Epoch  46 Batch   35/269   train_loss = 3.535\n",
      "Epoch  46 Batch   36/269   train_loss = 3.532\n",
      "Epoch  46 Batch   37/269   train_loss = 3.726\n",
      "Epoch  46 Batch   38/269   train_loss = 3.480\n",
      "Epoch  46 Batch   39/269   train_loss = 3.796\n",
      "Epoch  46 Batch   40/269   train_loss = 3.537\n",
      "Epoch  46 Batch   41/269   train_loss = 3.574\n",
      "Epoch  46 Batch   42/269   train_loss = 4.072\n",
      "Epoch  46 Batch   43/269   train_loss = 3.970\n",
      "Epoch  46 Batch   44/269   train_loss = 3.687\n",
      "Epoch  46 Batch   45/269   train_loss = 3.413\n",
      "Epoch  46 Batch   46/269   train_loss = 3.833\n",
      "Epoch  46 Batch   47/269   train_loss = 3.438\n",
      "Epoch  46 Batch   48/269   train_loss = 3.917\n",
      "Epoch  46 Batch   49/269   train_loss = 3.936\n",
      "Epoch  46 Batch   50/269   train_loss = 3.832\n",
      "Epoch  46 Batch   51/269   train_loss = 3.759\n",
      "Epoch  46 Batch   52/269   train_loss = 3.762\n",
      "Epoch  46 Batch   53/269   train_loss = 3.936\n",
      "Epoch  46 Batch   54/269   train_loss = 3.761\n",
      "Epoch  46 Batch   55/269   train_loss = 3.852\n",
      "Epoch  46 Batch   56/269   train_loss = 3.633\n",
      "Epoch  46 Batch   57/269   train_loss = 3.882\n",
      "Epoch  46 Batch   58/269   train_loss = 3.538\n",
      "Epoch  46 Batch   59/269   train_loss = 3.638\n",
      "Epoch  46 Batch   60/269   train_loss = 3.765\n",
      "Epoch  46 Batch   61/269   train_loss = 3.988\n",
      "Epoch  46 Batch   62/269   train_loss = 3.579\n",
      "Epoch  46 Batch   63/269   train_loss = 3.945\n",
      "Epoch  46 Batch   64/269   train_loss = 3.637\n",
      "Epoch  46 Batch   65/269   train_loss = 3.839\n",
      "Epoch  46 Batch   66/269   train_loss = 3.715\n",
      "Epoch  46 Batch   67/269   train_loss = 3.670\n",
      "Epoch  46 Batch   68/269   train_loss = 3.794\n",
      "Epoch  46 Batch   69/269   train_loss = 3.809\n",
      "Epoch  46 Batch   70/269   train_loss = 3.436\n",
      "Epoch  46 Batch   71/269   train_loss = 3.828\n",
      "Epoch  46 Batch   72/269   train_loss = 3.484\n",
      "Epoch  46 Batch   73/269   train_loss = 3.682\n",
      "Epoch  46 Batch   74/269   train_loss = 3.660\n",
      "Epoch  46 Batch   75/269   train_loss = 3.851\n",
      "Epoch  46 Batch   76/269   train_loss = 3.848\n",
      "Epoch  46 Batch   77/269   train_loss = 3.449\n",
      "Epoch  46 Batch   78/269   train_loss = 3.846\n",
      "Epoch  46 Batch   79/269   train_loss = 3.981\n",
      "Epoch  46 Batch   80/269   train_loss = 3.849\n",
      "Epoch  46 Batch   81/269   train_loss = 3.651\n",
      "Epoch  46 Batch   82/269   train_loss = 3.889\n",
      "Epoch  46 Batch   83/269   train_loss = 3.651\n",
      "Epoch  46 Batch   84/269   train_loss = 3.920\n",
      "Epoch  46 Batch   85/269   train_loss = 4.073\n",
      "Epoch  46 Batch   86/269   train_loss = 4.058\n",
      "Epoch  46 Batch   87/269   train_loss = 3.719\n",
      "Epoch  46 Batch   88/269   train_loss = 3.818\n",
      "Epoch  46 Batch   89/269   train_loss = 3.787\n",
      "Epoch  46 Batch   90/269   train_loss = 3.598\n",
      "Epoch  46 Batch   91/269   train_loss = 3.889\n",
      "Epoch  46 Batch   92/269   train_loss = 3.868\n",
      "Epoch  46 Batch   93/269   train_loss = 3.567\n",
      "Epoch  46 Batch   94/269   train_loss = 4.069\n",
      "Epoch  46 Batch   95/269   train_loss = 3.497\n",
      "Epoch  46 Batch   96/269   train_loss = 3.666\n",
      "Epoch  46 Batch   97/269   train_loss = 3.697\n",
      "Epoch  46 Batch   98/269   train_loss = 3.913\n",
      "Epoch  46 Batch   99/269   train_loss = 3.689\n",
      "Epoch  46 Batch  100/269   train_loss = 3.712\n",
      "Epoch  46 Batch  101/269   train_loss = 3.882\n",
      "Epoch  46 Batch  102/269   train_loss = 3.577\n",
      "Epoch  46 Batch  103/269   train_loss = 3.639\n",
      "Epoch  46 Batch  104/269   train_loss = 3.598\n",
      "Epoch  46 Batch  105/269   train_loss = 3.897\n",
      "Epoch  46 Batch  106/269   train_loss = 3.963\n",
      "Epoch  46 Batch  107/269   train_loss = 3.987\n",
      "Epoch  46 Batch  108/269   train_loss = 3.758\n",
      "Epoch  46 Batch  109/269   train_loss = 3.788\n",
      "Epoch  46 Batch  110/269   train_loss = 3.734\n",
      "Epoch  46 Batch  111/269   train_loss = 3.868\n",
      "Epoch  46 Batch  112/269   train_loss = 3.885\n",
      "Epoch  46 Batch  113/269   train_loss = 3.770\n",
      "Epoch  46 Batch  114/269   train_loss = 3.754\n",
      "Epoch  46 Batch  115/269   train_loss = 3.576\n",
      "Epoch  46 Batch  116/269   train_loss = 3.599\n",
      "Epoch  46 Batch  117/269   train_loss = 3.609\n",
      "Epoch  46 Batch  118/269   train_loss = 3.956\n",
      "Epoch  46 Batch  119/269   train_loss = 3.506\n",
      "Epoch  46 Batch  120/269   train_loss = 3.823\n",
      "Epoch  46 Batch  121/269   train_loss = 3.644\n",
      "Epoch  46 Batch  122/269   train_loss = 3.757\n",
      "Epoch  46 Batch  123/269   train_loss = 3.594\n",
      "Epoch  46 Batch  124/269   train_loss = 3.922\n",
      "Epoch  46 Batch  125/269   train_loss = 3.521\n",
      "Epoch  46 Batch  126/269   train_loss = 3.424\n",
      "Epoch  46 Batch  127/269   train_loss = 3.688\n",
      "Epoch  46 Batch  128/269   train_loss = 3.667\n",
      "Epoch  46 Batch  129/269   train_loss = 3.673\n",
      "Epoch  46 Batch  130/269   train_loss = 3.802\n",
      "Epoch  46 Batch  131/269   train_loss = 3.375\n",
      "Epoch  46 Batch  132/269   train_loss = 3.732\n",
      "Epoch  46 Batch  133/269   train_loss = 3.522\n",
      "Epoch  46 Batch  134/269   train_loss = 3.956\n",
      "Epoch  46 Batch  135/269   train_loss = 3.588\n",
      "Epoch  46 Batch  136/269   train_loss = 3.495\n",
      "Epoch  46 Batch  137/269   train_loss = 3.810\n",
      "Epoch  46 Batch  138/269   train_loss = 3.614\n",
      "Epoch  46 Batch  139/269   train_loss = 3.764\n",
      "Epoch  46 Batch  140/269   train_loss = 3.810\n",
      "Epoch  46 Batch  141/269   train_loss = 4.070\n",
      "Epoch  46 Batch  142/269   train_loss = 3.527\n",
      "Epoch  46 Batch  143/269   train_loss = 3.728\n",
      "Epoch  46 Batch  144/269   train_loss = 3.903\n",
      "Epoch  46 Batch  145/269   train_loss = 3.671\n",
      "Epoch  46 Batch  146/269   train_loss = 3.955\n",
      "Epoch  46 Batch  147/269   train_loss = 3.509\n",
      "Epoch  46 Batch  148/269   train_loss = 3.833\n",
      "Epoch  46 Batch  149/269   train_loss = 3.684\n",
      "Epoch  46 Batch  150/269   train_loss = 3.819\n",
      "Epoch  46 Batch  151/269   train_loss = 3.765\n",
      "Epoch  46 Batch  152/269   train_loss = 3.725\n",
      "Epoch  46 Batch  153/269   train_loss = 3.757\n",
      "Epoch  46 Batch  154/269   train_loss = 3.626\n",
      "Epoch  46 Batch  155/269   train_loss = 3.637\n",
      "Epoch  46 Batch  156/269   train_loss = 4.124\n",
      "Epoch  46 Batch  157/269   train_loss = 3.784\n",
      "Epoch  46 Batch  158/269   train_loss = 3.848\n",
      "Epoch  46 Batch  159/269   train_loss = 3.796\n",
      "Epoch  46 Batch  160/269   train_loss = 3.654\n",
      "Epoch  46 Batch  161/269   train_loss = 3.816\n",
      "Epoch  46 Batch  162/269   train_loss = 3.805\n",
      "Epoch  46 Batch  163/269   train_loss = 3.705\n",
      "Epoch  46 Batch  164/269   train_loss = 3.908\n",
      "Epoch  46 Batch  165/269   train_loss = 3.871\n",
      "Epoch  46 Batch  166/269   train_loss = 3.763\n",
      "Epoch  46 Batch  167/269   train_loss = 3.638\n",
      "Epoch  46 Batch  168/269   train_loss = 3.713\n",
      "Epoch  46 Batch  169/269   train_loss = 4.054\n",
      "Epoch  46 Batch  170/269   train_loss = 3.551\n",
      "Epoch  46 Batch  171/269   train_loss = 3.730\n",
      "Epoch  46 Batch  172/269   train_loss = 3.568\n",
      "Epoch  46 Batch  173/269   train_loss = 3.777\n",
      "Epoch  46 Batch  174/269   train_loss = 3.926\n",
      "Epoch  46 Batch  175/269   train_loss = 3.832\n",
      "Epoch  46 Batch  176/269   train_loss = 3.769\n",
      "Epoch  46 Batch  177/269   train_loss = 3.711\n",
      "Epoch  46 Batch  178/269   train_loss = 4.010\n",
      "Epoch  46 Batch  179/269   train_loss = 3.754\n",
      "Epoch  46 Batch  180/269   train_loss = 3.588\n",
      "Epoch  46 Batch  181/269   train_loss = 3.851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  46 Batch  182/269   train_loss = 3.673\n",
      "Epoch  46 Batch  183/269   train_loss = 3.611\n",
      "Epoch  46 Batch  184/269   train_loss = 3.732\n",
      "Epoch  46 Batch  185/269   train_loss = 3.800\n",
      "Epoch  46 Batch  186/269   train_loss = 3.692\n",
      "Epoch  46 Batch  187/269   train_loss = 3.940\n",
      "Epoch  46 Batch  188/269   train_loss = 3.656\n",
      "Epoch  46 Batch  189/269   train_loss = 3.678\n",
      "Epoch  46 Batch  190/269   train_loss = 4.115\n",
      "Epoch  46 Batch  191/269   train_loss = 3.778\n",
      "Epoch  46 Batch  192/269   train_loss = 3.718\n",
      "Epoch  46 Batch  193/269   train_loss = 3.730\n",
      "Epoch  46 Batch  194/269   train_loss = 3.775\n",
      "Epoch  46 Batch  195/269   train_loss = 3.695\n",
      "Epoch  46 Batch  196/269   train_loss = 3.760\n",
      "Epoch  46 Batch  197/269   train_loss = 3.901\n",
      "Epoch  46 Batch  198/269   train_loss = 3.807\n",
      "Epoch  46 Batch  199/269   train_loss = 3.796\n",
      "Epoch  46 Batch  200/269   train_loss = 3.628\n",
      "Epoch  46 Batch  201/269   train_loss = 3.738\n",
      "Epoch  46 Batch  202/269   train_loss = 3.556\n",
      "Epoch  46 Batch  203/269   train_loss = 3.614\n",
      "Epoch  46 Batch  204/269   train_loss = 3.836\n",
      "Epoch  46 Batch  205/269   train_loss = 3.791\n",
      "Epoch  46 Batch  206/269   train_loss = 3.623\n",
      "Epoch  46 Batch  207/269   train_loss = 3.658\n",
      "Epoch  46 Batch  208/269   train_loss = 3.748\n",
      "Epoch  46 Batch  209/269   train_loss = 3.872\n",
      "Epoch  46 Batch  210/269   train_loss = 3.564\n",
      "Epoch  46 Batch  211/269   train_loss = 3.752\n",
      "Epoch  46 Batch  212/269   train_loss = 4.006\n",
      "Epoch  46 Batch  213/269   train_loss = 3.561\n",
      "Epoch  46 Batch  214/269   train_loss = 3.675\n",
      "Epoch  46 Batch  215/269   train_loss = 3.967\n",
      "Epoch  46 Batch  216/269   train_loss = 3.884\n",
      "Epoch  46 Batch  217/269   train_loss = 3.629\n",
      "Epoch  46 Batch  218/269   train_loss = 3.827\n",
      "Epoch  46 Batch  219/269   train_loss = 3.503\n",
      "Epoch  46 Batch  220/269   train_loss = 3.895\n",
      "Epoch  46 Batch  221/269   train_loss = 3.596\n",
      "Epoch  46 Batch  222/269   train_loss = 3.815\n",
      "Epoch  46 Batch  223/269   train_loss = 3.611\n",
      "Epoch  46 Batch  224/269   train_loss = 3.828\n",
      "Epoch  46 Batch  225/269   train_loss = 3.979\n",
      "Epoch  46 Batch  226/269   train_loss = 3.759\n",
      "Epoch  46 Batch  227/269   train_loss = 3.470\n",
      "Epoch  46 Batch  228/269   train_loss = 3.702\n",
      "Epoch  46 Batch  229/269   train_loss = 3.919\n",
      "Epoch  46 Batch  230/269   train_loss = 3.840\n",
      "Epoch  46 Batch  231/269   train_loss = 3.705\n",
      "Epoch  46 Batch  232/269   train_loss = 3.727\n",
      "Epoch  46 Batch  233/269   train_loss = 3.796\n",
      "Epoch  46 Batch  234/269   train_loss = 3.718\n",
      "Epoch  46 Batch  235/269   train_loss = 3.980\n",
      "Epoch  46 Batch  236/269   train_loss = 3.618\n",
      "Epoch  46 Batch  237/269   train_loss = 3.324\n",
      "Epoch  46 Batch  238/269   train_loss = 3.526\n",
      "Epoch  46 Batch  239/269   train_loss = 4.062\n",
      "Epoch  46 Batch  240/269   train_loss = 3.641\n",
      "Epoch  46 Batch  241/269   train_loss = 3.963\n",
      "Epoch  46 Batch  242/269   train_loss = 3.538\n",
      "Epoch  46 Batch  243/269   train_loss = 3.932\n",
      "Epoch  46 Batch  244/269   train_loss = 3.718\n",
      "Epoch  46 Batch  245/269   train_loss = 3.772\n",
      "Epoch  46 Batch  246/269   train_loss = 3.397\n",
      "Epoch  46 Batch  247/269   train_loss = 3.784\n",
      "Epoch  46 Batch  248/269   train_loss = 3.776\n",
      "Epoch  46 Batch  249/269   train_loss = 3.590\n",
      "Epoch  46 Batch  250/269   train_loss = 3.487\n",
      "Epoch  46 Batch  251/269   train_loss = 3.949\n",
      "Epoch  46 Batch  252/269   train_loss = 3.875\n",
      "Epoch  46 Batch  253/269   train_loss = 3.663\n",
      "Epoch  46 Batch  254/269   train_loss = 3.636\n",
      "Epoch  46 Batch  255/269   train_loss = 3.714\n",
      "Epoch  46 Batch  256/269   train_loss = 3.638\n",
      "Epoch  46 Batch  257/269   train_loss = 3.568\n",
      "Epoch  46 Batch  258/269   train_loss = 3.477\n",
      "Epoch  46 Batch  259/269   train_loss = 3.615\n",
      "Epoch  46 Batch  260/269   train_loss = 3.817\n",
      "Epoch  46 Batch  261/269   train_loss = 3.825\n",
      "Epoch  46 Batch  262/269   train_loss = 3.641\n",
      "Epoch  46 Batch  263/269   train_loss = 3.445\n",
      "Epoch  46 Batch  264/269   train_loss = 3.960\n",
      "Epoch  46 Batch  265/269   train_loss = 3.851\n",
      "Epoch  46 Batch  266/269   train_loss = 3.607\n",
      "Epoch  46 Batch  267/269   train_loss = 3.736\n",
      "Epoch  46 Batch  268/269   train_loss = 3.852\n",
      "Epoch  47 Batch    0/269   train_loss = 3.695\n",
      "Epoch  47 Batch    1/269   train_loss = 3.486\n",
      "Epoch  47 Batch    2/269   train_loss = 3.637\n",
      "Epoch  47 Batch    3/269   train_loss = 3.656\n",
      "Epoch  47 Batch    4/269   train_loss = 4.041\n",
      "Epoch  47 Batch    5/269   train_loss = 3.755\n",
      "Epoch  47 Batch    6/269   train_loss = 3.615\n",
      "Epoch  47 Batch    7/269   train_loss = 3.459\n",
      "Epoch  47 Batch    8/269   train_loss = 3.784\n",
      "Epoch  47 Batch    9/269   train_loss = 3.469\n",
      "Epoch  47 Batch   10/269   train_loss = 3.535\n",
      "Epoch  47 Batch   11/269   train_loss = 3.661\n",
      "Epoch  47 Batch   12/269   train_loss = 3.347\n",
      "Epoch  47 Batch   13/269   train_loss = 3.744\n",
      "Epoch  47 Batch   14/269   train_loss = 3.558\n",
      "Epoch  47 Batch   15/269   train_loss = 3.967\n",
      "Epoch  47 Batch   16/269   train_loss = 3.671\n",
      "Epoch  47 Batch   17/269   train_loss = 3.737\n",
      "Epoch  47 Batch   18/269   train_loss = 3.654\n",
      "Epoch  47 Batch   19/269   train_loss = 3.637\n",
      "Epoch  47 Batch   20/269   train_loss = 4.016\n",
      "Epoch  47 Batch   21/269   train_loss = 3.873\n",
      "Epoch  47 Batch   22/269   train_loss = 3.576\n",
      "Epoch  47 Batch   23/269   train_loss = 3.803\n",
      "Epoch  47 Batch   24/269   train_loss = 3.580\n",
      "Epoch  47 Batch   25/269   train_loss = 3.900\n",
      "Epoch  47 Batch   26/269   train_loss = 3.785\n",
      "Epoch  47 Batch   27/269   train_loss = 3.597\n",
      "Epoch  47 Batch   28/269   train_loss = 3.769\n",
      "Epoch  47 Batch   29/269   train_loss = 3.747\n",
      "Epoch  47 Batch   30/269   train_loss = 4.040\n",
      "Epoch  47 Batch   31/269   train_loss = 3.748\n",
      "Epoch  47 Batch   32/269   train_loss = 3.614\n",
      "Epoch  47 Batch   33/269   train_loss = 3.406\n",
      "Epoch  47 Batch   34/269   train_loss = 3.640\n",
      "Epoch  47 Batch   35/269   train_loss = 3.507\n",
      "Epoch  47 Batch   36/269   train_loss = 3.527\n",
      "Epoch  47 Batch   37/269   train_loss = 3.722\n",
      "Epoch  47 Batch   38/269   train_loss = 3.475\n",
      "Epoch  47 Batch   39/269   train_loss = 3.783\n",
      "Epoch  47 Batch   40/269   train_loss = 3.548\n",
      "Epoch  47 Batch   41/269   train_loss = 3.571\n",
      "Epoch  47 Batch   42/269   train_loss = 4.082\n",
      "Epoch  47 Batch   43/269   train_loss = 3.913\n",
      "Epoch  47 Batch   44/269   train_loss = 3.702\n",
      "Epoch  47 Batch   45/269   train_loss = 3.400\n",
      "Epoch  47 Batch   46/269   train_loss = 3.840\n",
      "Epoch  47 Batch   47/269   train_loss = 3.441\n",
      "Epoch  47 Batch   48/269   train_loss = 3.906\n",
      "Epoch  47 Batch   49/269   train_loss = 3.955\n",
      "Epoch  47 Batch   50/269   train_loss = 3.832\n",
      "Epoch  47 Batch   51/269   train_loss = 3.748\n",
      "Epoch  47 Batch   52/269   train_loss = 3.768\n",
      "Epoch  47 Batch   53/269   train_loss = 3.946\n",
      "Epoch  47 Batch   54/269   train_loss = 3.736\n",
      "Epoch  47 Batch   55/269   train_loss = 3.868\n",
      "Epoch  47 Batch   56/269   train_loss = 3.632\n",
      "Epoch  47 Batch   57/269   train_loss = 3.875\n",
      "Epoch  47 Batch   58/269   train_loss = 3.540\n",
      "Epoch  47 Batch   59/269   train_loss = 3.650\n",
      "Epoch  47 Batch   60/269   train_loss = 3.746\n",
      "Epoch  47 Batch   61/269   train_loss = 3.975\n",
      "Epoch  47 Batch   62/269   train_loss = 3.592\n",
      "Epoch  47 Batch   63/269   train_loss = 3.934\n",
      "Epoch  47 Batch   64/269   train_loss = 3.623\n",
      "Epoch  47 Batch   65/269   train_loss = 3.839\n",
      "Epoch  47 Batch   66/269   train_loss = 3.669\n",
      "Epoch  47 Batch   67/269   train_loss = 3.702\n",
      "Epoch  47 Batch   68/269   train_loss = 3.789\n",
      "Epoch  47 Batch   69/269   train_loss = 3.806\n",
      "Epoch  47 Batch   70/269   train_loss = 3.436\n",
      "Epoch  47 Batch   71/269   train_loss = 3.797\n",
      "Epoch  47 Batch   72/269   train_loss = 3.550\n",
      "Epoch  47 Batch   73/269   train_loss = 3.705\n",
      "Epoch  47 Batch   74/269   train_loss = 3.676\n",
      "Epoch  47 Batch   75/269   train_loss = 3.843\n",
      "Epoch  47 Batch   76/269   train_loss = 3.818\n",
      "Epoch  47 Batch   77/269   train_loss = 3.422\n",
      "Epoch  47 Batch   78/269   train_loss = 3.843\n",
      "Epoch  47 Batch   79/269   train_loss = 3.949\n",
      "Epoch  47 Batch   80/269   train_loss = 3.845\n",
      "Epoch  47 Batch   81/269   train_loss = 3.645\n",
      "Epoch  47 Batch   82/269   train_loss = 3.884\n",
      "Epoch  47 Batch   83/269   train_loss = 3.624\n",
      "Epoch  47 Batch   84/269   train_loss = 3.902\n",
      "Epoch  47 Batch   85/269   train_loss = 4.106\n",
      "Epoch  47 Batch   86/269   train_loss = 4.034\n",
      "Epoch  47 Batch   87/269   train_loss = 3.695\n",
      "Epoch  47 Batch   88/269   train_loss = 3.805\n",
      "Epoch  47 Batch   89/269   train_loss = 3.796\n",
      "Epoch  47 Batch   90/269   train_loss = 3.561\n",
      "Epoch  47 Batch   91/269   train_loss = 3.869\n",
      "Epoch  47 Batch   92/269   train_loss = 3.867\n",
      "Epoch  47 Batch   93/269   train_loss = 3.588\n",
      "Epoch  47 Batch   94/269   train_loss = 4.057\n",
      "Epoch  47 Batch   95/269   train_loss = 3.523\n",
      "Epoch  47 Batch   96/269   train_loss = 3.665\n",
      "Epoch  47 Batch   97/269   train_loss = 3.711\n",
      "Epoch  47 Batch   98/269   train_loss = 3.901\n",
      "Epoch  47 Batch   99/269   train_loss = 3.708\n",
      "Epoch  47 Batch  100/269   train_loss = 3.725\n",
      "Epoch  47 Batch  101/269   train_loss = 3.859\n",
      "Epoch  47 Batch  102/269   train_loss = 3.539\n",
      "Epoch  47 Batch  103/269   train_loss = 3.613\n",
      "Epoch  47 Batch  104/269   train_loss = 3.602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  47 Batch  105/269   train_loss = 3.876\n",
      "Epoch  47 Batch  106/269   train_loss = 3.981\n",
      "Epoch  47 Batch  107/269   train_loss = 4.001\n",
      "Epoch  47 Batch  108/269   train_loss = 3.733\n",
      "Epoch  47 Batch  109/269   train_loss = 3.789\n",
      "Epoch  47 Batch  110/269   train_loss = 3.754\n",
      "Epoch  47 Batch  111/269   train_loss = 3.863\n",
      "Epoch  47 Batch  112/269   train_loss = 3.882\n",
      "Epoch  47 Batch  113/269   train_loss = 3.745\n",
      "Epoch  47 Batch  114/269   train_loss = 3.800\n",
      "Epoch  47 Batch  115/269   train_loss = 3.587\n",
      "Epoch  47 Batch  116/269   train_loss = 3.575\n",
      "Epoch  47 Batch  117/269   train_loss = 3.599\n",
      "Epoch  47 Batch  118/269   train_loss = 3.967\n",
      "Epoch  47 Batch  119/269   train_loss = 3.522\n",
      "Epoch  47 Batch  120/269   train_loss = 3.812\n",
      "Epoch  47 Batch  121/269   train_loss = 3.652\n",
      "Epoch  47 Batch  122/269   train_loss = 3.767\n",
      "Epoch  47 Batch  123/269   train_loss = 3.576\n",
      "Epoch  47 Batch  124/269   train_loss = 3.938\n",
      "Epoch  47 Batch  125/269   train_loss = 3.501\n",
      "Epoch  47 Batch  126/269   train_loss = 3.428\n",
      "Epoch  47 Batch  127/269   train_loss = 3.665\n",
      "Epoch  47 Batch  128/269   train_loss = 3.661\n",
      "Epoch  47 Batch  129/269   train_loss = 3.698\n",
      "Epoch  47 Batch  130/269   train_loss = 3.819\n",
      "Epoch  47 Batch  131/269   train_loss = 3.386\n",
      "Epoch  47 Batch  132/269   train_loss = 3.740\n",
      "Epoch  47 Batch  133/269   train_loss = 3.527\n",
      "Epoch  47 Batch  134/269   train_loss = 3.926\n",
      "Epoch  47 Batch  135/269   train_loss = 3.590\n",
      "Epoch  47 Batch  136/269   train_loss = 3.506\n",
      "Epoch  47 Batch  137/269   train_loss = 3.808\n",
      "Epoch  47 Batch  138/269   train_loss = 3.599\n",
      "Epoch  47 Batch  139/269   train_loss = 3.761\n",
      "Epoch  47 Batch  140/269   train_loss = 3.800\n",
      "Epoch  47 Batch  141/269   train_loss = 4.036\n",
      "Epoch  47 Batch  142/269   train_loss = 3.496\n",
      "Epoch  47 Batch  143/269   train_loss = 3.728\n",
      "Epoch  47 Batch  144/269   train_loss = 3.880\n",
      "Epoch  47 Batch  145/269   train_loss = 3.661\n",
      "Epoch  47 Batch  146/269   train_loss = 3.961\n",
      "Epoch  47 Batch  147/269   train_loss = 3.504\n",
      "Epoch  47 Batch  148/269   train_loss = 3.837\n",
      "Epoch  47 Batch  149/269   train_loss = 3.683\n",
      "Epoch  47 Batch  150/269   train_loss = 3.858\n",
      "Epoch  47 Batch  151/269   train_loss = 3.757\n",
      "Epoch  47 Batch  152/269   train_loss = 3.727\n",
      "Epoch  47 Batch  153/269   train_loss = 3.772\n",
      "Epoch  47 Batch  154/269   train_loss = 3.595\n",
      "Epoch  47 Batch  155/269   train_loss = 3.635\n",
      "Epoch  47 Batch  156/269   train_loss = 4.088\n",
      "Epoch  47 Batch  157/269   train_loss = 3.775\n",
      "Epoch  47 Batch  158/269   train_loss = 3.837\n",
      "Epoch  47 Batch  159/269   train_loss = 3.783\n",
      "Epoch  47 Batch  160/269   train_loss = 3.677\n",
      "Epoch  47 Batch  161/269   train_loss = 3.825\n",
      "Epoch  47 Batch  162/269   train_loss = 3.817\n",
      "Epoch  47 Batch  163/269   train_loss = 3.699\n",
      "Epoch  47 Batch  164/269   train_loss = 3.915\n",
      "Epoch  47 Batch  165/269   train_loss = 3.864\n",
      "Epoch  47 Batch  166/269   train_loss = 3.782\n",
      "Epoch  47 Batch  167/269   train_loss = 3.645\n",
      "Epoch  47 Batch  168/269   train_loss = 3.725\n",
      "Epoch  47 Batch  169/269   train_loss = 4.075\n",
      "Epoch  47 Batch  170/269   train_loss = 3.518\n",
      "Epoch  47 Batch  171/269   train_loss = 3.740\n",
      "Epoch  47 Batch  172/269   train_loss = 3.600\n",
      "Epoch  47 Batch  173/269   train_loss = 3.752\n",
      "Epoch  47 Batch  174/269   train_loss = 3.908\n",
      "Epoch  47 Batch  175/269   train_loss = 3.835\n",
      "Epoch  47 Batch  176/269   train_loss = 3.757\n",
      "Epoch  47 Batch  177/269   train_loss = 3.713\n",
      "Epoch  47 Batch  178/269   train_loss = 3.998\n",
      "Epoch  47 Batch  179/269   train_loss = 3.713\n",
      "Epoch  47 Batch  180/269   train_loss = 3.591\n",
      "Epoch  47 Batch  181/269   train_loss = 3.846\n",
      "Epoch  47 Batch  182/269   train_loss = 3.656\n",
      "Epoch  47 Batch  183/269   train_loss = 3.586\n",
      "Epoch  47 Batch  184/269   train_loss = 3.749\n",
      "Epoch  47 Batch  185/269   train_loss = 3.786\n",
      "Epoch  47 Batch  186/269   train_loss = 3.684\n",
      "Epoch  47 Batch  187/269   train_loss = 3.926\n",
      "Epoch  47 Batch  188/269   train_loss = 3.689\n",
      "Epoch  47 Batch  189/269   train_loss = 3.679\n",
      "Epoch  47 Batch  190/269   train_loss = 4.117\n",
      "Epoch  47 Batch  191/269   train_loss = 3.779\n",
      "Epoch  47 Batch  192/269   train_loss = 3.697\n",
      "Epoch  47 Batch  193/269   train_loss = 3.714\n",
      "Epoch  47 Batch  194/269   train_loss = 3.786\n",
      "Epoch  47 Batch  195/269   train_loss = 3.780\n",
      "Epoch  47 Batch  196/269   train_loss = 3.746\n",
      "Epoch  47 Batch  197/269   train_loss = 3.847\n",
      "Epoch  47 Batch  198/269   train_loss = 3.809\n",
      "Epoch  47 Batch  199/269   train_loss = 3.829\n",
      "Epoch  47 Batch  200/269   train_loss = 3.619\n",
      "Epoch  47 Batch  201/269   train_loss = 3.718\n",
      "Epoch  47 Batch  202/269   train_loss = 3.551\n",
      "Epoch  47 Batch  203/269   train_loss = 3.605\n",
      "Epoch  47 Batch  204/269   train_loss = 3.807\n",
      "Epoch  47 Batch  205/269   train_loss = 3.760\n",
      "Epoch  47 Batch  206/269   train_loss = 3.609\n",
      "Epoch  47 Batch  207/269   train_loss = 3.615\n",
      "Epoch  47 Batch  208/269   train_loss = 3.717\n",
      "Epoch  47 Batch  209/269   train_loss = 3.850\n",
      "Epoch  47 Batch  210/269   train_loss = 3.564\n",
      "Epoch  47 Batch  211/269   train_loss = 3.723\n",
      "Epoch  47 Batch  212/269   train_loss = 3.996\n",
      "Epoch  47 Batch  213/269   train_loss = 3.558\n",
      "Epoch  47 Batch  214/269   train_loss = 3.675\n",
      "Epoch  47 Batch  215/269   train_loss = 3.945\n",
      "Epoch  47 Batch  216/269   train_loss = 3.919\n",
      "Epoch  47 Batch  217/269   train_loss = 3.655\n",
      "Epoch  47 Batch  218/269   train_loss = 3.799\n",
      "Epoch  47 Batch  219/269   train_loss = 3.483\n",
      "Epoch  47 Batch  220/269   train_loss = 3.918\n",
      "Epoch  47 Batch  221/269   train_loss = 3.587\n",
      "Epoch  47 Batch  222/269   train_loss = 3.792\n",
      "Epoch  47 Batch  223/269   train_loss = 3.636\n",
      "Epoch  47 Batch  224/269   train_loss = 3.845\n",
      "Epoch  47 Batch  225/269   train_loss = 3.972\n",
      "Epoch  47 Batch  226/269   train_loss = 3.751\n",
      "Epoch  47 Batch  227/269   train_loss = 3.446\n",
      "Epoch  47 Batch  228/269   train_loss = 3.747\n",
      "Epoch  47 Batch  229/269   train_loss = 3.908\n",
      "Epoch  47 Batch  230/269   train_loss = 3.839\n",
      "Epoch  47 Batch  231/269   train_loss = 3.694\n",
      "Epoch  47 Batch  232/269   train_loss = 3.715\n",
      "Epoch  47 Batch  233/269   train_loss = 3.771\n",
      "Epoch  47 Batch  234/269   train_loss = 3.755\n",
      "Epoch  47 Batch  235/269   train_loss = 3.960\n",
      "Epoch  47 Batch  236/269   train_loss = 3.613\n",
      "Epoch  47 Batch  237/269   train_loss = 3.302\n",
      "Epoch  47 Batch  238/269   train_loss = 3.555\n",
      "Epoch  47 Batch  239/269   train_loss = 4.029\n",
      "Epoch  47 Batch  240/269   train_loss = 3.641\n",
      "Epoch  47 Batch  241/269   train_loss = 3.969\n",
      "Epoch  47 Batch  242/269   train_loss = 3.530\n",
      "Epoch  47 Batch  243/269   train_loss = 3.917\n",
      "Epoch  47 Batch  244/269   train_loss = 3.731\n",
      "Epoch  47 Batch  245/269   train_loss = 3.760\n",
      "Epoch  47 Batch  246/269   train_loss = 3.380\n",
      "Epoch  47 Batch  247/269   train_loss = 3.817\n",
      "Epoch  47 Batch  248/269   train_loss = 3.766\n",
      "Epoch  47 Batch  249/269   train_loss = 3.642\n",
      "Epoch  47 Batch  250/269   train_loss = 3.524\n",
      "Epoch  47 Batch  251/269   train_loss = 3.969\n",
      "Epoch  47 Batch  252/269   train_loss = 3.866\n",
      "Epoch  47 Batch  253/269   train_loss = 3.723\n",
      "Epoch  47 Batch  254/269   train_loss = 3.661\n",
      "Epoch  47 Batch  255/269   train_loss = 3.714\n",
      "Epoch  47 Batch  256/269   train_loss = 3.658\n",
      "Epoch  47 Batch  257/269   train_loss = 3.584\n",
      "Epoch  47 Batch  258/269   train_loss = 3.504\n",
      "Epoch  47 Batch  259/269   train_loss = 3.631\n",
      "Epoch  47 Batch  260/269   train_loss = 3.830\n",
      "Epoch  47 Batch  261/269   train_loss = 3.854\n",
      "Epoch  47 Batch  262/269   train_loss = 3.640\n",
      "Epoch  47 Batch  263/269   train_loss = 3.494\n",
      "Epoch  47 Batch  264/269   train_loss = 3.974\n",
      "Epoch  47 Batch  265/269   train_loss = 3.861\n",
      "Epoch  47 Batch  266/269   train_loss = 3.641\n",
      "Epoch  47 Batch  267/269   train_loss = 3.759\n",
      "Epoch  47 Batch  268/269   train_loss = 3.854\n",
      "Epoch  48 Batch    0/269   train_loss = 3.763\n",
      "Epoch  48 Batch    1/269   train_loss = 3.481\n",
      "Epoch  48 Batch    2/269   train_loss = 3.615\n",
      "Epoch  48 Batch    3/269   train_loss = 3.687\n",
      "Epoch  48 Batch    4/269   train_loss = 4.057\n",
      "Epoch  48 Batch    5/269   train_loss = 3.774\n",
      "Epoch  48 Batch    6/269   train_loss = 3.647\n",
      "Epoch  48 Batch    7/269   train_loss = 3.461\n",
      "Epoch  48 Batch    8/269   train_loss = 3.824\n",
      "Epoch  48 Batch    9/269   train_loss = 3.469\n",
      "Epoch  48 Batch   10/269   train_loss = 3.583\n",
      "Epoch  48 Batch   11/269   train_loss = 3.664\n",
      "Epoch  48 Batch   12/269   train_loss = 3.396\n",
      "Epoch  48 Batch   13/269   train_loss = 3.738\n",
      "Epoch  48 Batch   14/269   train_loss = 3.573\n",
      "Epoch  48 Batch   15/269   train_loss = 3.980\n",
      "Epoch  48 Batch   16/269   train_loss = 3.686\n",
      "Epoch  48 Batch   17/269   train_loss = 3.748\n",
      "Epoch  48 Batch   18/269   train_loss = 3.644\n",
      "Epoch  48 Batch   19/269   train_loss = 3.660\n",
      "Epoch  48 Batch   20/269   train_loss = 4.058\n",
      "Epoch  48 Batch   21/269   train_loss = 3.908\n",
      "Epoch  48 Batch   22/269   train_loss = 3.592\n",
      "Epoch  48 Batch   23/269   train_loss = 3.805\n",
      "Epoch  48 Batch   24/269   train_loss = 3.604\n",
      "Epoch  48 Batch   25/269   train_loss = 3.933\n",
      "Epoch  48 Batch   26/269   train_loss = 3.753\n",
      "Epoch  48 Batch   27/269   train_loss = 3.626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  48 Batch   28/269   train_loss = 3.776\n",
      "Epoch  48 Batch   29/269   train_loss = 3.760\n",
      "Epoch  48 Batch   30/269   train_loss = 4.019\n",
      "Epoch  48 Batch   31/269   train_loss = 3.786\n",
      "Epoch  48 Batch   32/269   train_loss = 3.650\n",
      "Epoch  48 Batch   33/269   train_loss = 3.428\n",
      "Epoch  48 Batch   34/269   train_loss = 3.659\n",
      "Epoch  48 Batch   35/269   train_loss = 3.532\n",
      "Epoch  48 Batch   36/269   train_loss = 3.562\n",
      "Epoch  48 Batch   37/269   train_loss = 3.771\n",
      "Epoch  48 Batch   38/269   train_loss = 3.481\n",
      "Epoch  48 Batch   39/269   train_loss = 3.780\n",
      "Epoch  48 Batch   40/269   train_loss = 3.565\n",
      "Epoch  48 Batch   41/269   train_loss = 3.588\n",
      "Epoch  48 Batch   42/269   train_loss = 4.076\n",
      "Epoch  48 Batch   43/269   train_loss = 3.945\n",
      "Epoch  48 Batch   44/269   train_loss = 3.724\n",
      "Epoch  48 Batch   45/269   train_loss = 3.411\n",
      "Epoch  48 Batch   46/269   train_loss = 3.827\n",
      "Epoch  48 Batch   47/269   train_loss = 3.450\n",
      "Epoch  48 Batch   48/269   train_loss = 3.951\n",
      "Epoch  48 Batch   49/269   train_loss = 3.953\n",
      "Epoch  48 Batch   50/269   train_loss = 3.850\n",
      "Epoch  48 Batch   51/269   train_loss = 3.785\n",
      "Epoch  48 Batch   52/269   train_loss = 3.794\n",
      "Epoch  48 Batch   53/269   train_loss = 3.980\n",
      "Epoch  48 Batch   54/269   train_loss = 3.750\n",
      "Epoch  48 Batch   55/269   train_loss = 3.867\n",
      "Epoch  48 Batch   56/269   train_loss = 3.641\n",
      "Epoch  48 Batch   57/269   train_loss = 3.902\n",
      "Epoch  48 Batch   58/269   train_loss = 3.616\n",
      "Epoch  48 Batch   59/269   train_loss = 3.654\n",
      "Epoch  48 Batch   60/269   train_loss = 3.769\n",
      "Epoch  48 Batch   61/269   train_loss = 3.974\n",
      "Epoch  48 Batch   62/269   train_loss = 3.616\n",
      "Epoch  48 Batch   63/269   train_loss = 3.990\n",
      "Epoch  48 Batch   64/269   train_loss = 3.645\n",
      "Epoch  48 Batch   65/269   train_loss = 3.829\n",
      "Epoch  48 Batch   66/269   train_loss = 3.680\n",
      "Epoch  48 Batch   67/269   train_loss = 3.722\n",
      "Epoch  48 Batch   68/269   train_loss = 3.813\n",
      "Epoch  48 Batch   69/269   train_loss = 3.806\n",
      "Epoch  48 Batch   70/269   train_loss = 3.435\n",
      "Epoch  48 Batch   71/269   train_loss = 3.813\n",
      "Epoch  48 Batch   72/269   train_loss = 3.534\n",
      "Epoch  48 Batch   73/269   train_loss = 3.735\n",
      "Epoch  48 Batch   74/269   train_loss = 3.664\n",
      "Epoch  48 Batch   75/269   train_loss = 3.903\n",
      "Epoch  48 Batch   76/269   train_loss = 3.848\n",
      "Epoch  48 Batch   77/269   train_loss = 3.430\n",
      "Epoch  48 Batch   78/269   train_loss = 3.856\n",
      "Epoch  48 Batch   79/269   train_loss = 3.946\n",
      "Epoch  48 Batch   80/269   train_loss = 3.837\n",
      "Epoch  48 Batch   81/269   train_loss = 3.672\n",
      "Epoch  48 Batch   82/269   train_loss = 3.877\n",
      "Epoch  48 Batch   83/269   train_loss = 3.658\n",
      "Epoch  48 Batch   84/269   train_loss = 3.948\n",
      "Epoch  48 Batch   85/269   train_loss = 4.090\n",
      "Epoch  48 Batch   86/269   train_loss = 4.076\n",
      "Epoch  48 Batch   87/269   train_loss = 3.690\n",
      "Epoch  48 Batch   88/269   train_loss = 3.803\n",
      "Epoch  48 Batch   89/269   train_loss = 3.794\n",
      "Epoch  48 Batch   90/269   train_loss = 3.559\n",
      "Epoch  48 Batch   91/269   train_loss = 3.895\n",
      "Epoch  48 Batch   92/269   train_loss = 3.900\n",
      "Epoch  48 Batch   93/269   train_loss = 3.589\n",
      "Epoch  48 Batch   94/269   train_loss = 4.063\n",
      "Epoch  48 Batch   95/269   train_loss = 3.508\n",
      "Epoch  48 Batch   96/269   train_loss = 3.671\n",
      "Epoch  48 Batch   97/269   train_loss = 3.717\n",
      "Epoch  48 Batch   98/269   train_loss = 3.917\n",
      "Epoch  48 Batch   99/269   train_loss = 3.726\n",
      "Epoch  48 Batch  100/269   train_loss = 3.744\n",
      "Epoch  48 Batch  101/269   train_loss = 3.900\n",
      "Epoch  48 Batch  102/269   train_loss = 3.562\n",
      "Epoch  48 Batch  103/269   train_loss = 3.641\n",
      "Epoch  48 Batch  104/269   train_loss = 3.587\n",
      "Epoch  48 Batch  105/269   train_loss = 3.896\n",
      "Epoch  48 Batch  106/269   train_loss = 3.962\n",
      "Epoch  48 Batch  107/269   train_loss = 3.980\n",
      "Epoch  48 Batch  108/269   train_loss = 3.767\n",
      "Epoch  48 Batch  109/269   train_loss = 3.788\n",
      "Epoch  48 Batch  110/269   train_loss = 3.771\n",
      "Epoch  48 Batch  111/269   train_loss = 3.878\n",
      "Epoch  48 Batch  112/269   train_loss = 3.931\n",
      "Epoch  48 Batch  113/269   train_loss = 3.785\n",
      "Epoch  48 Batch  114/269   train_loss = 3.809\n",
      "Epoch  48 Batch  115/269   train_loss = 3.604\n",
      "Epoch  48 Batch  116/269   train_loss = 3.584\n",
      "Epoch  48 Batch  117/269   train_loss = 3.602\n",
      "Epoch  48 Batch  118/269   train_loss = 3.964\n",
      "Epoch  48 Batch  119/269   train_loss = 3.502\n",
      "Epoch  48 Batch  120/269   train_loss = 3.825\n",
      "Epoch  48 Batch  121/269   train_loss = 3.662\n",
      "Epoch  48 Batch  122/269   train_loss = 3.730\n",
      "Epoch  48 Batch  123/269   train_loss = 3.584\n",
      "Epoch  48 Batch  124/269   train_loss = 3.952\n",
      "Epoch  48 Batch  125/269   train_loss = 3.536\n",
      "Epoch  48 Batch  126/269   train_loss = 3.460\n",
      "Epoch  48 Batch  127/269   train_loss = 3.678\n",
      "Epoch  48 Batch  128/269   train_loss = 3.691\n",
      "Epoch  48 Batch  129/269   train_loss = 3.676\n",
      "Epoch  48 Batch  130/269   train_loss = 3.849\n",
      "Epoch  48 Batch  131/269   train_loss = 3.397\n",
      "Epoch  48 Batch  132/269   train_loss = 3.752\n",
      "Epoch  48 Batch  133/269   train_loss = 3.523\n",
      "Epoch  48 Batch  134/269   train_loss = 3.956\n",
      "Epoch  48 Batch  135/269   train_loss = 3.604\n",
      "Epoch  48 Batch  136/269   train_loss = 3.515\n",
      "Epoch  48 Batch  137/269   train_loss = 3.834\n",
      "Epoch  48 Batch  138/269   train_loss = 3.614\n",
      "Epoch  48 Batch  139/269   train_loss = 3.801\n",
      "Epoch  48 Batch  140/269   train_loss = 3.784\n",
      "Epoch  48 Batch  141/269   train_loss = 4.034\n",
      "Epoch  48 Batch  142/269   train_loss = 3.591\n",
      "Epoch  48 Batch  143/269   train_loss = 3.780\n",
      "Epoch  48 Batch  144/269   train_loss = 3.871\n",
      "Epoch  48 Batch  145/269   train_loss = 3.684\n",
      "Epoch  48 Batch  146/269   train_loss = 4.011\n",
      "Epoch  48 Batch  147/269   train_loss = 3.565\n",
      "Epoch  48 Batch  148/269   train_loss = 3.884\n",
      "Epoch  48 Batch  149/269   train_loss = 3.719\n",
      "Epoch  48 Batch  150/269   train_loss = 3.850\n",
      "Epoch  48 Batch  151/269   train_loss = 3.764\n",
      "Epoch  48 Batch  152/269   train_loss = 3.719\n",
      "Epoch  48 Batch  153/269   train_loss = 3.754\n",
      "Epoch  48 Batch  154/269   train_loss = 3.607\n",
      "Epoch  48 Batch  155/269   train_loss = 3.634\n",
      "Epoch  48 Batch  156/269   train_loss = 4.131\n",
      "Epoch  48 Batch  157/269   train_loss = 3.795\n",
      "Epoch  48 Batch  158/269   train_loss = 3.841\n",
      "Epoch  48 Batch  159/269   train_loss = 3.770\n",
      "Epoch  48 Batch  160/269   train_loss = 3.715\n",
      "Epoch  48 Batch  161/269   train_loss = 3.843\n",
      "Epoch  48 Batch  162/269   train_loss = 3.860\n",
      "Epoch  48 Batch  163/269   train_loss = 3.689\n",
      "Epoch  48 Batch  164/269   train_loss = 3.955\n",
      "Epoch  48 Batch  165/269   train_loss = 3.858\n",
      "Epoch  48 Batch  166/269   train_loss = 3.790\n",
      "Epoch  48 Batch  167/269   train_loss = 3.656\n",
      "Epoch  48 Batch  168/269   train_loss = 3.726\n",
      "Epoch  48 Batch  169/269   train_loss = 4.040\n",
      "Epoch  48 Batch  170/269   train_loss = 3.524\n",
      "Epoch  48 Batch  171/269   train_loss = 3.749\n",
      "Epoch  48 Batch  172/269   train_loss = 3.632\n",
      "Epoch  48 Batch  173/269   train_loss = 3.780\n",
      "Epoch  48 Batch  174/269   train_loss = 3.956\n",
      "Epoch  48 Batch  175/269   train_loss = 3.833\n",
      "Epoch  48 Batch  176/269   train_loss = 3.744\n",
      "Epoch  48 Batch  177/269   train_loss = 3.717\n",
      "Epoch  48 Batch  178/269   train_loss = 3.966\n",
      "Epoch  48 Batch  179/269   train_loss = 3.715\n",
      "Epoch  48 Batch  180/269   train_loss = 3.632\n",
      "Epoch  48 Batch  181/269   train_loss = 3.863\n",
      "Epoch  48 Batch  182/269   train_loss = 3.674\n",
      "Epoch  48 Batch  183/269   train_loss = 3.620\n",
      "Epoch  48 Batch  184/269   train_loss = 3.756\n",
      "Epoch  48 Batch  185/269   train_loss = 3.825\n",
      "Epoch  48 Batch  186/269   train_loss = 3.708\n",
      "Epoch  48 Batch  187/269   train_loss = 3.938\n",
      "Epoch  48 Batch  188/269   train_loss = 3.693\n",
      "Epoch  48 Batch  189/269   train_loss = 3.670\n",
      "Epoch  48 Batch  190/269   train_loss = 4.113\n",
      "Epoch  48 Batch  191/269   train_loss = 3.793\n",
      "Epoch  48 Batch  192/269   train_loss = 3.766\n",
      "Epoch  48 Batch  193/269   train_loss = 3.721\n",
      "Epoch  48 Batch  194/269   train_loss = 3.848\n",
      "Epoch  48 Batch  195/269   train_loss = 3.749\n",
      "Epoch  48 Batch  196/269   train_loss = 3.777\n",
      "Epoch  48 Batch  197/269   train_loss = 3.864\n",
      "Epoch  48 Batch  198/269   train_loss = 3.857\n",
      "Epoch  48 Batch  199/269   train_loss = 3.838\n",
      "Epoch  48 Batch  200/269   train_loss = 3.659\n",
      "Epoch  48 Batch  201/269   train_loss = 3.728\n",
      "Epoch  48 Batch  202/269   train_loss = 3.580\n",
      "Epoch  48 Batch  203/269   train_loss = 3.614\n",
      "Epoch  48 Batch  204/269   train_loss = 3.830\n",
      "Epoch  48 Batch  205/269   train_loss = 3.773\n",
      "Epoch  48 Batch  206/269   train_loss = 3.620\n",
      "Epoch  48 Batch  207/269   train_loss = 3.631\n",
      "Epoch  48 Batch  208/269   train_loss = 3.747\n",
      "Epoch  48 Batch  209/269   train_loss = 3.868\n",
      "Epoch  48 Batch  210/269   train_loss = 3.600\n",
      "Epoch  48 Batch  211/269   train_loss = 3.715\n",
      "Epoch  48 Batch  212/269   train_loss = 4.025\n",
      "Epoch  48 Batch  213/269   train_loss = 3.615\n",
      "Epoch  48 Batch  214/269   train_loss = 3.700\n",
      "Epoch  48 Batch  215/269   train_loss = 3.965\n",
      "Epoch  48 Batch  216/269   train_loss = 3.942\n",
      "Epoch  48 Batch  217/269   train_loss = 3.644\n",
      "Epoch  48 Batch  218/269   train_loss = 3.810\n",
      "Epoch  48 Batch  219/269   train_loss = 3.488\n",
      "Epoch  48 Batch  220/269   train_loss = 3.937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  48 Batch  221/269   train_loss = 3.578\n",
      "Epoch  48 Batch  222/269   train_loss = 3.813\n",
      "Epoch  48 Batch  223/269   train_loss = 3.618\n",
      "Epoch  48 Batch  224/269   train_loss = 3.855\n",
      "Epoch  48 Batch  225/269   train_loss = 3.959\n",
      "Epoch  48 Batch  226/269   train_loss = 3.763\n",
      "Epoch  48 Batch  227/269   train_loss = 3.459\n",
      "Epoch  48 Batch  228/269   train_loss = 3.716\n",
      "Epoch  48 Batch  229/269   train_loss = 3.942\n",
      "Epoch  48 Batch  230/269   train_loss = 3.851\n",
      "Epoch  48 Batch  231/269   train_loss = 3.709\n",
      "Epoch  48 Batch  232/269   train_loss = 3.731\n",
      "Epoch  48 Batch  233/269   train_loss = 3.807\n",
      "Epoch  48 Batch  234/269   train_loss = 3.745\n",
      "Epoch  48 Batch  235/269   train_loss = 3.955\n",
      "Epoch  48 Batch  236/269   train_loss = 3.639\n",
      "Epoch  48 Batch  237/269   train_loss = 3.310\n",
      "Epoch  48 Batch  238/269   train_loss = 3.522\n",
      "Epoch  48 Batch  239/269   train_loss = 4.043\n",
      "Epoch  48 Batch  240/269   train_loss = 3.666\n",
      "Epoch  48 Batch  241/269   train_loss = 3.914\n",
      "Epoch  48 Batch  242/269   train_loss = 3.539\n",
      "Epoch  48 Batch  243/269   train_loss = 3.934\n",
      "Epoch  48 Batch  244/269   train_loss = 3.761\n",
      "Epoch  48 Batch  245/269   train_loss = 3.748\n",
      "Epoch  48 Batch  246/269   train_loss = 3.376\n",
      "Epoch  48 Batch  247/269   train_loss = 3.812\n",
      "Epoch  48 Batch  248/269   train_loss = 3.759\n",
      "Epoch  48 Batch  249/269   train_loss = 3.653\n",
      "Epoch  48 Batch  250/269   train_loss = 3.543\n",
      "Epoch  48 Batch  251/269   train_loss = 3.956\n",
      "Epoch  48 Batch  252/269   train_loss = 3.881\n",
      "Epoch  48 Batch  253/269   train_loss = 3.667\n",
      "Epoch  48 Batch  254/269   train_loss = 3.626\n",
      "Epoch  48 Batch  255/269   train_loss = 3.707\n",
      "Epoch  48 Batch  256/269   train_loss = 3.677\n",
      "Epoch  48 Batch  257/269   train_loss = 3.568\n",
      "Epoch  48 Batch  258/269   train_loss = 3.471\n",
      "Epoch  48 Batch  259/269   train_loss = 3.638\n",
      "Epoch  48 Batch  260/269   train_loss = 3.830\n",
      "Epoch  48 Batch  261/269   train_loss = 3.847\n",
      "Epoch  48 Batch  262/269   train_loss = 3.651\n",
      "Epoch  48 Batch  263/269   train_loss = 3.484\n",
      "Epoch  48 Batch  264/269   train_loss = 3.980\n",
      "Epoch  48 Batch  265/269   train_loss = 3.865\n",
      "Epoch  48 Batch  266/269   train_loss = 3.629\n",
      "Epoch  48 Batch  267/269   train_loss = 3.729\n",
      "Epoch  48 Batch  268/269   train_loss = 3.834\n",
      "Epoch  49 Batch    0/269   train_loss = 3.739\n",
      "Epoch  49 Batch    1/269   train_loss = 3.494\n",
      "Epoch  49 Batch    2/269   train_loss = 3.622\n",
      "Epoch  49 Batch    3/269   train_loss = 3.678\n",
      "Epoch  49 Batch    4/269   train_loss = 4.087\n",
      "Epoch  49 Batch    5/269   train_loss = 3.768\n",
      "Epoch  49 Batch    6/269   train_loss = 3.613\n",
      "Epoch  49 Batch    7/269   train_loss = 3.445\n",
      "Epoch  49 Batch    8/269   train_loss = 3.787\n",
      "Epoch  49 Batch    9/269   train_loss = 3.486\n",
      "Epoch  49 Batch   10/269   train_loss = 3.535\n",
      "Epoch  49 Batch   11/269   train_loss = 3.660\n",
      "Epoch  49 Batch   12/269   train_loss = 3.424\n",
      "Epoch  49 Batch   13/269   train_loss = 3.754\n",
      "Epoch  49 Batch   14/269   train_loss = 3.594\n",
      "Epoch  49 Batch   15/269   train_loss = 3.956\n",
      "Epoch  49 Batch   16/269   train_loss = 3.693\n",
      "Epoch  49 Batch   17/269   train_loss = 3.713\n",
      "Epoch  49 Batch   18/269   train_loss = 3.651\n",
      "Epoch  49 Batch   19/269   train_loss = 3.662\n",
      "Epoch  49 Batch   20/269   train_loss = 4.078\n",
      "Epoch  49 Batch   21/269   train_loss = 3.912\n",
      "Epoch  49 Batch   22/269   train_loss = 3.589\n",
      "Epoch  49 Batch   23/269   train_loss = 3.845\n",
      "Epoch  49 Batch   24/269   train_loss = 3.583\n",
      "Epoch  49 Batch   25/269   train_loss = 3.903\n",
      "Epoch  49 Batch   26/269   train_loss = 3.761\n",
      "Epoch  49 Batch   27/269   train_loss = 3.636\n",
      "Epoch  49 Batch   28/269   train_loss = 3.773\n",
      "Epoch  49 Batch   29/269   train_loss = 3.771\n",
      "Epoch  49 Batch   30/269   train_loss = 4.029\n",
      "Epoch  49 Batch   31/269   train_loss = 3.763\n",
      "Epoch  49 Batch   32/269   train_loss = 3.623\n",
      "Epoch  49 Batch   33/269   train_loss = 3.431\n",
      "Epoch  49 Batch   34/269   train_loss = 3.658\n",
      "Epoch  49 Batch   35/269   train_loss = 3.514\n",
      "Epoch  49 Batch   36/269   train_loss = 3.568\n",
      "Epoch  49 Batch   37/269   train_loss = 3.747\n",
      "Epoch  49 Batch   38/269   train_loss = 3.468\n",
      "Epoch  49 Batch   39/269   train_loss = 3.762\n",
      "Epoch  49 Batch   40/269   train_loss = 3.584\n",
      "Epoch  49 Batch   41/269   train_loss = 3.584\n",
      "Epoch  49 Batch   42/269   train_loss = 4.093\n",
      "Epoch  49 Batch   43/269   train_loss = 3.914\n",
      "Epoch  49 Batch   44/269   train_loss = 3.732\n",
      "Epoch  49 Batch   45/269   train_loss = 3.385\n",
      "Epoch  49 Batch   46/269   train_loss = 3.810\n",
      "Epoch  49 Batch   47/269   train_loss = 3.497\n",
      "Epoch  49 Batch   48/269   train_loss = 3.973\n",
      "Epoch  49 Batch   49/269   train_loss = 3.963\n",
      "Epoch  49 Batch   50/269   train_loss = 3.852\n",
      "Epoch  49 Batch   51/269   train_loss = 3.753\n",
      "Epoch  49 Batch   52/269   train_loss = 3.765\n",
      "Epoch  49 Batch   53/269   train_loss = 3.949\n",
      "Epoch  49 Batch   54/269   train_loss = 3.736\n",
      "Epoch  49 Batch   55/269   train_loss = 3.800\n",
      "Epoch  49 Batch   56/269   train_loss = 3.618\n",
      "Epoch  49 Batch   57/269   train_loss = 3.876\n",
      "Epoch  49 Batch   58/269   train_loss = 3.630\n",
      "Epoch  49 Batch   59/269   train_loss = 3.630\n",
      "Epoch  49 Batch   60/269   train_loss = 3.775\n",
      "Epoch  49 Batch   61/269   train_loss = 3.967\n",
      "Epoch  49 Batch   62/269   train_loss = 3.571\n",
      "Epoch  49 Batch   63/269   train_loss = 3.956\n",
      "Epoch  49 Batch   64/269   train_loss = 3.688\n",
      "Epoch  49 Batch   65/269   train_loss = 3.789\n",
      "Epoch  49 Batch   66/269   train_loss = 3.671\n",
      "Epoch  49 Batch   67/269   train_loss = 3.691\n",
      "Epoch  49 Batch   68/269   train_loss = 3.777\n",
      "Epoch  49 Batch   69/269   train_loss = 3.798\n",
      "Epoch  49 Batch   70/269   train_loss = 3.458\n",
      "Epoch  49 Batch   71/269   train_loss = 3.790\n",
      "Epoch  49 Batch   72/269   train_loss = 3.512\n",
      "Epoch  49 Batch   73/269   train_loss = 3.708\n",
      "Epoch  49 Batch   74/269   train_loss = 3.659\n",
      "Epoch  49 Batch   75/269   train_loss = 3.880\n",
      "Epoch  49 Batch   76/269   train_loss = 3.830\n",
      "Epoch  49 Batch   77/269   train_loss = 3.473\n",
      "Epoch  49 Batch   78/269   train_loss = 3.868\n",
      "Epoch  49 Batch   79/269   train_loss = 3.922\n",
      "Epoch  49 Batch   80/269   train_loss = 3.837\n",
      "Epoch  49 Batch   81/269   train_loss = 3.665\n",
      "Epoch  49 Batch   82/269   train_loss = 3.888\n",
      "Epoch  49 Batch   83/269   train_loss = 3.636\n",
      "Epoch  49 Batch   84/269   train_loss = 3.953\n",
      "Epoch  49 Batch   85/269   train_loss = 4.076\n",
      "Epoch  49 Batch   86/269   train_loss = 4.075\n",
      "Epoch  49 Batch   87/269   train_loss = 3.689\n",
      "Epoch  49 Batch   88/269   train_loss = 3.812\n",
      "Epoch  49 Batch   89/269   train_loss = 3.807\n",
      "Epoch  49 Batch   90/269   train_loss = 3.572\n",
      "Epoch  49 Batch   91/269   train_loss = 3.866\n",
      "Epoch  49 Batch   92/269   train_loss = 3.871\n",
      "Epoch  49 Batch   93/269   train_loss = 3.596\n",
      "Epoch  49 Batch   94/269   train_loss = 4.069\n",
      "Epoch  49 Batch   95/269   train_loss = 3.525\n",
      "Epoch  49 Batch   96/269   train_loss = 3.656\n",
      "Epoch  49 Batch   97/269   train_loss = 3.704\n",
      "Epoch  49 Batch   98/269   train_loss = 3.948\n",
      "Epoch  49 Batch   99/269   train_loss = 3.733\n",
      "Epoch  49 Batch  100/269   train_loss = 3.745\n",
      "Epoch  49 Batch  101/269   train_loss = 3.888\n",
      "Epoch  49 Batch  102/269   train_loss = 3.538\n",
      "Epoch  49 Batch  103/269   train_loss = 3.623\n",
      "Epoch  49 Batch  104/269   train_loss = 3.585\n",
      "Epoch  49 Batch  105/269   train_loss = 3.907\n",
      "Epoch  49 Batch  106/269   train_loss = 3.987\n",
      "Epoch  49 Batch  107/269   train_loss = 4.001\n",
      "Epoch  49 Batch  108/269   train_loss = 3.738\n",
      "Epoch  49 Batch  109/269   train_loss = 3.765\n",
      "Epoch  49 Batch  110/269   train_loss = 3.741\n",
      "Epoch  49 Batch  111/269   train_loss = 3.859\n",
      "Epoch  49 Batch  112/269   train_loss = 3.907\n",
      "Epoch  49 Batch  113/269   train_loss = 3.783\n",
      "Epoch  49 Batch  114/269   train_loss = 3.790\n",
      "Epoch  49 Batch  115/269   train_loss = 3.579\n",
      "Epoch  49 Batch  116/269   train_loss = 3.589\n",
      "Epoch  49 Batch  117/269   train_loss = 3.586\n",
      "Epoch  49 Batch  118/269   train_loss = 3.972\n",
      "Epoch  49 Batch  119/269   train_loss = 3.484\n",
      "Epoch  49 Batch  120/269   train_loss = 3.807\n",
      "Epoch  49 Batch  121/269   train_loss = 3.652\n",
      "Epoch  49 Batch  122/269   train_loss = 3.755\n",
      "Epoch  49 Batch  123/269   train_loss = 3.569\n",
      "Epoch  49 Batch  124/269   train_loss = 3.912\n",
      "Epoch  49 Batch  125/269   train_loss = 3.538\n",
      "Epoch  49 Batch  126/269   train_loss = 3.452\n",
      "Epoch  49 Batch  127/269   train_loss = 3.699\n",
      "Epoch  49 Batch  128/269   train_loss = 3.663\n",
      "Epoch  49 Batch  129/269   train_loss = 3.680\n",
      "Epoch  49 Batch  130/269   train_loss = 3.869\n",
      "Epoch  49 Batch  131/269   train_loss = 3.433\n",
      "Epoch  49 Batch  132/269   train_loss = 3.751\n",
      "Epoch  49 Batch  133/269   train_loss = 3.497\n",
      "Epoch  49 Batch  134/269   train_loss = 3.934\n",
      "Epoch  49 Batch  135/269   train_loss = 3.579\n",
      "Epoch  49 Batch  136/269   train_loss = 3.502\n",
      "Epoch  49 Batch  137/269   train_loss = 3.827\n",
      "Epoch  49 Batch  138/269   train_loss = 3.616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  49 Batch  139/269   train_loss = 3.764\n",
      "Epoch  49 Batch  140/269   train_loss = 3.772\n",
      "Epoch  49 Batch  141/269   train_loss = 4.037\n",
      "Epoch  49 Batch  142/269   train_loss = 3.544\n",
      "Epoch  49 Batch  143/269   train_loss = 3.764\n",
      "Epoch  49 Batch  144/269   train_loss = 3.856\n",
      "Epoch  49 Batch  145/269   train_loss = 3.650\n",
      "Epoch  49 Batch  146/269   train_loss = 3.956\n",
      "Epoch  49 Batch  147/269   train_loss = 3.512\n",
      "Epoch  49 Batch  148/269   train_loss = 3.825\n",
      "Epoch  49 Batch  149/269   train_loss = 3.683\n",
      "Epoch  49 Batch  150/269   train_loss = 3.812\n",
      "Epoch  49 Batch  151/269   train_loss = 3.760\n",
      "Epoch  49 Batch  152/269   train_loss = 3.732\n",
      "Epoch  49 Batch  153/269   train_loss = 3.742\n",
      "Epoch  49 Batch  154/269   train_loss = 3.628\n",
      "Epoch  49 Batch  155/269   train_loss = 3.635\n",
      "Epoch  49 Batch  156/269   train_loss = 4.083\n",
      "Epoch  49 Batch  157/269   train_loss = 3.773\n",
      "Epoch  49 Batch  158/269   train_loss = 3.809\n",
      "Epoch  49 Batch  159/269   train_loss = 3.772\n",
      "Epoch  49 Batch  160/269   train_loss = 3.687\n",
      "Epoch  49 Batch  161/269   train_loss = 3.801\n",
      "Epoch  49 Batch  162/269   train_loss = 3.843\n",
      "Epoch  49 Batch  163/269   train_loss = 3.666\n",
      "Epoch  49 Batch  164/269   train_loss = 3.908\n",
      "Epoch  49 Batch  165/269   train_loss = 3.866\n",
      "Epoch  49 Batch  166/269   train_loss = 3.793\n",
      "Epoch  49 Batch  167/269   train_loss = 3.648\n",
      "Epoch  49 Batch  168/269   train_loss = 3.709\n",
      "Epoch  49 Batch  169/269   train_loss = 4.059\n",
      "Epoch  49 Batch  170/269   train_loss = 3.515\n",
      "Epoch  49 Batch  171/269   train_loss = 3.748\n",
      "Epoch  49 Batch  172/269   train_loss = 3.585\n",
      "Epoch  49 Batch  173/269   train_loss = 3.727\n",
      "Epoch  49 Batch  174/269   train_loss = 3.966\n",
      "Epoch  49 Batch  175/269   train_loss = 3.796\n",
      "Epoch  49 Batch  176/269   train_loss = 3.724\n",
      "Epoch  49 Batch  177/269   train_loss = 3.692\n",
      "Epoch  49 Batch  178/269   train_loss = 3.943\n",
      "Epoch  49 Batch  179/269   train_loss = 3.739\n",
      "Epoch  49 Batch  180/269   train_loss = 3.609\n",
      "Epoch  49 Batch  181/269   train_loss = 3.843\n",
      "Epoch  49 Batch  182/269   train_loss = 3.645\n",
      "Epoch  49 Batch  183/269   train_loss = 3.592\n",
      "Epoch  49 Batch  184/269   train_loss = 3.748\n",
      "Epoch  49 Batch  185/269   train_loss = 3.819\n",
      "Epoch  49 Batch  186/269   train_loss = 3.695\n",
      "Epoch  49 Batch  187/269   train_loss = 3.921\n",
      "Epoch  49 Batch  188/269   train_loss = 3.695\n",
      "Epoch  49 Batch  189/269   train_loss = 3.645\n",
      "Epoch  49 Batch  190/269   train_loss = 4.104\n",
      "Epoch  49 Batch  191/269   train_loss = 3.796\n",
      "Epoch  49 Batch  192/269   train_loss = 3.724\n",
      "Epoch  49 Batch  193/269   train_loss = 3.740\n",
      "Epoch  49 Batch  194/269   train_loss = 3.824\n",
      "Epoch  49 Batch  195/269   train_loss = 3.711\n",
      "Epoch  49 Batch  196/269   train_loss = 3.757\n",
      "Epoch  49 Batch  197/269   train_loss = 3.856\n",
      "Epoch  49 Batch  198/269   train_loss = 3.784\n",
      "Epoch  49 Batch  199/269   train_loss = 3.815\n",
      "Epoch  49 Batch  200/269   train_loss = 3.636\n",
      "Epoch  49 Batch  201/269   train_loss = 3.735\n",
      "Epoch  49 Batch  202/269   train_loss = 3.553\n",
      "Epoch  49 Batch  203/269   train_loss = 3.610\n",
      "Epoch  49 Batch  204/269   train_loss = 3.868\n",
      "Epoch  49 Batch  205/269   train_loss = 3.768\n",
      "Epoch  49 Batch  206/269   train_loss = 3.630\n",
      "Epoch  49 Batch  207/269   train_loss = 3.642\n",
      "Epoch  49 Batch  208/269   train_loss = 3.735\n",
      "Epoch  49 Batch  209/269   train_loss = 3.854\n",
      "Epoch  49 Batch  210/269   train_loss = 3.591\n",
      "Epoch  49 Batch  211/269   train_loss = 3.731\n",
      "Epoch  49 Batch  212/269   train_loss = 4.002\n",
      "Epoch  49 Batch  213/269   train_loss = 3.609\n",
      "Epoch  49 Batch  214/269   train_loss = 3.721\n",
      "Epoch  49 Batch  215/269   train_loss = 3.952\n",
      "Epoch  49 Batch  216/269   train_loss = 3.927\n",
      "Epoch  49 Batch  217/269   train_loss = 3.625\n",
      "Epoch  49 Batch  218/269   train_loss = 3.805\n",
      "Epoch  49 Batch  219/269   train_loss = 3.526\n",
      "Epoch  49 Batch  220/269   train_loss = 3.928\n",
      "Epoch  49 Batch  221/269   train_loss = 3.582\n",
      "Epoch  49 Batch  222/269   train_loss = 3.803\n",
      "Epoch  49 Batch  223/269   train_loss = 3.600\n",
      "Epoch  49 Batch  224/269   train_loss = 3.834\n",
      "Epoch  49 Batch  225/269   train_loss = 3.955\n",
      "Epoch  49 Batch  226/269   train_loss = 3.748\n",
      "Epoch  49 Batch  227/269   train_loss = 3.448\n",
      "Epoch  49 Batch  228/269   train_loss = 3.727\n",
      "Epoch  49 Batch  229/269   train_loss = 3.923\n",
      "Epoch  49 Batch  230/269   train_loss = 3.832\n",
      "Epoch  49 Batch  231/269   train_loss = 3.716\n",
      "Epoch  49 Batch  232/269   train_loss = 3.732\n",
      "Epoch  49 Batch  233/269   train_loss = 3.786\n",
      "Epoch  49 Batch  234/269   train_loss = 3.726\n",
      "Epoch  49 Batch  235/269   train_loss = 4.000\n",
      "Epoch  49 Batch  236/269   train_loss = 3.620\n",
      "Epoch  49 Batch  237/269   train_loss = 3.291\n",
      "Epoch  49 Batch  238/269   train_loss = 3.506\n",
      "Epoch  49 Batch  239/269   train_loss = 4.046\n",
      "Epoch  49 Batch  240/269   train_loss = 3.655\n",
      "Epoch  49 Batch  241/269   train_loss = 3.928\n",
      "Epoch  49 Batch  242/269   train_loss = 3.521\n",
      "Epoch  49 Batch  243/269   train_loss = 3.918\n",
      "Epoch  49 Batch  244/269   train_loss = 3.740\n",
      "Epoch  49 Batch  245/269   train_loss = 3.773\n",
      "Epoch  49 Batch  246/269   train_loss = 3.352\n",
      "Epoch  49 Batch  247/269   train_loss = 3.808\n",
      "Epoch  49 Batch  248/269   train_loss = 3.790\n",
      "Epoch  49 Batch  249/269   train_loss = 3.651\n",
      "Epoch  49 Batch  250/269   train_loss = 3.534\n",
      "Epoch  49 Batch  251/269   train_loss = 3.941\n",
      "Epoch  49 Batch  252/269   train_loss = 3.891\n",
      "Epoch  49 Batch  253/269   train_loss = 3.653\n",
      "Epoch  49 Batch  254/269   train_loss = 3.619\n",
      "Epoch  49 Batch  255/269   train_loss = 3.696\n",
      "Epoch  49 Batch  256/269   train_loss = 3.618\n",
      "Epoch  49 Batch  257/269   train_loss = 3.585\n",
      "Epoch  49 Batch  258/269   train_loss = 3.469\n",
      "Epoch  49 Batch  259/269   train_loss = 3.591\n",
      "Epoch  49 Batch  260/269   train_loss = 3.798\n",
      "Epoch  49 Batch  261/269   train_loss = 3.845\n",
      "Epoch  49 Batch  262/269   train_loss = 3.639\n",
      "Epoch  49 Batch  263/269   train_loss = 3.481\n",
      "Epoch  49 Batch  264/269   train_loss = 3.942\n",
      "Epoch  49 Batch  265/269   train_loss = 3.850\n",
      "Epoch  49 Batch  266/269   train_loss = 3.631\n",
      "Epoch  49 Batch  267/269   train_loss = 3.730\n",
      "Epoch  49 Batch  268/269   train_loss = 3.849\n",
      "Epoch  50 Batch    0/269   train_loss = 3.711\n",
      "Epoch  50 Batch    1/269   train_loss = 3.497\n",
      "Epoch  50 Batch    2/269   train_loss = 3.625\n",
      "Epoch  50 Batch    3/269   train_loss = 3.676\n",
      "Epoch  50 Batch    4/269   train_loss = 4.031\n",
      "Epoch  50 Batch    5/269   train_loss = 3.772\n",
      "Epoch  50 Batch    6/269   train_loss = 3.610\n",
      "Epoch  50 Batch    7/269   train_loss = 3.437\n",
      "Epoch  50 Batch    8/269   train_loss = 3.807\n",
      "Epoch  50 Batch    9/269   train_loss = 3.479\n",
      "Epoch  50 Batch   10/269   train_loss = 3.543\n",
      "Epoch  50 Batch   11/269   train_loss = 3.652\n",
      "Epoch  50 Batch   12/269   train_loss = 3.406\n",
      "Epoch  50 Batch   13/269   train_loss = 3.730\n",
      "Epoch  50 Batch   14/269   train_loss = 3.617\n",
      "Epoch  50 Batch   15/269   train_loss = 3.970\n",
      "Epoch  50 Batch   16/269   train_loss = 3.677\n",
      "Epoch  50 Batch   17/269   train_loss = 3.731\n",
      "Epoch  50 Batch   18/269   train_loss = 3.655\n",
      "Epoch  50 Batch   19/269   train_loss = 3.650\n",
      "Epoch  50 Batch   20/269   train_loss = 4.058\n",
      "Epoch  50 Batch   21/269   train_loss = 3.884\n",
      "Epoch  50 Batch   22/269   train_loss = 3.596\n",
      "Epoch  50 Batch   23/269   train_loss = 3.791\n",
      "Epoch  50 Batch   24/269   train_loss = 3.567\n",
      "Epoch  50 Batch   25/269   train_loss = 3.902\n",
      "Epoch  50 Batch   26/269   train_loss = 3.764\n",
      "Epoch  50 Batch   27/269   train_loss = 3.589\n",
      "Epoch  50 Batch   28/269   train_loss = 3.751\n",
      "Epoch  50 Batch   29/269   train_loss = 3.771\n",
      "Epoch  50 Batch   30/269   train_loss = 4.025\n",
      "Epoch  50 Batch   31/269   train_loss = 3.753\n",
      "Epoch  50 Batch   32/269   train_loss = 3.615\n",
      "Epoch  50 Batch   33/269   train_loss = 3.410\n",
      "Epoch  50 Batch   34/269   train_loss = 3.636\n",
      "Epoch  50 Batch   35/269   train_loss = 3.537\n",
      "Epoch  50 Batch   36/269   train_loss = 3.561\n",
      "Epoch  50 Batch   37/269   train_loss = 3.745\n",
      "Epoch  50 Batch   38/269   train_loss = 3.467\n",
      "Epoch  50 Batch   39/269   train_loss = 3.755\n",
      "Epoch  50 Batch   40/269   train_loss = 3.598\n",
      "Epoch  50 Batch   41/269   train_loss = 3.568\n",
      "Epoch  50 Batch   42/269   train_loss = 4.063\n",
      "Epoch  50 Batch   43/269   train_loss = 3.901\n",
      "Epoch  50 Batch   44/269   train_loss = 3.728\n",
      "Epoch  50 Batch   45/269   train_loss = 3.375\n",
      "Epoch  50 Batch   46/269   train_loss = 3.786\n",
      "Epoch  50 Batch   47/269   train_loss = 3.473\n",
      "Epoch  50 Batch   48/269   train_loss = 3.919\n",
      "Epoch  50 Batch   49/269   train_loss = 3.949\n",
      "Epoch  50 Batch   50/269   train_loss = 3.831\n",
      "Epoch  50 Batch   51/269   train_loss = 3.749\n",
      "Epoch  50 Batch   52/269   train_loss = 3.749\n",
      "Epoch  50 Batch   53/269   train_loss = 3.942\n",
      "Epoch  50 Batch   54/269   train_loss = 3.729\n",
      "Epoch  50 Batch   55/269   train_loss = 3.825\n",
      "Epoch  50 Batch   56/269   train_loss = 3.607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 Batch   57/269   train_loss = 3.915\n",
      "Epoch  50 Batch   58/269   train_loss = 3.605\n",
      "Epoch  50 Batch   59/269   train_loss = 3.621\n",
      "Epoch  50 Batch   60/269   train_loss = 3.765\n",
      "Epoch  50 Batch   61/269   train_loss = 3.952\n",
      "Epoch  50 Batch   62/269   train_loss = 3.572\n",
      "Epoch  50 Batch   63/269   train_loss = 3.948\n",
      "Epoch  50 Batch   64/269   train_loss = 3.659\n",
      "Epoch  50 Batch   65/269   train_loss = 3.775\n",
      "Epoch  50 Batch   66/269   train_loss = 3.668\n",
      "Epoch  50 Batch   67/269   train_loss = 3.667\n",
      "Epoch  50 Batch   68/269   train_loss = 3.772\n",
      "Epoch  50 Batch   69/269   train_loss = 3.767\n",
      "Epoch  50 Batch   70/269   train_loss = 3.436\n",
      "Epoch  50 Batch   71/269   train_loss = 3.788\n",
      "Epoch  50 Batch   72/269   train_loss = 3.541\n",
      "Epoch  50 Batch   73/269   train_loss = 3.697\n",
      "Epoch  50 Batch   74/269   train_loss = 3.666\n",
      "Epoch  50 Batch   75/269   train_loss = 3.851\n",
      "Epoch  50 Batch   76/269   train_loss = 3.792\n",
      "Epoch  50 Batch   77/269   train_loss = 3.445\n",
      "Epoch  50 Batch   78/269   train_loss = 3.850\n",
      "Epoch  50 Batch   79/269   train_loss = 3.919\n",
      "Epoch  50 Batch   80/269   train_loss = 3.850\n",
      "Epoch  50 Batch   81/269   train_loss = 3.637\n",
      "Epoch  50 Batch   82/269   train_loss = 3.852\n",
      "Epoch  50 Batch   83/269   train_loss = 3.636\n",
      "Epoch  50 Batch   84/269   train_loss = 3.956\n",
      "Epoch  50 Batch   85/269   train_loss = 4.055\n",
      "Epoch  50 Batch   86/269   train_loss = 4.036\n",
      "Epoch  50 Batch   87/269   train_loss = 3.699\n",
      "Epoch  50 Batch   88/269   train_loss = 3.791\n",
      "Epoch  50 Batch   89/269   train_loss = 3.779\n",
      "Epoch  50 Batch   90/269   train_loss = 3.560\n",
      "Epoch  50 Batch   91/269   train_loss = 3.881\n",
      "Epoch  50 Batch   92/269   train_loss = 3.892\n",
      "Epoch  50 Batch   93/269   train_loss = 3.566\n",
      "Epoch  50 Batch   94/269   train_loss = 4.071\n",
      "Epoch  50 Batch   95/269   train_loss = 3.494\n",
      "Epoch  50 Batch   96/269   train_loss = 3.656\n",
      "Epoch  50 Batch   97/269   train_loss = 3.685\n",
      "Epoch  50 Batch   98/269   train_loss = 3.936\n",
      "Epoch  50 Batch   99/269   train_loss = 3.731\n",
      "Epoch  50 Batch  100/269   train_loss = 3.717\n",
      "Epoch  50 Batch  101/269   train_loss = 3.861\n",
      "Epoch  50 Batch  102/269   train_loss = 3.526\n",
      "Epoch  50 Batch  103/269   train_loss = 3.620\n",
      "Epoch  50 Batch  104/269   train_loss = 3.590\n",
      "Epoch  50 Batch  105/269   train_loss = 3.902\n",
      "Epoch  50 Batch  106/269   train_loss = 3.974\n",
      "Epoch  50 Batch  107/269   train_loss = 3.969\n",
      "Epoch  50 Batch  108/269   train_loss = 3.745\n",
      "Epoch  50 Batch  109/269   train_loss = 3.764\n",
      "Epoch  50 Batch  110/269   train_loss = 3.754\n",
      "Epoch  50 Batch  111/269   train_loss = 3.854\n",
      "Epoch  50 Batch  112/269   train_loss = 3.876\n",
      "Epoch  50 Batch  113/269   train_loss = 3.755\n",
      "Epoch  50 Batch  114/269   train_loss = 3.753\n",
      "Epoch  50 Batch  115/269   train_loss = 3.567\n",
      "Epoch  50 Batch  116/269   train_loss = 3.593\n",
      "Epoch  50 Batch  117/269   train_loss = 3.584\n",
      "Epoch  50 Batch  118/269   train_loss = 3.950\n",
      "Epoch  50 Batch  119/269   train_loss = 3.481\n",
      "Epoch  50 Batch  120/269   train_loss = 3.774\n",
      "Epoch  50 Batch  121/269   train_loss = 3.662\n",
      "Epoch  50 Batch  122/269   train_loss = 3.726\n",
      "Epoch  50 Batch  123/269   train_loss = 3.559\n",
      "Epoch  50 Batch  124/269   train_loss = 3.881\n",
      "Epoch  50 Batch  125/269   train_loss = 3.509\n",
      "Epoch  50 Batch  126/269   train_loss = 3.467\n",
      "Epoch  50 Batch  127/269   train_loss = 3.710\n",
      "Epoch  50 Batch  128/269   train_loss = 3.668\n",
      "Epoch  50 Batch  129/269   train_loss = 3.665\n",
      "Epoch  50 Batch  130/269   train_loss = 3.834\n",
      "Epoch  50 Batch  131/269   train_loss = 3.406\n",
      "Epoch  50 Batch  132/269   train_loss = 3.720\n",
      "Epoch  50 Batch  133/269   train_loss = 3.510\n",
      "Epoch  50 Batch  134/269   train_loss = 3.947\n",
      "Epoch  50 Batch  135/269   train_loss = 3.586\n",
      "Epoch  50 Batch  136/269   train_loss = 3.473\n",
      "Epoch  50 Batch  137/269   train_loss = 3.806\n",
      "Epoch  50 Batch  138/269   train_loss = 3.602\n",
      "Epoch  50 Batch  139/269   train_loss = 3.734\n",
      "Epoch  50 Batch  140/269   train_loss = 3.762\n",
      "Epoch  50 Batch  141/269   train_loss = 4.026\n",
      "Epoch  50 Batch  142/269   train_loss = 3.526\n",
      "Epoch  50 Batch  143/269   train_loss = 3.720\n",
      "Epoch  50 Batch  144/269   train_loss = 3.874\n",
      "Epoch  50 Batch  145/269   train_loss = 3.700\n",
      "Epoch  50 Batch  146/269   train_loss = 3.963\n",
      "Epoch  50 Batch  147/269   train_loss = 3.517\n",
      "Epoch  50 Batch  148/269   train_loss = 3.831\n",
      "Epoch  50 Batch  149/269   train_loss = 3.672\n",
      "Epoch  50 Batch  150/269   train_loss = 3.815\n",
      "Epoch  50 Batch  151/269   train_loss = 3.781\n",
      "Epoch  50 Batch  152/269   train_loss = 3.723\n",
      "Epoch  50 Batch  153/269   train_loss = 3.762\n",
      "Epoch  50 Batch  154/269   train_loss = 3.623\n",
      "Epoch  50 Batch  155/269   train_loss = 3.619\n",
      "Epoch  50 Batch  156/269   train_loss = 4.087\n",
      "Epoch  50 Batch  157/269   train_loss = 3.765\n",
      "Epoch  50 Batch  158/269   train_loss = 3.806\n",
      "Epoch  50 Batch  159/269   train_loss = 3.759\n",
      "Epoch  50 Batch  160/269   train_loss = 3.680\n",
      "Epoch  50 Batch  161/269   train_loss = 3.798\n",
      "Epoch  50 Batch  162/269   train_loss = 3.806\n",
      "Epoch  50 Batch  163/269   train_loss = 3.675\n",
      "Epoch  50 Batch  164/269   train_loss = 3.905\n",
      "Epoch  50 Batch  165/269   train_loss = 3.870\n",
      "Epoch  50 Batch  166/269   train_loss = 3.755\n",
      "Epoch  50 Batch  167/269   train_loss = 3.630\n",
      "Epoch  50 Batch  168/269   train_loss = 3.716\n",
      "Epoch  50 Batch  169/269   train_loss = 4.039\n",
      "Epoch  50 Batch  170/269   train_loss = 3.518\n",
      "Epoch  50 Batch  171/269   train_loss = 3.784\n",
      "Epoch  50 Batch  172/269   train_loss = 3.593\n",
      "Epoch  50 Batch  173/269   train_loss = 3.714\n",
      "Epoch  50 Batch  174/269   train_loss = 3.956\n",
      "Epoch  50 Batch  175/269   train_loss = 3.777\n",
      "Epoch  50 Batch  176/269   train_loss = 3.727\n",
      "Epoch  50 Batch  177/269   train_loss = 3.679\n",
      "Epoch  50 Batch  178/269   train_loss = 3.936\n",
      "Epoch  50 Batch  179/269   train_loss = 3.716\n",
      "Epoch  50 Batch  180/269   train_loss = 3.596\n",
      "Epoch  50 Batch  181/269   train_loss = 3.833\n",
      "Epoch  50 Batch  182/269   train_loss = 3.657\n",
      "Epoch  50 Batch  183/269   train_loss = 3.574\n",
      "Epoch  50 Batch  184/269   train_loss = 3.727\n",
      "Epoch  50 Batch  185/269   train_loss = 3.819\n",
      "Epoch  50 Batch  186/269   train_loss = 3.679\n",
      "Epoch  50 Batch  187/269   train_loss = 3.912\n",
      "Epoch  50 Batch  188/269   train_loss = 3.710\n",
      "Epoch  50 Batch  189/269   train_loss = 3.657\n",
      "Epoch  50 Batch  190/269   train_loss = 4.085\n",
      "Epoch  50 Batch  191/269   train_loss = 3.781\n",
      "Epoch  50 Batch  192/269   train_loss = 3.732\n",
      "Epoch  50 Batch  193/269   train_loss = 3.725\n",
      "Epoch  50 Batch  194/269   train_loss = 3.824\n",
      "Epoch  50 Batch  195/269   train_loss = 3.700\n",
      "Epoch  50 Batch  196/269   train_loss = 3.758\n",
      "Epoch  50 Batch  197/269   train_loss = 3.895\n",
      "Epoch  50 Batch  198/269   train_loss = 3.807\n",
      "Epoch  50 Batch  199/269   train_loss = 3.810\n",
      "Epoch  50 Batch  200/269   train_loss = 3.630\n",
      "Epoch  50 Batch  201/269   train_loss = 3.726\n",
      "Epoch  50 Batch  202/269   train_loss = 3.523\n",
      "Epoch  50 Batch  203/269   train_loss = 3.609\n",
      "Epoch  50 Batch  204/269   train_loss = 3.860\n",
      "Epoch  50 Batch  205/269   train_loss = 3.779\n",
      "Epoch  50 Batch  206/269   train_loss = 3.623\n",
      "Epoch  50 Batch  207/269   train_loss = 3.675\n",
      "Epoch  50 Batch  208/269   train_loss = 3.735\n",
      "Epoch  50 Batch  209/269   train_loss = 3.826\n",
      "Epoch  50 Batch  210/269   train_loss = 3.566\n",
      "Epoch  50 Batch  211/269   train_loss = 3.713\n",
      "Epoch  50 Batch  212/269   train_loss = 4.043\n",
      "Epoch  50 Batch  213/269   train_loss = 3.582\n",
      "Epoch  50 Batch  214/269   train_loss = 3.720\n",
      "Epoch  50 Batch  215/269   train_loss = 3.956\n",
      "Epoch  50 Batch  216/269   train_loss = 3.911\n",
      "Epoch  50 Batch  217/269   train_loss = 3.629\n",
      "Epoch  50 Batch  218/269   train_loss = 3.812\n",
      "Epoch  50 Batch  219/269   train_loss = 3.518\n",
      "Epoch  50 Batch  220/269   train_loss = 3.944\n",
      "Epoch  50 Batch  221/269   train_loss = 3.581\n",
      "Epoch  50 Batch  222/269   train_loss = 3.794\n",
      "Epoch  50 Batch  223/269   train_loss = 3.593\n",
      "Epoch  50 Batch  224/269   train_loss = 3.844\n",
      "Epoch  50 Batch  225/269   train_loss = 3.949\n",
      "Epoch  50 Batch  226/269   train_loss = 3.751\n",
      "Epoch  50 Batch  227/269   train_loss = 3.430\n",
      "Epoch  50 Batch  228/269   train_loss = 3.745\n",
      "Epoch  50 Batch  229/269   train_loss = 3.900\n",
      "Epoch  50 Batch  230/269   train_loss = 3.828\n",
      "Epoch  50 Batch  231/269   train_loss = 3.689\n",
      "Epoch  50 Batch  232/269   train_loss = 3.763\n",
      "Epoch  50 Batch  233/269   train_loss = 3.779\n",
      "Epoch  50 Batch  234/269   train_loss = 3.712\n",
      "Epoch  50 Batch  235/269   train_loss = 3.986\n",
      "Epoch  50 Batch  236/269   train_loss = 3.613\n",
      "Epoch  50 Batch  237/269   train_loss = 3.282\n",
      "Epoch  50 Batch  238/269   train_loss = 3.501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 Batch  239/269   train_loss = 4.055\n",
      "Epoch  50 Batch  240/269   train_loss = 3.657\n",
      "Epoch  50 Batch  241/269   train_loss = 3.930\n",
      "Epoch  50 Batch  242/269   train_loss = 3.523\n",
      "Epoch  50 Batch  243/269   train_loss = 3.980\n",
      "Epoch  50 Batch  244/269   train_loss = 3.721\n",
      "Epoch  50 Batch  245/269   train_loss = 3.747\n",
      "Epoch  50 Batch  246/269   train_loss = 3.361\n",
      "Epoch  50 Batch  247/269   train_loss = 3.778\n",
      "Epoch  50 Batch  248/269   train_loss = 3.778\n",
      "Epoch  50 Batch  249/269   train_loss = 3.604\n",
      "Epoch  50 Batch  250/269   train_loss = 3.525\n",
      "Epoch  50 Batch  251/269   train_loss = 3.953\n",
      "Epoch  50 Batch  252/269   train_loss = 3.876\n",
      "Epoch  50 Batch  253/269   train_loss = 3.659\n",
      "Epoch  50 Batch  254/269   train_loss = 3.631\n",
      "Epoch  50 Batch  255/269   train_loss = 3.684\n",
      "Epoch  50 Batch  256/269   train_loss = 3.624\n",
      "Epoch  50 Batch  257/269   train_loss = 3.564\n",
      "Epoch  50 Batch  258/269   train_loss = 3.447\n",
      "Epoch  50 Batch  259/269   train_loss = 3.577\n",
      "Epoch  50 Batch  260/269   train_loss = 3.781\n",
      "Epoch  50 Batch  261/269   train_loss = 3.854\n",
      "Epoch  50 Batch  262/269   train_loss = 3.638\n",
      "Epoch  50 Batch  263/269   train_loss = 3.509\n",
      "Epoch  50 Batch  264/269   train_loss = 3.938\n",
      "Epoch  50 Batch  265/269   train_loss = 3.817\n",
      "Epoch  50 Batch  266/269   train_loss = 3.628\n",
      "Epoch  50 Batch  267/269   train_loss = 3.738\n",
      "Epoch  50 Batch  268/269   train_loss = 3.884\n",
      "Epoch  51 Batch    0/269   train_loss = 3.669\n",
      "Epoch  51 Batch    1/269   train_loss = 3.512\n",
      "Epoch  51 Batch    2/269   train_loss = 3.633\n",
      "Epoch  51 Batch    3/269   train_loss = 3.652\n",
      "Epoch  51 Batch    4/269   train_loss = 4.076\n",
      "Epoch  51 Batch    5/269   train_loss = 3.759\n",
      "Epoch  51 Batch    6/269   train_loss = 3.628\n",
      "Epoch  51 Batch    7/269   train_loss = 3.466\n",
      "Epoch  51 Batch    8/269   train_loss = 3.809\n",
      "Epoch  51 Batch    9/269   train_loss = 3.476\n",
      "Epoch  51 Batch   10/269   train_loss = 3.583\n",
      "Epoch  51 Batch   11/269   train_loss = 3.643\n",
      "Epoch  51 Batch   12/269   train_loss = 3.396\n",
      "Epoch  51 Batch   13/269   train_loss = 3.725\n",
      "Epoch  51 Batch   14/269   train_loss = 3.583\n",
      "Epoch  51 Batch   15/269   train_loss = 3.927\n",
      "Epoch  51 Batch   16/269   train_loss = 3.692\n",
      "Epoch  51 Batch   17/269   train_loss = 3.709\n",
      "Epoch  51 Batch   18/269   train_loss = 3.678\n",
      "Epoch  51 Batch   19/269   train_loss = 3.650\n",
      "Epoch  51 Batch   20/269   train_loss = 4.059\n",
      "Epoch  51 Batch   21/269   train_loss = 3.909\n",
      "Epoch  51 Batch   22/269   train_loss = 3.573\n",
      "Epoch  51 Batch   23/269   train_loss = 3.841\n",
      "Epoch  51 Batch   24/269   train_loss = 3.549\n",
      "Epoch  51 Batch   25/269   train_loss = 3.892\n",
      "Epoch  51 Batch   26/269   train_loss = 3.740\n",
      "Epoch  51 Batch   27/269   train_loss = 3.577\n",
      "Epoch  51 Batch   28/269   train_loss = 3.763\n",
      "Epoch  51 Batch   29/269   train_loss = 3.770\n",
      "Epoch  51 Batch   30/269   train_loss = 4.031\n",
      "Epoch  51 Batch   31/269   train_loss = 3.785\n",
      "Epoch  51 Batch   32/269   train_loss = 3.641\n",
      "Epoch  51 Batch   33/269   train_loss = 3.410\n",
      "Epoch  51 Batch   34/269   train_loss = 3.623\n",
      "Epoch  51 Batch   35/269   train_loss = 3.534\n",
      "Epoch  51 Batch   36/269   train_loss = 3.563\n",
      "Epoch  51 Batch   37/269   train_loss = 3.734\n",
      "Epoch  51 Batch   38/269   train_loss = 3.459\n",
      "Epoch  51 Batch   39/269   train_loss = 3.758\n",
      "Epoch  51 Batch   40/269   train_loss = 3.598\n",
      "Epoch  51 Batch   41/269   train_loss = 3.574\n",
      "Epoch  51 Batch   42/269   train_loss = 4.024\n",
      "Epoch  51 Batch   43/269   train_loss = 3.917\n",
      "Epoch  51 Batch   44/269   train_loss = 3.723\n",
      "Epoch  51 Batch   45/269   train_loss = 3.396\n",
      "Epoch  51 Batch   46/269   train_loss = 3.785\n",
      "Epoch  51 Batch   47/269   train_loss = 3.438\n",
      "Epoch  51 Batch   48/269   train_loss = 3.906\n",
      "Epoch  51 Batch   49/269   train_loss = 3.930\n",
      "Epoch  51 Batch   50/269   train_loss = 3.837\n",
      "Epoch  51 Batch   51/269   train_loss = 3.745\n",
      "Epoch  51 Batch   52/269   train_loss = 3.731\n",
      "Epoch  51 Batch   53/269   train_loss = 3.961\n",
      "Epoch  51 Batch   54/269   train_loss = 3.707\n",
      "Epoch  51 Batch   55/269   train_loss = 3.833\n",
      "Epoch  51 Batch   56/269   train_loss = 3.592\n",
      "Epoch  51 Batch   57/269   train_loss = 3.877\n",
      "Epoch  51 Batch   58/269   train_loss = 3.582\n",
      "Epoch  51 Batch   59/269   train_loss = 3.611\n",
      "Epoch  51 Batch   60/269   train_loss = 3.749\n",
      "Epoch  51 Batch   61/269   train_loss = 3.950\n",
      "Epoch  51 Batch   62/269   train_loss = 3.579\n",
      "Epoch  51 Batch   63/269   train_loss = 3.933\n",
      "Epoch  51 Batch   64/269   train_loss = 3.640\n",
      "Epoch  51 Batch   65/269   train_loss = 3.752\n",
      "Epoch  51 Batch   66/269   train_loss = 3.654\n",
      "Epoch  51 Batch   67/269   train_loss = 3.682\n",
      "Epoch  51 Batch   68/269   train_loss = 3.784\n",
      "Epoch  51 Batch   69/269   train_loss = 3.778\n",
      "Epoch  51 Batch   70/269   train_loss = 3.445\n",
      "Epoch  51 Batch   71/269   train_loss = 3.785\n",
      "Epoch  51 Batch   72/269   train_loss = 3.503\n",
      "Epoch  51 Batch   73/269   train_loss = 3.693\n",
      "Epoch  51 Batch   74/269   train_loss = 3.633\n",
      "Epoch  51 Batch   75/269   train_loss = 3.847\n",
      "Epoch  51 Batch   76/269   train_loss = 3.782\n",
      "Epoch  51 Batch   77/269   train_loss = 3.453\n",
      "Epoch  51 Batch   78/269   train_loss = 3.845\n",
      "Epoch  51 Batch   79/269   train_loss = 3.911\n",
      "Epoch  51 Batch   80/269   train_loss = 3.836\n",
      "Epoch  51 Batch   81/269   train_loss = 3.649\n",
      "Epoch  51 Batch   82/269   train_loss = 3.863\n",
      "Epoch  51 Batch   83/269   train_loss = 3.612\n",
      "Epoch  51 Batch   84/269   train_loss = 3.952\n",
      "Epoch  51 Batch   85/269   train_loss = 4.062\n",
      "Epoch  51 Batch   86/269   train_loss = 4.035\n",
      "Epoch  51 Batch   87/269   train_loss = 3.657\n",
      "Epoch  51 Batch   88/269   train_loss = 3.778\n",
      "Epoch  51 Batch   89/269   train_loss = 3.775\n",
      "Epoch  51 Batch   90/269   train_loss = 3.589\n",
      "Epoch  51 Batch   91/269   train_loss = 3.863\n",
      "Epoch  51 Batch   92/269   train_loss = 3.867\n",
      "Epoch  51 Batch   93/269   train_loss = 3.585\n",
      "Epoch  51 Batch   94/269   train_loss = 4.116\n",
      "Epoch  51 Batch   95/269   train_loss = 3.489\n",
      "Epoch  51 Batch   96/269   train_loss = 3.657\n",
      "Epoch  51 Batch   97/269   train_loss = 3.685\n",
      "Epoch  51 Batch   98/269   train_loss = 3.913\n",
      "Epoch  51 Batch   99/269   train_loss = 3.724\n",
      "Epoch  51 Batch  100/269   train_loss = 3.695\n",
      "Epoch  51 Batch  101/269   train_loss = 3.854\n",
      "Epoch  51 Batch  102/269   train_loss = 3.508\n",
      "Epoch  51 Batch  103/269   train_loss = 3.631\n",
      "Epoch  51 Batch  104/269   train_loss = 3.564\n",
      "Epoch  51 Batch  105/269   train_loss = 3.884\n",
      "Epoch  51 Batch  106/269   train_loss = 3.981\n",
      "Epoch  51 Batch  107/269   train_loss = 3.987\n",
      "Epoch  51 Batch  108/269   train_loss = 3.755\n",
      "Epoch  51 Batch  109/269   train_loss = 3.758\n",
      "Epoch  51 Batch  110/269   train_loss = 3.719\n",
      "Epoch  51 Batch  111/269   train_loss = 3.852\n",
      "Epoch  51 Batch  112/269   train_loss = 3.863\n",
      "Epoch  51 Batch  113/269   train_loss = 3.752\n",
      "Epoch  51 Batch  114/269   train_loss = 3.760\n",
      "Epoch  51 Batch  115/269   train_loss = 3.566\n",
      "Epoch  51 Batch  116/269   train_loss = 3.558\n",
      "Epoch  51 Batch  117/269   train_loss = 3.598\n",
      "Epoch  51 Batch  118/269   train_loss = 3.936\n",
      "Epoch  51 Batch  119/269   train_loss = 3.477\n",
      "Epoch  51 Batch  120/269   train_loss = 3.791\n",
      "Epoch  51 Batch  121/269   train_loss = 3.664\n",
      "Epoch  51 Batch  122/269   train_loss = 3.714\n",
      "Epoch  51 Batch  123/269   train_loss = 3.547\n",
      "Epoch  51 Batch  124/269   train_loss = 3.892\n",
      "Epoch  51 Batch  125/269   train_loss = 3.488\n",
      "Epoch  51 Batch  126/269   train_loss = 3.453\n",
      "Epoch  51 Batch  127/269   train_loss = 3.714\n",
      "Epoch  51 Batch  128/269   train_loss = 3.651\n",
      "Epoch  51 Batch  129/269   train_loss = 3.665\n",
      "Epoch  51 Batch  130/269   train_loss = 3.837\n",
      "Epoch  51 Batch  131/269   train_loss = 3.370\n",
      "Epoch  51 Batch  132/269   train_loss = 3.698\n",
      "Epoch  51 Batch  133/269   train_loss = 3.500\n",
      "Epoch  51 Batch  134/269   train_loss = 3.966\n",
      "Epoch  51 Batch  135/269   train_loss = 3.573\n",
      "Epoch  51 Batch  136/269   train_loss = 3.462\n",
      "Epoch  51 Batch  137/269   train_loss = 3.791\n",
      "Epoch  51 Batch  138/269   train_loss = 3.598\n",
      "Epoch  51 Batch  139/269   train_loss = 3.747\n",
      "Epoch  51 Batch  140/269   train_loss = 3.774\n",
      "Epoch  51 Batch  141/269   train_loss = 4.039\n",
      "Epoch  51 Batch  142/269   train_loss = 3.510\n",
      "Epoch  51 Batch  143/269   train_loss = 3.755\n",
      "Epoch  51 Batch  144/269   train_loss = 3.887\n",
      "Epoch  51 Batch  145/269   train_loss = 3.646\n",
      "Epoch  51 Batch  146/269   train_loss = 3.938\n",
      "Epoch  51 Batch  147/269   train_loss = 3.519\n",
      "Epoch  51 Batch  148/269   train_loss = 3.830\n",
      "Epoch  51 Batch  149/269   train_loss = 3.663\n",
      "Epoch  51 Batch  150/269   train_loss = 3.817\n",
      "Epoch  51 Batch  151/269   train_loss = 3.747\n",
      "Epoch  51 Batch  152/269   train_loss = 3.728\n",
      "Epoch  51 Batch  153/269   train_loss = 3.749\n",
      "Epoch  51 Batch  154/269   train_loss = 3.640\n",
      "Epoch  51 Batch  155/269   train_loss = 3.642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  51 Batch  156/269   train_loss = 4.084\n",
      "Epoch  51 Batch  157/269   train_loss = 3.787\n",
      "Epoch  51 Batch  158/269   train_loss = 3.793\n",
      "Epoch  51 Batch  159/269   train_loss = 3.750\n",
      "Epoch  51 Batch  160/269   train_loss = 3.682\n",
      "Epoch  51 Batch  161/269   train_loss = 3.803\n",
      "Epoch  51 Batch  162/269   train_loss = 3.783\n",
      "Epoch  51 Batch  163/269   train_loss = 3.677\n",
      "Epoch  51 Batch  164/269   train_loss = 3.900\n",
      "Epoch  51 Batch  165/269   train_loss = 3.847\n",
      "Epoch  51 Batch  166/269   train_loss = 3.762\n",
      "Epoch  51 Batch  167/269   train_loss = 3.615\n",
      "Epoch  51 Batch  168/269   train_loss = 3.731\n",
      "Epoch  51 Batch  169/269   train_loss = 4.048\n",
      "Epoch  51 Batch  170/269   train_loss = 3.486\n",
      "Epoch  51 Batch  171/269   train_loss = 3.762\n",
      "Epoch  51 Batch  172/269   train_loss = 3.589\n",
      "Epoch  51 Batch  173/269   train_loss = 3.739\n",
      "Epoch  51 Batch  174/269   train_loss = 3.963\n",
      "Epoch  51 Batch  175/269   train_loss = 3.781\n",
      "Epoch  51 Batch  176/269   train_loss = 3.744\n",
      "Epoch  51 Batch  177/269   train_loss = 3.660\n",
      "Epoch  51 Batch  178/269   train_loss = 3.939\n",
      "Epoch  51 Batch  179/269   train_loss = 3.747\n",
      "Epoch  51 Batch  180/269   train_loss = 3.600\n",
      "Epoch  51 Batch  181/269   train_loss = 3.824\n",
      "Epoch  51 Batch  182/269   train_loss = 3.648\n",
      "Epoch  51 Batch  183/269   train_loss = 3.585\n",
      "Epoch  51 Batch  184/269   train_loss = 3.727\n",
      "Epoch  51 Batch  185/269   train_loss = 3.819\n",
      "Epoch  51 Batch  186/269   train_loss = 3.696\n",
      "Epoch  51 Batch  187/269   train_loss = 3.907\n",
      "Epoch  51 Batch  188/269   train_loss = 3.685\n",
      "Epoch  51 Batch  189/269   train_loss = 3.643\n",
      "Epoch  51 Batch  190/269   train_loss = 4.081\n",
      "Epoch  51 Batch  191/269   train_loss = 3.792\n",
      "Epoch  51 Batch  192/269   train_loss = 3.757\n",
      "Epoch  51 Batch  193/269   train_loss = 3.714\n",
      "Epoch  51 Batch  194/269   train_loss = 3.841\n",
      "Epoch  51 Batch  195/269   train_loss = 3.721\n",
      "Epoch  51 Batch  196/269   train_loss = 3.743\n",
      "Epoch  51 Batch  197/269   train_loss = 3.859\n",
      "Epoch  51 Batch  198/269   train_loss = 3.785\n",
      "Epoch  51 Batch  199/269   train_loss = 3.797\n",
      "Epoch  51 Batch  200/269   train_loss = 3.629\n",
      "Epoch  51 Batch  201/269   train_loss = 3.725\n",
      "Epoch  51 Batch  202/269   train_loss = 3.516\n",
      "Epoch  51 Batch  203/269   train_loss = 3.603\n",
      "Epoch  51 Batch  204/269   train_loss = 3.824\n",
      "Epoch  51 Batch  205/269   train_loss = 3.779\n",
      "Epoch  51 Batch  206/269   train_loss = 3.665\n",
      "Epoch  51 Batch  207/269   train_loss = 3.649\n",
      "Epoch  51 Batch  208/269   train_loss = 3.764\n",
      "Epoch  51 Batch  209/269   train_loss = 3.831\n",
      "Epoch  51 Batch  210/269   train_loss = 3.545\n",
      "Epoch  51 Batch  211/269   train_loss = 3.708\n",
      "Epoch  51 Batch  212/269   train_loss = 4.025\n",
      "Epoch  51 Batch  213/269   train_loss = 3.612\n",
      "Epoch  51 Batch  214/269   train_loss = 3.713\n",
      "Epoch  51 Batch  215/269   train_loss = 3.955\n",
      "Epoch  51 Batch  216/269   train_loss = 3.918\n",
      "Epoch  51 Batch  217/269   train_loss = 3.631\n",
      "Epoch  51 Batch  218/269   train_loss = 3.800\n",
      "Epoch  51 Batch  219/269   train_loss = 3.507\n",
      "Epoch  51 Batch  220/269   train_loss = 3.903\n",
      "Epoch  51 Batch  221/269   train_loss = 3.563\n",
      "Epoch  51 Batch  222/269   train_loss = 3.775\n",
      "Epoch  51 Batch  223/269   train_loss = 3.572\n",
      "Epoch  51 Batch  224/269   train_loss = 3.835\n",
      "Epoch  51 Batch  225/269   train_loss = 3.941\n",
      "Epoch  51 Batch  226/269   train_loss = 3.749\n",
      "Epoch  51 Batch  227/269   train_loss = 3.417\n",
      "Epoch  51 Batch  228/269   train_loss = 3.749\n",
      "Epoch  51 Batch  229/269   train_loss = 3.909\n",
      "Epoch  51 Batch  230/269   train_loss = 3.833\n",
      "Epoch  51 Batch  231/269   train_loss = 3.671\n",
      "Epoch  51 Batch  232/269   train_loss = 3.744\n",
      "Epoch  51 Batch  233/269   train_loss = 3.778\n",
      "Epoch  51 Batch  234/269   train_loss = 3.695\n",
      "Epoch  51 Batch  235/269   train_loss = 3.984\n",
      "Epoch  51 Batch  236/269   train_loss = 3.619\n",
      "Epoch  51 Batch  237/269   train_loss = 3.284\n",
      "Epoch  51 Batch  238/269   train_loss = 3.487\n",
      "Epoch  51 Batch  239/269   train_loss = 4.026\n",
      "Epoch  51 Batch  240/269   train_loss = 3.642\n",
      "Epoch  51 Batch  241/269   train_loss = 3.963\n",
      "Epoch  51 Batch  242/269   train_loss = 3.515\n",
      "Epoch  51 Batch  243/269   train_loss = 3.915\n",
      "Epoch  51 Batch  244/269   train_loss = 3.734\n",
      "Epoch  51 Batch  245/269   train_loss = 3.735\n",
      "Epoch  51 Batch  246/269   train_loss = 3.347\n",
      "Epoch  51 Batch  247/269   train_loss = 3.799\n",
      "Epoch  51 Batch  248/269   train_loss = 3.774\n",
      "Epoch  51 Batch  249/269   train_loss = 3.608\n",
      "Epoch  51 Batch  250/269   train_loss = 3.524\n",
      "Epoch  51 Batch  251/269   train_loss = 3.946\n",
      "Epoch  51 Batch  252/269   train_loss = 3.879\n",
      "Epoch  51 Batch  253/269   train_loss = 3.628\n",
      "Epoch  51 Batch  254/269   train_loss = 3.635\n",
      "Epoch  51 Batch  255/269   train_loss = 3.692\n",
      "Epoch  51 Batch  256/269   train_loss = 3.615\n",
      "Epoch  51 Batch  257/269   train_loss = 3.568\n",
      "Epoch  51 Batch  258/269   train_loss = 3.443\n",
      "Epoch  51 Batch  259/269   train_loss = 3.572\n",
      "Epoch  51 Batch  260/269   train_loss = 3.778\n",
      "Epoch  51 Batch  261/269   train_loss = 3.848\n",
      "Epoch  51 Batch  262/269   train_loss = 3.635\n",
      "Epoch  51 Batch  263/269   train_loss = 3.527\n",
      "Epoch  51 Batch  264/269   train_loss = 3.942\n",
      "Epoch  51 Batch  265/269   train_loss = 3.816\n",
      "Epoch  51 Batch  266/269   train_loss = 3.661\n",
      "Epoch  51 Batch  267/269   train_loss = 3.725\n",
      "Epoch  51 Batch  268/269   train_loss = 3.875\n",
      "Epoch  52 Batch    0/269   train_loss = 3.666\n",
      "Epoch  52 Batch    1/269   train_loss = 3.490\n",
      "Epoch  52 Batch    2/269   train_loss = 3.632\n",
      "Epoch  52 Batch    3/269   train_loss = 3.654\n",
      "Epoch  52 Batch    4/269   train_loss = 4.079\n",
      "Epoch  52 Batch    5/269   train_loss = 3.747\n",
      "Epoch  52 Batch    6/269   train_loss = 3.610\n",
      "Epoch  52 Batch    7/269   train_loss = 3.455\n",
      "Epoch  52 Batch    8/269   train_loss = 3.801\n",
      "Epoch  52 Batch    9/269   train_loss = 3.467\n",
      "Epoch  52 Batch   10/269   train_loss = 3.569\n",
      "Epoch  52 Batch   11/269   train_loss = 3.657\n",
      "Epoch  52 Batch   12/269   train_loss = 3.386\n",
      "Epoch  52 Batch   13/269   train_loss = 3.757\n",
      "Epoch  52 Batch   14/269   train_loss = 3.557\n",
      "Epoch  52 Batch   15/269   train_loss = 3.925\n",
      "Epoch  52 Batch   16/269   train_loss = 3.668\n",
      "Epoch  52 Batch   17/269   train_loss = 3.714\n",
      "Epoch  52 Batch   18/269   train_loss = 3.665\n",
      "Epoch  52 Batch   19/269   train_loss = 3.638\n",
      "Epoch  52 Batch   20/269   train_loss = 4.020\n",
      "Epoch  52 Batch   21/269   train_loss = 3.869\n",
      "Epoch  52 Batch   22/269   train_loss = 3.601\n",
      "Epoch  52 Batch   23/269   train_loss = 3.789\n",
      "Epoch  52 Batch   24/269   train_loss = 3.543\n",
      "Epoch  52 Batch   25/269   train_loss = 3.892\n",
      "Epoch  52 Batch   26/269   train_loss = 3.744\n",
      "Epoch  52 Batch   27/269   train_loss = 3.577\n",
      "Epoch  52 Batch   28/269   train_loss = 3.754\n",
      "Epoch  52 Batch   29/269   train_loss = 3.756\n",
      "Epoch  52 Batch   30/269   train_loss = 4.030\n",
      "Epoch  52 Batch   31/269   train_loss = 3.771\n",
      "Epoch  52 Batch   32/269   train_loss = 3.637\n",
      "Epoch  52 Batch   33/269   train_loss = 3.412\n",
      "Epoch  52 Batch   34/269   train_loss = 3.633\n",
      "Epoch  52 Batch   35/269   train_loss = 3.508\n",
      "Epoch  52 Batch   36/269   train_loss = 3.552\n",
      "Epoch  52 Batch   37/269   train_loss = 3.748\n",
      "Epoch  52 Batch   38/269   train_loss = 3.444\n",
      "Epoch  52 Batch   39/269   train_loss = 3.749\n",
      "Epoch  52 Batch   40/269   train_loss = 3.582\n",
      "Epoch  52 Batch   41/269   train_loss = 3.594\n",
      "Epoch  52 Batch   42/269   train_loss = 4.037\n",
      "Epoch  52 Batch   43/269   train_loss = 3.911\n",
      "Epoch  52 Batch   44/269   train_loss = 3.723\n",
      "Epoch  52 Batch   45/269   train_loss = 3.381\n",
      "Epoch  52 Batch   46/269   train_loss = 3.787\n",
      "Epoch  52 Batch   47/269   train_loss = 3.445\n",
      "Epoch  52 Batch   48/269   train_loss = 3.901\n",
      "Epoch  52 Batch   49/269   train_loss = 3.921\n",
      "Epoch  52 Batch   50/269   train_loss = 3.834\n",
      "Epoch  52 Batch   51/269   train_loss = 3.730\n",
      "Epoch  52 Batch   52/269   train_loss = 3.724\n",
      "Epoch  52 Batch   53/269   train_loss = 3.954\n",
      "Epoch  52 Batch   54/269   train_loss = 3.708\n",
      "Epoch  52 Batch   55/269   train_loss = 3.803\n",
      "Epoch  52 Batch   56/269   train_loss = 3.602\n",
      "Epoch  52 Batch   57/269   train_loss = 3.873\n",
      "Epoch  52 Batch   58/269   train_loss = 3.562\n",
      "Epoch  52 Batch   59/269   train_loss = 3.623\n",
      "Epoch  52 Batch   60/269   train_loss = 3.743\n",
      "Epoch  52 Batch   61/269   train_loss = 3.919\n",
      "Epoch  52 Batch   62/269   train_loss = 3.586\n",
      "Epoch  52 Batch   63/269   train_loss = 3.923\n",
      "Epoch  52 Batch   64/269   train_loss = 3.641\n",
      "Epoch  52 Batch   65/269   train_loss = 3.770\n",
      "Epoch  52 Batch   66/269   train_loss = 3.644\n",
      "Epoch  52 Batch   67/269   train_loss = 3.659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  52 Batch   68/269   train_loss = 3.785\n",
      "Epoch  52 Batch   69/269   train_loss = 3.782\n",
      "Epoch  52 Batch   70/269   train_loss = 3.452\n",
      "Epoch  52 Batch   71/269   train_loss = 3.775\n",
      "Epoch  52 Batch   72/269   train_loss = 3.471\n",
      "Epoch  52 Batch   73/269   train_loss = 3.686\n",
      "Epoch  52 Batch   74/269   train_loss = 3.632\n",
      "Epoch  52 Batch   75/269   train_loss = 3.832\n",
      "Epoch  52 Batch   76/269   train_loss = 3.775\n",
      "Epoch  52 Batch   77/269   train_loss = 3.451\n",
      "Epoch  52 Batch   78/269   train_loss = 3.844\n",
      "Epoch  52 Batch   79/269   train_loss = 3.912\n",
      "Epoch  52 Batch   80/269   train_loss = 3.819\n",
      "Epoch  52 Batch   81/269   train_loss = 3.676\n",
      "Epoch  52 Batch   82/269   train_loss = 3.862\n",
      "Epoch  52 Batch   83/269   train_loss = 3.620\n",
      "Epoch  52 Batch   84/269   train_loss = 3.945\n",
      "Epoch  52 Batch   85/269   train_loss = 4.072\n",
      "Epoch  52 Batch   86/269   train_loss = 4.039\n",
      "Epoch  52 Batch   87/269   train_loss = 3.653\n",
      "Epoch  52 Batch   88/269   train_loss = 3.816\n",
      "Epoch  52 Batch   89/269   train_loss = 3.770\n",
      "Epoch  52 Batch   90/269   train_loss = 3.543\n",
      "Epoch  52 Batch   91/269   train_loss = 3.840\n",
      "Epoch  52 Batch   92/269   train_loss = 3.850\n",
      "Epoch  52 Batch   93/269   train_loss = 3.602\n",
      "Epoch  52 Batch   94/269   train_loss = 4.100\n",
      "Epoch  52 Batch   95/269   train_loss = 3.492\n",
      "Epoch  52 Batch   96/269   train_loss = 3.643\n",
      "Epoch  52 Batch   97/269   train_loss = 3.671\n",
      "Epoch  52 Batch   98/269   train_loss = 3.921\n",
      "Epoch  52 Batch   99/269   train_loss = 3.706\n",
      "Epoch  52 Batch  100/269   train_loss = 3.694\n",
      "Epoch  52 Batch  101/269   train_loss = 3.848\n",
      "Epoch  52 Batch  102/269   train_loss = 3.510\n",
      "Epoch  52 Batch  103/269   train_loss = 3.602\n",
      "Epoch  52 Batch  104/269   train_loss = 3.552\n",
      "Epoch  52 Batch  105/269   train_loss = 3.896\n",
      "Epoch  52 Batch  106/269   train_loss = 3.947\n",
      "Epoch  52 Batch  107/269   train_loss = 4.047\n",
      "Epoch  52 Batch  108/269   train_loss = 3.750\n",
      "Epoch  52 Batch  109/269   train_loss = 3.751\n",
      "Epoch  52 Batch  110/269   train_loss = 3.737\n",
      "Epoch  52 Batch  111/269   train_loss = 3.851\n",
      "Epoch  52 Batch  112/269   train_loss = 3.865\n",
      "Epoch  52 Batch  113/269   train_loss = 3.750\n",
      "Epoch  52 Batch  114/269   train_loss = 3.729\n",
      "Epoch  52 Batch  115/269   train_loss = 3.554\n",
      "Epoch  52 Batch  116/269   train_loss = 3.574\n",
      "Epoch  52 Batch  117/269   train_loss = 3.586\n",
      "Epoch  52 Batch  118/269   train_loss = 3.941\n",
      "Epoch  52 Batch  119/269   train_loss = 3.466\n",
      "Epoch  52 Batch  120/269   train_loss = 3.760\n",
      "Epoch  52 Batch  121/269   train_loss = 3.657\n",
      "Epoch  52 Batch  122/269   train_loss = 3.723\n",
      "Epoch  52 Batch  123/269   train_loss = 3.550\n",
      "Epoch  52 Batch  124/269   train_loss = 3.877\n",
      "Epoch  52 Batch  125/269   train_loss = 3.503\n",
      "Epoch  52 Batch  126/269   train_loss = 3.451\n",
      "Epoch  52 Batch  127/269   train_loss = 3.686\n",
      "Epoch  52 Batch  128/269   train_loss = 3.642\n",
      "Epoch  52 Batch  129/269   train_loss = 3.642\n",
      "Epoch  52 Batch  130/269   train_loss = 3.810\n",
      "Epoch  52 Batch  131/269   train_loss = 3.368\n",
      "Epoch  52 Batch  132/269   train_loss = 3.717\n",
      "Epoch  52 Batch  133/269   train_loss = 3.477\n",
      "Epoch  52 Batch  134/269   train_loss = 3.928\n",
      "Epoch  52 Batch  135/269   train_loss = 3.563\n",
      "Epoch  52 Batch  136/269   train_loss = 3.458\n",
      "Epoch  52 Batch  137/269   train_loss = 3.783\n",
      "Epoch  52 Batch  138/269   train_loss = 3.591\n",
      "Epoch  52 Batch  139/269   train_loss = 3.733\n",
      "Epoch  52 Batch  140/269   train_loss = 3.778\n",
      "Epoch  52 Batch  141/269   train_loss = 4.063\n",
      "Epoch  52 Batch  142/269   train_loss = 3.496\n",
      "Epoch  52 Batch  143/269   train_loss = 3.705\n",
      "Epoch  52 Batch  144/269   train_loss = 3.926\n",
      "Epoch  52 Batch  145/269   train_loss = 3.659\n",
      "Epoch  52 Batch  146/269   train_loss = 3.918\n",
      "Epoch  52 Batch  147/269   train_loss = 3.500\n",
      "Epoch  52 Batch  148/269   train_loss = 3.833\n",
      "Epoch  52 Batch  149/269   train_loss = 3.656\n",
      "Epoch  52 Batch  150/269   train_loss = 3.818\n",
      "Epoch  52 Batch  151/269   train_loss = 3.757\n",
      "Epoch  52 Batch  152/269   train_loss = 3.695\n",
      "Epoch  52 Batch  153/269   train_loss = 3.728\n",
      "Epoch  52 Batch  154/269   train_loss = 3.605\n",
      "Epoch  52 Batch  155/269   train_loss = 3.617\n",
      "Epoch  52 Batch  156/269   train_loss = 4.085\n",
      "Epoch  52 Batch  157/269   train_loss = 3.769\n",
      "Epoch  52 Batch  158/269   train_loss = 3.774\n",
      "Epoch  52 Batch  159/269   train_loss = 3.776\n",
      "Epoch  52 Batch  160/269   train_loss = 3.705\n",
      "Epoch  52 Batch  161/269   train_loss = 3.803\n",
      "Epoch  52 Batch  162/269   train_loss = 3.830\n",
      "Epoch  52 Batch  163/269   train_loss = 3.688\n",
      "Epoch  52 Batch  164/269   train_loss = 3.885\n",
      "Epoch  52 Batch  165/269   train_loss = 3.853\n",
      "Epoch  52 Batch  166/269   train_loss = 3.770\n",
      "Epoch  52 Batch  167/269   train_loss = 3.610\n",
      "Epoch  52 Batch  168/269   train_loss = 3.706\n",
      "Epoch  52 Batch  169/269   train_loss = 4.040\n",
      "Epoch  52 Batch  170/269   train_loss = 3.494\n",
      "Epoch  52 Batch  171/269   train_loss = 3.748\n",
      "Epoch  52 Batch  172/269   train_loss = 3.578\n",
      "Epoch  52 Batch  173/269   train_loss = 3.742\n",
      "Epoch  52 Batch  174/269   train_loss = 3.951\n",
      "Epoch  52 Batch  175/269   train_loss = 3.772\n",
      "Epoch  52 Batch  176/269   train_loss = 3.715\n",
      "Epoch  52 Batch  177/269   train_loss = 3.679\n",
      "Epoch  52 Batch  178/269   train_loss = 3.935\n",
      "Epoch  52 Batch  179/269   train_loss = 3.744\n",
      "Epoch  52 Batch  180/269   train_loss = 3.578\n",
      "Epoch  52 Batch  181/269   train_loss = 3.820\n",
      "Epoch  52 Batch  182/269   train_loss = 3.626\n",
      "Epoch  52 Batch  183/269   train_loss = 3.597\n",
      "Epoch  52 Batch  184/269   train_loss = 3.690\n",
      "Epoch  52 Batch  185/269   train_loss = 3.811\n",
      "Epoch  52 Batch  186/269   train_loss = 3.668\n",
      "Epoch  52 Batch  187/269   train_loss = 3.894\n",
      "Epoch  52 Batch  188/269   train_loss = 3.675\n",
      "Epoch  52 Batch  189/269   train_loss = 3.634\n",
      "Epoch  52 Batch  190/269   train_loss = 4.094\n",
      "Epoch  52 Batch  191/269   train_loss = 3.798\n",
      "Epoch  52 Batch  192/269   train_loss = 3.743\n",
      "Epoch  52 Batch  193/269   train_loss = 3.739\n",
      "Epoch  52 Batch  194/269   train_loss = 3.800\n",
      "Epoch  52 Batch  195/269   train_loss = 3.696\n",
      "Epoch  52 Batch  196/269   train_loss = 3.738\n",
      "Epoch  52 Batch  197/269   train_loss = 3.852\n",
      "Epoch  52 Batch  198/269   train_loss = 3.791\n",
      "Epoch  52 Batch  199/269   train_loss = 3.788\n",
      "Epoch  52 Batch  200/269   train_loss = 3.589\n",
      "Epoch  52 Batch  201/269   train_loss = 3.732\n",
      "Epoch  52 Batch  202/269   train_loss = 3.508\n",
      "Epoch  52 Batch  203/269   train_loss = 3.601\n",
      "Epoch  52 Batch  204/269   train_loss = 3.823\n",
      "Epoch  52 Batch  205/269   train_loss = 3.764\n",
      "Epoch  52 Batch  206/269   train_loss = 3.665\n",
      "Epoch  52 Batch  207/269   train_loss = 3.633\n",
      "Epoch  52 Batch  208/269   train_loss = 3.764\n",
      "Epoch  52 Batch  209/269   train_loss = 3.833\n",
      "Epoch  52 Batch  210/269   train_loss = 3.563\n",
      "Epoch  52 Batch  211/269   train_loss = 3.700\n",
      "Epoch  52 Batch  212/269   train_loss = 4.017\n",
      "Epoch  52 Batch  213/269   train_loss = 3.602\n",
      "Epoch  52 Batch  214/269   train_loss = 3.710\n",
      "Epoch  52 Batch  215/269   train_loss = 3.944\n",
      "Epoch  52 Batch  216/269   train_loss = 3.913\n",
      "Epoch  52 Batch  217/269   train_loss = 3.614\n",
      "Epoch  52 Batch  218/269   train_loss = 3.818\n",
      "Epoch  52 Batch  219/269   train_loss = 3.482\n",
      "Epoch  52 Batch  220/269   train_loss = 3.916\n",
      "Epoch  52 Batch  221/269   train_loss = 3.561\n",
      "Epoch  52 Batch  222/269   train_loss = 3.762\n",
      "Epoch  52 Batch  223/269   train_loss = 3.553\n",
      "Epoch  52 Batch  224/269   train_loss = 3.839\n",
      "Epoch  52 Batch  225/269   train_loss = 3.930\n",
      "Epoch  52 Batch  226/269   train_loss = 3.715\n",
      "Epoch  52 Batch  227/269   train_loss = 3.413\n",
      "Epoch  52 Batch  228/269   train_loss = 3.742\n",
      "Epoch  52 Batch  229/269   train_loss = 3.926\n",
      "Epoch  52 Batch  230/269   train_loss = 3.845\n",
      "Epoch  52 Batch  231/269   train_loss = 3.666\n",
      "Epoch  52 Batch  232/269   train_loss = 3.741\n",
      "Epoch  52 Batch  233/269   train_loss = 3.777\n",
      "Epoch  52 Batch  234/269   train_loss = 3.701\n",
      "Epoch  52 Batch  235/269   train_loss = 3.990\n",
      "Epoch  52 Batch  236/269   train_loss = 3.605\n",
      "Epoch  52 Batch  237/269   train_loss = 3.275\n",
      "Epoch  52 Batch  238/269   train_loss = 3.482\n",
      "Epoch  52 Batch  239/269   train_loss = 4.017\n",
      "Epoch  52 Batch  240/269   train_loss = 3.654\n",
      "Epoch  52 Batch  241/269   train_loss = 3.929\n",
      "Epoch  52 Batch  242/269   train_loss = 3.517\n",
      "Epoch  52 Batch  243/269   train_loss = 3.913\n",
      "Epoch  52 Batch  244/269   train_loss = 3.734\n",
      "Epoch  52 Batch  245/269   train_loss = 3.761\n",
      "Epoch  52 Batch  246/269   train_loss = 3.362\n",
      "Epoch  52 Batch  247/269   train_loss = 3.820\n",
      "Epoch  52 Batch  248/269   train_loss = 3.775\n",
      "Epoch  52 Batch  249/269   train_loss = 3.606\n",
      "Epoch  52 Batch  250/269   train_loss = 3.516\n",
      "Epoch  52 Batch  251/269   train_loss = 3.953\n",
      "Epoch  52 Batch  252/269   train_loss = 3.902\n",
      "Epoch  52 Batch  253/269   train_loss = 3.610\n",
      "Epoch  52 Batch  254/269   train_loss = 3.633\n",
      "Epoch  52 Batch  255/269   train_loss = 3.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  52 Batch  256/269   train_loss = 3.600\n",
      "Epoch  52 Batch  257/269   train_loss = 3.569\n",
      "Epoch  52 Batch  258/269   train_loss = 3.442\n",
      "Epoch  52 Batch  259/269   train_loss = 3.579\n",
      "Epoch  52 Batch  260/269   train_loss = 3.790\n",
      "Epoch  52 Batch  261/269   train_loss = 3.843\n",
      "Epoch  52 Batch  262/269   train_loss = 3.659\n",
      "Epoch  52 Batch  263/269   train_loss = 3.525\n",
      "Epoch  52 Batch  264/269   train_loss = 3.923\n",
      "Epoch  52 Batch  265/269   train_loss = 3.821\n",
      "Epoch  52 Batch  266/269   train_loss = 3.631\n",
      "Epoch  52 Batch  267/269   train_loss = 3.722\n",
      "Epoch  52 Batch  268/269   train_loss = 3.861\n",
      "Epoch  53 Batch    0/269   train_loss = 3.647\n",
      "Epoch  53 Batch    1/269   train_loss = 3.507\n",
      "Epoch  53 Batch    2/269   train_loss = 3.598\n",
      "Epoch  53 Batch    3/269   train_loss = 3.670\n",
      "Epoch  53 Batch    4/269   train_loss = 4.070\n",
      "Epoch  53 Batch    5/269   train_loss = 3.762\n",
      "Epoch  53 Batch    6/269   train_loss = 3.603\n",
      "Epoch  53 Batch    7/269   train_loss = 3.424\n",
      "Epoch  53 Batch    8/269   train_loss = 3.812\n",
      "Epoch  53 Batch    9/269   train_loss = 3.443\n",
      "Epoch  53 Batch   10/269   train_loss = 3.551\n",
      "Epoch  53 Batch   11/269   train_loss = 3.650\n",
      "Epoch  53 Batch   12/269   train_loss = 3.370\n",
      "Epoch  53 Batch   13/269   train_loss = 3.749\n",
      "Epoch  53 Batch   14/269   train_loss = 3.551\n",
      "Epoch  53 Batch   15/269   train_loss = 3.930\n",
      "Epoch  53 Batch   16/269   train_loss = 3.661\n",
      "Epoch  53 Batch   17/269   train_loss = 3.685\n",
      "Epoch  53 Batch   18/269   train_loss = 3.647\n",
      "Epoch  53 Batch   19/269   train_loss = 3.640\n",
      "Epoch  53 Batch   20/269   train_loss = 4.031\n",
      "Epoch  53 Batch   21/269   train_loss = 3.872\n",
      "Epoch  53 Batch   22/269   train_loss = 3.585\n",
      "Epoch  53 Batch   23/269   train_loss = 3.803\n",
      "Epoch  53 Batch   24/269   train_loss = 3.545\n",
      "Epoch  53 Batch   25/269   train_loss = 3.880\n",
      "Epoch  53 Batch   26/269   train_loss = 3.755\n",
      "Epoch  53 Batch   27/269   train_loss = 3.586\n",
      "Epoch  53 Batch   28/269   train_loss = 3.729\n",
      "Epoch  53 Batch   29/269   train_loss = 3.775\n",
      "Epoch  53 Batch   30/269   train_loss = 4.024\n",
      "Epoch  53 Batch   31/269   train_loss = 3.772\n",
      "Epoch  53 Batch   32/269   train_loss = 3.618\n",
      "Epoch  53 Batch   33/269   train_loss = 3.415\n",
      "Epoch  53 Batch   34/269   train_loss = 3.660\n",
      "Epoch  53 Batch   35/269   train_loss = 3.509\n",
      "Epoch  53 Batch   36/269   train_loss = 3.549\n",
      "Epoch  53 Batch   37/269   train_loss = 3.729\n",
      "Epoch  53 Batch   38/269   train_loss = 3.434\n",
      "Epoch  53 Batch   39/269   train_loss = 3.750\n",
      "Epoch  53 Batch   40/269   train_loss = 3.583\n",
      "Epoch  53 Batch   41/269   train_loss = 3.593\n",
      "Epoch  53 Batch   42/269   train_loss = 4.047\n",
      "Epoch  53 Batch   43/269   train_loss = 3.901\n",
      "Epoch  53 Batch   44/269   train_loss = 3.732\n",
      "Epoch  53 Batch   45/269   train_loss = 3.368\n",
      "Epoch  53 Batch   46/269   train_loss = 3.797\n",
      "Epoch  53 Batch   47/269   train_loss = 3.438\n",
      "Epoch  53 Batch   48/269   train_loss = 3.894\n",
      "Epoch  53 Batch   49/269   train_loss = 3.913\n",
      "Epoch  53 Batch   50/269   train_loss = 3.833\n",
      "Epoch  53 Batch   51/269   train_loss = 3.743\n",
      "Epoch  53 Batch   52/269   train_loss = 3.733\n",
      "Epoch  53 Batch   53/269   train_loss = 3.951\n",
      "Epoch  53 Batch   54/269   train_loss = 3.706\n",
      "Epoch  53 Batch   55/269   train_loss = 3.834\n",
      "Epoch  53 Batch   56/269   train_loss = 3.579\n",
      "Epoch  53 Batch   57/269   train_loss = 3.875\n",
      "Epoch  53 Batch   58/269   train_loss = 3.563\n",
      "Epoch  53 Batch   59/269   train_loss = 3.596\n",
      "Epoch  53 Batch   60/269   train_loss = 3.746\n",
      "Epoch  53 Batch   61/269   train_loss = 3.943\n",
      "Epoch  53 Batch   62/269   train_loss = 3.588\n",
      "Epoch  53 Batch   63/269   train_loss = 3.928\n",
      "Epoch  53 Batch   64/269   train_loss = 3.606\n",
      "Epoch  53 Batch   65/269   train_loss = 3.762\n",
      "Epoch  53 Batch   66/269   train_loss = 3.630\n",
      "Epoch  53 Batch   67/269   train_loss = 3.653\n",
      "Epoch  53 Batch   68/269   train_loss = 3.767\n",
      "Epoch  53 Batch   69/269   train_loss = 3.791\n",
      "Epoch  53 Batch   70/269   train_loss = 3.437\n",
      "Epoch  53 Batch   71/269   train_loss = 3.787\n",
      "Epoch  53 Batch   72/269   train_loss = 3.480\n",
      "Epoch  53 Batch   73/269   train_loss = 3.685\n",
      "Epoch  53 Batch   74/269   train_loss = 3.631\n",
      "Epoch  53 Batch   75/269   train_loss = 3.836\n",
      "Epoch  53 Batch   76/269   train_loss = 3.793\n",
      "Epoch  53 Batch   77/269   train_loss = 3.430\n",
      "Epoch  53 Batch   78/269   train_loss = 3.841\n",
      "Epoch  53 Batch   79/269   train_loss = 3.916\n",
      "Epoch  53 Batch   80/269   train_loss = 3.866\n",
      "Epoch  53 Batch   81/269   train_loss = 3.673\n",
      "Epoch  53 Batch   82/269   train_loss = 3.853\n",
      "Epoch  53 Batch   83/269   train_loss = 3.609\n",
      "Epoch  53 Batch   84/269   train_loss = 3.962\n",
      "Epoch  53 Batch   85/269   train_loss = 4.089\n",
      "Epoch  53 Batch   86/269   train_loss = 4.036\n",
      "Epoch  53 Batch   87/269   train_loss = 3.648\n",
      "Epoch  53 Batch   88/269   train_loss = 3.785\n",
      "Epoch  53 Batch   89/269   train_loss = 3.752\n",
      "Epoch  53 Batch   90/269   train_loss = 3.562\n",
      "Epoch  53 Batch   91/269   train_loss = 3.834\n",
      "Epoch  53 Batch   92/269   train_loss = 3.885\n",
      "Epoch  53 Batch   93/269   train_loss = 3.578\n",
      "Epoch  53 Batch   94/269   train_loss = 4.088\n",
      "Epoch  53 Batch   95/269   train_loss = 3.502\n",
      "Epoch  53 Batch   96/269   train_loss = 3.615\n",
      "Epoch  53 Batch   97/269   train_loss = 3.670\n",
      "Epoch  53 Batch   98/269   train_loss = 3.916\n",
      "Epoch  53 Batch   99/269   train_loss = 3.707\n",
      "Epoch  53 Batch  100/269   train_loss = 3.681\n",
      "Epoch  53 Batch  101/269   train_loss = 3.814\n",
      "Epoch  53 Batch  102/269   train_loss = 3.515\n",
      "Epoch  53 Batch  103/269   train_loss = 3.632\n",
      "Epoch  53 Batch  104/269   train_loss = 3.578\n",
      "Epoch  53 Batch  105/269   train_loss = 3.882\n",
      "Epoch  53 Batch  106/269   train_loss = 3.968\n",
      "Epoch  53 Batch  107/269   train_loss = 3.951\n",
      "Epoch  53 Batch  108/269   train_loss = 3.737\n",
      "Epoch  53 Batch  109/269   train_loss = 3.759\n",
      "Epoch  53 Batch  110/269   train_loss = 3.752\n",
      "Epoch  53 Batch  111/269   train_loss = 3.864\n",
      "Epoch  53 Batch  112/269   train_loss = 3.855\n",
      "Epoch  53 Batch  113/269   train_loss = 3.741\n",
      "Epoch  53 Batch  114/269   train_loss = 3.738\n",
      "Epoch  53 Batch  115/269   train_loss = 3.553\n",
      "Epoch  53 Batch  116/269   train_loss = 3.548\n",
      "Epoch  53 Batch  117/269   train_loss = 3.586\n",
      "Epoch  53 Batch  118/269   train_loss = 3.935\n",
      "Epoch  53 Batch  119/269   train_loss = 3.444\n",
      "Epoch  53 Batch  120/269   train_loss = 3.764\n",
      "Epoch  53 Batch  121/269   train_loss = 3.631\n",
      "Epoch  53 Batch  122/269   train_loss = 3.716\n",
      "Epoch  53 Batch  123/269   train_loss = 3.534\n",
      "Epoch  53 Batch  124/269   train_loss = 3.860\n",
      "Epoch  53 Batch  125/269   train_loss = 3.496\n",
      "Epoch  53 Batch  126/269   train_loss = 3.454\n",
      "Epoch  53 Batch  127/269   train_loss = 3.693\n",
      "Epoch  53 Batch  128/269   train_loss = 3.637\n",
      "Epoch  53 Batch  129/269   train_loss = 3.654\n",
      "Epoch  53 Batch  130/269   train_loss = 3.769\n",
      "Epoch  53 Batch  131/269   train_loss = 3.362\n",
      "Epoch  53 Batch  132/269   train_loss = 3.729\n",
      "Epoch  53 Batch  133/269   train_loss = 3.479\n",
      "Epoch  53 Batch  134/269   train_loss = 3.924\n",
      "Epoch  53 Batch  135/269   train_loss = 3.568\n",
      "Epoch  53 Batch  136/269   train_loss = 3.458\n",
      "Epoch  53 Batch  137/269   train_loss = 3.786\n",
      "Epoch  53 Batch  138/269   train_loss = 3.579\n",
      "Epoch  53 Batch  139/269   train_loss = 3.736\n",
      "Epoch  53 Batch  140/269   train_loss = 3.782\n",
      "Epoch  53 Batch  141/269   train_loss = 4.020\n",
      "Epoch  53 Batch  142/269   train_loss = 3.503\n",
      "Epoch  53 Batch  143/269   train_loss = 3.697\n",
      "Epoch  53 Batch  144/269   train_loss = 3.888\n",
      "Epoch  53 Batch  145/269   train_loss = 3.639\n",
      "Epoch  53 Batch  146/269   train_loss = 3.937\n",
      "Epoch  53 Batch  147/269   train_loss = 3.519\n",
      "Epoch  53 Batch  148/269   train_loss = 3.832\n",
      "Epoch  53 Batch  149/269   train_loss = 3.659\n",
      "Epoch  53 Batch  150/269   train_loss = 3.816\n",
      "Epoch  53 Batch  151/269   train_loss = 3.762\n",
      "Epoch  53 Batch  152/269   train_loss = 3.733\n",
      "Epoch  53 Batch  153/269   train_loss = 3.738\n",
      "Epoch  53 Batch  154/269   train_loss = 3.609\n",
      "Epoch  53 Batch  155/269   train_loss = 3.604\n",
      "Epoch  53 Batch  156/269   train_loss = 4.084\n",
      "Epoch  53 Batch  157/269   train_loss = 3.764\n",
      "Epoch  53 Batch  158/269   train_loss = 3.800\n",
      "Epoch  53 Batch  159/269   train_loss = 3.767\n",
      "Epoch  53 Batch  160/269   train_loss = 3.690\n",
      "Epoch  53 Batch  161/269   train_loss = 3.810\n",
      "Epoch  53 Batch  162/269   train_loss = 3.802\n",
      "Epoch  53 Batch  163/269   train_loss = 3.651\n",
      "Epoch  53 Batch  164/269   train_loss = 3.887\n",
      "Epoch  53 Batch  165/269   train_loss = 3.878\n",
      "Epoch  53 Batch  166/269   train_loss = 3.788\n",
      "Epoch  53 Batch  167/269   train_loss = 3.625\n",
      "Epoch  53 Batch  168/269   train_loss = 3.690\n",
      "Epoch  53 Batch  169/269   train_loss = 4.037\n",
      "Epoch  53 Batch  170/269   train_loss = 3.458\n",
      "Epoch  53 Batch  171/269   train_loss = 3.780\n",
      "Epoch  53 Batch  172/269   train_loss = 3.580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  53 Batch  173/269   train_loss = 3.747\n",
      "Epoch  53 Batch  174/269   train_loss = 3.917\n",
      "Epoch  53 Batch  175/269   train_loss = 3.758\n",
      "Epoch  53 Batch  176/269   train_loss = 3.735\n",
      "Epoch  53 Batch  177/269   train_loss = 3.706\n",
      "Epoch  53 Batch  178/269   train_loss = 3.928\n",
      "Epoch  53 Batch  179/269   train_loss = 3.728\n",
      "Epoch  53 Batch  180/269   train_loss = 3.596\n",
      "Epoch  53 Batch  181/269   train_loss = 3.831\n",
      "Epoch  53 Batch  182/269   train_loss = 3.625\n",
      "Epoch  53 Batch  183/269   train_loss = 3.578\n",
      "Epoch  53 Batch  184/269   train_loss = 3.716\n",
      "Epoch  53 Batch  185/269   train_loss = 3.780\n",
      "Epoch  53 Batch  186/269   train_loss = 3.668\n",
      "Epoch  53 Batch  187/269   train_loss = 3.891\n",
      "Epoch  53 Batch  188/269   train_loss = 3.687\n",
      "Epoch  53 Batch  189/269   train_loss = 3.613\n",
      "Epoch  53 Batch  190/269   train_loss = 4.080\n",
      "Epoch  53 Batch  191/269   train_loss = 3.791\n",
      "Epoch  53 Batch  192/269   train_loss = 3.784\n",
      "Epoch  53 Batch  193/269   train_loss = 3.704\n",
      "Epoch  53 Batch  194/269   train_loss = 3.780\n",
      "Epoch  53 Batch  195/269   train_loss = 3.694\n",
      "Epoch  53 Batch  196/269   train_loss = 3.740\n",
      "Epoch  53 Batch  197/269   train_loss = 3.865\n",
      "Epoch  53 Batch  198/269   train_loss = 3.796\n",
      "Epoch  53 Batch  199/269   train_loss = 3.773\n",
      "Epoch  53 Batch  200/269   train_loss = 3.580\n",
      "Epoch  53 Batch  201/269   train_loss = 3.733\n",
      "Epoch  53 Batch  202/269   train_loss = 3.495\n",
      "Epoch  53 Batch  203/269   train_loss = 3.622\n",
      "Epoch  53 Batch  204/269   train_loss = 3.817\n",
      "Epoch  53 Batch  205/269   train_loss = 3.758\n",
      "Epoch  53 Batch  206/269   train_loss = 3.661\n",
      "Epoch  53 Batch  207/269   train_loss = 3.635\n",
      "Epoch  53 Batch  208/269   train_loss = 3.754\n",
      "Epoch  53 Batch  209/269   train_loss = 3.841\n",
      "Epoch  53 Batch  210/269   train_loss = 3.565\n",
      "Epoch  53 Batch  211/269   train_loss = 3.678\n",
      "Epoch  53 Batch  212/269   train_loss = 4.026\n",
      "Epoch  53 Batch  213/269   train_loss = 3.610\n",
      "Epoch  53 Batch  214/269   train_loss = 3.698\n",
      "Epoch  53 Batch  215/269   train_loss = 3.943\n",
      "Epoch  53 Batch  216/269   train_loss = 3.923\n",
      "Epoch  53 Batch  217/269   train_loss = 3.610\n",
      "Epoch  53 Batch  218/269   train_loss = 3.808\n",
      "Epoch  53 Batch  219/269   train_loss = 3.486\n",
      "Epoch  53 Batch  220/269   train_loss = 3.890\n",
      "Epoch  53 Batch  221/269   train_loss = 3.564\n",
      "Epoch  53 Batch  222/269   train_loss = 3.758\n",
      "Epoch  53 Batch  223/269   train_loss = 3.553\n",
      "Epoch  53 Batch  224/269   train_loss = 3.863\n",
      "Epoch  53 Batch  225/269   train_loss = 3.919\n",
      "Epoch  53 Batch  226/269   train_loss = 3.702\n",
      "Epoch  53 Batch  227/269   train_loss = 3.422\n",
      "Epoch  53 Batch  228/269   train_loss = 3.730\n",
      "Epoch  53 Batch  229/269   train_loss = 3.924\n",
      "Epoch  53 Batch  230/269   train_loss = 3.842\n",
      "Epoch  53 Batch  231/269   train_loss = 3.661\n",
      "Epoch  53 Batch  232/269   train_loss = 3.774\n",
      "Epoch  53 Batch  233/269   train_loss = 3.777\n",
      "Epoch  53 Batch  234/269   train_loss = 3.690\n",
      "Epoch  53 Batch  235/269   train_loss = 3.983\n",
      "Epoch  53 Batch  236/269   train_loss = 3.609\n",
      "Epoch  53 Batch  237/269   train_loss = 3.286\n",
      "Epoch  53 Batch  238/269   train_loss = 3.475\n",
      "Epoch  53 Batch  239/269   train_loss = 4.013\n",
      "Epoch  53 Batch  240/269   train_loss = 3.628\n",
      "Epoch  53 Batch  241/269   train_loss = 3.948\n",
      "Epoch  53 Batch  242/269   train_loss = 3.514\n",
      "Epoch  53 Batch  243/269   train_loss = 3.930\n",
      "Epoch  53 Batch  244/269   train_loss = 3.761\n",
      "Epoch  53 Batch  245/269   train_loss = 3.744\n",
      "Epoch  53 Batch  246/269   train_loss = 3.381\n",
      "Epoch  53 Batch  247/269   train_loss = 3.829\n",
      "Epoch  53 Batch  248/269   train_loss = 3.739\n",
      "Epoch  53 Batch  249/269   train_loss = 3.601\n",
      "Epoch  53 Batch  250/269   train_loss = 3.511\n",
      "Epoch  53 Batch  251/269   train_loss = 3.944\n",
      "Epoch  53 Batch  252/269   train_loss = 3.885\n",
      "Epoch  53 Batch  253/269   train_loss = 3.610\n",
      "Epoch  53 Batch  254/269   train_loss = 3.623\n",
      "Epoch  53 Batch  255/269   train_loss = 3.686\n",
      "Epoch  53 Batch  256/269   train_loss = 3.616\n",
      "Epoch  53 Batch  257/269   train_loss = 3.565\n",
      "Epoch  53 Batch  258/269   train_loss = 3.422\n",
      "Epoch  53 Batch  259/269   train_loss = 3.578\n",
      "Epoch  53 Batch  260/269   train_loss = 3.800\n",
      "Epoch  53 Batch  261/269   train_loss = 3.843\n",
      "Epoch  53 Batch  262/269   train_loss = 3.627\n",
      "Epoch  53 Batch  263/269   train_loss = 3.527\n",
      "Epoch  53 Batch  264/269   train_loss = 3.929\n",
      "Epoch  53 Batch  265/269   train_loss = 3.843\n",
      "Epoch  53 Batch  266/269   train_loss = 3.618\n",
      "Epoch  53 Batch  267/269   train_loss = 3.725\n",
      "Epoch  53 Batch  268/269   train_loss = 3.856\n",
      "Epoch  54 Batch    0/269   train_loss = 3.635\n",
      "Epoch  54 Batch    1/269   train_loss = 3.487\n",
      "Epoch  54 Batch    2/269   train_loss = 3.596\n",
      "Epoch  54 Batch    3/269   train_loss = 3.651\n",
      "Epoch  54 Batch    4/269   train_loss = 4.079\n",
      "Epoch  54 Batch    5/269   train_loss = 3.759\n",
      "Epoch  54 Batch    6/269   train_loss = 3.618\n",
      "Epoch  54 Batch    7/269   train_loss = 3.442\n",
      "Epoch  54 Batch    8/269   train_loss = 3.791\n",
      "Epoch  54 Batch    9/269   train_loss = 3.426\n",
      "Epoch  54 Batch   10/269   train_loss = 3.547\n",
      "Epoch  54 Batch   11/269   train_loss = 3.681\n",
      "Epoch  54 Batch   12/269   train_loss = 3.360\n",
      "Epoch  54 Batch   13/269   train_loss = 3.739\n",
      "Epoch  54 Batch   14/269   train_loss = 3.561\n",
      "Epoch  54 Batch   15/269   train_loss = 3.935\n",
      "Epoch  54 Batch   16/269   train_loss = 3.682\n",
      "Epoch  54 Batch   17/269   train_loss = 3.672\n",
      "Epoch  54 Batch   18/269   train_loss = 3.632\n",
      "Epoch  54 Batch   19/269   train_loss = 3.650\n",
      "Epoch  54 Batch   20/269   train_loss = 4.014\n",
      "Epoch  54 Batch   21/269   train_loss = 3.874\n",
      "Epoch  54 Batch   22/269   train_loss = 3.569\n",
      "Epoch  54 Batch   23/269   train_loss = 3.791\n",
      "Epoch  54 Batch   24/269   train_loss = 3.541\n",
      "Epoch  54 Batch   25/269   train_loss = 3.892\n",
      "Epoch  54 Batch   26/269   train_loss = 3.753\n",
      "Epoch  54 Batch   27/269   train_loss = 3.616\n",
      "Epoch  54 Batch   28/269   train_loss = 3.754\n",
      "Epoch  54 Batch   29/269   train_loss = 3.761\n",
      "Epoch  54 Batch   30/269   train_loss = 4.021\n",
      "Epoch  54 Batch   31/269   train_loss = 3.757\n",
      "Epoch  54 Batch   32/269   train_loss = 3.626\n",
      "Epoch  54 Batch   33/269   train_loss = 3.394\n",
      "Epoch  54 Batch   34/269   train_loss = 3.661\n",
      "Epoch  54 Batch   35/269   train_loss = 3.525\n",
      "Epoch  54 Batch   36/269   train_loss = 3.535\n",
      "Epoch  54 Batch   37/269   train_loss = 3.726\n",
      "Epoch  54 Batch   38/269   train_loss = 3.419\n",
      "Epoch  54 Batch   39/269   train_loss = 3.730\n",
      "Epoch  54 Batch   40/269   train_loss = 3.574\n",
      "Epoch  54 Batch   41/269   train_loss = 3.565\n",
      "Epoch  54 Batch   42/269   train_loss = 4.037\n",
      "Epoch  54 Batch   43/269   train_loss = 3.934\n",
      "Epoch  54 Batch   44/269   train_loss = 3.716\n",
      "Epoch  54 Batch   45/269   train_loss = 3.372\n",
      "Epoch  54 Batch   46/269   train_loss = 3.785\n",
      "Epoch  54 Batch   47/269   train_loss = 3.416\n",
      "Epoch  54 Batch   48/269   train_loss = 3.891\n",
      "Epoch  54 Batch   49/269   train_loss = 3.929\n",
      "Epoch  54 Batch   50/269   train_loss = 3.817\n",
      "Epoch  54 Batch   51/269   train_loss = 3.754\n",
      "Epoch  54 Batch   52/269   train_loss = 3.725\n",
      "Epoch  54 Batch   53/269   train_loss = 3.960\n",
      "Epoch  54 Batch   54/269   train_loss = 3.708\n",
      "Epoch  54 Batch   55/269   train_loss = 3.834\n",
      "Epoch  54 Batch   56/269   train_loss = 3.589\n",
      "Epoch  54 Batch   57/269   train_loss = 3.875\n",
      "Epoch  54 Batch   58/269   train_loss = 3.543\n",
      "Epoch  54 Batch   59/269   train_loss = 3.607\n",
      "Epoch  54 Batch   60/269   train_loss = 3.781\n",
      "Epoch  54 Batch   61/269   train_loss = 3.949\n",
      "Epoch  54 Batch   62/269   train_loss = 3.603\n",
      "Epoch  54 Batch   63/269   train_loss = 3.918\n",
      "Epoch  54 Batch   64/269   train_loss = 3.613\n",
      "Epoch  54 Batch   65/269   train_loss = 3.769\n",
      "Epoch  54 Batch   66/269   train_loss = 3.623\n",
      "Epoch  54 Batch   67/269   train_loss = 3.675\n",
      "Epoch  54 Batch   68/269   train_loss = 3.751\n",
      "Epoch  54 Batch   69/269   train_loss = 3.767\n",
      "Epoch  54 Batch   70/269   train_loss = 3.450\n",
      "Epoch  54 Batch   71/269   train_loss = 3.793\n",
      "Epoch  54 Batch   72/269   train_loss = 3.473\n",
      "Epoch  54 Batch   73/269   train_loss = 3.707\n",
      "Epoch  54 Batch   74/269   train_loss = 3.609\n",
      "Epoch  54 Batch   75/269   train_loss = 3.842\n",
      "Epoch  54 Batch   76/269   train_loss = 3.779\n",
      "Epoch  54 Batch   77/269   train_loss = 3.413\n",
      "Epoch  54 Batch   78/269   train_loss = 3.850\n",
      "Epoch  54 Batch   79/269   train_loss = 3.915\n",
      "Epoch  54 Batch   80/269   train_loss = 3.833\n",
      "Epoch  54 Batch   81/269   train_loss = 3.678\n",
      "Epoch  54 Batch   82/269   train_loss = 3.841\n",
      "Epoch  54 Batch   83/269   train_loss = 3.609\n",
      "Epoch  54 Batch   84/269   train_loss = 3.930\n",
      "Epoch  54 Batch   85/269   train_loss = 4.071\n",
      "Epoch  54 Batch   86/269   train_loss = 4.032\n",
      "Epoch  54 Batch   87/269   train_loss = 3.662\n",
      "Epoch  54 Batch   88/269   train_loss = 3.789\n",
      "Epoch  54 Batch   89/269   train_loss = 3.771\n",
      "Epoch  54 Batch   90/269   train_loss = 3.618\n",
      "Epoch  54 Batch   91/269   train_loss = 3.849\n",
      "Epoch  54 Batch   92/269   train_loss = 3.891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  54 Batch   93/269   train_loss = 3.576\n",
      "Epoch  54 Batch   94/269   train_loss = 4.062\n",
      "Epoch  54 Batch   95/269   train_loss = 3.470\n",
      "Epoch  54 Batch   96/269   train_loss = 3.616\n",
      "Epoch  54 Batch   97/269   train_loss = 3.692\n",
      "Epoch  54 Batch   98/269   train_loss = 3.919\n",
      "Epoch  54 Batch   99/269   train_loss = 3.742\n",
      "Epoch  54 Batch  100/269   train_loss = 3.678\n",
      "Epoch  54 Batch  101/269   train_loss = 3.808\n",
      "Epoch  54 Batch  102/269   train_loss = 3.536\n",
      "Epoch  54 Batch  103/269   train_loss = 3.602\n",
      "Epoch  54 Batch  104/269   train_loss = 3.593\n",
      "Epoch  54 Batch  105/269   train_loss = 3.888\n",
      "Epoch  54 Batch  106/269   train_loss = 3.989\n",
      "Epoch  54 Batch  107/269   train_loss = 3.966\n",
      "Epoch  54 Batch  108/269   train_loss = 3.730\n",
      "Epoch  54 Batch  109/269   train_loss = 3.754\n",
      "Epoch  54 Batch  110/269   train_loss = 3.747\n",
      "Epoch  54 Batch  111/269   train_loss = 3.870\n",
      "Epoch  54 Batch  112/269   train_loss = 3.845\n",
      "Epoch  54 Batch  113/269   train_loss = 3.757\n",
      "Epoch  54 Batch  114/269   train_loss = 3.734\n",
      "Epoch  54 Batch  115/269   train_loss = 3.581\n",
      "Epoch  54 Batch  116/269   train_loss = 3.543\n",
      "Epoch  54 Batch  117/269   train_loss = 3.573\n",
      "Epoch  54 Batch  118/269   train_loss = 3.938\n",
      "Epoch  54 Batch  119/269   train_loss = 3.449\n",
      "Epoch  54 Batch  120/269   train_loss = 3.767\n",
      "Epoch  54 Batch  121/269   train_loss = 3.656\n",
      "Epoch  54 Batch  122/269   train_loss = 3.717\n",
      "Epoch  54 Batch  123/269   train_loss = 3.537\n",
      "Epoch  54 Batch  124/269   train_loss = 3.888\n",
      "Epoch  54 Batch  125/269   train_loss = 3.497\n",
      "Epoch  54 Batch  126/269   train_loss = 3.442\n",
      "Epoch  54 Batch  127/269   train_loss = 3.656\n",
      "Epoch  54 Batch  128/269   train_loss = 3.646\n",
      "Epoch  54 Batch  129/269   train_loss = 3.651\n",
      "Epoch  54 Batch  130/269   train_loss = 3.792\n",
      "Epoch  54 Batch  131/269   train_loss = 3.359\n",
      "Epoch  54 Batch  132/269   train_loss = 3.750\n",
      "Epoch  54 Batch  133/269   train_loss = 3.488\n",
      "Epoch  54 Batch  134/269   train_loss = 3.955\n",
      "Epoch  54 Batch  135/269   train_loss = 3.570\n",
      "Epoch  54 Batch  136/269   train_loss = 3.459\n",
      "Epoch  54 Batch  137/269   train_loss = 3.808\n",
      "Epoch  54 Batch  138/269   train_loss = 3.556\n",
      "Epoch  54 Batch  139/269   train_loss = 3.733\n",
      "Epoch  54 Batch  140/269   train_loss = 3.753\n",
      "Epoch  54 Batch  141/269   train_loss = 4.065\n",
      "Epoch  54 Batch  142/269   train_loss = 3.489\n",
      "Epoch  54 Batch  143/269   train_loss = 3.707\n",
      "Epoch  54 Batch  144/269   train_loss = 3.881\n",
      "Epoch  54 Batch  145/269   train_loss = 3.648\n",
      "Epoch  54 Batch  146/269   train_loss = 3.914\n",
      "Epoch  54 Batch  147/269   train_loss = 3.498\n",
      "Epoch  54 Batch  148/269   train_loss = 3.808\n",
      "Epoch  54 Batch  149/269   train_loss = 3.666\n",
      "Epoch  54 Batch  150/269   train_loss = 3.796\n",
      "Epoch  54 Batch  151/269   train_loss = 3.772\n",
      "Epoch  54 Batch  152/269   train_loss = 3.726\n",
      "Epoch  54 Batch  153/269   train_loss = 3.729\n",
      "Epoch  54 Batch  154/269   train_loss = 3.611\n",
      "Epoch  54 Batch  155/269   train_loss = 3.613\n",
      "Epoch  54 Batch  156/269   train_loss = 4.094\n",
      "Epoch  54 Batch  157/269   train_loss = 3.758\n",
      "Epoch  54 Batch  158/269   train_loss = 3.798\n",
      "Epoch  54 Batch  159/269   train_loss = 3.744\n",
      "Epoch  54 Batch  160/269   train_loss = 3.679\n",
      "Epoch  54 Batch  161/269   train_loss = 3.833\n",
      "Epoch  54 Batch  162/269   train_loss = 3.792\n",
      "Epoch  54 Batch  163/269   train_loss = 3.658\n",
      "Epoch  54 Batch  164/269   train_loss = 3.889\n",
      "Epoch  54 Batch  165/269   train_loss = 3.870\n",
      "Epoch  54 Batch  166/269   train_loss = 3.788\n",
      "Epoch  54 Batch  167/269   train_loss = 3.625\n",
      "Epoch  54 Batch  168/269   train_loss = 3.702\n",
      "Epoch  54 Batch  169/269   train_loss = 4.047\n",
      "Epoch  54 Batch  170/269   train_loss = 3.452\n",
      "Epoch  54 Batch  171/269   train_loss = 3.778\n",
      "Epoch  54 Batch  172/269   train_loss = 3.589\n",
      "Epoch  54 Batch  173/269   train_loss = 3.724\n",
      "Epoch  54 Batch  174/269   train_loss = 3.904\n",
      "Epoch  54 Batch  175/269   train_loss = 3.752\n",
      "Epoch  54 Batch  176/269   train_loss = 3.712\n",
      "Epoch  54 Batch  177/269   train_loss = 3.660\n",
      "Epoch  54 Batch  178/269   train_loss = 3.916\n",
      "Epoch  54 Batch  179/269   train_loss = 3.719\n",
      "Epoch  54 Batch  180/269   train_loss = 3.584\n",
      "Epoch  54 Batch  181/269   train_loss = 3.841\n",
      "Epoch  54 Batch  182/269   train_loss = 3.633\n",
      "Epoch  54 Batch  183/269   train_loss = 3.607\n",
      "Epoch  54 Batch  184/269   train_loss = 3.713\n",
      "Epoch  54 Batch  185/269   train_loss = 3.786\n",
      "Epoch  54 Batch  186/269   train_loss = 3.670\n",
      "Epoch  54 Batch  187/269   train_loss = 3.921\n",
      "Epoch  54 Batch  188/269   train_loss = 3.681\n",
      "Epoch  54 Batch  189/269   train_loss = 3.598\n",
      "Epoch  54 Batch  190/269   train_loss = 4.079\n",
      "Epoch  54 Batch  191/269   train_loss = 3.789\n",
      "Epoch  54 Batch  192/269   train_loss = 3.750\n",
      "Epoch  54 Batch  193/269   train_loss = 3.688\n",
      "Epoch  54 Batch  194/269   train_loss = 3.776\n",
      "Epoch  54 Batch  195/269   train_loss = 3.700\n",
      "Epoch  54 Batch  196/269   train_loss = 3.744\n",
      "Epoch  54 Batch  197/269   train_loss = 3.844\n",
      "Epoch  54 Batch  198/269   train_loss = 3.758\n",
      "Epoch  54 Batch  199/269   train_loss = 3.786\n",
      "Epoch  54 Batch  200/269   train_loss = 3.572\n",
      "Epoch  54 Batch  201/269   train_loss = 3.719\n",
      "Epoch  54 Batch  202/269   train_loss = 3.500\n",
      "Epoch  54 Batch  203/269   train_loss = 3.601\n",
      "Epoch  54 Batch  204/269   train_loss = 3.819\n",
      "Epoch  54 Batch  205/269   train_loss = 3.753\n",
      "Epoch  54 Batch  206/269   train_loss = 3.699\n",
      "Epoch  54 Batch  207/269   train_loss = 3.672\n",
      "Epoch  54 Batch  208/269   train_loss = 3.726\n",
      "Epoch  54 Batch  209/269   train_loss = 3.813\n",
      "Epoch  54 Batch  210/269   train_loss = 3.570\n",
      "Epoch  54 Batch  211/269   train_loss = 3.703\n",
      "Epoch  54 Batch  212/269   train_loss = 4.014\n",
      "Epoch  54 Batch  213/269   train_loss = 3.592\n",
      "Epoch  54 Batch  214/269   train_loss = 3.701\n",
      "Epoch  54 Batch  215/269   train_loss = 3.937\n",
      "Epoch  54 Batch  216/269   train_loss = 3.906\n",
      "Epoch  54 Batch  217/269   train_loss = 3.616\n",
      "Epoch  54 Batch  218/269   train_loss = 3.812\n",
      "Epoch  54 Batch  219/269   train_loss = 3.476\n",
      "Epoch  54 Batch  220/269   train_loss = 3.882\n",
      "Epoch  54 Batch  221/269   train_loss = 3.575\n",
      "Epoch  54 Batch  222/269   train_loss = 3.740\n",
      "Epoch  54 Batch  223/269   train_loss = 3.537\n",
      "Epoch  54 Batch  224/269   train_loss = 3.875\n",
      "Epoch  54 Batch  225/269   train_loss = 3.913\n",
      "Epoch  54 Batch  226/269   train_loss = 3.701\n",
      "Epoch  54 Batch  227/269   train_loss = 3.413\n",
      "Epoch  54 Batch  228/269   train_loss = 3.775\n",
      "Epoch  54 Batch  229/269   train_loss = 3.891\n",
      "Epoch  54 Batch  230/269   train_loss = 3.860\n",
      "Epoch  54 Batch  231/269   train_loss = 3.663\n",
      "Epoch  54 Batch  232/269   train_loss = 3.787\n",
      "Epoch  54 Batch  233/269   train_loss = 3.790\n",
      "Epoch  54 Batch  234/269   train_loss = 3.691\n",
      "Epoch  54 Batch  235/269   train_loss = 3.957\n",
      "Epoch  54 Batch  236/269   train_loss = 3.610\n",
      "Epoch  54 Batch  237/269   train_loss = 3.272\n",
      "Epoch  54 Batch  238/269   train_loss = 3.469\n",
      "Epoch  54 Batch  239/269   train_loss = 4.041\n",
      "Epoch  54 Batch  240/269   train_loss = 3.618\n",
      "Epoch  54 Batch  241/269   train_loss = 3.939\n",
      "Epoch  54 Batch  242/269   train_loss = 3.535\n",
      "Epoch  54 Batch  243/269   train_loss = 3.924\n",
      "Epoch  54 Batch  244/269   train_loss = 3.755\n",
      "Epoch  54 Batch  245/269   train_loss = 3.755\n",
      "Epoch  54 Batch  246/269   train_loss = 3.365\n",
      "Epoch  54 Batch  247/269   train_loss = 3.807\n",
      "Epoch  54 Batch  248/269   train_loss = 3.737\n",
      "Epoch  54 Batch  249/269   train_loss = 3.606\n",
      "Epoch  54 Batch  250/269   train_loss = 3.539\n",
      "Epoch  54 Batch  251/269   train_loss = 3.923\n",
      "Epoch  54 Batch  252/269   train_loss = 3.889\n",
      "Epoch  54 Batch  253/269   train_loss = 3.590\n",
      "Epoch  54 Batch  254/269   train_loss = 3.632\n",
      "Epoch  54 Batch  255/269   train_loss = 3.688\n",
      "Epoch  54 Batch  256/269   train_loss = 3.609\n",
      "Epoch  54 Batch  257/269   train_loss = 3.559\n",
      "Epoch  54 Batch  258/269   train_loss = 3.427\n",
      "Epoch  54 Batch  259/269   train_loss = 3.595\n",
      "Epoch  54 Batch  260/269   train_loss = 3.791\n",
      "Epoch  54 Batch  261/269   train_loss = 3.851\n",
      "Epoch  54 Batch  262/269   train_loss = 3.611\n",
      "Epoch  54 Batch  263/269   train_loss = 3.537\n",
      "Epoch  54 Batch  264/269   train_loss = 3.917\n",
      "Epoch  54 Batch  265/269   train_loss = 3.823\n",
      "Epoch  54 Batch  266/269   train_loss = 3.637\n",
      "Epoch  54 Batch  267/269   train_loss = 3.723\n",
      "Epoch  54 Batch  268/269   train_loss = 3.892\n",
      "Epoch  55 Batch    0/269   train_loss = 3.674\n",
      "Epoch  55 Batch    1/269   train_loss = 3.473\n",
      "Epoch  55 Batch    2/269   train_loss = 3.585\n",
      "Epoch  55 Batch    3/269   train_loss = 3.671\n",
      "Epoch  55 Batch    4/269   train_loss = 4.069\n",
      "Epoch  55 Batch    5/269   train_loss = 3.802\n",
      "Epoch  55 Batch    6/269   train_loss = 3.598\n",
      "Epoch  55 Batch    7/269   train_loss = 3.459\n",
      "Epoch  55 Batch    8/269   train_loss = 3.797\n",
      "Epoch  55 Batch    9/269   train_loss = 3.455\n",
      "Epoch  55 Batch   10/269   train_loss = 3.595\n",
      "Epoch  55 Batch   11/269   train_loss = 3.666\n",
      "Epoch  55 Batch   12/269   train_loss = 3.357\n",
      "Epoch  55 Batch   13/269   train_loss = 3.712\n",
      "Epoch  55 Batch   14/269   train_loss = 3.566\n",
      "Epoch  55 Batch   15/269   train_loss = 3.913\n",
      "Epoch  55 Batch   16/269   train_loss = 3.667\n",
      "Epoch  55 Batch   17/269   train_loss = 3.680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  55 Batch   18/269   train_loss = 3.651\n",
      "Epoch  55 Batch   19/269   train_loss = 3.640\n",
      "Epoch  55 Batch   20/269   train_loss = 4.005\n",
      "Epoch  55 Batch   21/269   train_loss = 3.878\n",
      "Epoch  55 Batch   22/269   train_loss = 3.576\n",
      "Epoch  55 Batch   23/269   train_loss = 3.835\n",
      "Epoch  55 Batch   24/269   train_loss = 3.569\n",
      "Epoch  55 Batch   25/269   train_loss = 3.886\n",
      "Epoch  55 Batch   26/269   train_loss = 3.773\n",
      "Epoch  55 Batch   27/269   train_loss = 3.614\n",
      "Epoch  55 Batch   28/269   train_loss = 3.749\n",
      "Epoch  55 Batch   29/269   train_loss = 3.774\n",
      "Epoch  55 Batch   30/269   train_loss = 4.029\n",
      "Epoch  55 Batch   31/269   train_loss = 3.731\n",
      "Epoch  55 Batch   32/269   train_loss = 3.622\n",
      "Epoch  55 Batch   33/269   train_loss = 3.375\n",
      "Epoch  55 Batch   34/269   train_loss = 3.655\n",
      "Epoch  55 Batch   35/269   train_loss = 3.501\n",
      "Epoch  55 Batch   36/269   train_loss = 3.537\n",
      "Epoch  55 Batch   37/269   train_loss = 3.734\n",
      "Epoch  55 Batch   38/269   train_loss = 3.460\n",
      "Epoch  55 Batch   39/269   train_loss = 3.734\n",
      "Epoch  55 Batch   40/269   train_loss = 3.587\n",
      "Epoch  55 Batch   41/269   train_loss = 3.577\n",
      "Epoch  55 Batch   42/269   train_loss = 4.043\n",
      "Epoch  55 Batch   43/269   train_loss = 3.916\n",
      "Epoch  55 Batch   44/269   train_loss = 3.713\n",
      "Epoch  55 Batch   45/269   train_loss = 3.363\n",
      "Epoch  55 Batch   46/269   train_loss = 3.771\n",
      "Epoch  55 Batch   47/269   train_loss = 3.421\n",
      "Epoch  55 Batch   48/269   train_loss = 3.912\n",
      "Epoch  55 Batch   49/269   train_loss = 3.926\n",
      "Epoch  55 Batch   50/269   train_loss = 3.850\n",
      "Epoch  55 Batch   51/269   train_loss = 3.736\n",
      "Epoch  55 Batch   52/269   train_loss = 3.737\n",
      "Epoch  55 Batch   53/269   train_loss = 3.963\n",
      "Epoch  55 Batch   54/269   train_loss = 3.700\n",
      "Epoch  55 Batch   55/269   train_loss = 3.845\n",
      "Epoch  55 Batch   56/269   train_loss = 3.586\n",
      "Epoch  55 Batch   57/269   train_loss = 3.879\n",
      "Epoch  55 Batch   58/269   train_loss = 3.575\n",
      "Epoch  55 Batch   59/269   train_loss = 3.606\n",
      "Epoch  55 Batch   60/269   train_loss = 3.753\n",
      "Epoch  55 Batch   61/269   train_loss = 3.965\n",
      "Epoch  55 Batch   62/269   train_loss = 3.571\n",
      "Epoch  55 Batch   63/269   train_loss = 3.923\n",
      "Epoch  55 Batch   64/269   train_loss = 3.606\n",
      "Epoch  55 Batch   65/269   train_loss = 3.735\n",
      "Epoch  55 Batch   66/269   train_loss = 3.630\n",
      "Epoch  55 Batch   67/269   train_loss = 3.655\n",
      "Epoch  55 Batch   68/269   train_loss = 3.762\n",
      "Epoch  55 Batch   69/269   train_loss = 3.764\n",
      "Epoch  55 Batch   70/269   train_loss = 3.451\n",
      "Epoch  55 Batch   71/269   train_loss = 3.810\n",
      "Epoch  55 Batch   72/269   train_loss = 3.471\n",
      "Epoch  55 Batch   73/269   train_loss = 3.683\n",
      "Epoch  55 Batch   74/269   train_loss = 3.611\n",
      "Epoch  55 Batch   75/269   train_loss = 3.837\n",
      "Epoch  55 Batch   76/269   train_loss = 3.776\n",
      "Epoch  55 Batch   77/269   train_loss = 3.456\n",
      "Epoch  55 Batch   78/269   train_loss = 3.848\n",
      "Epoch  55 Batch   79/269   train_loss = 3.908\n",
      "Epoch  55 Batch   80/269   train_loss = 3.838\n",
      "Epoch  55 Batch   81/269   train_loss = 3.663\n",
      "Epoch  55 Batch   82/269   train_loss = 3.868\n",
      "Epoch  55 Batch   83/269   train_loss = 3.604\n",
      "Epoch  55 Batch   84/269   train_loss = 3.947\n",
      "Epoch  55 Batch   85/269   train_loss = 4.064\n",
      "Epoch  55 Batch   86/269   train_loss = 4.017\n",
      "Epoch  55 Batch   87/269   train_loss = 3.643\n",
      "Epoch  55 Batch   88/269   train_loss = 3.801\n",
      "Epoch  55 Batch   89/269   train_loss = 3.741\n",
      "Epoch  55 Batch   90/269   train_loss = 3.564\n",
      "Epoch  55 Batch   91/269   train_loss = 3.867\n",
      "Epoch  55 Batch   92/269   train_loss = 3.926\n",
      "Epoch  55 Batch   93/269   train_loss = 3.584\n",
      "Epoch  55 Batch   94/269   train_loss = 4.049\n",
      "Epoch  55 Batch   95/269   train_loss = 3.522\n",
      "Epoch  55 Batch   96/269   train_loss = 3.583\n",
      "Epoch  55 Batch   97/269   train_loss = 3.664\n",
      "Epoch  55 Batch   98/269   train_loss = 3.918\n",
      "Epoch  55 Batch   99/269   train_loss = 3.713\n",
      "Epoch  55 Batch  100/269   train_loss = 3.688\n",
      "Epoch  55 Batch  101/269   train_loss = 3.816\n",
      "Epoch  55 Batch  102/269   train_loss = 3.536\n",
      "Epoch  55 Batch  103/269   train_loss = 3.604\n",
      "Epoch  55 Batch  104/269   train_loss = 3.594\n",
      "Epoch  55 Batch  105/269   train_loss = 3.894\n",
      "Epoch  55 Batch  106/269   train_loss = 4.011\n",
      "Epoch  55 Batch  107/269   train_loss = 4.005\n",
      "Epoch  55 Batch  108/269   train_loss = 3.749\n",
      "Epoch  55 Batch  109/269   train_loss = 3.753\n",
      "Epoch  55 Batch  110/269   train_loss = 3.719\n",
      "Epoch  55 Batch  111/269   train_loss = 3.846\n",
      "Epoch  55 Batch  112/269   train_loss = 3.856\n",
      "Epoch  55 Batch  113/269   train_loss = 3.744\n",
      "Epoch  55 Batch  114/269   train_loss = 3.715\n",
      "Epoch  55 Batch  115/269   train_loss = 3.587\n",
      "Epoch  55 Batch  116/269   train_loss = 3.578\n",
      "Epoch  55 Batch  117/269   train_loss = 3.593\n",
      "Epoch  55 Batch  118/269   train_loss = 3.928\n",
      "Epoch  55 Batch  119/269   train_loss = 3.448\n",
      "Epoch  55 Batch  120/269   train_loss = 3.775\n",
      "Epoch  55 Batch  121/269   train_loss = 3.670\n",
      "Epoch  55 Batch  122/269   train_loss = 3.722\n",
      "Epoch  55 Batch  123/269   train_loss = 3.523\n",
      "Epoch  55 Batch  124/269   train_loss = 3.890\n",
      "Epoch  55 Batch  125/269   train_loss = 3.495\n",
      "Epoch  55 Batch  126/269   train_loss = 3.419\n",
      "Epoch  55 Batch  127/269   train_loss = 3.689\n",
      "Epoch  55 Batch  128/269   train_loss = 3.642\n",
      "Epoch  55 Batch  129/269   train_loss = 3.645\n",
      "Epoch  55 Batch  130/269   train_loss = 3.775\n",
      "Epoch  55 Batch  131/269   train_loss = 3.385\n",
      "Epoch  55 Batch  132/269   train_loss = 3.751\n",
      "Epoch  55 Batch  133/269   train_loss = 3.478\n",
      "Epoch  55 Batch  134/269   train_loss = 3.930\n",
      "Epoch  55 Batch  135/269   train_loss = 3.581\n",
      "Epoch  55 Batch  136/269   train_loss = 3.449\n",
      "Epoch  55 Batch  137/269   train_loss = 3.803\n",
      "Epoch  55 Batch  138/269   train_loss = 3.565\n",
      "Epoch  55 Batch  139/269   train_loss = 3.706\n",
      "Epoch  55 Batch  140/269   train_loss = 3.796\n",
      "Epoch  55 Batch  141/269   train_loss = 4.065\n",
      "Epoch  55 Batch  142/269   train_loss = 3.486\n",
      "Epoch  55 Batch  143/269   train_loss = 3.689\n",
      "Epoch  55 Batch  144/269   train_loss = 3.882\n",
      "Epoch  55 Batch  145/269   train_loss = 3.658\n",
      "Epoch  55 Batch  146/269   train_loss = 3.910\n",
      "Epoch  55 Batch  147/269   train_loss = 3.500\n",
      "Epoch  55 Batch  148/269   train_loss = 3.813\n",
      "Epoch  55 Batch  149/269   train_loss = 3.647\n",
      "Epoch  55 Batch  150/269   train_loss = 3.797\n",
      "Epoch  55 Batch  151/269   train_loss = 3.795\n",
      "Epoch  55 Batch  152/269   train_loss = 3.689\n",
      "Epoch  55 Batch  153/269   train_loss = 3.734\n",
      "Epoch  55 Batch  154/269   train_loss = 3.635\n",
      "Epoch  55 Batch  155/269   train_loss = 3.598\n",
      "Epoch  55 Batch  156/269   train_loss = 4.061\n",
      "Epoch  55 Batch  157/269   train_loss = 3.777\n",
      "Epoch  55 Batch  158/269   train_loss = 3.807\n",
      "Epoch  55 Batch  159/269   train_loss = 3.778\n",
      "Epoch  55 Batch  160/269   train_loss = 3.671\n",
      "Epoch  55 Batch  161/269   train_loss = 3.788\n",
      "Epoch  55 Batch  162/269   train_loss = 3.804\n",
      "Epoch  55 Batch  163/269   train_loss = 3.646\n",
      "Epoch  55 Batch  164/269   train_loss = 3.882\n",
      "Epoch  55 Batch  165/269   train_loss = 3.852\n",
      "Epoch  55 Batch  166/269   train_loss = 3.793\n",
      "Epoch  55 Batch  167/269   train_loss = 3.582\n",
      "Epoch  55 Batch  168/269   train_loss = 3.706\n",
      "Epoch  55 Batch  169/269   train_loss = 4.053\n",
      "Epoch  55 Batch  170/269   train_loss = 3.467\n",
      "Epoch  55 Batch  171/269   train_loss = 3.761\n",
      "Epoch  55 Batch  172/269   train_loss = 3.576\n",
      "Epoch  55 Batch  173/269   train_loss = 3.728\n",
      "Epoch  55 Batch  174/269   train_loss = 3.943\n",
      "Epoch  55 Batch  175/269   train_loss = 3.762\n",
      "Epoch  55 Batch  176/269   train_loss = 3.693\n",
      "Epoch  55 Batch  177/269   train_loss = 3.686\n",
      "Epoch  55 Batch  178/269   train_loss = 3.945\n",
      "Epoch  55 Batch  179/269   train_loss = 3.725\n",
      "Epoch  55 Batch  180/269   train_loss = 3.591\n",
      "Epoch  55 Batch  181/269   train_loss = 3.842\n",
      "Epoch  55 Batch  182/269   train_loss = 3.647\n",
      "Epoch  55 Batch  183/269   train_loss = 3.611\n",
      "Epoch  55 Batch  184/269   train_loss = 3.718\n",
      "Epoch  55 Batch  185/269   train_loss = 3.814\n",
      "Epoch  55 Batch  186/269   train_loss = 3.689\n",
      "Epoch  55 Batch  187/269   train_loss = 3.917\n",
      "Epoch  55 Batch  188/269   train_loss = 3.673\n",
      "Epoch  55 Batch  189/269   train_loss = 3.618\n",
      "Epoch  55 Batch  190/269   train_loss = 4.094\n",
      "Epoch  55 Batch  191/269   train_loss = 3.807\n",
      "Epoch  55 Batch  192/269   train_loss = 3.734\n",
      "Epoch  55 Batch  193/269   train_loss = 3.748\n",
      "Epoch  55 Batch  194/269   train_loss = 3.769\n",
      "Epoch  55 Batch  195/269   train_loss = 3.693\n",
      "Epoch  55 Batch  196/269   train_loss = 3.787\n",
      "Epoch  55 Batch  197/269   train_loss = 3.898\n",
      "Epoch  55 Batch  198/269   train_loss = 3.776\n",
      "Epoch  55 Batch  199/269   train_loss = 3.794\n",
      "Epoch  55 Batch  200/269   train_loss = 3.604\n",
      "Epoch  55 Batch  201/269   train_loss = 3.731\n",
      "Epoch  55 Batch  202/269   train_loss = 3.500\n",
      "Epoch  55 Batch  203/269   train_loss = 3.628\n",
      "Epoch  55 Batch  204/269   train_loss = 3.842\n",
      "Epoch  55 Batch  205/269   train_loss = 3.749\n",
      "Epoch  55 Batch  206/269   train_loss = 3.693\n",
      "Epoch  55 Batch  207/269   train_loss = 3.637\n",
      "Epoch  55 Batch  208/269   train_loss = 3.742\n",
      "Epoch  55 Batch  209/269   train_loss = 3.809\n",
      "Epoch  55 Batch  210/269   train_loss = 3.554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  55 Batch  211/269   train_loss = 3.707\n",
      "Epoch  55 Batch  212/269   train_loss = 4.035\n",
      "Epoch  55 Batch  213/269   train_loss = 3.627\n",
      "Epoch  55 Batch  214/269   train_loss = 3.708\n",
      "Epoch  55 Batch  215/269   train_loss = 3.947\n",
      "Epoch  55 Batch  216/269   train_loss = 3.918\n",
      "Epoch  55 Batch  217/269   train_loss = 3.600\n",
      "Epoch  55 Batch  218/269   train_loss = 3.800\n",
      "Epoch  55 Batch  219/269   train_loss = 3.483\n",
      "Epoch  55 Batch  220/269   train_loss = 3.856\n",
      "Epoch  55 Batch  221/269   train_loss = 3.573\n",
      "Epoch  55 Batch  222/269   train_loss = 3.755\n",
      "Epoch  55 Batch  223/269   train_loss = 3.549\n",
      "Epoch  55 Batch  224/269   train_loss = 3.874\n",
      "Epoch  55 Batch  225/269   train_loss = 3.906\n",
      "Epoch  55 Batch  226/269   train_loss = 3.683\n",
      "Epoch  55 Batch  227/269   train_loss = 3.437\n",
      "Epoch  55 Batch  228/269   train_loss = 3.788\n",
      "Epoch  55 Batch  229/269   train_loss = 3.949\n",
      "Epoch  55 Batch  230/269   train_loss = 3.825\n",
      "Epoch  55 Batch  231/269   train_loss = 3.664\n",
      "Epoch  55 Batch  232/269   train_loss = 3.764\n",
      "Epoch  55 Batch  233/269   train_loss = 3.776\n",
      "Epoch  55 Batch  234/269   train_loss = 3.758\n",
      "Epoch  55 Batch  235/269   train_loss = 3.969\n",
      "Epoch  55 Batch  236/269   train_loss = 3.611\n",
      "Epoch  55 Batch  237/269   train_loss = 3.289\n",
      "Epoch  55 Batch  238/269   train_loss = 3.483\n",
      "Epoch  55 Batch  239/269   train_loss = 4.021\n",
      "Epoch  55 Batch  240/269   train_loss = 3.583\n",
      "Epoch  55 Batch  241/269   train_loss = 3.932\n",
      "Epoch  55 Batch  242/269   train_loss = 3.539\n",
      "Epoch  55 Batch  243/269   train_loss = 3.896\n",
      "Epoch  55 Batch  244/269   train_loss = 3.725\n",
      "Epoch  55 Batch  245/269   train_loss = 3.751\n",
      "Epoch  55 Batch  246/269   train_loss = 3.394\n",
      "Epoch  55 Batch  247/269   train_loss = 3.823\n",
      "Epoch  55 Batch  248/269   train_loss = 3.763\n",
      "Epoch  55 Batch  249/269   train_loss = 3.611\n",
      "Epoch  55 Batch  250/269   train_loss = 3.537\n",
      "Epoch  55 Batch  251/269   train_loss = 3.920\n",
      "Epoch  55 Batch  252/269   train_loss = 3.872\n",
      "Epoch  55 Batch  253/269   train_loss = 3.611\n",
      "Epoch  55 Batch  254/269   train_loss = 3.624\n",
      "Epoch  55 Batch  255/269   train_loss = 3.676\n",
      "Epoch  55 Batch  256/269   train_loss = 3.615\n",
      "Epoch  55 Batch  257/269   train_loss = 3.543\n",
      "Epoch  55 Batch  258/269   train_loss = 3.387\n",
      "Epoch  55 Batch  259/269   train_loss = 3.597\n",
      "Epoch  55 Batch  260/269   train_loss = 3.762\n",
      "Epoch  55 Batch  261/269   train_loss = 3.828\n",
      "Epoch  55 Batch  262/269   train_loss = 3.606\n",
      "Epoch  55 Batch  263/269   train_loss = 3.525\n",
      "Epoch  55 Batch  264/269   train_loss = 3.929\n",
      "Epoch  55 Batch  265/269   train_loss = 3.801\n",
      "Epoch  55 Batch  266/269   train_loss = 3.609\n",
      "Epoch  55 Batch  267/269   train_loss = 3.736\n",
      "Epoch  55 Batch  268/269   train_loss = 3.850\n",
      "Epoch  56 Batch    0/269   train_loss = 3.644\n",
      "Epoch  56 Batch    1/269   train_loss = 3.463\n",
      "Epoch  56 Batch    2/269   train_loss = 3.595\n",
      "Epoch  56 Batch    3/269   train_loss = 3.652\n",
      "Epoch  56 Batch    4/269   train_loss = 4.074\n",
      "Epoch  56 Batch    5/269   train_loss = 3.767\n",
      "Epoch  56 Batch    6/269   train_loss = 3.624\n",
      "Epoch  56 Batch    7/269   train_loss = 3.433\n",
      "Epoch  56 Batch    8/269   train_loss = 3.796\n",
      "Epoch  56 Batch    9/269   train_loss = 3.466\n",
      "Epoch  56 Batch   10/269   train_loss = 3.545\n",
      "Epoch  56 Batch   11/269   train_loss = 3.648\n",
      "Epoch  56 Batch   12/269   train_loss = 3.358\n",
      "Epoch  56 Batch   13/269   train_loss = 3.729\n",
      "Epoch  56 Batch   14/269   train_loss = 3.566\n",
      "Epoch  56 Batch   15/269   train_loss = 3.932\n",
      "Epoch  56 Batch   16/269   train_loss = 3.662\n",
      "Epoch  56 Batch   17/269   train_loss = 3.693\n",
      "Epoch  56 Batch   18/269   train_loss = 3.676\n",
      "Epoch  56 Batch   19/269   train_loss = 3.670\n",
      "Epoch  56 Batch   20/269   train_loss = 4.004\n",
      "Epoch  56 Batch   21/269   train_loss = 3.868\n",
      "Epoch  56 Batch   22/269   train_loss = 3.590\n",
      "Epoch  56 Batch   23/269   train_loss = 3.802\n",
      "Epoch  56 Batch   24/269   train_loss = 3.526\n",
      "Epoch  56 Batch   25/269   train_loss = 3.874\n",
      "Epoch  56 Batch   26/269   train_loss = 3.740\n",
      "Epoch  56 Batch   27/269   train_loss = 3.613\n",
      "Epoch  56 Batch   28/269   train_loss = 3.708\n",
      "Epoch  56 Batch   29/269   train_loss = 3.773\n",
      "Epoch  56 Batch   30/269   train_loss = 4.049\n",
      "Epoch  56 Batch   31/269   train_loss = 3.727\n",
      "Epoch  56 Batch   32/269   train_loss = 3.629\n",
      "Epoch  56 Batch   33/269   train_loss = 3.474\n",
      "Epoch  56 Batch   34/269   train_loss = 3.617\n",
      "Epoch  56 Batch   35/269   train_loss = 3.507\n",
      "Epoch  56 Batch   36/269   train_loss = 3.528\n",
      "Epoch  56 Batch   37/269   train_loss = 3.739\n",
      "Epoch  56 Batch   38/269   train_loss = 3.479\n",
      "Epoch  56 Batch   39/269   train_loss = 3.756\n",
      "Epoch  56 Batch   40/269   train_loss = 3.572\n",
      "Epoch  56 Batch   41/269   train_loss = 3.562\n",
      "Epoch  56 Batch   42/269   train_loss = 4.025\n",
      "Epoch  56 Batch   43/269   train_loss = 3.919\n",
      "Epoch  56 Batch   44/269   train_loss = 3.730\n",
      "Epoch  56 Batch   45/269   train_loss = 3.389\n",
      "Epoch  56 Batch   46/269   train_loss = 3.777\n",
      "Epoch  56 Batch   47/269   train_loss = 3.410\n",
      "Epoch  56 Batch   48/269   train_loss = 3.904\n",
      "Epoch  56 Batch   49/269   train_loss = 3.875\n",
      "Epoch  56 Batch   50/269   train_loss = 3.859\n",
      "Epoch  56 Batch   51/269   train_loss = 3.728\n",
      "Epoch  56 Batch   52/269   train_loss = 3.735\n",
      "Epoch  56 Batch   53/269   train_loss = 3.999\n",
      "Epoch  56 Batch   54/269   train_loss = 3.701\n",
      "Epoch  56 Batch   55/269   train_loss = 3.825\n",
      "Epoch  56 Batch   56/269   train_loss = 3.607\n",
      "Epoch  56 Batch   57/269   train_loss = 3.850\n",
      "Epoch  56 Batch   58/269   train_loss = 3.557\n",
      "Epoch  56 Batch   59/269   train_loss = 3.620\n",
      "Epoch  56 Batch   60/269   train_loss = 3.752\n",
      "Epoch  56 Batch   61/269   train_loss = 3.949\n",
      "Epoch  56 Batch   62/269   train_loss = 3.564\n",
      "Epoch  56 Batch   63/269   train_loss = 3.898\n",
      "Epoch  56 Batch   64/269   train_loss = 3.602\n",
      "Epoch  56 Batch   65/269   train_loss = 3.792\n",
      "Epoch  56 Batch   66/269   train_loss = 3.617\n",
      "Epoch  56 Batch   67/269   train_loss = 3.674\n",
      "Epoch  56 Batch   68/269   train_loss = 3.766\n",
      "Epoch  56 Batch   69/269   train_loss = 3.754\n",
      "Epoch  56 Batch   70/269   train_loss = 3.431\n",
      "Epoch  56 Batch   71/269   train_loss = 3.801\n",
      "Epoch  56 Batch   72/269   train_loss = 3.469\n",
      "Epoch  56 Batch   73/269   train_loss = 3.661\n",
      "Epoch  56 Batch   74/269   train_loss = 3.611\n",
      "Epoch  56 Batch   75/269   train_loss = 3.812\n",
      "Epoch  56 Batch   76/269   train_loss = 3.803\n",
      "Epoch  56 Batch   77/269   train_loss = 3.435\n",
      "Epoch  56 Batch   78/269   train_loss = 3.855\n",
      "Epoch  56 Batch   79/269   train_loss = 3.906\n",
      "Epoch  56 Batch   80/269   train_loss = 3.841\n",
      "Epoch  56 Batch   81/269   train_loss = 3.681\n",
      "Epoch  56 Batch   82/269   train_loss = 3.868\n",
      "Epoch  56 Batch   83/269   train_loss = 3.626\n",
      "Epoch  56 Batch   84/269   train_loss = 3.911\n",
      "Epoch  56 Batch   85/269   train_loss = 4.081\n",
      "Epoch  56 Batch   86/269   train_loss = 4.027\n",
      "Epoch  56 Batch   87/269   train_loss = 3.644\n",
      "Epoch  56 Batch   88/269   train_loss = 3.790\n",
      "Epoch  56 Batch   89/269   train_loss = 3.745\n",
      "Epoch  56 Batch   90/269   train_loss = 3.552\n",
      "Epoch  56 Batch   91/269   train_loss = 3.878\n",
      "Epoch  56 Batch   92/269   train_loss = 3.863\n",
      "Epoch  56 Batch   93/269   train_loss = 3.556\n",
      "Epoch  56 Batch   94/269   train_loss = 4.043\n",
      "Epoch  56 Batch   95/269   train_loss = 3.520\n",
      "Epoch  56 Batch   96/269   train_loss = 3.614\n",
      "Epoch  56 Batch   97/269   train_loss = 3.695\n",
      "Epoch  56 Batch   98/269   train_loss = 3.896\n",
      "Epoch  56 Batch   99/269   train_loss = 3.704\n",
      "Epoch  56 Batch  100/269   train_loss = 3.657\n",
      "Epoch  56 Batch  101/269   train_loss = 3.804\n",
      "Epoch  56 Batch  102/269   train_loss = 3.519\n",
      "Epoch  56 Batch  103/269   train_loss = 3.624\n",
      "Epoch  56 Batch  104/269   train_loss = 3.568\n",
      "Epoch  56 Batch  105/269   train_loss = 3.916\n",
      "Epoch  56 Batch  106/269   train_loss = 3.960\n",
      "Epoch  56 Batch  107/269   train_loss = 4.000\n",
      "Epoch  56 Batch  108/269   train_loss = 3.753\n",
      "Epoch  56 Batch  109/269   train_loss = 3.749\n",
      "Epoch  56 Batch  110/269   train_loss = 3.760\n",
      "Epoch  56 Batch  111/269   train_loss = 3.872\n",
      "Epoch  56 Batch  112/269   train_loss = 3.854\n",
      "Epoch  56 Batch  113/269   train_loss = 3.771\n",
      "Epoch  56 Batch  114/269   train_loss = 3.750\n",
      "Epoch  56 Batch  115/269   train_loss = 3.566\n",
      "Epoch  56 Batch  116/269   train_loss = 3.571\n",
      "Epoch  56 Batch  117/269   train_loss = 3.588\n",
      "Epoch  56 Batch  118/269   train_loss = 3.939\n",
      "Epoch  56 Batch  119/269   train_loss = 3.441\n",
      "Epoch  56 Batch  120/269   train_loss = 3.805\n",
      "Epoch  56 Batch  121/269   train_loss = 3.664\n",
      "Epoch  56 Batch  122/269   train_loss = 3.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  56 Batch  123/269   train_loss = 3.552\n",
      "Epoch  56 Batch  124/269   train_loss = 3.901\n",
      "Epoch  56 Batch  125/269   train_loss = 3.479\n",
      "Epoch  56 Batch  126/269   train_loss = 3.441\n",
      "Epoch  56 Batch  127/269   train_loss = 3.652\n",
      "Epoch  56 Batch  128/269   train_loss = 3.643\n",
      "Epoch  56 Batch  129/269   train_loss = 3.650\n",
      "Epoch  56 Batch  130/269   train_loss = 3.785\n",
      "Epoch  56 Batch  131/269   train_loss = 3.363\n",
      "Epoch  56 Batch  132/269   train_loss = 3.731\n",
      "Epoch  56 Batch  133/269   train_loss = 3.502\n",
      "Epoch  56 Batch  134/269   train_loss = 3.931\n",
      "Epoch  56 Batch  135/269   train_loss = 3.577\n",
      "Epoch  56 Batch  136/269   train_loss = 3.457\n",
      "Epoch  56 Batch  137/269   train_loss = 3.780\n",
      "Epoch  56 Batch  138/269   train_loss = 3.546\n",
      "Epoch  56 Batch  139/269   train_loss = 3.695\n",
      "Epoch  56 Batch  140/269   train_loss = 3.771\n",
      "Epoch  56 Batch  141/269   train_loss = 4.075\n",
      "Epoch  56 Batch  142/269   train_loss = 3.479\n",
      "Epoch  56 Batch  143/269   train_loss = 3.695\n",
      "Epoch  56 Batch  144/269   train_loss = 3.909\n",
      "Epoch  56 Batch  145/269   train_loss = 3.678\n",
      "Epoch  56 Batch  146/269   train_loss = 3.890\n",
      "Epoch  56 Batch  147/269   train_loss = 3.490\n",
      "Epoch  56 Batch  148/269   train_loss = 3.832\n",
      "Epoch  56 Batch  149/269   train_loss = 3.636\n",
      "Epoch  56 Batch  150/269   train_loss = 3.814\n",
      "Epoch  56 Batch  151/269   train_loss = 3.793\n",
      "Epoch  56 Batch  152/269   train_loss = 3.703\n",
      "Epoch  56 Batch  153/269   train_loss = 3.767\n",
      "Epoch  56 Batch  154/269   train_loss = 3.612\n",
      "Epoch  56 Batch  155/269   train_loss = 3.612\n",
      "Epoch  56 Batch  156/269   train_loss = 4.102\n",
      "Epoch  56 Batch  157/269   train_loss = 3.754\n",
      "Epoch  56 Batch  158/269   train_loss = 3.818\n",
      "Epoch  56 Batch  159/269   train_loss = 3.755\n",
      "Epoch  56 Batch  160/269   train_loss = 3.672\n",
      "Epoch  56 Batch  161/269   train_loss = 3.802\n",
      "Epoch  56 Batch  162/269   train_loss = 3.814\n",
      "Epoch  56 Batch  163/269   train_loss = 3.643\n",
      "Epoch  56 Batch  164/269   train_loss = 3.864\n",
      "Epoch  56 Batch  165/269   train_loss = 3.874\n",
      "Epoch  56 Batch  166/269   train_loss = 3.781\n",
      "Epoch  56 Batch  167/269   train_loss = 3.602\n",
      "Epoch  56 Batch  168/269   train_loss = 3.708\n",
      "Epoch  56 Batch  169/269   train_loss = 4.034\n",
      "Epoch  56 Batch  170/269   train_loss = 3.463\n",
      "Epoch  56 Batch  171/269   train_loss = 3.764\n",
      "Epoch  56 Batch  172/269   train_loss = 3.562\n",
      "Epoch  56 Batch  173/269   train_loss = 3.697\n",
      "Epoch  56 Batch  174/269   train_loss = 3.933\n",
      "Epoch  56 Batch  175/269   train_loss = 3.771\n",
      "Epoch  56 Batch  176/269   train_loss = 3.691\n",
      "Epoch  56 Batch  177/269   train_loss = 3.694\n",
      "Epoch  56 Batch  178/269   train_loss = 3.920\n",
      "Epoch  56 Batch  179/269   train_loss = 3.727\n",
      "Epoch  56 Batch  180/269   train_loss = 3.637\n",
      "Epoch  56 Batch  181/269   train_loss = 3.841\n",
      "Epoch  56 Batch  182/269   train_loss = 3.622\n",
      "Epoch  56 Batch  183/269   train_loss = 3.645\n",
      "Epoch  56 Batch  184/269   train_loss = 3.708\n",
      "Epoch  56 Batch  185/269   train_loss = 3.805\n",
      "Epoch  56 Batch  186/269   train_loss = 3.671\n",
      "Epoch  56 Batch  187/269   train_loss = 3.925\n",
      "Epoch  56 Batch  188/269   train_loss = 3.684\n",
      "Epoch  56 Batch  189/269   train_loss = 3.669\n",
      "Epoch  56 Batch  190/269   train_loss = 4.078\n",
      "Epoch  56 Batch  191/269   train_loss = 3.771\n",
      "Epoch  56 Batch  192/269   train_loss = 3.755\n",
      "Epoch  56 Batch  193/269   train_loss = 3.710\n",
      "Epoch  56 Batch  194/269   train_loss = 3.772\n",
      "Epoch  56 Batch  195/269   train_loss = 3.706\n",
      "Epoch  56 Batch  196/269   train_loss = 3.713\n",
      "Epoch  56 Batch  197/269   train_loss = 3.882\n",
      "Epoch  56 Batch  198/269   train_loss = 3.794\n",
      "Epoch  56 Batch  199/269   train_loss = 3.804\n",
      "Epoch  56 Batch  200/269   train_loss = 3.598\n",
      "Epoch  56 Batch  201/269   train_loss = 3.761\n",
      "Epoch  56 Batch  202/269   train_loss = 3.510\n",
      "Epoch  56 Batch  203/269   train_loss = 3.606\n",
      "Epoch  56 Batch  204/269   train_loss = 3.828\n",
      "Epoch  56 Batch  205/269   train_loss = 3.756\n",
      "Epoch  56 Batch  206/269   train_loss = 3.682\n",
      "Epoch  56 Batch  207/269   train_loss = 3.621\n",
      "Epoch  56 Batch  208/269   train_loss = 3.729\n",
      "Epoch  56 Batch  209/269   train_loss = 3.805\n",
      "Epoch  56 Batch  210/269   train_loss = 3.549\n",
      "Epoch  56 Batch  211/269   train_loss = 3.690\n",
      "Epoch  56 Batch  212/269   train_loss = 4.033\n",
      "Epoch  56 Batch  213/269   train_loss = 3.643\n",
      "Epoch  56 Batch  214/269   train_loss = 3.707\n",
      "Epoch  56 Batch  215/269   train_loss = 3.927\n",
      "Epoch  56 Batch  216/269   train_loss = 3.918\n",
      "Epoch  56 Batch  217/269   train_loss = 3.607\n",
      "Epoch  56 Batch  218/269   train_loss = 3.784\n",
      "Epoch  56 Batch  219/269   train_loss = 3.477\n",
      "Epoch  56 Batch  220/269   train_loss = 3.849\n",
      "Epoch  56 Batch  221/269   train_loss = 3.597\n",
      "Epoch  56 Batch  222/269   train_loss = 3.750\n",
      "Epoch  56 Batch  223/269   train_loss = 3.568\n",
      "Epoch  56 Batch  224/269   train_loss = 3.852\n",
      "Epoch  56 Batch  225/269   train_loss = 3.906\n",
      "Epoch  56 Batch  226/269   train_loss = 3.689\n",
      "Epoch  56 Batch  227/269   train_loss = 3.447\n",
      "Epoch  56 Batch  228/269   train_loss = 3.740\n",
      "Epoch  56 Batch  229/269   train_loss = 3.914\n",
      "Epoch  56 Batch  230/269   train_loss = 3.826\n",
      "Epoch  56 Batch  231/269   train_loss = 3.664\n",
      "Epoch  56 Batch  232/269   train_loss = 3.743\n",
      "Epoch  56 Batch  233/269   train_loss = 3.786\n",
      "Epoch  56 Batch  234/269   train_loss = 3.724\n",
      "Epoch  56 Batch  235/269   train_loss = 3.967\n",
      "Epoch  56 Batch  236/269   train_loss = 3.592\n",
      "Epoch  56 Batch  237/269   train_loss = 3.256\n",
      "Epoch  56 Batch  238/269   train_loss = 3.488\n",
      "Epoch  56 Batch  239/269   train_loss = 4.019\n",
      "Epoch  56 Batch  240/269   train_loss = 3.589\n",
      "Epoch  56 Batch  241/269   train_loss = 3.933\n",
      "Epoch  56 Batch  242/269   train_loss = 3.522\n",
      "Epoch  56 Batch  243/269   train_loss = 3.922\n",
      "Epoch  56 Batch  244/269   train_loss = 3.723\n",
      "Epoch  56 Batch  245/269   train_loss = 3.739\n",
      "Epoch  56 Batch  246/269   train_loss = 3.398\n",
      "Epoch  56 Batch  247/269   train_loss = 3.813\n",
      "Epoch  56 Batch  248/269   train_loss = 3.744\n",
      "Epoch  56 Batch  249/269   train_loss = 3.611\n",
      "Epoch  56 Batch  250/269   train_loss = 3.540\n",
      "Epoch  56 Batch  251/269   train_loss = 3.927\n",
      "Epoch  56 Batch  252/269   train_loss = 3.853\n",
      "Epoch  56 Batch  253/269   train_loss = 3.631\n",
      "Epoch  56 Batch  254/269   train_loss = 3.612\n",
      "Epoch  56 Batch  255/269   train_loss = 3.680\n",
      "Epoch  56 Batch  256/269   train_loss = 3.631\n",
      "Epoch  56 Batch  257/269   train_loss = 3.544\n",
      "Epoch  56 Batch  258/269   train_loss = 3.417\n",
      "Epoch  56 Batch  259/269   train_loss = 3.591\n",
      "Epoch  56 Batch  260/269   train_loss = 3.763\n",
      "Epoch  56 Batch  261/269   train_loss = 3.878\n",
      "Epoch  56 Batch  262/269   train_loss = 3.582\n",
      "Epoch  56 Batch  263/269   train_loss = 3.543\n",
      "Epoch  56 Batch  264/269   train_loss = 3.912\n",
      "Epoch  56 Batch  265/269   train_loss = 3.799\n",
      "Epoch  56 Batch  266/269   train_loss = 3.606\n",
      "Epoch  56 Batch  267/269   train_loss = 3.723\n",
      "Epoch  56 Batch  268/269   train_loss = 3.820\n",
      "Epoch  57 Batch    0/269   train_loss = 3.656\n",
      "Epoch  57 Batch    1/269   train_loss = 3.470\n",
      "Epoch  57 Batch    2/269   train_loss = 3.598\n",
      "Epoch  57 Batch    3/269   train_loss = 3.655\n",
      "Epoch  57 Batch    4/269   train_loss = 4.044\n",
      "Epoch  57 Batch    5/269   train_loss = 3.758\n",
      "Epoch  57 Batch    6/269   train_loss = 3.634\n",
      "Epoch  57 Batch    7/269   train_loss = 3.477\n",
      "Epoch  57 Batch    8/269   train_loss = 3.763\n",
      "Epoch  57 Batch    9/269   train_loss = 3.442\n",
      "Epoch  57 Batch   10/269   train_loss = 3.622\n",
      "Epoch  57 Batch   11/269   train_loss = 3.666\n",
      "Epoch  57 Batch   12/269   train_loss = 3.413\n",
      "Epoch  57 Batch   13/269   train_loss = 3.719\n",
      "Epoch  57 Batch   14/269   train_loss = 3.559\n",
      "Epoch  57 Batch   15/269   train_loss = 3.920\n",
      "Epoch  57 Batch   16/269   train_loss = 3.644\n",
      "Epoch  57 Batch   17/269   train_loss = 3.685\n",
      "Epoch  57 Batch   18/269   train_loss = 3.673\n",
      "Epoch  57 Batch   19/269   train_loss = 3.631\n",
      "Epoch  57 Batch   20/269   train_loss = 4.001\n",
      "Epoch  57 Batch   21/269   train_loss = 3.854\n",
      "Epoch  57 Batch   22/269   train_loss = 3.604\n",
      "Epoch  57 Batch   23/269   train_loss = 3.787\n",
      "Epoch  57 Batch   24/269   train_loss = 3.512\n",
      "Epoch  57 Batch   25/269   train_loss = 3.880\n",
      "Epoch  57 Batch   26/269   train_loss = 3.726\n",
      "Epoch  57 Batch   27/269   train_loss = 3.627\n",
      "Epoch  57 Batch   28/269   train_loss = 3.709\n",
      "Epoch  57 Batch   29/269   train_loss = 3.752\n",
      "Epoch  57 Batch   30/269   train_loss = 4.067\n",
      "Epoch  57 Batch   31/269   train_loss = 3.712\n",
      "Epoch  57 Batch   32/269   train_loss = 3.643\n",
      "Epoch  57 Batch   33/269   train_loss = 3.396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  57 Batch   34/269   train_loss = 3.611\n",
      "Epoch  57 Batch   35/269   train_loss = 3.527\n",
      "Epoch  57 Batch   36/269   train_loss = 3.513\n",
      "Epoch  57 Batch   37/269   train_loss = 3.713\n",
      "Epoch  57 Batch   38/269   train_loss = 3.461\n",
      "Epoch  57 Batch   39/269   train_loss = 3.746\n",
      "Epoch  57 Batch   40/269   train_loss = 3.560\n",
      "Epoch  57 Batch   41/269   train_loss = 3.580\n",
      "Epoch  57 Batch   42/269   train_loss = 4.031\n",
      "Epoch  57 Batch   43/269   train_loss = 3.948\n",
      "Epoch  57 Batch   44/269   train_loss = 3.724\n",
      "Epoch  57 Batch   45/269   train_loss = 3.417\n",
      "Epoch  57 Batch   46/269   train_loss = 3.770\n",
      "Epoch  57 Batch   47/269   train_loss = 3.404\n",
      "Epoch  57 Batch   48/269   train_loss = 3.901\n",
      "Epoch  57 Batch   49/269   train_loss = 3.875\n",
      "Epoch  57 Batch   50/269   train_loss = 3.845\n",
      "Epoch  57 Batch   51/269   train_loss = 3.752\n",
      "Epoch  57 Batch   52/269   train_loss = 3.749\n",
      "Epoch  57 Batch   53/269   train_loss = 3.968\n",
      "Epoch  57 Batch   54/269   train_loss = 3.744\n",
      "Epoch  57 Batch   55/269   train_loss = 3.826\n",
      "Epoch  57 Batch   56/269   train_loss = 3.589\n",
      "Epoch  57 Batch   57/269   train_loss = 3.892\n",
      "Epoch  57 Batch   58/269   train_loss = 3.596\n",
      "Epoch  57 Batch   59/269   train_loss = 3.604\n",
      "Epoch  57 Batch   60/269   train_loss = 3.779\n",
      "Epoch  57 Batch   61/269   train_loss = 3.931\n",
      "Epoch  57 Batch   62/269   train_loss = 3.582\n",
      "Epoch  57 Batch   63/269   train_loss = 3.899\n",
      "Epoch  57 Batch   64/269   train_loss = 3.608\n",
      "Epoch  57 Batch   65/269   train_loss = 3.756\n",
      "Epoch  57 Batch   66/269   train_loss = 3.628\n",
      "Epoch  57 Batch   67/269   train_loss = 3.703\n",
      "Epoch  57 Batch   68/269   train_loss = 3.749\n",
      "Epoch  57 Batch   69/269   train_loss = 3.813\n",
      "Epoch  57 Batch   70/269   train_loss = 3.430\n",
      "Epoch  57 Batch   71/269   train_loss = 3.806\n",
      "Epoch  57 Batch   72/269   train_loss = 3.468\n",
      "Epoch  57 Batch   73/269   train_loss = 3.678\n",
      "Epoch  57 Batch   74/269   train_loss = 3.626\n",
      "Epoch  57 Batch   75/269   train_loss = 3.804\n",
      "Epoch  57 Batch   76/269   train_loss = 3.795\n",
      "Epoch  57 Batch   77/269   train_loss = 3.439\n",
      "Epoch  57 Batch   78/269   train_loss = 3.830\n",
      "Epoch  57 Batch   79/269   train_loss = 3.895\n",
      "Epoch  57 Batch   80/269   train_loss = 3.854\n",
      "Epoch  57 Batch   81/269   train_loss = 3.679\n",
      "Epoch  57 Batch   82/269   train_loss = 3.851\n",
      "Epoch  57 Batch   83/269   train_loss = 3.639\n",
      "Epoch  57 Batch   84/269   train_loss = 3.918\n",
      "Epoch  57 Batch   85/269   train_loss = 4.081\n",
      "Epoch  57 Batch   86/269   train_loss = 4.060\n",
      "Epoch  57 Batch   87/269   train_loss = 3.669\n",
      "Epoch  57 Batch   88/269   train_loss = 3.793\n",
      "Epoch  57 Batch   89/269   train_loss = 3.755\n",
      "Epoch  57 Batch   90/269   train_loss = 3.584\n",
      "Epoch  57 Batch   91/269   train_loss = 3.877\n",
      "Epoch  57 Batch   92/269   train_loss = 3.853\n",
      "Epoch  57 Batch   93/269   train_loss = 3.582\n",
      "Epoch  57 Batch   94/269   train_loss = 4.074\n",
      "Epoch  57 Batch   95/269   train_loss = 3.520\n",
      "Epoch  57 Batch   96/269   train_loss = 3.633\n",
      "Epoch  57 Batch   97/269   train_loss = 3.702\n",
      "Epoch  57 Batch   98/269   train_loss = 3.922\n",
      "Epoch  57 Batch   99/269   train_loss = 3.720\n",
      "Epoch  57 Batch  100/269   train_loss = 3.713\n",
      "Epoch  57 Batch  101/269   train_loss = 3.799\n",
      "Epoch  57 Batch  102/269   train_loss = 3.520\n",
      "Epoch  57 Batch  103/269   train_loss = 3.646\n",
      "Epoch  57 Batch  104/269   train_loss = 3.524\n",
      "Epoch  57 Batch  105/269   train_loss = 3.890\n",
      "Epoch  57 Batch  106/269   train_loss = 3.988\n",
      "Epoch  57 Batch  107/269   train_loss = 3.997\n",
      "Epoch  57 Batch  108/269   train_loss = 3.703\n",
      "Epoch  57 Batch  109/269   train_loss = 3.748\n",
      "Epoch  57 Batch  110/269   train_loss = 3.746\n",
      "Epoch  57 Batch  111/269   train_loss = 3.845\n",
      "Epoch  57 Batch  112/269   train_loss = 3.871\n",
      "Epoch  57 Batch  113/269   train_loss = 3.742\n",
      "Epoch  57 Batch  114/269   train_loss = 3.740\n",
      "Epoch  57 Batch  115/269   train_loss = 3.582\n",
      "Epoch  57 Batch  116/269   train_loss = 3.560\n",
      "Epoch  57 Batch  117/269   train_loss = 3.576\n",
      "Epoch  57 Batch  118/269   train_loss = 3.945\n",
      "Epoch  57 Batch  119/269   train_loss = 3.464\n",
      "Epoch  57 Batch  120/269   train_loss = 3.758\n",
      "Epoch  57 Batch  121/269   train_loss = 3.667\n",
      "Epoch  57 Batch  122/269   train_loss = 3.727\n",
      "Epoch  57 Batch  123/269   train_loss = 3.521\n",
      "Epoch  57 Batch  124/269   train_loss = 3.879\n",
      "Epoch  57 Batch  125/269   train_loss = 3.507\n",
      "Epoch  57 Batch  126/269   train_loss = 3.430\n",
      "Epoch  57 Batch  127/269   train_loss = 3.646\n",
      "Epoch  57 Batch  128/269   train_loss = 3.630\n",
      "Epoch  57 Batch  129/269   train_loss = 3.651\n",
      "Epoch  57 Batch  130/269   train_loss = 3.794\n",
      "Epoch  57 Batch  131/269   train_loss = 3.367\n",
      "Epoch  57 Batch  132/269   train_loss = 3.735\n",
      "Epoch  57 Batch  133/269   train_loss = 3.471\n",
      "Epoch  57 Batch  134/269   train_loss = 3.914\n",
      "Epoch  57 Batch  135/269   train_loss = 3.575\n",
      "Epoch  57 Batch  136/269   train_loss = 3.428\n",
      "Epoch  57 Batch  137/269   train_loss = 3.776\n",
      "Epoch  57 Batch  138/269   train_loss = 3.523\n",
      "Epoch  57 Batch  139/269   train_loss = 3.706\n",
      "Epoch  57 Batch  140/269   train_loss = 3.783\n",
      "Epoch  57 Batch  141/269   train_loss = 4.063\n",
      "Epoch  57 Batch  142/269   train_loss = 3.505\n",
      "Epoch  57 Batch  143/269   train_loss = 3.709\n",
      "Epoch  57 Batch  144/269   train_loss = 3.895\n",
      "Epoch  57 Batch  145/269   train_loss = 3.679\n",
      "Epoch  57 Batch  146/269   train_loss = 3.919\n",
      "Epoch  57 Batch  147/269   train_loss = 3.486\n",
      "Epoch  57 Batch  148/269   train_loss = 3.832\n",
      "Epoch  57 Batch  149/269   train_loss = 3.649\n",
      "Epoch  57 Batch  150/269   train_loss = 3.825\n",
      "Epoch  57 Batch  151/269   train_loss = 3.765\n",
      "Epoch  57 Batch  152/269   train_loss = 3.722\n",
      "Epoch  57 Batch  153/269   train_loss = 3.754\n",
      "Epoch  57 Batch  154/269   train_loss = 3.616\n",
      "Epoch  57 Batch  155/269   train_loss = 3.602\n",
      "Epoch  57 Batch  156/269   train_loss = 4.090\n",
      "Epoch  57 Batch  157/269   train_loss = 3.769\n",
      "Epoch  57 Batch  158/269   train_loss = 3.793\n",
      "Epoch  57 Batch  159/269   train_loss = 3.770\n",
      "Epoch  57 Batch  160/269   train_loss = 3.676\n",
      "Epoch  57 Batch  161/269   train_loss = 3.788\n",
      "Epoch  57 Batch  162/269   train_loss = 3.800\n",
      "Epoch  57 Batch  163/269   train_loss = 3.621\n",
      "Epoch  57 Batch  164/269   train_loss = 3.884\n",
      "Epoch  57 Batch  165/269   train_loss = 3.880\n",
      "Epoch  57 Batch  166/269   train_loss = 3.787\n",
      "Epoch  57 Batch  167/269   train_loss = 3.606\n",
      "Epoch  57 Batch  168/269   train_loss = 3.736\n",
      "Epoch  57 Batch  169/269   train_loss = 4.078\n",
      "Epoch  57 Batch  170/269   train_loss = 3.469\n",
      "Epoch  57 Batch  171/269   train_loss = 3.774\n",
      "Epoch  57 Batch  172/269   train_loss = 3.585\n",
      "Epoch  57 Batch  173/269   train_loss = 3.721\n",
      "Epoch  57 Batch  174/269   train_loss = 3.905\n",
      "Epoch  57 Batch  175/269   train_loss = 3.755\n",
      "Epoch  57 Batch  176/269   train_loss = 3.710\n",
      "Epoch  57 Batch  177/269   train_loss = 3.684\n",
      "Epoch  57 Batch  178/269   train_loss = 3.923\n",
      "Epoch  57 Batch  179/269   train_loss = 3.700\n",
      "Epoch  57 Batch  180/269   train_loss = 3.607\n",
      "Epoch  57 Batch  181/269   train_loss = 3.805\n",
      "Epoch  57 Batch  182/269   train_loss = 3.611\n",
      "Epoch  57 Batch  183/269   train_loss = 3.617\n",
      "Epoch  57 Batch  184/269   train_loss = 3.734\n",
      "Epoch  57 Batch  185/269   train_loss = 3.810\n",
      "Epoch  57 Batch  186/269   train_loss = 3.666\n",
      "Epoch  57 Batch  187/269   train_loss = 3.906\n",
      "Epoch  57 Batch  188/269   train_loss = 3.695\n",
      "Epoch  57 Batch  189/269   train_loss = 3.590\n",
      "Epoch  57 Batch  190/269   train_loss = 4.072\n",
      "Epoch  57 Batch  191/269   train_loss = 3.767\n",
      "Epoch  57 Batch  192/269   train_loss = 3.765\n",
      "Epoch  57 Batch  193/269   train_loss = 3.727\n",
      "Epoch  57 Batch  194/269   train_loss = 3.756\n",
      "Epoch  57 Batch  195/269   train_loss = 3.721\n",
      "Epoch  57 Batch  196/269   train_loss = 3.747\n",
      "Epoch  57 Batch  197/269   train_loss = 3.839\n",
      "Epoch  57 Batch  198/269   train_loss = 3.793\n",
      "Epoch  57 Batch  199/269   train_loss = 3.815\n",
      "Epoch  57 Batch  200/269   train_loss = 3.600\n",
      "Epoch  57 Batch  201/269   train_loss = 3.725\n",
      "Epoch  57 Batch  202/269   train_loss = 3.514\n",
      "Epoch  57 Batch  203/269   train_loss = 3.648\n",
      "Epoch  57 Batch  204/269   train_loss = 3.820\n",
      "Epoch  57 Batch  205/269   train_loss = 3.761\n",
      "Epoch  57 Batch  206/269   train_loss = 3.687\n",
      "Epoch  57 Batch  207/269   train_loss = 3.628\n",
      "Epoch  57 Batch  208/269   train_loss = 3.744\n",
      "Epoch  57 Batch  209/269   train_loss = 3.852\n",
      "Epoch  57 Batch  210/269   train_loss = 3.596\n",
      "Epoch  57 Batch  211/269   train_loss = 3.690\n",
      "Epoch  57 Batch  212/269   train_loss = 4.050\n",
      "Epoch  57 Batch  213/269   train_loss = 3.604\n",
      "Epoch  57 Batch  214/269   train_loss = 3.724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  57 Batch  215/269   train_loss = 3.965\n",
      "Epoch  57 Batch  216/269   train_loss = 3.920\n",
      "Epoch  57 Batch  217/269   train_loss = 3.609\n",
      "Epoch  57 Batch  218/269   train_loss = 3.804\n",
      "Epoch  57 Batch  219/269   train_loss = 3.493\n",
      "Epoch  57 Batch  220/269   train_loss = 3.887\n",
      "Epoch  57 Batch  221/269   train_loss = 3.639\n",
      "Epoch  57 Batch  222/269   train_loss = 3.788\n",
      "Epoch  57 Batch  223/269   train_loss = 3.556\n",
      "Epoch  57 Batch  224/269   train_loss = 3.855\n",
      "Epoch  57 Batch  225/269   train_loss = 3.898\n",
      "Epoch  57 Batch  226/269   train_loss = 3.695\n",
      "Epoch  57 Batch  227/269   train_loss = 3.429\n",
      "Epoch  57 Batch  228/269   train_loss = 3.746\n",
      "Epoch  57 Batch  229/269   train_loss = 3.959\n",
      "Epoch  57 Batch  230/269   train_loss = 3.831\n",
      "Epoch  57 Batch  231/269   train_loss = 3.701\n",
      "Epoch  57 Batch  232/269   train_loss = 3.760\n",
      "Epoch  57 Batch  233/269   train_loss = 3.772\n",
      "Epoch  57 Batch  234/269   train_loss = 3.718\n",
      "Epoch  57 Batch  235/269   train_loss = 3.991\n",
      "Epoch  57 Batch  236/269   train_loss = 3.613\n",
      "Epoch  57 Batch  237/269   train_loss = 3.247\n",
      "Epoch  57 Batch  238/269   train_loss = 3.531\n",
      "Epoch  57 Batch  239/269   train_loss = 4.030\n",
      "Epoch  57 Batch  240/269   train_loss = 3.602\n",
      "Epoch  57 Batch  241/269   train_loss = 3.955\n",
      "Epoch  57 Batch  242/269   train_loss = 3.531\n",
      "Epoch  57 Batch  243/269   train_loss = 3.917\n",
      "Epoch  57 Batch  244/269   train_loss = 3.752\n",
      "Epoch  57 Batch  245/269   train_loss = 3.742\n",
      "Epoch  57 Batch  246/269   train_loss = 3.385\n",
      "Epoch  57 Batch  247/269   train_loss = 3.814\n",
      "Epoch  57 Batch  248/269   train_loss = 3.767\n",
      "Epoch  57 Batch  249/269   train_loss = 3.610\n",
      "Epoch  57 Batch  250/269   train_loss = 3.557\n",
      "Epoch  57 Batch  251/269   train_loss = 3.945\n",
      "Epoch  57 Batch  252/269   train_loss = 3.886\n",
      "Epoch  57 Batch  253/269   train_loss = 3.663\n",
      "Epoch  57 Batch  254/269   train_loss = 3.646\n",
      "Epoch  57 Batch  255/269   train_loss = 3.691\n",
      "Epoch  57 Batch  256/269   train_loss = 3.623\n",
      "Epoch  57 Batch  257/269   train_loss = 3.576\n",
      "Epoch  57 Batch  258/269   train_loss = 3.447\n",
      "Epoch  57 Batch  259/269   train_loss = 3.597\n",
      "Epoch  57 Batch  260/269   train_loss = 3.800\n",
      "Epoch  57 Batch  261/269   train_loss = 3.875\n",
      "Epoch  57 Batch  262/269   train_loss = 3.608\n",
      "Epoch  57 Batch  263/269   train_loss = 3.571\n",
      "Epoch  57 Batch  264/269   train_loss = 3.922\n",
      "Epoch  57 Batch  265/269   train_loss = 3.804\n",
      "Epoch  57 Batch  266/269   train_loss = 3.640\n",
      "Epoch  57 Batch  267/269   train_loss = 3.732\n",
      "Epoch  57 Batch  268/269   train_loss = 3.837\n",
      "Epoch  58 Batch    0/269   train_loss = 3.649\n",
      "Epoch  58 Batch    1/269   train_loss = 3.495\n",
      "Epoch  58 Batch    2/269   train_loss = 3.596\n",
      "Epoch  58 Batch    3/269   train_loss = 3.668\n",
      "Epoch  58 Batch    4/269   train_loss = 4.065\n",
      "Epoch  58 Batch    5/269   train_loss = 3.769\n",
      "Epoch  58 Batch    6/269   train_loss = 3.677\n",
      "Epoch  58 Batch    7/269   train_loss = 3.525\n",
      "Epoch  58 Batch    8/269   train_loss = 3.769\n",
      "Epoch  58 Batch    9/269   train_loss = 3.455\n",
      "Epoch  58 Batch   10/269   train_loss = 3.609\n",
      "Epoch  58 Batch   11/269   train_loss = 3.686\n",
      "Epoch  58 Batch   12/269   train_loss = 3.371\n",
      "Epoch  58 Batch   13/269   train_loss = 3.741\n",
      "Epoch  58 Batch   14/269   train_loss = 3.574\n",
      "Epoch  58 Batch   15/269   train_loss = 3.926\n",
      "Epoch  58 Batch   16/269   train_loss = 3.671\n",
      "Epoch  58 Batch   17/269   train_loss = 3.693\n",
      "Epoch  58 Batch   18/269   train_loss = 3.646\n",
      "Epoch  58 Batch   19/269   train_loss = 3.649\n",
      "Epoch  58 Batch   20/269   train_loss = 4.013\n",
      "Epoch  58 Batch   21/269   train_loss = 3.873\n",
      "Epoch  58 Batch   22/269   train_loss = 3.590\n",
      "Epoch  58 Batch   23/269   train_loss = 3.815\n",
      "Epoch  58 Batch   24/269   train_loss = 3.535\n",
      "Epoch  58 Batch   25/269   train_loss = 3.864\n",
      "Epoch  58 Batch   26/269   train_loss = 3.787\n",
      "Epoch  58 Batch   27/269   train_loss = 3.623\n",
      "Epoch  58 Batch   28/269   train_loss = 3.779\n",
      "Epoch  58 Batch   29/269   train_loss = 3.780\n",
      "Epoch  58 Batch   30/269   train_loss = 4.051\n",
      "Epoch  58 Batch   31/269   train_loss = 3.732\n",
      "Epoch  58 Batch   32/269   train_loss = 3.623\n",
      "Epoch  58 Batch   33/269   train_loss = 3.383\n",
      "Epoch  58 Batch   34/269   train_loss = 3.619\n",
      "Epoch  58 Batch   35/269   train_loss = 3.514\n",
      "Epoch  58 Batch   36/269   train_loss = 3.542\n",
      "Epoch  58 Batch   37/269   train_loss = 3.767\n",
      "Epoch  58 Batch   38/269   train_loss = 3.450\n",
      "Epoch  58 Batch   39/269   train_loss = 3.764\n",
      "Epoch  58 Batch   40/269   train_loss = 3.601\n",
      "Epoch  58 Batch   41/269   train_loss = 3.565\n",
      "Epoch  58 Batch   42/269   train_loss = 4.057\n",
      "Epoch  58 Batch   43/269   train_loss = 3.954\n",
      "Epoch  58 Batch   44/269   train_loss = 3.718\n",
      "Epoch  58 Batch   45/269   train_loss = 3.428\n",
      "Epoch  58 Batch   46/269   train_loss = 3.788\n",
      "Epoch  58 Batch   47/269   train_loss = 3.455\n",
      "Epoch  58 Batch   48/269   train_loss = 3.901\n",
      "Epoch  58 Batch   49/269   train_loss = 3.894\n",
      "Epoch  58 Batch   50/269   train_loss = 3.828\n",
      "Epoch  58 Batch   51/269   train_loss = 3.758\n",
      "Epoch  58 Batch   52/269   train_loss = 3.764\n",
      "Epoch  58 Batch   53/269   train_loss = 3.976\n",
      "Epoch  58 Batch   54/269   train_loss = 3.746\n",
      "Epoch  58 Batch   55/269   train_loss = 3.839\n",
      "Epoch  58 Batch   56/269   train_loss = 3.617\n",
      "Epoch  58 Batch   57/269   train_loss = 3.900\n",
      "Epoch  58 Batch   58/269   train_loss = 3.589\n",
      "Epoch  58 Batch   59/269   train_loss = 3.617\n",
      "Epoch  58 Batch   60/269   train_loss = 3.772\n",
      "Epoch  58 Batch   61/269   train_loss = 3.935\n",
      "Epoch  58 Batch   62/269   train_loss = 3.612\n",
      "Epoch  58 Batch   63/269   train_loss = 3.914\n",
      "Epoch  58 Batch   64/269   train_loss = 3.610\n",
      "Epoch  58 Batch   65/269   train_loss = 3.758\n",
      "Epoch  58 Batch   66/269   train_loss = 3.675\n",
      "Epoch  58 Batch   67/269   train_loss = 3.693\n",
      "Epoch  58 Batch   68/269   train_loss = 3.782\n",
      "Epoch  58 Batch   69/269   train_loss = 3.787\n",
      "Epoch  58 Batch   70/269   train_loss = 3.423\n",
      "Epoch  58 Batch   71/269   train_loss = 3.816\n",
      "Epoch  58 Batch   72/269   train_loss = 3.492\n",
      "Epoch  58 Batch   73/269   train_loss = 3.689\n",
      "Epoch  58 Batch   74/269   train_loss = 3.649\n",
      "Epoch  58 Batch   75/269   train_loss = 3.825\n",
      "Epoch  58 Batch   76/269   train_loss = 3.795\n",
      "Epoch  58 Batch   77/269   train_loss = 3.448\n",
      "Epoch  58 Batch   78/269   train_loss = 3.857\n",
      "Epoch  58 Batch   79/269   train_loss = 3.910\n",
      "Epoch  58 Batch   80/269   train_loss = 3.839\n",
      "Epoch  58 Batch   81/269   train_loss = 3.697\n",
      "Epoch  58 Batch   82/269   train_loss = 3.866\n",
      "Epoch  58 Batch   83/269   train_loss = 3.625\n",
      "Epoch  58 Batch   84/269   train_loss = 3.920\n",
      "Epoch  58 Batch   85/269   train_loss = 4.088\n",
      "Epoch  58 Batch   86/269   train_loss = 4.046\n",
      "Epoch  58 Batch   87/269   train_loss = 3.705\n",
      "Epoch  58 Batch   88/269   train_loss = 3.819\n",
      "Epoch  58 Batch   89/269   train_loss = 3.758\n",
      "Epoch  58 Batch   90/269   train_loss = 3.580\n",
      "Epoch  58 Batch   91/269   train_loss = 3.862\n",
      "Epoch  58 Batch   92/269   train_loss = 3.872\n",
      "Epoch  58 Batch   93/269   train_loss = 3.588\n",
      "Epoch  58 Batch   94/269   train_loss = 4.066\n",
      "Epoch  58 Batch   95/269   train_loss = 3.532\n",
      "Epoch  58 Batch   96/269   train_loss = 3.629\n",
      "Epoch  58 Batch   97/269   train_loss = 3.722\n",
      "Epoch  58 Batch   98/269   train_loss = 3.922\n",
      "Epoch  58 Batch   99/269   train_loss = 3.698\n",
      "Epoch  58 Batch  100/269   train_loss = 3.741\n",
      "Epoch  58 Batch  101/269   train_loss = 3.851\n",
      "Epoch  58 Batch  102/269   train_loss = 3.532\n",
      "Epoch  58 Batch  103/269   train_loss = 3.653\n",
      "Epoch  58 Batch  104/269   train_loss = 3.571\n",
      "Epoch  58 Batch  105/269   train_loss = 3.884\n",
      "Epoch  58 Batch  106/269   train_loss = 3.979\n",
      "Epoch  58 Batch  107/269   train_loss = 3.982\n",
      "Epoch  58 Batch  108/269   train_loss = 3.738\n",
      "Epoch  58 Batch  109/269   train_loss = 3.758\n",
      "Epoch  58 Batch  110/269   train_loss = 3.743\n",
      "Epoch  58 Batch  111/269   train_loss = 3.853\n",
      "Epoch  58 Batch  112/269   train_loss = 3.888\n",
      "Epoch  58 Batch  113/269   train_loss = 3.730\n",
      "Epoch  58 Batch  114/269   train_loss = 3.727\n",
      "Epoch  58 Batch  115/269   train_loss = 3.588\n",
      "Epoch  58 Batch  116/269   train_loss = 3.568\n",
      "Epoch  58 Batch  117/269   train_loss = 3.603\n",
      "Epoch  58 Batch  118/269   train_loss = 3.968\n",
      "Epoch  58 Batch  119/269   train_loss = 3.500\n",
      "Epoch  58 Batch  120/269   train_loss = 3.757\n",
      "Epoch  58 Batch  121/269   train_loss = 3.694\n",
      "Epoch  58 Batch  122/269   train_loss = 3.744\n",
      "Epoch  58 Batch  123/269   train_loss = 3.538\n",
      "Epoch  58 Batch  124/269   train_loss = 3.888\n",
      "Epoch  58 Batch  125/269   train_loss = 3.503\n",
      "Epoch  58 Batch  126/269   train_loss = 3.405\n",
      "Epoch  58 Batch  127/269   train_loss = 3.654\n",
      "Epoch  58 Batch  128/269   train_loss = 3.654\n",
      "Epoch  58 Batch  129/269   train_loss = 3.645\n",
      "Epoch  58 Batch  130/269   train_loss = 3.781\n",
      "Epoch  58 Batch  131/269   train_loss = 3.381\n",
      "Epoch  58 Batch  132/269   train_loss = 3.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  58 Batch  133/269   train_loss = 3.465\n",
      "Epoch  58 Batch  134/269   train_loss = 3.898\n",
      "Epoch  58 Batch  135/269   train_loss = 3.600\n",
      "Epoch  58 Batch  136/269   train_loss = 3.460\n",
      "Epoch  58 Batch  137/269   train_loss = 3.806\n",
      "Epoch  58 Batch  138/269   train_loss = 3.539\n",
      "Epoch  58 Batch  139/269   train_loss = 3.688\n",
      "Epoch  58 Batch  140/269   train_loss = 3.770\n",
      "Epoch  58 Batch  141/269   train_loss = 4.091\n",
      "Epoch  58 Batch  142/269   train_loss = 3.521\n",
      "Epoch  58 Batch  143/269   train_loss = 3.701\n",
      "Epoch  58 Batch  144/269   train_loss = 3.911\n",
      "Epoch  58 Batch  145/269   train_loss = 3.675\n",
      "Epoch  58 Batch  146/269   train_loss = 3.913\n",
      "Epoch  58 Batch  147/269   train_loss = 3.477\n",
      "Epoch  58 Batch  148/269   train_loss = 3.833\n",
      "Epoch  58 Batch  149/269   train_loss = 3.679\n",
      "Epoch  58 Batch  150/269   train_loss = 3.818\n",
      "Epoch  58 Batch  151/269   train_loss = 3.755\n",
      "Epoch  58 Batch  152/269   train_loss = 3.692\n",
      "Epoch  58 Batch  153/269   train_loss = 3.750\n",
      "Epoch  58 Batch  154/269   train_loss = 3.588\n",
      "Epoch  58 Batch  155/269   train_loss = 3.568\n",
      "Epoch  58 Batch  156/269   train_loss = 4.079\n",
      "Epoch  58 Batch  157/269   train_loss = 3.767\n",
      "Epoch  58 Batch  158/269   train_loss = 3.785\n",
      "Epoch  58 Batch  159/269   train_loss = 3.751\n",
      "Epoch  58 Batch  160/269   train_loss = 3.651\n",
      "Epoch  58 Batch  161/269   train_loss = 3.814\n",
      "Epoch  58 Batch  162/269   train_loss = 3.833\n",
      "Epoch  58 Batch  163/269   train_loss = 3.605\n",
      "Epoch  58 Batch  164/269   train_loss = 3.952\n",
      "Epoch  58 Batch  165/269   train_loss = 3.920\n",
      "Epoch  58 Batch  166/269   train_loss = 3.783\n",
      "Epoch  58 Batch  167/269   train_loss = 3.661\n",
      "Epoch  58 Batch  168/269   train_loss = 3.739\n",
      "Epoch  58 Batch  169/269   train_loss = 4.036\n",
      "Epoch  58 Batch  170/269   train_loss = 3.445\n",
      "Epoch  58 Batch  171/269   train_loss = 3.765\n",
      "Epoch  58 Batch  172/269   train_loss = 3.573\n",
      "Epoch  58 Batch  173/269   train_loss = 3.711\n",
      "Epoch  58 Batch  174/269   train_loss = 3.903\n",
      "Epoch  58 Batch  175/269   train_loss = 3.789\n",
      "Epoch  58 Batch  176/269   train_loss = 3.706\n",
      "Epoch  58 Batch  177/269   train_loss = 3.692\n",
      "Epoch  58 Batch  178/269   train_loss = 3.957\n",
      "Epoch  58 Batch  179/269   train_loss = 3.710\n",
      "Epoch  58 Batch  180/269   train_loss = 3.602\n",
      "Epoch  58 Batch  181/269   train_loss = 3.836\n",
      "Epoch  58 Batch  182/269   train_loss = 3.652\n",
      "Epoch  58 Batch  183/269   train_loss = 3.644\n",
      "Epoch  58 Batch  184/269   train_loss = 3.742\n",
      "Epoch  58 Batch  185/269   train_loss = 3.846\n",
      "Epoch  58 Batch  186/269   train_loss = 3.676\n",
      "Epoch  58 Batch  187/269   train_loss = 3.909\n",
      "Epoch  58 Batch  188/269   train_loss = 3.719\n",
      "Epoch  58 Batch  189/269   train_loss = 3.630\n",
      "Epoch  58 Batch  190/269   train_loss = 4.091\n",
      "Epoch  58 Batch  191/269   train_loss = 3.789\n",
      "Epoch  58 Batch  192/269   train_loss = 3.758\n",
      "Epoch  58 Batch  193/269   train_loss = 3.744\n",
      "Epoch  58 Batch  194/269   train_loss = 3.763\n",
      "Epoch  58 Batch  195/269   train_loss = 3.731\n",
      "Epoch  58 Batch  196/269   train_loss = 3.740\n",
      "Epoch  58 Batch  197/269   train_loss = 3.875\n",
      "Epoch  58 Batch  198/269   train_loss = 3.794\n",
      "Epoch  58 Batch  199/269   train_loss = 3.802\n",
      "Epoch  58 Batch  200/269   train_loss = 3.616\n",
      "Epoch  58 Batch  201/269   train_loss = 3.755\n",
      "Epoch  58 Batch  202/269   train_loss = 3.537\n",
      "Epoch  58 Batch  203/269   train_loss = 3.636\n",
      "Epoch  58 Batch  204/269   train_loss = 3.805\n",
      "Epoch  58 Batch  205/269   train_loss = 3.798\n",
      "Epoch  58 Batch  206/269   train_loss = 3.686\n",
      "Epoch  58 Batch  207/269   train_loss = 3.614\n",
      "Epoch  58 Batch  208/269   train_loss = 3.779\n",
      "Epoch  58 Batch  209/269   train_loss = 3.824\n",
      "Epoch  58 Batch  210/269   train_loss = 3.641\n",
      "Epoch  58 Batch  211/269   train_loss = 3.670\n",
      "Epoch  58 Batch  212/269   train_loss = 4.072\n",
      "Epoch  58 Batch  213/269   train_loss = 3.613\n",
      "Epoch  58 Batch  214/269   train_loss = 3.727\n",
      "Epoch  58 Batch  215/269   train_loss = 3.972\n",
      "Epoch  58 Batch  216/269   train_loss = 3.917\n",
      "Epoch  58 Batch  217/269   train_loss = 3.591\n",
      "Epoch  58 Batch  218/269   train_loss = 3.775\n",
      "Epoch  58 Batch  219/269   train_loss = 3.485\n",
      "Epoch  58 Batch  220/269   train_loss = 3.861\n",
      "Epoch  58 Batch  221/269   train_loss = 3.590\n",
      "Epoch  58 Batch  222/269   train_loss = 3.785\n",
      "Epoch  58 Batch  223/269   train_loss = 3.565\n",
      "Epoch  58 Batch  224/269   train_loss = 3.848\n",
      "Epoch  58 Batch  225/269   train_loss = 3.913\n",
      "Epoch  58 Batch  226/269   train_loss = 3.694\n",
      "Epoch  58 Batch  227/269   train_loss = 3.406\n",
      "Epoch  58 Batch  228/269   train_loss = 3.745\n",
      "Epoch  58 Batch  229/269   train_loss = 3.923\n",
      "Epoch  58 Batch  230/269   train_loss = 3.857\n",
      "Epoch  58 Batch  231/269   train_loss = 3.692\n",
      "Epoch  58 Batch  232/269   train_loss = 3.707\n",
      "Epoch  58 Batch  233/269   train_loss = 3.798\n",
      "Epoch  58 Batch  234/269   train_loss = 3.707\n",
      "Epoch  58 Batch  235/269   train_loss = 4.012\n",
      "Epoch  58 Batch  236/269   train_loss = 3.583\n",
      "Epoch  58 Batch  237/269   train_loss = 3.237\n",
      "Epoch  58 Batch  238/269   train_loss = 3.525\n",
      "Epoch  58 Batch  239/269   train_loss = 4.004\n",
      "Epoch  58 Batch  240/269   train_loss = 3.587\n",
      "Epoch  58 Batch  241/269   train_loss = 3.970\n",
      "Epoch  58 Batch  242/269   train_loss = 3.548\n",
      "Epoch  58 Batch  243/269   train_loss = 3.909\n",
      "Epoch  58 Batch  244/269   train_loss = 3.752\n",
      "Epoch  58 Batch  245/269   train_loss = 3.759\n",
      "Epoch  58 Batch  246/269   train_loss = 3.372\n",
      "Epoch  58 Batch  247/269   train_loss = 3.800\n",
      "Epoch  58 Batch  248/269   train_loss = 3.765\n",
      "Epoch  58 Batch  249/269   train_loss = 3.638\n",
      "Epoch  58 Batch  250/269   train_loss = 3.524\n",
      "Epoch  58 Batch  251/269   train_loss = 3.950\n",
      "Epoch  58 Batch  252/269   train_loss = 3.887\n",
      "Epoch  58 Batch  253/269   train_loss = 3.631\n",
      "Epoch  58 Batch  254/269   train_loss = 3.617\n",
      "Epoch  58 Batch  255/269   train_loss = 3.690\n",
      "Epoch  58 Batch  256/269   train_loss = 3.611\n",
      "Epoch  58 Batch  257/269   train_loss = 3.581\n",
      "Epoch  58 Batch  258/269   train_loss = 3.449\n",
      "Epoch  58 Batch  259/269   train_loss = 3.562\n",
      "Epoch  58 Batch  260/269   train_loss = 3.806\n",
      "Epoch  58 Batch  261/269   train_loss = 3.881\n",
      "Epoch  58 Batch  262/269   train_loss = 3.634\n",
      "Epoch  58 Batch  263/269   train_loss = 3.538\n",
      "Epoch  58 Batch  264/269   train_loss = 3.915\n",
      "Epoch  58 Batch  265/269   train_loss = 3.828\n",
      "Epoch  58 Batch  266/269   train_loss = 3.633\n",
      "Epoch  58 Batch  267/269   train_loss = 3.726\n",
      "Epoch  58 Batch  268/269   train_loss = 3.838\n",
      "Epoch  59 Batch    0/269   train_loss = 3.682\n",
      "Epoch  59 Batch    1/269   train_loss = 3.532\n",
      "Epoch  59 Batch    2/269   train_loss = 3.573\n",
      "Epoch  59 Batch    3/269   train_loss = 3.707\n",
      "Epoch  59 Batch    4/269   train_loss = 4.061\n",
      "Epoch  59 Batch    5/269   train_loss = 3.788\n",
      "Epoch  59 Batch    6/269   train_loss = 3.675\n",
      "Epoch  59 Batch    7/269   train_loss = 3.518\n",
      "Epoch  59 Batch    8/269   train_loss = 3.782\n",
      "Epoch  59 Batch    9/269   train_loss = 3.452\n",
      "Epoch  59 Batch   10/269   train_loss = 3.593\n",
      "Epoch  59 Batch   11/269   train_loss = 3.645\n",
      "Epoch  59 Batch   12/269   train_loss = 3.383\n",
      "Epoch  59 Batch   13/269   train_loss = 3.738\n",
      "Epoch  59 Batch   14/269   train_loss = 3.610\n",
      "Epoch  59 Batch   15/269   train_loss = 3.905\n",
      "Epoch  59 Batch   16/269   train_loss = 3.619\n",
      "Epoch  59 Batch   17/269   train_loss = 3.718\n",
      "Epoch  59 Batch   18/269   train_loss = 3.644\n",
      "Epoch  59 Batch   19/269   train_loss = 3.637\n",
      "Epoch  59 Batch   20/269   train_loss = 4.002\n",
      "Epoch  59 Batch   21/269   train_loss = 3.898\n",
      "Epoch  59 Batch   22/269   train_loss = 3.567\n",
      "Epoch  59 Batch   23/269   train_loss = 3.795\n",
      "Epoch  59 Batch   24/269   train_loss = 3.521\n",
      "Epoch  59 Batch   25/269   train_loss = 3.905\n",
      "Epoch  59 Batch   26/269   train_loss = 3.766\n",
      "Epoch  59 Batch   27/269   train_loss = 3.644\n",
      "Epoch  59 Batch   28/269   train_loss = 3.800\n",
      "Epoch  59 Batch   29/269   train_loss = 3.773\n",
      "Epoch  59 Batch   30/269   train_loss = 4.041\n",
      "Epoch  59 Batch   31/269   train_loss = 3.739\n",
      "Epoch  59 Batch   32/269   train_loss = 3.620\n",
      "Epoch  59 Batch   33/269   train_loss = 3.366\n",
      "Epoch  59 Batch   34/269   train_loss = 3.611\n",
      "Epoch  59 Batch   35/269   train_loss = 3.520\n",
      "Epoch  59 Batch   36/269   train_loss = 3.525\n",
      "Epoch  59 Batch   37/269   train_loss = 3.734\n",
      "Epoch  59 Batch   38/269   train_loss = 3.453\n",
      "Epoch  59 Batch   39/269   train_loss = 3.764\n",
      "Epoch  59 Batch   40/269   train_loss = 3.583\n",
      "Epoch  59 Batch   41/269   train_loss = 3.562\n",
      "Epoch  59 Batch   42/269   train_loss = 4.057\n",
      "Epoch  59 Batch   43/269   train_loss = 3.962\n",
      "Epoch  59 Batch   44/269   train_loss = 3.729\n",
      "Epoch  59 Batch   45/269   train_loss = 3.397\n",
      "Epoch  59 Batch   46/269   train_loss = 3.757\n",
      "Epoch  59 Batch   47/269   train_loss = 3.429\n",
      "Epoch  59 Batch   48/269   train_loss = 3.929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  59 Batch   49/269   train_loss = 3.888\n",
      "Epoch  59 Batch   50/269   train_loss = 3.846\n",
      "Epoch  59 Batch   51/269   train_loss = 3.734\n",
      "Epoch  59 Batch   52/269   train_loss = 3.753\n",
      "Epoch  59 Batch   53/269   train_loss = 3.970\n",
      "Epoch  59 Batch   54/269   train_loss = 3.723\n",
      "Epoch  59 Batch   55/269   train_loss = 3.861\n",
      "Epoch  59 Batch   56/269   train_loss = 3.631\n",
      "Epoch  59 Batch   57/269   train_loss = 3.924\n",
      "Epoch  59 Batch   58/269   train_loss = 3.610\n",
      "Epoch  59 Batch   59/269   train_loss = 3.621\n",
      "Epoch  59 Batch   60/269   train_loss = 3.780\n",
      "Epoch  59 Batch   61/269   train_loss = 3.944\n",
      "Epoch  59 Batch   62/269   train_loss = 3.601\n",
      "Epoch  59 Batch   63/269   train_loss = 3.903\n",
      "Epoch  59 Batch   64/269   train_loss = 3.607\n",
      "Epoch  59 Batch   65/269   train_loss = 3.754\n",
      "Epoch  59 Batch   66/269   train_loss = 3.622\n",
      "Epoch  59 Batch   67/269   train_loss = 3.714\n",
      "Epoch  59 Batch   68/269   train_loss = 3.762\n",
      "Epoch  59 Batch   69/269   train_loss = 3.765\n",
      "Epoch  59 Batch   70/269   train_loss = 3.434\n",
      "Epoch  59 Batch   71/269   train_loss = 3.820\n",
      "Epoch  59 Batch   72/269   train_loss = 3.489\n",
      "Epoch  59 Batch   73/269   train_loss = 3.672\n",
      "Epoch  59 Batch   74/269   train_loss = 3.655\n",
      "Epoch  59 Batch   75/269   train_loss = 3.847\n",
      "Epoch  59 Batch   76/269   train_loss = 3.752\n",
      "Epoch  59 Batch   77/269   train_loss = 3.428\n",
      "Epoch  59 Batch   78/269   train_loss = 3.854\n",
      "Epoch  59 Batch   79/269   train_loss = 3.869\n",
      "Epoch  59 Batch   80/269   train_loss = 3.834\n",
      "Epoch  59 Batch   81/269   train_loss = 3.691\n",
      "Epoch  59 Batch   82/269   train_loss = 3.834\n",
      "Epoch  59 Batch   83/269   train_loss = 3.606\n",
      "Epoch  59 Batch   84/269   train_loss = 3.951\n",
      "Epoch  59 Batch   85/269   train_loss = 4.051\n",
      "Epoch  59 Batch   86/269   train_loss = 4.104\n",
      "Epoch  59 Batch   87/269   train_loss = 3.655\n",
      "Epoch  59 Batch   88/269   train_loss = 3.805\n",
      "Epoch  59 Batch   89/269   train_loss = 3.743\n",
      "Epoch  59 Batch   90/269   train_loss = 3.597\n",
      "Epoch  59 Batch   91/269   train_loss = 3.849\n",
      "Epoch  59 Batch   92/269   train_loss = 3.847\n",
      "Epoch  59 Batch   93/269   train_loss = 3.580\n",
      "Epoch  59 Batch   94/269   train_loss = 4.076\n",
      "Epoch  59 Batch   95/269   train_loss = 3.551\n",
      "Epoch  59 Batch   96/269   train_loss = 3.638\n",
      "Epoch  59 Batch   97/269   train_loss = 3.731\n",
      "Epoch  59 Batch   98/269   train_loss = 3.934\n",
      "Epoch  59 Batch   99/269   train_loss = 3.727\n",
      "Epoch  59 Batch  100/269   train_loss = 3.735\n",
      "Epoch  59 Batch  101/269   train_loss = 3.870\n",
      "Epoch  59 Batch  102/269   train_loss = 3.541\n",
      "Epoch  59 Batch  103/269   train_loss = 3.671\n",
      "Epoch  59 Batch  104/269   train_loss = 3.570\n",
      "Epoch  59 Batch  105/269   train_loss = 3.918\n",
      "Epoch  59 Batch  106/269   train_loss = 3.988\n",
      "Epoch  59 Batch  107/269   train_loss = 4.003\n",
      "Epoch  59 Batch  108/269   train_loss = 3.775\n",
      "Epoch  59 Batch  109/269   train_loss = 3.748\n",
      "Epoch  59 Batch  110/269   train_loss = 3.754\n",
      "Epoch  59 Batch  111/269   train_loss = 3.919\n",
      "Epoch  59 Batch  112/269   train_loss = 3.888\n",
      "Epoch  59 Batch  113/269   train_loss = 3.764\n",
      "Epoch  59 Batch  114/269   train_loss = 3.717\n",
      "Epoch  59 Batch  115/269   train_loss = 3.587\n",
      "Epoch  59 Batch  116/269   train_loss = 3.579\n",
      "Epoch  59 Batch  117/269   train_loss = 3.611\n",
      "Epoch  59 Batch  118/269   train_loss = 3.988\n",
      "Epoch  59 Batch  119/269   train_loss = 3.472\n",
      "Epoch  59 Batch  120/269   train_loss = 3.778\n",
      "Epoch  59 Batch  121/269   train_loss = 3.698\n",
      "Epoch  59 Batch  122/269   train_loss = 3.765\n",
      "Epoch  59 Batch  123/269   train_loss = 3.525\n",
      "Epoch  59 Batch  124/269   train_loss = 3.908\n",
      "Epoch  59 Batch  125/269   train_loss = 3.500\n",
      "Epoch  59 Batch  126/269   train_loss = 3.432\n",
      "Epoch  59 Batch  127/269   train_loss = 3.653\n",
      "Epoch  59 Batch  128/269   train_loss = 3.651\n",
      "Epoch  59 Batch  129/269   train_loss = 3.645\n",
      "Epoch  59 Batch  130/269   train_loss = 3.803\n",
      "Epoch  59 Batch  131/269   train_loss = 3.418\n",
      "Epoch  59 Batch  132/269   train_loss = 3.753\n",
      "Epoch  59 Batch  133/269   train_loss = 3.470\n",
      "Epoch  59 Batch  134/269   train_loss = 3.910\n",
      "Epoch  59 Batch  135/269   train_loss = 3.595\n",
      "Epoch  59 Batch  136/269   train_loss = 3.421\n",
      "Epoch  59 Batch  137/269   train_loss = 3.773\n",
      "Epoch  59 Batch  138/269   train_loss = 3.527\n",
      "Epoch  59 Batch  139/269   train_loss = 3.708\n",
      "Epoch  59 Batch  140/269   train_loss = 3.747\n",
      "Epoch  59 Batch  141/269   train_loss = 4.081\n",
      "Epoch  59 Batch  142/269   train_loss = 3.510\n",
      "Epoch  59 Batch  143/269   train_loss = 3.735\n",
      "Epoch  59 Batch  144/269   train_loss = 3.938\n",
      "Epoch  59 Batch  145/269   train_loss = 3.642\n",
      "Epoch  59 Batch  146/269   train_loss = 3.933\n",
      "Epoch  59 Batch  147/269   train_loss = 3.460\n",
      "Epoch  59 Batch  148/269   train_loss = 3.811\n",
      "Epoch  59 Batch  149/269   train_loss = 3.664\n",
      "Epoch  59 Batch  150/269   train_loss = 3.803\n",
      "Epoch  59 Batch  151/269   train_loss = 3.758\n",
      "Epoch  59 Batch  152/269   train_loss = 3.689\n",
      "Epoch  59 Batch  153/269   train_loss = 3.744\n",
      "Epoch  59 Batch  154/269   train_loss = 3.622\n",
      "Epoch  59 Batch  155/269   train_loss = 3.578\n",
      "Epoch  59 Batch  156/269   train_loss = 4.103\n",
      "Epoch  59 Batch  157/269   train_loss = 3.771\n",
      "Epoch  59 Batch  158/269   train_loss = 3.787\n",
      "Epoch  59 Batch  159/269   train_loss = 3.740\n",
      "Epoch  59 Batch  160/269   train_loss = 3.648\n",
      "Epoch  59 Batch  161/269   train_loss = 3.821\n",
      "Epoch  59 Batch  162/269   train_loss = 3.841\n",
      "Epoch  59 Batch  163/269   train_loss = 3.591\n",
      "Epoch  59 Batch  164/269   train_loss = 3.930\n",
      "Epoch  59 Batch  165/269   train_loss = 3.913\n",
      "Epoch  59 Batch  166/269   train_loss = 3.777\n",
      "Epoch  59 Batch  167/269   train_loss = 3.686\n",
      "Epoch  59 Batch  168/269   train_loss = 3.760\n",
      "Epoch  59 Batch  169/269   train_loss = 3.997\n",
      "Epoch  59 Batch  170/269   train_loss = 3.468\n",
      "Epoch  59 Batch  171/269   train_loss = 3.769\n",
      "Epoch  59 Batch  172/269   train_loss = 3.584\n",
      "Epoch  59 Batch  173/269   train_loss = 3.740\n",
      "Epoch  59 Batch  174/269   train_loss = 3.946\n",
      "Epoch  59 Batch  175/269   train_loss = 3.814\n",
      "Epoch  59 Batch  176/269   train_loss = 3.766\n",
      "Epoch  59 Batch  177/269   train_loss = 3.816\n",
      "Epoch  59 Batch  178/269   train_loss = 3.963\n",
      "Epoch  59 Batch  179/269   train_loss = 3.707\n",
      "Epoch  59 Batch  180/269   train_loss = 3.584\n",
      "Epoch  59 Batch  181/269   train_loss = 3.829\n",
      "Epoch  59 Batch  182/269   train_loss = 3.666\n",
      "Epoch  59 Batch  183/269   train_loss = 3.633\n",
      "Epoch  59 Batch  184/269   train_loss = 3.747\n",
      "Epoch  59 Batch  185/269   train_loss = 3.820\n",
      "Epoch  59 Batch  186/269   train_loss = 3.693\n",
      "Epoch  59 Batch  187/269   train_loss = 3.910\n",
      "Epoch  59 Batch  188/269   train_loss = 3.691\n",
      "Epoch  59 Batch  189/269   train_loss = 3.630\n",
      "Epoch  59 Batch  190/269   train_loss = 4.100\n",
      "Epoch  59 Batch  191/269   train_loss = 3.800\n",
      "Epoch  59 Batch  192/269   train_loss = 3.736\n",
      "Epoch  59 Batch  193/269   train_loss = 3.732\n",
      "Epoch  59 Batch  194/269   train_loss = 3.770\n",
      "Epoch  59 Batch  195/269   train_loss = 3.748\n",
      "Epoch  59 Batch  196/269   train_loss = 3.737\n",
      "Epoch  59 Batch  197/269   train_loss = 3.886\n",
      "Epoch  59 Batch  198/269   train_loss = 3.795\n",
      "Epoch  59 Batch  199/269   train_loss = 3.835\n",
      "Epoch  59 Batch  200/269   train_loss = 3.614\n",
      "Epoch  59 Batch  201/269   train_loss = 3.791\n",
      "Epoch  59 Batch  202/269   train_loss = 3.580\n",
      "Epoch  59 Batch  203/269   train_loss = 3.709\n",
      "Epoch  59 Batch  204/269   train_loss = 3.806\n",
      "Epoch  59 Batch  205/269   train_loss = 3.831\n",
      "Epoch  59 Batch  206/269   train_loss = 3.673\n",
      "Epoch  59 Batch  207/269   train_loss = 3.642\n",
      "Epoch  59 Batch  208/269   train_loss = 3.776\n",
      "Epoch  59 Batch  209/269   train_loss = 3.827\n",
      "Epoch  59 Batch  210/269   train_loss = 3.640\n",
      "Epoch  59 Batch  211/269   train_loss = 3.684\n",
      "Epoch  59 Batch  212/269   train_loss = 4.105\n",
      "Epoch  59 Batch  213/269   train_loss = 3.633\n",
      "Epoch  59 Batch  214/269   train_loss = 3.720\n",
      "Epoch  59 Batch  215/269   train_loss = 3.944\n",
      "Epoch  59 Batch  216/269   train_loss = 3.922\n",
      "Epoch  59 Batch  217/269   train_loss = 3.625\n",
      "Epoch  59 Batch  218/269   train_loss = 3.768\n",
      "Epoch  59 Batch  219/269   train_loss = 3.499\n",
      "Epoch  59 Batch  220/269   train_loss = 3.921\n",
      "Epoch  59 Batch  221/269   train_loss = 3.595\n",
      "Epoch  59 Batch  222/269   train_loss = 3.809\n",
      "Epoch  59 Batch  223/269   train_loss = 3.568\n",
      "Epoch  59 Batch  224/269   train_loss = 3.820\n",
      "Epoch  59 Batch  225/269   train_loss = 3.894\n",
      "Epoch  59 Batch  226/269   train_loss = 3.702\n",
      "Epoch  59 Batch  227/269   train_loss = 3.437\n",
      "Epoch  59 Batch  228/269   train_loss = 3.744\n",
      "Epoch  59 Batch  229/269   train_loss = 3.908\n",
      "Epoch  59 Batch  230/269   train_loss = 3.873\n",
      "Epoch  59 Batch  231/269   train_loss = 3.726\n",
      "Epoch  59 Batch  232/269   train_loss = 3.726\n",
      "Epoch  59 Batch  233/269   train_loss = 3.830\n",
      "Epoch  59 Batch  234/269   train_loss = 3.714\n",
      "Epoch  59 Batch  235/269   train_loss = 3.993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  59 Batch  236/269   train_loss = 3.619\n",
      "Epoch  59 Batch  237/269   train_loss = 3.236\n",
      "Epoch  59 Batch  238/269   train_loss = 3.547\n",
      "Epoch  59 Batch  239/269   train_loss = 4.048\n",
      "Epoch  59 Batch  240/269   train_loss = 3.592\n",
      "Epoch  59 Batch  241/269   train_loss = 4.003\n",
      "Epoch  59 Batch  242/269   train_loss = 3.570\n",
      "Epoch  59 Batch  243/269   train_loss = 3.967\n",
      "Epoch  59 Batch  244/269   train_loss = 3.747\n",
      "Epoch  59 Batch  245/269   train_loss = 3.730\n",
      "Epoch  59 Batch  246/269   train_loss = 3.404\n",
      "Epoch  59 Batch  247/269   train_loss = 3.783\n",
      "Epoch  59 Batch  248/269   train_loss = 3.749\n",
      "Epoch  59 Batch  249/269   train_loss = 3.650\n",
      "Epoch  59 Batch  250/269   train_loss = 3.524\n",
      "Epoch  59 Batch  251/269   train_loss = 3.980\n",
      "Epoch  59 Batch  252/269   train_loss = 3.890\n",
      "Epoch  59 Batch  253/269   train_loss = 3.616\n",
      "Epoch  59 Batch  254/269   train_loss = 3.645\n",
      "Epoch  59 Batch  255/269   train_loss = 3.729\n",
      "Epoch  59 Batch  256/269   train_loss = 3.615\n",
      "Epoch  59 Batch  257/269   train_loss = 3.605\n",
      "Epoch  59 Batch  258/269   train_loss = 3.455\n",
      "Epoch  59 Batch  259/269   train_loss = 3.579\n",
      "Epoch  59 Batch  260/269   train_loss = 3.811\n",
      "Epoch  59 Batch  261/269   train_loss = 3.833\n",
      "Epoch  59 Batch  262/269   train_loss = 3.648\n",
      "Epoch  59 Batch  263/269   train_loss = 3.551\n",
      "Epoch  59 Batch  264/269   train_loss = 3.977\n",
      "Epoch  59 Batch  265/269   train_loss = 3.802\n",
      "Epoch  59 Batch  266/269   train_loss = 3.615\n",
      "Epoch  59 Batch  267/269   train_loss = 3.825\n",
      "Epoch  59 Batch  268/269   train_loss = 3.878\n",
      "Epoch  60 Batch    0/269   train_loss = 3.698\n",
      "Epoch  60 Batch    1/269   train_loss = 3.510\n",
      "Epoch  60 Batch    2/269   train_loss = 3.618\n",
      "Epoch  60 Batch    3/269   train_loss = 3.633\n",
      "Epoch  60 Batch    4/269   train_loss = 4.096\n",
      "Epoch  60 Batch    5/269   train_loss = 3.743\n",
      "Epoch  60 Batch    6/269   train_loss = 3.661\n",
      "Epoch  60 Batch    7/269   train_loss = 3.483\n",
      "Epoch  60 Batch    8/269   train_loss = 3.786\n",
      "Epoch  60 Batch    9/269   train_loss = 3.480\n",
      "Epoch  60 Batch   10/269   train_loss = 3.642\n",
      "Epoch  60 Batch   11/269   train_loss = 3.649\n",
      "Epoch  60 Batch   12/269   train_loss = 3.385\n",
      "Epoch  60 Batch   13/269   train_loss = 3.725\n",
      "Epoch  60 Batch   14/269   train_loss = 3.602\n",
      "Epoch  60 Batch   15/269   train_loss = 3.895\n",
      "Epoch  60 Batch   16/269   train_loss = 3.637\n",
      "Epoch  60 Batch   17/269   train_loss = 3.710\n",
      "Epoch  60 Batch   18/269   train_loss = 3.650\n",
      "Epoch  60 Batch   19/269   train_loss = 3.651\n",
      "Epoch  60 Batch   20/269   train_loss = 4.022\n",
      "Epoch  60 Batch   21/269   train_loss = 3.878\n",
      "Epoch  60 Batch   22/269   train_loss = 3.580\n",
      "Epoch  60 Batch   23/269   train_loss = 3.837\n",
      "Epoch  60 Batch   24/269   train_loss = 3.524\n",
      "Epoch  60 Batch   25/269   train_loss = 3.926\n",
      "Epoch  60 Batch   26/269   train_loss = 3.810\n",
      "Epoch  60 Batch   27/269   train_loss = 3.622\n",
      "Epoch  60 Batch   28/269   train_loss = 3.771\n",
      "Epoch  60 Batch   29/269   train_loss = 3.800\n",
      "Epoch  60 Batch   30/269   train_loss = 4.037\n",
      "Epoch  60 Batch   31/269   train_loss = 3.728\n",
      "Epoch  60 Batch   32/269   train_loss = 3.653\n",
      "Epoch  60 Batch   33/269   train_loss = 3.389\n",
      "Epoch  60 Batch   34/269   train_loss = 3.606\n",
      "Epoch  60 Batch   35/269   train_loss = 3.524\n",
      "Epoch  60 Batch   36/269   train_loss = 3.520\n",
      "Epoch  60 Batch   37/269   train_loss = 3.766\n",
      "Epoch  60 Batch   38/269   train_loss = 3.469\n",
      "Epoch  60 Batch   39/269   train_loss = 3.766\n",
      "Epoch  60 Batch   40/269   train_loss = 3.593\n",
      "Epoch  60 Batch   41/269   train_loss = 3.590\n",
      "Epoch  60 Batch   42/269   train_loss = 4.053\n",
      "Epoch  60 Batch   43/269   train_loss = 3.965\n",
      "Epoch  60 Batch   44/269   train_loss = 3.729\n",
      "Epoch  60 Batch   45/269   train_loss = 3.427\n",
      "Epoch  60 Batch   46/269   train_loss = 3.775\n",
      "Epoch  60 Batch   47/269   train_loss = 3.434\n",
      "Epoch  60 Batch   48/269   train_loss = 3.891\n",
      "Epoch  60 Batch   49/269   train_loss = 3.916\n",
      "Epoch  60 Batch   50/269   train_loss = 3.823\n",
      "Epoch  60 Batch   51/269   train_loss = 3.787\n",
      "Epoch  60 Batch   52/269   train_loss = 3.739\n",
      "Epoch  60 Batch   53/269   train_loss = 3.984\n",
      "Epoch  60 Batch   54/269   train_loss = 3.704\n",
      "Epoch  60 Batch   55/269   train_loss = 3.875\n",
      "Epoch  60 Batch   56/269   train_loss = 3.622\n",
      "Epoch  60 Batch   57/269   train_loss = 3.949\n",
      "Epoch  60 Batch   58/269   train_loss = 3.606\n",
      "Epoch  60 Batch   59/269   train_loss = 3.619\n",
      "Epoch  60 Batch   60/269   train_loss = 3.814\n",
      "Epoch  60 Batch   61/269   train_loss = 3.901\n",
      "Epoch  60 Batch   62/269   train_loss = 3.593\n",
      "Epoch  60 Batch   63/269   train_loss = 3.916\n",
      "Epoch  60 Batch   64/269   train_loss = 3.634\n",
      "Epoch  60 Batch   65/269   train_loss = 3.774\n",
      "Epoch  60 Batch   66/269   train_loss = 3.672\n",
      "Epoch  60 Batch   67/269   train_loss = 3.706\n",
      "Epoch  60 Batch   68/269   train_loss = 3.781\n",
      "Epoch  60 Batch   69/269   train_loss = 3.782\n",
      "Epoch  60 Batch   70/269   train_loss = 3.450\n",
      "Epoch  60 Batch   71/269   train_loss = 3.833\n",
      "Epoch  60 Batch   72/269   train_loss = 3.509\n",
      "Epoch  60 Batch   73/269   train_loss = 3.716\n",
      "Epoch  60 Batch   74/269   train_loss = 3.644\n",
      "Epoch  60 Batch   75/269   train_loss = 3.824\n",
      "Epoch  60 Batch   76/269   train_loss = 3.808\n",
      "Epoch  60 Batch   77/269   train_loss = 3.403\n",
      "Epoch  60 Batch   78/269   train_loss = 3.884\n",
      "Epoch  60 Batch   79/269   train_loss = 3.898\n",
      "Epoch  60 Batch   80/269   train_loss = 3.905\n",
      "Epoch  60 Batch   81/269   train_loss = 3.679\n",
      "Epoch  60 Batch   82/269   train_loss = 3.840\n",
      "Epoch  60 Batch   83/269   train_loss = 3.658\n",
      "Epoch  60 Batch   84/269   train_loss = 3.991\n",
      "Epoch  60 Batch   85/269   train_loss = 4.118\n",
      "Epoch  60 Batch   86/269   train_loss = 4.141\n",
      "Epoch  60 Batch   87/269   train_loss = 3.706\n",
      "Epoch  60 Batch   88/269   train_loss = 3.804\n",
      "Epoch  60 Batch   89/269   train_loss = 3.767\n",
      "Epoch  60 Batch   90/269   train_loss = 3.637\n",
      "Epoch  60 Batch   91/269   train_loss = 3.870\n",
      "Epoch  60 Batch   92/269   train_loss = 3.863\n",
      "Epoch  60 Batch   93/269   train_loss = 3.580\n",
      "Epoch  60 Batch   94/269   train_loss = 4.131\n",
      "Epoch  60 Batch   95/269   train_loss = 3.545\n",
      "Epoch  60 Batch   96/269   train_loss = 3.755\n",
      "Epoch  60 Batch   97/269   train_loss = 3.704\n",
      "Epoch  60 Batch   98/269   train_loss = 3.966\n",
      "Epoch  60 Batch   99/269   train_loss = 3.714\n",
      "Epoch  60 Batch  100/269   train_loss = 3.768\n",
      "Epoch  60 Batch  101/269   train_loss = 3.879\n",
      "Epoch  60 Batch  102/269   train_loss = 3.544\n",
      "Epoch  60 Batch  103/269   train_loss = 3.703\n",
      "Epoch  60 Batch  104/269   train_loss = 3.621\n",
      "Epoch  60 Batch  105/269   train_loss = 3.942\n",
      "Epoch  60 Batch  106/269   train_loss = 3.986\n",
      "Epoch  60 Batch  107/269   train_loss = 4.058\n",
      "Epoch  60 Batch  108/269   train_loss = 3.783\n",
      "Epoch  60 Batch  109/269   train_loss = 3.789\n",
      "Epoch  60 Batch  110/269   train_loss = 3.776\n",
      "Epoch  60 Batch  111/269   train_loss = 3.974\n",
      "Epoch  60 Batch  112/269   train_loss = 3.885\n",
      "Epoch  60 Batch  113/269   train_loss = 3.742\n",
      "Epoch  60 Batch  114/269   train_loss = 3.729\n",
      "Epoch  60 Batch  115/269   train_loss = 3.625\n",
      "Epoch  60 Batch  116/269   train_loss = 3.569\n",
      "Epoch  60 Batch  117/269   train_loss = 3.620\n",
      "Epoch  60 Batch  118/269   train_loss = 3.962\n",
      "Epoch  60 Batch  119/269   train_loss = 3.533\n",
      "Epoch  60 Batch  120/269   train_loss = 3.789\n",
      "Epoch  60 Batch  121/269   train_loss = 3.678\n",
      "Epoch  60 Batch  122/269   train_loss = 3.769\n",
      "Epoch  60 Batch  123/269   train_loss = 3.519\n",
      "Epoch  60 Batch  124/269   train_loss = 3.906\n",
      "Epoch  60 Batch  125/269   train_loss = 3.484\n",
      "Epoch  60 Batch  126/269   train_loss = 3.426\n",
      "Epoch  60 Batch  127/269   train_loss = 3.671\n",
      "Epoch  60 Batch  128/269   train_loss = 3.669\n",
      "Epoch  60 Batch  129/269   train_loss = 3.639\n",
      "Epoch  60 Batch  130/269   train_loss = 3.796\n",
      "Epoch  60 Batch  131/269   train_loss = 3.410\n",
      "Epoch  60 Batch  132/269   train_loss = 3.774\n",
      "Epoch  60 Batch  133/269   train_loss = 3.481\n",
      "Epoch  60 Batch  134/269   train_loss = 3.889\n",
      "Epoch  60 Batch  135/269   train_loss = 3.624\n",
      "Epoch  60 Batch  136/269   train_loss = 3.437\n",
      "Epoch  60 Batch  137/269   train_loss = 3.782\n",
      "Epoch  60 Batch  138/269   train_loss = 3.568\n",
      "Epoch  60 Batch  139/269   train_loss = 3.686\n",
      "Epoch  60 Batch  140/269   train_loss = 3.768\n",
      "Epoch  60 Batch  141/269   train_loss = 4.051\n",
      "Epoch  60 Batch  142/269   train_loss = 3.500\n",
      "Epoch  60 Batch  143/269   train_loss = 3.711\n",
      "Epoch  60 Batch  144/269   train_loss = 3.948\n",
      "Epoch  60 Batch  145/269   train_loss = 3.642\n",
      "Epoch  60 Batch  146/269   train_loss = 3.942\n",
      "Epoch  60 Batch  147/269   train_loss = 3.501\n",
      "Epoch  60 Batch  148/269   train_loss = 3.849\n",
      "Epoch  60 Batch  149/269   train_loss = 3.622\n",
      "Epoch  60 Batch  150/269   train_loss = 3.852\n",
      "Epoch  60 Batch  151/269   train_loss = 3.785\n",
      "Epoch  60 Batch  152/269   train_loss = 3.716\n",
      "Epoch  60 Batch  153/269   train_loss = 3.750\n",
      "Epoch  60 Batch  154/269   train_loss = 3.643\n",
      "Epoch  60 Batch  155/269   train_loss = 3.620\n",
      "Epoch  60 Batch  156/269   train_loss = 4.080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60 Batch  157/269   train_loss = 3.754\n",
      "Epoch  60 Batch  158/269   train_loss = 3.766\n",
      "Epoch  60 Batch  159/269   train_loss = 3.774\n",
      "Epoch  60 Batch  160/269   train_loss = 3.672\n",
      "Epoch  60 Batch  161/269   train_loss = 3.872\n",
      "Epoch  60 Batch  162/269   train_loss = 3.805\n",
      "Epoch  60 Batch  163/269   train_loss = 3.603\n",
      "Epoch  60 Batch  164/269   train_loss = 3.942\n",
      "Epoch  60 Batch  165/269   train_loss = 3.898\n",
      "Epoch  60 Batch  166/269   train_loss = 3.806\n",
      "Epoch  60 Batch  167/269   train_loss = 3.610\n",
      "Epoch  60 Batch  168/269   train_loss = 3.783\n",
      "Epoch  60 Batch  169/269   train_loss = 4.023\n",
      "Epoch  60 Batch  170/269   train_loss = 3.472\n",
      "Epoch  60 Batch  171/269   train_loss = 3.798\n",
      "Epoch  60 Batch  172/269   train_loss = 3.592\n",
      "Epoch  60 Batch  173/269   train_loss = 3.753\n",
      "Epoch  60 Batch  174/269   train_loss = 3.949\n",
      "Epoch  60 Batch  175/269   train_loss = 3.827\n",
      "Epoch  60 Batch  176/269   train_loss = 3.734\n",
      "Epoch  60 Batch  177/269   train_loss = 3.753\n",
      "Epoch  60 Batch  178/269   train_loss = 3.936\n",
      "Epoch  60 Batch  179/269   train_loss = 3.725\n",
      "Epoch  60 Batch  180/269   train_loss = 3.573\n",
      "Epoch  60 Batch  181/269   train_loss = 3.841\n",
      "Epoch  60 Batch  182/269   train_loss = 3.679\n",
      "Epoch  60 Batch  183/269   train_loss = 3.626\n",
      "Epoch  60 Batch  184/269   train_loss = 3.778\n",
      "Epoch  60 Batch  185/269   train_loss = 3.865\n",
      "Epoch  60 Batch  186/269   train_loss = 3.694\n",
      "Epoch  60 Batch  187/269   train_loss = 3.893\n",
      "Epoch  60 Batch  188/269   train_loss = 3.696\n",
      "Epoch  60 Batch  189/269   train_loss = 3.621\n",
      "Epoch  60 Batch  190/269   train_loss = 4.103\n",
      "Epoch  60 Batch  191/269   train_loss = 3.787\n",
      "Epoch  60 Batch  192/269   train_loss = 3.744\n",
      "Epoch  60 Batch  193/269   train_loss = 3.720\n",
      "Epoch  60 Batch  194/269   train_loss = 3.789\n",
      "Epoch  60 Batch  195/269   train_loss = 3.765\n",
      "Epoch  60 Batch  196/269   train_loss = 3.750\n",
      "Epoch  60 Batch  197/269   train_loss = 3.880\n",
      "Epoch  60 Batch  198/269   train_loss = 3.801\n",
      "Epoch  60 Batch  199/269   train_loss = 3.849\n",
      "Epoch  60 Batch  200/269   train_loss = 3.606\n",
      "Epoch  60 Batch  201/269   train_loss = 3.750\n",
      "Epoch  60 Batch  202/269   train_loss = 3.522\n",
      "Epoch  60 Batch  203/269   train_loss = 3.692\n",
      "Epoch  60 Batch  204/269   train_loss = 3.824\n",
      "Epoch  60 Batch  205/269   train_loss = 3.799\n",
      "Epoch  60 Batch  206/269   train_loss = 3.717\n",
      "Epoch  60 Batch  207/269   train_loss = 3.650\n",
      "Epoch  60 Batch  208/269   train_loss = 3.745\n",
      "Epoch  60 Batch  209/269   train_loss = 3.813\n",
      "Epoch  60 Batch  210/269   train_loss = 3.647\n",
      "Epoch  60 Batch  211/269   train_loss = 3.646\n",
      "Epoch  60 Batch  212/269   train_loss = 4.051\n",
      "Epoch  60 Batch  213/269   train_loss = 3.598\n",
      "Epoch  60 Batch  214/269   train_loss = 3.747\n",
      "Epoch  60 Batch  215/269   train_loss = 3.964\n",
      "Epoch  60 Batch  216/269   train_loss = 3.904\n",
      "Epoch  60 Batch  217/269   train_loss = 3.636\n",
      "Epoch  60 Batch  218/269   train_loss = 3.802\n",
      "Epoch  60 Batch  219/269   train_loss = 3.486\n",
      "Epoch  60 Batch  220/269   train_loss = 3.925\n",
      "Epoch  60 Batch  221/269   train_loss = 3.591\n",
      "Epoch  60 Batch  222/269   train_loss = 3.827\n",
      "Epoch  60 Batch  223/269   train_loss = 3.547\n",
      "Epoch  60 Batch  224/269   train_loss = 3.808\n",
      "Epoch  60 Batch  225/269   train_loss = 3.919\n",
      "Epoch  60 Batch  226/269   train_loss = 3.716\n",
      "Epoch  60 Batch  227/269   train_loss = 3.441\n",
      "Epoch  60 Batch  228/269   train_loss = 3.743\n",
      "Epoch  60 Batch  229/269   train_loss = 3.925\n",
      "Epoch  60 Batch  230/269   train_loss = 3.863\n",
      "Epoch  60 Batch  231/269   train_loss = 3.770\n",
      "Epoch  60 Batch  232/269   train_loss = 3.723\n",
      "Epoch  60 Batch  233/269   train_loss = 3.836\n",
      "Epoch  60 Batch  234/269   train_loss = 3.705\n",
      "Epoch  60 Batch  235/269   train_loss = 3.968\n",
      "Epoch  60 Batch  236/269   train_loss = 3.575\n",
      "Epoch  60 Batch  237/269   train_loss = 3.263\n",
      "Epoch  60 Batch  238/269   train_loss = 3.549\n",
      "Epoch  60 Batch  239/269   train_loss = 4.011\n",
      "Epoch  60 Batch  240/269   train_loss = 3.596\n",
      "Epoch  60 Batch  241/269   train_loss = 3.968\n",
      "Epoch  60 Batch  242/269   train_loss = 3.535\n",
      "Epoch  60 Batch  243/269   train_loss = 3.929\n",
      "Epoch  60 Batch  244/269   train_loss = 3.738\n",
      "Epoch  60 Batch  245/269   train_loss = 3.716\n",
      "Epoch  60 Batch  246/269   train_loss = 3.394\n",
      "Epoch  60 Batch  247/269   train_loss = 3.789\n",
      "Epoch  60 Batch  248/269   train_loss = 3.753\n",
      "Epoch  60 Batch  249/269   train_loss = 3.643\n",
      "Epoch  60 Batch  250/269   train_loss = 3.535\n",
      "Epoch  60 Batch  251/269   train_loss = 3.965\n",
      "Epoch  60 Batch  252/269   train_loss = 3.886\n",
      "Epoch  60 Batch  253/269   train_loss = 3.622\n",
      "Epoch  60 Batch  254/269   train_loss = 3.617\n",
      "Epoch  60 Batch  255/269   train_loss = 3.708\n",
      "Epoch  60 Batch  256/269   train_loss = 3.613\n",
      "Epoch  60 Batch  257/269   train_loss = 3.571\n",
      "Epoch  60 Batch  258/269   train_loss = 3.462\n",
      "Epoch  60 Batch  259/269   train_loss = 3.619\n",
      "Epoch  60 Batch  260/269   train_loss = 3.849\n",
      "Epoch  60 Batch  261/269   train_loss = 3.818\n",
      "Epoch  60 Batch  262/269   train_loss = 3.664\n",
      "Epoch  60 Batch  263/269   train_loss = 3.498\n",
      "Epoch  60 Batch  264/269   train_loss = 3.942\n",
      "Epoch  60 Batch  265/269   train_loss = 3.827\n",
      "Epoch  60 Batch  266/269   train_loss = 3.596\n",
      "Epoch  60 Batch  267/269   train_loss = 3.831\n",
      "Epoch  60 Batch  268/269   train_loss = 3.859\n",
      "Epoch  61 Batch    0/269   train_loss = 3.691\n",
      "Epoch  61 Batch    1/269   train_loss = 3.494\n",
      "Epoch  61 Batch    2/269   train_loss = 3.590\n",
      "Epoch  61 Batch    3/269   train_loss = 3.645\n",
      "Epoch  61 Batch    4/269   train_loss = 4.096\n",
      "Epoch  61 Batch    5/269   train_loss = 3.748\n",
      "Epoch  61 Batch    6/269   train_loss = 3.621\n",
      "Epoch  61 Batch    7/269   train_loss = 3.463\n",
      "Epoch  61 Batch    8/269   train_loss = 3.806\n",
      "Epoch  61 Batch    9/269   train_loss = 3.486\n",
      "Epoch  61 Batch   10/269   train_loss = 3.584\n",
      "Epoch  61 Batch   11/269   train_loss = 3.665\n",
      "Epoch  61 Batch   12/269   train_loss = 3.399\n",
      "Epoch  61 Batch   13/269   train_loss = 3.738\n",
      "Epoch  61 Batch   14/269   train_loss = 3.621\n",
      "Epoch  61 Batch   15/269   train_loss = 3.901\n",
      "Epoch  61 Batch   16/269   train_loss = 3.635\n",
      "Epoch  61 Batch   17/269   train_loss = 3.697\n",
      "Epoch  61 Batch   18/269   train_loss = 3.663\n",
      "Epoch  61 Batch   19/269   train_loss = 3.649\n",
      "Epoch  61 Batch   20/269   train_loss = 4.019\n",
      "Epoch  61 Batch   21/269   train_loss = 3.870\n",
      "Epoch  61 Batch   22/269   train_loss = 3.574\n",
      "Epoch  61 Batch   23/269   train_loss = 3.799\n",
      "Epoch  61 Batch   24/269   train_loss = 3.526\n",
      "Epoch  61 Batch   25/269   train_loss = 3.900\n",
      "Epoch  61 Batch   26/269   train_loss = 3.782\n",
      "Epoch  61 Batch   27/269   train_loss = 3.623\n",
      "Epoch  61 Batch   28/269   train_loss = 3.799\n",
      "Epoch  61 Batch   29/269   train_loss = 3.789\n",
      "Epoch  61 Batch   30/269   train_loss = 4.110\n",
      "Epoch  61 Batch   31/269   train_loss = 3.749\n",
      "Epoch  61 Batch   32/269   train_loss = 3.619\n",
      "Epoch  61 Batch   33/269   train_loss = 3.427\n",
      "Epoch  61 Batch   34/269   train_loss = 3.612\n",
      "Epoch  61 Batch   35/269   train_loss = 3.525\n",
      "Epoch  61 Batch   36/269   train_loss = 3.530\n",
      "Epoch  61 Batch   37/269   train_loss = 3.751\n",
      "Epoch  61 Batch   38/269   train_loss = 3.456\n",
      "Epoch  61 Batch   39/269   train_loss = 3.784\n",
      "Epoch  61 Batch   40/269   train_loss = 3.590\n",
      "Epoch  61 Batch   41/269   train_loss = 3.589\n",
      "Epoch  61 Batch   42/269   train_loss = 4.062\n",
      "Epoch  61 Batch   43/269   train_loss = 3.949\n",
      "Epoch  61 Batch   44/269   train_loss = 3.751\n",
      "Epoch  61 Batch   45/269   train_loss = 3.397\n",
      "Epoch  61 Batch   46/269   train_loss = 3.776\n",
      "Epoch  61 Batch   47/269   train_loss = 3.447\n",
      "Epoch  61 Batch   48/269   train_loss = 3.893\n",
      "Epoch  61 Batch   49/269   train_loss = 3.876\n",
      "Epoch  61 Batch   50/269   train_loss = 3.838\n",
      "Epoch  61 Batch   51/269   train_loss = 3.728\n",
      "Epoch  61 Batch   52/269   train_loss = 3.765\n",
      "Epoch  61 Batch   53/269   train_loss = 3.967\n",
      "Epoch  61 Batch   54/269   train_loss = 3.692\n",
      "Epoch  61 Batch   55/269   train_loss = 3.890\n",
      "Epoch  61 Batch   56/269   train_loss = 3.633\n",
      "Epoch  61 Batch   57/269   train_loss = 3.952\n",
      "Epoch  61 Batch   58/269   train_loss = 3.575\n",
      "Epoch  61 Batch   59/269   train_loss = 3.636\n",
      "Epoch  61 Batch   60/269   train_loss = 3.791\n",
      "Epoch  61 Batch   61/269   train_loss = 3.925\n",
      "Epoch  61 Batch   62/269   train_loss = 3.544\n",
      "Epoch  61 Batch   63/269   train_loss = 3.890\n",
      "Epoch  61 Batch   64/269   train_loss = 3.617\n",
      "Epoch  61 Batch   65/269   train_loss = 3.737\n",
      "Epoch  61 Batch   66/269   train_loss = 3.663\n",
      "Epoch  61 Batch   67/269   train_loss = 3.679\n",
      "Epoch  61 Batch   68/269   train_loss = 3.775\n",
      "Epoch  61 Batch   69/269   train_loss = 3.763\n",
      "Epoch  61 Batch   70/269   train_loss = 3.432\n",
      "Epoch  61 Batch   71/269   train_loss = 3.818\n",
      "Epoch  61 Batch   72/269   train_loss = 3.468\n",
      "Epoch  61 Batch   73/269   train_loss = 3.685\n",
      "Epoch  61 Batch   74/269   train_loss = 3.644\n",
      "Epoch  61 Batch   75/269   train_loss = 3.847\n",
      "Epoch  61 Batch   76/269   train_loss = 3.826\n",
      "Epoch  61 Batch   77/269   train_loss = 3.390\n",
      "Epoch  61 Batch   78/269   train_loss = 3.856\n",
      "Epoch  61 Batch   79/269   train_loss = 3.898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  61 Batch   80/269   train_loss = 3.824\n",
      "Epoch  61 Batch   81/269   train_loss = 3.638\n",
      "Epoch  61 Batch   82/269   train_loss = 3.832\n",
      "Epoch  61 Batch   83/269   train_loss = 3.626\n",
      "Epoch  61 Batch   84/269   train_loss = 3.941\n",
      "Epoch  61 Batch   85/269   train_loss = 4.101\n",
      "Epoch  61 Batch   86/269   train_loss = 4.084\n",
      "Epoch  61 Batch   87/269   train_loss = 3.671\n",
      "Epoch  61 Batch   88/269   train_loss = 3.790\n",
      "Epoch  61 Batch   89/269   train_loss = 3.757\n",
      "Epoch  61 Batch   90/269   train_loss = 3.652\n",
      "Epoch  61 Batch   91/269   train_loss = 3.842\n",
      "Epoch  61 Batch   92/269   train_loss = 3.887\n",
      "Epoch  61 Batch   93/269   train_loss = 3.578\n",
      "Epoch  61 Batch   94/269   train_loss = 4.092\n",
      "Epoch  61 Batch   95/269   train_loss = 3.538\n",
      "Epoch  61 Batch   96/269   train_loss = 3.653\n",
      "Epoch  61 Batch   97/269   train_loss = 3.711\n",
      "Epoch  61 Batch   98/269   train_loss = 3.980\n",
      "Epoch  61 Batch   99/269   train_loss = 3.747\n",
      "Epoch  61 Batch  100/269   train_loss = 3.765\n",
      "Epoch  61 Batch  101/269   train_loss = 3.856\n",
      "Epoch  61 Batch  102/269   train_loss = 3.519\n",
      "Epoch  61 Batch  103/269   train_loss = 3.647\n",
      "Epoch  61 Batch  104/269   train_loss = 3.588\n",
      "Epoch  61 Batch  105/269   train_loss = 3.920\n",
      "Epoch  61 Batch  106/269   train_loss = 3.969\n",
      "Epoch  61 Batch  107/269   train_loss = 3.996\n",
      "Epoch  61 Batch  108/269   train_loss = 3.738\n",
      "Epoch  61 Batch  109/269   train_loss = 3.735\n",
      "Epoch  61 Batch  110/269   train_loss = 3.750\n",
      "Epoch  61 Batch  111/269   train_loss = 3.872\n",
      "Epoch  61 Batch  112/269   train_loss = 3.884\n",
      "Epoch  61 Batch  113/269   train_loss = 3.731\n",
      "Epoch  61 Batch  114/269   train_loss = 3.708\n",
      "Epoch  61 Batch  115/269   train_loss = 3.606\n",
      "Epoch  61 Batch  116/269   train_loss = 3.562\n",
      "Epoch  61 Batch  117/269   train_loss = 3.635\n",
      "Epoch  61 Batch  118/269   train_loss = 3.927\n",
      "Epoch  61 Batch  119/269   train_loss = 3.488\n",
      "Epoch  61 Batch  120/269   train_loss = 3.746\n",
      "Epoch  61 Batch  121/269   train_loss = 3.680\n",
      "Epoch  61 Batch  122/269   train_loss = 3.714\n",
      "Epoch  61 Batch  123/269   train_loss = 3.531\n",
      "Epoch  61 Batch  124/269   train_loss = 3.920\n",
      "Epoch  61 Batch  125/269   train_loss = 3.509\n",
      "Epoch  61 Batch  126/269   train_loss = 3.419\n",
      "Epoch  61 Batch  127/269   train_loss = 3.675\n",
      "Epoch  61 Batch  128/269   train_loss = 3.625\n",
      "Epoch  61 Batch  129/269   train_loss = 3.667\n",
      "Epoch  61 Batch  130/269   train_loss = 3.761\n",
      "Epoch  61 Batch  131/269   train_loss = 3.434\n",
      "Epoch  61 Batch  132/269   train_loss = 3.743\n",
      "Epoch  61 Batch  133/269   train_loss = 3.499\n",
      "Epoch  61 Batch  134/269   train_loss = 3.860\n",
      "Epoch  61 Batch  135/269   train_loss = 3.590\n",
      "Epoch  61 Batch  136/269   train_loss = 3.408\n",
      "Epoch  61 Batch  137/269   train_loss = 3.804\n",
      "Epoch  61 Batch  138/269   train_loss = 3.524\n",
      "Epoch  61 Batch  139/269   train_loss = 3.663\n",
      "Epoch  61 Batch  140/269   train_loss = 3.775\n",
      "Epoch  61 Batch  141/269   train_loss = 4.057\n",
      "Epoch  61 Batch  142/269   train_loss = 3.505\n",
      "Epoch  61 Batch  143/269   train_loss = 3.681\n",
      "Epoch  61 Batch  144/269   train_loss = 3.921\n",
      "Epoch  61 Batch  145/269   train_loss = 3.644\n",
      "Epoch  61 Batch  146/269   train_loss = 3.929\n",
      "Epoch  61 Batch  147/269   train_loss = 3.496\n",
      "Epoch  61 Batch  148/269   train_loss = 3.845\n",
      "Epoch  61 Batch  149/269   train_loss = 3.617\n",
      "Epoch  61 Batch  150/269   train_loss = 3.838\n",
      "Epoch  61 Batch  151/269   train_loss = 3.776\n",
      "Epoch  61 Batch  152/269   train_loss = 3.748\n",
      "Epoch  61 Batch  153/269   train_loss = 3.720\n",
      "Epoch  61 Batch  154/269   train_loss = 3.604\n",
      "Epoch  61 Batch  155/269   train_loss = 3.635\n",
      "Epoch  61 Batch  156/269   train_loss = 4.068\n",
      "Epoch  61 Batch  157/269   train_loss = 3.794\n",
      "Epoch  61 Batch  158/269   train_loss = 3.777\n",
      "Epoch  61 Batch  159/269   train_loss = 3.822\n",
      "Epoch  61 Batch  160/269   train_loss = 3.665\n",
      "Epoch  61 Batch  161/269   train_loss = 3.874\n",
      "Epoch  61 Batch  162/269   train_loss = 3.832\n",
      "Epoch  61 Batch  163/269   train_loss = 3.616\n",
      "Epoch  61 Batch  164/269   train_loss = 3.933\n",
      "Epoch  61 Batch  165/269   train_loss = 3.918\n",
      "Epoch  61 Batch  166/269   train_loss = 3.830\n",
      "Epoch  61 Batch  167/269   train_loss = 3.608\n",
      "Epoch  61 Batch  168/269   train_loss = 3.753\n",
      "Epoch  61 Batch  169/269   train_loss = 4.003\n",
      "Epoch  61 Batch  170/269   train_loss = 3.487\n",
      "Epoch  61 Batch  171/269   train_loss = 3.795\n",
      "Epoch  61 Batch  172/269   train_loss = 3.548\n",
      "Epoch  61 Batch  173/269   train_loss = 3.711\n",
      "Epoch  61 Batch  174/269   train_loss = 3.898\n",
      "Epoch  61 Batch  175/269   train_loss = 3.786\n",
      "Epoch  61 Batch  176/269   train_loss = 3.740\n",
      "Epoch  61 Batch  177/269   train_loss = 3.722\n",
      "Epoch  61 Batch  178/269   train_loss = 3.942\n",
      "Epoch  61 Batch  179/269   train_loss = 3.726\n",
      "Epoch  61 Batch  180/269   train_loss = 3.567\n",
      "Epoch  61 Batch  181/269   train_loss = 3.810\n",
      "Epoch  61 Batch  182/269   train_loss = 3.668\n",
      "Epoch  61 Batch  183/269   train_loss = 3.636\n",
      "Epoch  61 Batch  184/269   train_loss = 3.747\n",
      "Epoch  61 Batch  185/269   train_loss = 3.845\n",
      "Epoch  61 Batch  186/269   train_loss = 3.694\n",
      "Epoch  61 Batch  187/269   train_loss = 3.948\n",
      "Epoch  61 Batch  188/269   train_loss = 3.653\n",
      "Epoch  61 Batch  189/269   train_loss = 3.635\n",
      "Epoch  61 Batch  190/269   train_loss = 4.057\n",
      "Epoch  61 Batch  191/269   train_loss = 3.752\n",
      "Epoch  61 Batch  192/269   train_loss = 3.760\n",
      "Epoch  61 Batch  193/269   train_loss = 3.708\n",
      "Epoch  61 Batch  194/269   train_loss = 3.785\n",
      "Epoch  61 Batch  195/269   train_loss = 3.739\n",
      "Epoch  61 Batch  196/269   train_loss = 3.733\n",
      "Epoch  61 Batch  197/269   train_loss = 3.936\n",
      "Epoch  61 Batch  198/269   train_loss = 3.782\n",
      "Epoch  61 Batch  199/269   train_loss = 3.845\n",
      "Epoch  61 Batch  200/269   train_loss = 3.635\n",
      "Epoch  61 Batch  201/269   train_loss = 3.745\n",
      "Epoch  61 Batch  202/269   train_loss = 3.515\n",
      "Epoch  61 Batch  203/269   train_loss = 3.682\n",
      "Epoch  61 Batch  204/269   train_loss = 3.797\n",
      "Epoch  61 Batch  205/269   train_loss = 3.795\n",
      "Epoch  61 Batch  206/269   train_loss = 3.665\n",
      "Epoch  61 Batch  207/269   train_loss = 3.605\n",
      "Epoch  61 Batch  208/269   train_loss = 3.754\n",
      "Epoch  61 Batch  209/269   train_loss = 3.804\n",
      "Epoch  61 Batch  210/269   train_loss = 3.628\n",
      "Epoch  61 Batch  211/269   train_loss = 3.629\n",
      "Epoch  61 Batch  212/269   train_loss = 4.047\n",
      "Epoch  61 Batch  213/269   train_loss = 3.614\n",
      "Epoch  61 Batch  214/269   train_loss = 3.697\n",
      "Epoch  61 Batch  215/269   train_loss = 3.946\n",
      "Epoch  61 Batch  216/269   train_loss = 3.880\n",
      "Epoch  61 Batch  217/269   train_loss = 3.587\n",
      "Epoch  61 Batch  218/269   train_loss = 3.785\n",
      "Epoch  61 Batch  219/269   train_loss = 3.466\n",
      "Epoch  61 Batch  220/269   train_loss = 3.900\n",
      "Epoch  61 Batch  221/269   train_loss = 3.595\n",
      "Epoch  61 Batch  222/269   train_loss = 3.821\n",
      "Epoch  61 Batch  223/269   train_loss = 3.518\n",
      "Epoch  61 Batch  224/269   train_loss = 3.791\n",
      "Epoch  61 Batch  225/269   train_loss = 3.885\n",
      "Epoch  61 Batch  226/269   train_loss = 3.698\n",
      "Epoch  61 Batch  227/269   train_loss = 3.447\n",
      "Epoch  61 Batch  228/269   train_loss = 3.727\n",
      "Epoch  61 Batch  229/269   train_loss = 3.908\n",
      "Epoch  61 Batch  230/269   train_loss = 3.842\n",
      "Epoch  61 Batch  231/269   train_loss = 3.729\n",
      "Epoch  61 Batch  232/269   train_loss = 3.698\n",
      "Epoch  61 Batch  233/269   train_loss = 3.794\n",
      "Epoch  61 Batch  234/269   train_loss = 3.731\n",
      "Epoch  61 Batch  235/269   train_loss = 3.993\n",
      "Epoch  61 Batch  236/269   train_loss = 3.604\n",
      "Epoch  61 Batch  237/269   train_loss = 3.260\n",
      "Epoch  61 Batch  238/269   train_loss = 3.511\n",
      "Epoch  61 Batch  239/269   train_loss = 4.003\n",
      "Epoch  61 Batch  240/269   train_loss = 3.640\n",
      "Epoch  61 Batch  241/269   train_loss = 3.971\n",
      "Epoch  61 Batch  242/269   train_loss = 3.519\n",
      "Epoch  61 Batch  243/269   train_loss = 3.915\n",
      "Epoch  61 Batch  244/269   train_loss = 3.714\n",
      "Epoch  61 Batch  245/269   train_loss = 3.696\n",
      "Epoch  61 Batch  246/269   train_loss = 3.397\n",
      "Epoch  61 Batch  247/269   train_loss = 3.780\n",
      "Epoch  61 Batch  248/269   train_loss = 3.754\n",
      "Epoch  61 Batch  249/269   train_loss = 3.657\n",
      "Epoch  61 Batch  250/269   train_loss = 3.510\n",
      "Epoch  61 Batch  251/269   train_loss = 3.950\n",
      "Epoch  61 Batch  252/269   train_loss = 3.890\n",
      "Epoch  61 Batch  253/269   train_loss = 3.615\n",
      "Epoch  61 Batch  254/269   train_loss = 3.608\n",
      "Epoch  61 Batch  255/269   train_loss = 3.717\n",
      "Epoch  61 Batch  256/269   train_loss = 3.616\n",
      "Epoch  61 Batch  257/269   train_loss = 3.563\n",
      "Epoch  61 Batch  258/269   train_loss = 3.434\n",
      "Epoch  61 Batch  259/269   train_loss = 3.588\n",
      "Epoch  61 Batch  260/269   train_loss = 3.863\n",
      "Epoch  61 Batch  261/269   train_loss = 3.817\n",
      "Epoch  61 Batch  262/269   train_loss = 3.666\n",
      "Epoch  61 Batch  263/269   train_loss = 3.512\n",
      "Epoch  61 Batch  264/269   train_loss = 3.966\n",
      "Epoch  61 Batch  265/269   train_loss = 3.831\n",
      "Epoch  61 Batch  266/269   train_loss = 3.607\n",
      "Epoch  61 Batch  267/269   train_loss = 3.720\n",
      "Epoch  61 Batch  268/269   train_loss = 3.824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  62 Batch    0/269   train_loss = 3.660\n",
      "Epoch  62 Batch    1/269   train_loss = 3.474\n",
      "Epoch  62 Batch    2/269   train_loss = 3.598\n",
      "Epoch  62 Batch    3/269   train_loss = 3.662\n",
      "Epoch  62 Batch    4/269   train_loss = 4.098\n",
      "Epoch  62 Batch    5/269   train_loss = 3.693\n",
      "Epoch  62 Batch    6/269   train_loss = 3.621\n",
      "Epoch  62 Batch    7/269   train_loss = 3.467\n",
      "Epoch  62 Batch    8/269   train_loss = 3.780\n",
      "Epoch  62 Batch    9/269   train_loss = 3.470\n",
      "Epoch  62 Batch   10/269   train_loss = 3.604\n",
      "Epoch  62 Batch   11/269   train_loss = 3.631\n",
      "Epoch  62 Batch   12/269   train_loss = 3.382\n",
      "Epoch  62 Batch   13/269   train_loss = 3.719\n",
      "Epoch  62 Batch   14/269   train_loss = 3.596\n",
      "Epoch  62 Batch   15/269   train_loss = 3.903\n",
      "Epoch  62 Batch   16/269   train_loss = 3.641\n",
      "Epoch  62 Batch   17/269   train_loss = 3.695\n",
      "Epoch  62 Batch   18/269   train_loss = 3.662\n",
      "Epoch  62 Batch   19/269   train_loss = 3.652\n",
      "Epoch  62 Batch   20/269   train_loss = 4.005\n",
      "Epoch  62 Batch   21/269   train_loss = 3.882\n",
      "Epoch  62 Batch   22/269   train_loss = 3.569\n",
      "Epoch  62 Batch   23/269   train_loss = 3.802\n",
      "Epoch  62 Batch   24/269   train_loss = 3.514\n",
      "Epoch  62 Batch   25/269   train_loss = 3.865\n",
      "Epoch  62 Batch   26/269   train_loss = 3.767\n",
      "Epoch  62 Batch   27/269   train_loss = 3.603\n",
      "Epoch  62 Batch   28/269   train_loss = 3.770\n",
      "Epoch  62 Batch   29/269   train_loss = 3.752\n",
      "Epoch  62 Batch   30/269   train_loss = 4.057\n",
      "Epoch  62 Batch   31/269   train_loss = 3.725\n",
      "Epoch  62 Batch   32/269   train_loss = 3.648\n",
      "Epoch  62 Batch   33/269   train_loss = 3.407\n",
      "Epoch  62 Batch   34/269   train_loss = 3.600\n",
      "Epoch  62 Batch   35/269   train_loss = 3.496\n",
      "Epoch  62 Batch   36/269   train_loss = 3.523\n",
      "Epoch  62 Batch   37/269   train_loss = 3.752\n",
      "Epoch  62 Batch   38/269   train_loss = 3.440\n",
      "Epoch  62 Batch   39/269   train_loss = 3.801\n",
      "Epoch  62 Batch   40/269   train_loss = 3.567\n",
      "Epoch  62 Batch   41/269   train_loss = 3.560\n",
      "Epoch  62 Batch   42/269   train_loss = 4.036\n",
      "Epoch  62 Batch   43/269   train_loss = 3.969\n",
      "Epoch  62 Batch   44/269   train_loss = 3.758\n",
      "Epoch  62 Batch   45/269   train_loss = 3.412\n",
      "Epoch  62 Batch   46/269   train_loss = 3.751\n",
      "Epoch  62 Batch   47/269   train_loss = 3.456\n",
      "Epoch  62 Batch   48/269   train_loss = 3.869\n",
      "Epoch  62 Batch   49/269   train_loss = 3.898\n",
      "Epoch  62 Batch   50/269   train_loss = 3.811\n",
      "Epoch  62 Batch   51/269   train_loss = 3.719\n",
      "Epoch  62 Batch   52/269   train_loss = 3.712\n",
      "Epoch  62 Batch   53/269   train_loss = 3.959\n",
      "Epoch  62 Batch   54/269   train_loss = 3.702\n",
      "Epoch  62 Batch   55/269   train_loss = 3.844\n",
      "Epoch  62 Batch   56/269   train_loss = 3.615\n",
      "Epoch  62 Batch   57/269   train_loss = 3.918\n",
      "Epoch  62 Batch   58/269   train_loss = 3.530\n",
      "Epoch  62 Batch   59/269   train_loss = 3.651\n",
      "Epoch  62 Batch   60/269   train_loss = 3.755\n",
      "Epoch  62 Batch   61/269   train_loss = 3.942\n",
      "Epoch  62 Batch   62/269   train_loss = 3.562\n",
      "Epoch  62 Batch   63/269   train_loss = 3.886\n",
      "Epoch  62 Batch   64/269   train_loss = 3.615\n",
      "Epoch  62 Batch   65/269   train_loss = 3.743\n",
      "Epoch  62 Batch   66/269   train_loss = 3.634\n",
      "Epoch  62 Batch   67/269   train_loss = 3.649\n",
      "Epoch  62 Batch   68/269   train_loss = 3.750\n",
      "Epoch  62 Batch   69/269   train_loss = 3.737\n",
      "Epoch  62 Batch   70/269   train_loss = 3.432\n",
      "Epoch  62 Batch   71/269   train_loss = 3.781\n",
      "Epoch  62 Batch   72/269   train_loss = 3.481\n",
      "Epoch  62 Batch   73/269   train_loss = 3.677\n",
      "Epoch  62 Batch   74/269   train_loss = 3.644\n",
      "Epoch  62 Batch   75/269   train_loss = 3.841\n",
      "Epoch  62 Batch   76/269   train_loss = 3.820\n",
      "Epoch  62 Batch   77/269   train_loss = 3.412\n",
      "Epoch  62 Batch   78/269   train_loss = 3.852\n",
      "Epoch  62 Batch   79/269   train_loss = 3.895\n",
      "Epoch  62 Batch   80/269   train_loss = 3.802\n",
      "Epoch  62 Batch   81/269   train_loss = 3.633\n",
      "Epoch  62 Batch   82/269   train_loss = 3.832\n",
      "Epoch  62 Batch   83/269   train_loss = 3.607\n",
      "Epoch  62 Batch   84/269   train_loss = 3.913\n",
      "Epoch  62 Batch   85/269   train_loss = 4.090\n",
      "Epoch  62 Batch   86/269   train_loss = 4.061\n",
      "Epoch  62 Batch   87/269   train_loss = 3.653\n",
      "Epoch  62 Batch   88/269   train_loss = 3.762\n",
      "Epoch  62 Batch   89/269   train_loss = 3.772\n",
      "Epoch  62 Batch   90/269   train_loss = 3.628\n",
      "Epoch  62 Batch   91/269   train_loss = 3.854\n",
      "Epoch  62 Batch   92/269   train_loss = 3.858\n",
      "Epoch  62 Batch   93/269   train_loss = 3.579\n",
      "Epoch  62 Batch   94/269   train_loss = 4.060\n",
      "Epoch  62 Batch   95/269   train_loss = 3.546\n",
      "Epoch  62 Batch   96/269   train_loss = 3.614\n",
      "Epoch  62 Batch   97/269   train_loss = 3.701\n",
      "Epoch  62 Batch   98/269   train_loss = 3.954\n",
      "Epoch  62 Batch   99/269   train_loss = 3.763\n",
      "Epoch  62 Batch  100/269   train_loss = 3.740\n",
      "Epoch  62 Batch  101/269   train_loss = 3.847\n",
      "Epoch  62 Batch  102/269   train_loss = 3.522\n",
      "Epoch  62 Batch  103/269   train_loss = 3.628\n",
      "Epoch  62 Batch  104/269   train_loss = 3.583\n",
      "Epoch  62 Batch  105/269   train_loss = 3.891\n",
      "Epoch  62 Batch  106/269   train_loss = 3.952\n",
      "Epoch  62 Batch  107/269   train_loss = 3.977\n",
      "Epoch  62 Batch  108/269   train_loss = 3.715\n",
      "Epoch  62 Batch  109/269   train_loss = 3.744\n",
      "Epoch  62 Batch  110/269   train_loss = 3.753\n",
      "Epoch  62 Batch  111/269   train_loss = 3.880\n",
      "Epoch  62 Batch  112/269   train_loss = 3.885\n",
      "Epoch  62 Batch  113/269   train_loss = 3.712\n",
      "Epoch  62 Batch  114/269   train_loss = 3.694\n",
      "Epoch  62 Batch  115/269   train_loss = 3.602\n",
      "Epoch  62 Batch  116/269   train_loss = 3.556\n",
      "Epoch  62 Batch  117/269   train_loss = 3.594\n",
      "Epoch  62 Batch  118/269   train_loss = 3.920\n",
      "Epoch  62 Batch  119/269   train_loss = 3.472\n",
      "Epoch  62 Batch  120/269   train_loss = 3.747\n",
      "Epoch  62 Batch  121/269   train_loss = 3.665\n",
      "Epoch  62 Batch  122/269   train_loss = 3.724\n",
      "Epoch  62 Batch  123/269   train_loss = 3.536\n",
      "Epoch  62 Batch  124/269   train_loss = 3.913\n",
      "Epoch  62 Batch  125/269   train_loss = 3.519\n",
      "Epoch  62 Batch  126/269   train_loss = 3.418\n",
      "Epoch  62 Batch  127/269   train_loss = 3.692\n",
      "Epoch  62 Batch  128/269   train_loss = 3.658\n",
      "Epoch  62 Batch  129/269   train_loss = 3.666\n",
      "Epoch  62 Batch  130/269   train_loss = 3.743\n",
      "Epoch  62 Batch  131/269   train_loss = 3.409\n",
      "Epoch  62 Batch  132/269   train_loss = 3.768\n",
      "Epoch  62 Batch  133/269   train_loss = 3.501\n",
      "Epoch  62 Batch  134/269   train_loss = 3.854\n",
      "Epoch  62 Batch  135/269   train_loss = 3.590\n",
      "Epoch  62 Batch  136/269   train_loss = 3.420\n",
      "Epoch  62 Batch  137/269   train_loss = 3.804\n",
      "Epoch  62 Batch  138/269   train_loss = 3.539\n",
      "Epoch  62 Batch  139/269   train_loss = 3.672\n",
      "Epoch  62 Batch  140/269   train_loss = 3.760\n",
      "Epoch  62 Batch  141/269   train_loss = 4.079\n",
      "Epoch  62 Batch  142/269   train_loss = 3.497\n",
      "Epoch  62 Batch  143/269   train_loss = 3.690\n",
      "Epoch  62 Batch  144/269   train_loss = 3.935\n",
      "Epoch  62 Batch  145/269   train_loss = 3.631\n",
      "Epoch  62 Batch  146/269   train_loss = 3.894\n",
      "Epoch  62 Batch  147/269   train_loss = 3.482\n",
      "Epoch  62 Batch  148/269   train_loss = 3.843\n",
      "Epoch  62 Batch  149/269   train_loss = 3.592\n",
      "Epoch  62 Batch  150/269   train_loss = 3.810\n",
      "Epoch  62 Batch  151/269   train_loss = 3.762\n",
      "Epoch  62 Batch  152/269   train_loss = 3.712\n",
      "Epoch  62 Batch  153/269   train_loss = 3.730\n",
      "Epoch  62 Batch  154/269   train_loss = 3.586\n",
      "Epoch  62 Batch  155/269   train_loss = 3.601\n",
      "Epoch  62 Batch  156/269   train_loss = 4.063\n",
      "Epoch  62 Batch  157/269   train_loss = 3.765\n",
      "Epoch  62 Batch  158/269   train_loss = 3.775\n",
      "Epoch  62 Batch  159/269   train_loss = 3.732\n",
      "Epoch  62 Batch  160/269   train_loss = 3.656\n",
      "Epoch  62 Batch  161/269   train_loss = 3.823\n",
      "Epoch  62 Batch  162/269   train_loss = 3.825\n",
      "Epoch  62 Batch  163/269   train_loss = 3.605\n",
      "Epoch  62 Batch  164/269   train_loss = 3.925\n",
      "Epoch  62 Batch  165/269   train_loss = 3.924\n",
      "Epoch  62 Batch  166/269   train_loss = 3.789\n",
      "Epoch  62 Batch  167/269   train_loss = 3.581\n",
      "Epoch  62 Batch  168/269   train_loss = 3.748\n",
      "Epoch  62 Batch  169/269   train_loss = 4.008\n",
      "Epoch  62 Batch  170/269   train_loss = 3.458\n",
      "Epoch  62 Batch  171/269   train_loss = 3.758\n",
      "Epoch  62 Batch  172/269   train_loss = 3.545\n",
      "Epoch  62 Batch  173/269   train_loss = 3.690\n",
      "Epoch  62 Batch  174/269   train_loss = 3.894\n",
      "Epoch  62 Batch  175/269   train_loss = 3.768\n",
      "Epoch  62 Batch  176/269   train_loss = 3.730\n",
      "Epoch  62 Batch  177/269   train_loss = 3.708\n",
      "Epoch  62 Batch  178/269   train_loss = 3.919\n",
      "Epoch  62 Batch  179/269   train_loss = 3.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  62 Batch  180/269   train_loss = 3.560\n",
      "Epoch  62 Batch  181/269   train_loss = 3.836\n",
      "Epoch  62 Batch  182/269   train_loss = 3.663\n",
      "Epoch  62 Batch  183/269   train_loss = 3.637\n",
      "Epoch  62 Batch  184/269   train_loss = 3.749\n",
      "Epoch  62 Batch  185/269   train_loss = 3.788\n",
      "Epoch  62 Batch  186/269   train_loss = 3.682\n",
      "Epoch  62 Batch  187/269   train_loss = 3.905\n",
      "Epoch  62 Batch  188/269   train_loss = 3.654\n",
      "Epoch  62 Batch  189/269   train_loss = 3.616\n",
      "Epoch  62 Batch  190/269   train_loss = 4.081\n",
      "Epoch  62 Batch  191/269   train_loss = 3.786\n",
      "Epoch  62 Batch  192/269   train_loss = 3.753\n",
      "Epoch  62 Batch  193/269   train_loss = 3.723\n",
      "Epoch  62 Batch  194/269   train_loss = 3.782\n",
      "Epoch  62 Batch  195/269   train_loss = 3.721\n",
      "Epoch  62 Batch  196/269   train_loss = 3.750\n",
      "Epoch  62 Batch  197/269   train_loss = 3.913\n",
      "Epoch  62 Batch  198/269   train_loss = 3.775\n",
      "Epoch  62 Batch  199/269   train_loss = 3.847\n",
      "Epoch  62 Batch  200/269   train_loss = 3.582\n",
      "Epoch  62 Batch  201/269   train_loss = 3.745\n",
      "Epoch  62 Batch  202/269   train_loss = 3.538\n",
      "Epoch  62 Batch  203/269   train_loss = 3.640\n",
      "Epoch  62 Batch  204/269   train_loss = 3.802\n",
      "Epoch  62 Batch  205/269   train_loss = 3.795\n",
      "Epoch  62 Batch  206/269   train_loss = 3.685\n",
      "Epoch  62 Batch  207/269   train_loss = 3.594\n",
      "Epoch  62 Batch  208/269   train_loss = 3.763\n",
      "Epoch  62 Batch  209/269   train_loss = 3.799\n",
      "Epoch  62 Batch  210/269   train_loss = 3.623\n",
      "Epoch  62 Batch  211/269   train_loss = 3.624\n",
      "Epoch  62 Batch  212/269   train_loss = 4.010\n",
      "Epoch  62 Batch  213/269   train_loss = 3.602\n",
      "Epoch  62 Batch  214/269   train_loss = 3.709\n",
      "Epoch  62 Batch  215/269   train_loss = 3.946\n",
      "Epoch  62 Batch  216/269   train_loss = 3.871\n",
      "Epoch  62 Batch  217/269   train_loss = 3.595\n",
      "Epoch  62 Batch  218/269   train_loss = 3.773\n",
      "Epoch  62 Batch  219/269   train_loss = 3.503\n",
      "Epoch  62 Batch  220/269   train_loss = 3.915\n",
      "Epoch  62 Batch  221/269   train_loss = 3.582\n",
      "Epoch  62 Batch  222/269   train_loss = 3.772\n",
      "Epoch  62 Batch  223/269   train_loss = 3.556\n",
      "Epoch  62 Batch  224/269   train_loss = 3.802\n",
      "Epoch  62 Batch  225/269   train_loss = 3.870\n",
      "Epoch  62 Batch  226/269   train_loss = 3.692\n",
      "Epoch  62 Batch  227/269   train_loss = 3.448\n",
      "Epoch  62 Batch  228/269   train_loss = 3.705\n",
      "Epoch  62 Batch  229/269   train_loss = 3.919\n",
      "Epoch  62 Batch  230/269   train_loss = 3.852\n",
      "Epoch  62 Batch  231/269   train_loss = 3.725\n",
      "Epoch  62 Batch  232/269   train_loss = 3.713\n",
      "Epoch  62 Batch  233/269   train_loss = 3.796\n",
      "Epoch  62 Batch  234/269   train_loss = 3.738\n",
      "Epoch  62 Batch  235/269   train_loss = 3.985\n",
      "Epoch  62 Batch  236/269   train_loss = 3.600\n",
      "Epoch  62 Batch  237/269   train_loss = 3.256\n",
      "Epoch  62 Batch  238/269   train_loss = 3.492\n",
      "Epoch  62 Batch  239/269   train_loss = 3.995\n",
      "Epoch  62 Batch  240/269   train_loss = 3.624\n",
      "Epoch  62 Batch  241/269   train_loss = 3.973\n",
      "Epoch  62 Batch  242/269   train_loss = 3.506\n",
      "Epoch  62 Batch  243/269   train_loss = 3.899\n",
      "Epoch  62 Batch  244/269   train_loss = 3.734\n",
      "Epoch  62 Batch  245/269   train_loss = 3.706\n",
      "Epoch  62 Batch  246/269   train_loss = 3.381\n",
      "Epoch  62 Batch  247/269   train_loss = 3.812\n",
      "Epoch  62 Batch  248/269   train_loss = 3.708\n",
      "Epoch  62 Batch  249/269   train_loss = 3.669\n",
      "Epoch  62 Batch  250/269   train_loss = 3.510\n",
      "Epoch  62 Batch  251/269   train_loss = 3.940\n",
      "Epoch  62 Batch  252/269   train_loss = 3.903\n",
      "Epoch  62 Batch  253/269   train_loss = 3.624\n",
      "Epoch  62 Batch  254/269   train_loss = 3.604\n",
      "Epoch  62 Batch  255/269   train_loss = 3.732\n",
      "Epoch  62 Batch  256/269   train_loss = 3.606\n",
      "Epoch  62 Batch  257/269   train_loss = 3.560\n",
      "Epoch  62 Batch  258/269   train_loss = 3.484\n",
      "Epoch  62 Batch  259/269   train_loss = 3.586\n",
      "Epoch  62 Batch  260/269   train_loss = 3.881\n",
      "Epoch  62 Batch  261/269   train_loss = 3.830\n",
      "Epoch  62 Batch  262/269   train_loss = 3.644\n",
      "Epoch  62 Batch  263/269   train_loss = 3.534\n",
      "Epoch  62 Batch  264/269   train_loss = 3.959\n",
      "Epoch  62 Batch  265/269   train_loss = 3.801\n",
      "Epoch  62 Batch  266/269   train_loss = 3.623\n",
      "Epoch  62 Batch  267/269   train_loss = 3.740\n",
      "Epoch  62 Batch  268/269   train_loss = 3.823\n",
      "Epoch  63 Batch    0/269   train_loss = 3.661\n",
      "Epoch  63 Batch    1/269   train_loss = 3.469\n",
      "Epoch  63 Batch    2/269   train_loss = 3.595\n",
      "Epoch  63 Batch    3/269   train_loss = 3.699\n",
      "Epoch  63 Batch    4/269   train_loss = 4.104\n",
      "Epoch  63 Batch    5/269   train_loss = 3.684\n",
      "Epoch  63 Batch    6/269   train_loss = 3.652\n",
      "Epoch  63 Batch    7/269   train_loss = 3.452\n",
      "Epoch  63 Batch    8/269   train_loss = 3.782\n",
      "Epoch  63 Batch    9/269   train_loss = 3.458\n",
      "Epoch  63 Batch   10/269   train_loss = 3.609\n",
      "Epoch  63 Batch   11/269   train_loss = 3.652\n",
      "Epoch  63 Batch   12/269   train_loss = 3.382\n",
      "Epoch  63 Batch   13/269   train_loss = 3.712\n",
      "Epoch  63 Batch   14/269   train_loss = 3.627\n",
      "Epoch  63 Batch   15/269   train_loss = 3.910\n",
      "Epoch  63 Batch   16/269   train_loss = 3.625\n",
      "Epoch  63 Batch   17/269   train_loss = 3.688\n",
      "Epoch  63 Batch   18/269   train_loss = 3.643\n",
      "Epoch  63 Batch   19/269   train_loss = 3.658\n",
      "Epoch  63 Batch   20/269   train_loss = 4.010\n",
      "Epoch  63 Batch   21/269   train_loss = 3.866\n",
      "Epoch  63 Batch   22/269   train_loss = 3.572\n",
      "Epoch  63 Batch   23/269   train_loss = 3.784\n",
      "Epoch  63 Batch   24/269   train_loss = 3.512\n",
      "Epoch  63 Batch   25/269   train_loss = 3.898\n",
      "Epoch  63 Batch   26/269   train_loss = 3.769\n",
      "Epoch  63 Batch   27/269   train_loss = 3.600\n",
      "Epoch  63 Batch   28/269   train_loss = 3.770\n",
      "Epoch  63 Batch   29/269   train_loss = 3.736\n",
      "Epoch  63 Batch   30/269   train_loss = 4.073\n",
      "Epoch  63 Batch   31/269   train_loss = 3.712\n",
      "Epoch  63 Batch   32/269   train_loss = 3.623\n",
      "Epoch  63 Batch   33/269   train_loss = 3.396\n",
      "Epoch  63 Batch   34/269   train_loss = 3.595\n",
      "Epoch  63 Batch   35/269   train_loss = 3.502\n",
      "Epoch  63 Batch   36/269   train_loss = 3.518\n",
      "Epoch  63 Batch   37/269   train_loss = 3.742\n",
      "Epoch  63 Batch   38/269   train_loss = 3.432\n",
      "Epoch  63 Batch   39/269   train_loss = 3.806\n",
      "Epoch  63 Batch   40/269   train_loss = 3.560\n",
      "Epoch  63 Batch   41/269   train_loss = 3.557\n",
      "Epoch  63 Batch   42/269   train_loss = 4.027\n",
      "Epoch  63 Batch   43/269   train_loss = 3.957\n",
      "Epoch  63 Batch   44/269   train_loss = 3.753\n",
      "Epoch  63 Batch   45/269   train_loss = 3.410\n",
      "Epoch  63 Batch   46/269   train_loss = 3.805\n",
      "Epoch  63 Batch   47/269   train_loss = 3.499\n",
      "Epoch  63 Batch   48/269   train_loss = 3.872\n",
      "Epoch  63 Batch   49/269   train_loss = 3.893\n",
      "Epoch  63 Batch   50/269   train_loss = 3.801\n",
      "Epoch  63 Batch   51/269   train_loss = 3.714\n",
      "Epoch  63 Batch   52/269   train_loss = 3.738\n",
      "Epoch  63 Batch   53/269   train_loss = 3.945\n",
      "Epoch  63 Batch   54/269   train_loss = 3.696\n",
      "Epoch  63 Batch   55/269   train_loss = 3.832\n",
      "Epoch  63 Batch   56/269   train_loss = 3.633\n",
      "Epoch  63 Batch   57/269   train_loss = 3.893\n",
      "Epoch  63 Batch   58/269   train_loss = 3.512\n",
      "Epoch  63 Batch   59/269   train_loss = 3.616\n",
      "Epoch  63 Batch   60/269   train_loss = 3.813\n",
      "Epoch  63 Batch   61/269   train_loss = 3.915\n",
      "Epoch  63 Batch   62/269   train_loss = 3.549\n",
      "Epoch  63 Batch   63/269   train_loss = 3.887\n",
      "Epoch  63 Batch   64/269   train_loss = 3.615\n",
      "Epoch  63 Batch   65/269   train_loss = 3.725\n",
      "Epoch  63 Batch   66/269   train_loss = 3.701\n",
      "Epoch  63 Batch   67/269   train_loss = 3.608\n",
      "Epoch  63 Batch   68/269   train_loss = 3.775\n",
      "Epoch  63 Batch   69/269   train_loss = 3.742\n",
      "Epoch  63 Batch   70/269   train_loss = 3.427\n",
      "Epoch  63 Batch   71/269   train_loss = 3.772\n",
      "Epoch  63 Batch   72/269   train_loss = 3.512\n",
      "Epoch  63 Batch   73/269   train_loss = 3.661\n",
      "Epoch  63 Batch   74/269   train_loss = 3.647\n",
      "Epoch  63 Batch   75/269   train_loss = 3.837\n",
      "Epoch  63 Batch   76/269   train_loss = 3.810\n",
      "Epoch  63 Batch   77/269   train_loss = 3.421\n",
      "Epoch  63 Batch   78/269   train_loss = 3.835\n",
      "Epoch  63 Batch   79/269   train_loss = 3.892\n",
      "Epoch  63 Batch   80/269   train_loss = 3.807\n",
      "Epoch  63 Batch   81/269   train_loss = 3.633\n",
      "Epoch  63 Batch   82/269   train_loss = 3.812\n",
      "Epoch  63 Batch   83/269   train_loss = 3.621\n",
      "Epoch  63 Batch   84/269   train_loss = 3.954\n",
      "Epoch  63 Batch   85/269   train_loss = 4.085\n",
      "Epoch  63 Batch   86/269   train_loss = 4.050\n",
      "Epoch  63 Batch   87/269   train_loss = 3.658\n",
      "Epoch  63 Batch   88/269   train_loss = 3.760\n",
      "Epoch  63 Batch   89/269   train_loss = 3.739\n",
      "Epoch  63 Batch   90/269   train_loss = 3.610\n",
      "Epoch  63 Batch   91/269   train_loss = 3.852\n",
      "Epoch  63 Batch   92/269   train_loss = 3.871\n",
      "Epoch  63 Batch   93/269   train_loss = 3.574\n",
      "Epoch  63 Batch   94/269   train_loss = 4.063\n",
      "Epoch  63 Batch   95/269   train_loss = 3.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  63 Batch   96/269   train_loss = 3.672\n",
      "Epoch  63 Batch   97/269   train_loss = 3.687\n",
      "Epoch  63 Batch   98/269   train_loss = 3.964\n",
      "Epoch  63 Batch   99/269   train_loss = 3.790\n",
      "Epoch  63 Batch  100/269   train_loss = 3.751\n",
      "Epoch  63 Batch  101/269   train_loss = 3.791\n",
      "Epoch  63 Batch  102/269   train_loss = 3.497\n",
      "Epoch  63 Batch  103/269   train_loss = 3.629\n",
      "Epoch  63 Batch  104/269   train_loss = 3.554\n",
      "Epoch  63 Batch  105/269   train_loss = 3.882\n",
      "Epoch  63 Batch  106/269   train_loss = 3.963\n",
      "Epoch  63 Batch  107/269   train_loss = 3.976\n",
      "Epoch  63 Batch  108/269   train_loss = 3.714\n",
      "Epoch  63 Batch  109/269   train_loss = 3.743\n",
      "Epoch  63 Batch  110/269   train_loss = 3.729\n",
      "Epoch  63 Batch  111/269   train_loss = 3.863\n",
      "Epoch  63 Batch  112/269   train_loss = 3.885\n",
      "Epoch  63 Batch  113/269   train_loss = 3.704\n",
      "Epoch  63 Batch  114/269   train_loss = 3.695\n",
      "Epoch  63 Batch  115/269   train_loss = 3.611\n",
      "Epoch  63 Batch  116/269   train_loss = 3.569\n",
      "Epoch  63 Batch  117/269   train_loss = 3.616\n",
      "Epoch  63 Batch  118/269   train_loss = 3.920\n",
      "Epoch  63 Batch  119/269   train_loss = 3.489\n",
      "Epoch  63 Batch  120/269   train_loss = 3.732\n",
      "Epoch  63 Batch  121/269   train_loss = 3.672\n",
      "Epoch  63 Batch  122/269   train_loss = 3.693\n",
      "Epoch  63 Batch  123/269   train_loss = 3.548\n",
      "Epoch  63 Batch  124/269   train_loss = 3.922\n",
      "Epoch  63 Batch  125/269   train_loss = 3.503\n",
      "Epoch  63 Batch  126/269   train_loss = 3.411\n",
      "Epoch  63 Batch  127/269   train_loss = 3.659\n",
      "Epoch  63 Batch  128/269   train_loss = 3.655\n",
      "Epoch  63 Batch  129/269   train_loss = 3.663\n",
      "Epoch  63 Batch  130/269   train_loss = 3.746\n",
      "Epoch  63 Batch  131/269   train_loss = 3.406\n",
      "Epoch  63 Batch  132/269   train_loss = 3.764\n",
      "Epoch  63 Batch  133/269   train_loss = 3.501\n",
      "Epoch  63 Batch  134/269   train_loss = 3.895\n",
      "Epoch  63 Batch  135/269   train_loss = 3.583\n",
      "Epoch  63 Batch  136/269   train_loss = 3.439\n",
      "Epoch  63 Batch  137/269   train_loss = 3.821\n",
      "Epoch  63 Batch  138/269   train_loss = 3.552\n",
      "Epoch  63 Batch  139/269   train_loss = 3.689\n",
      "Epoch  63 Batch  140/269   train_loss = 3.747\n",
      "Epoch  63 Batch  141/269   train_loss = 4.109\n",
      "Epoch  63 Batch  142/269   train_loss = 3.519\n",
      "Epoch  63 Batch  143/269   train_loss = 3.737\n",
      "Epoch  63 Batch  144/269   train_loss = 3.918\n",
      "Epoch  63 Batch  145/269   train_loss = 3.646\n",
      "Epoch  63 Batch  146/269   train_loss = 3.912\n",
      "Epoch  63 Batch  147/269   train_loss = 3.498\n",
      "Epoch  63 Batch  148/269   train_loss = 3.824\n",
      "Epoch  63 Batch  149/269   train_loss = 3.608\n",
      "Epoch  63 Batch  150/269   train_loss = 3.806\n",
      "Epoch  63 Batch  151/269   train_loss = 3.766\n",
      "Epoch  63 Batch  152/269   train_loss = 3.713\n",
      "Epoch  63 Batch  153/269   train_loss = 3.754\n",
      "Epoch  63 Batch  154/269   train_loss = 3.590\n",
      "Epoch  63 Batch  155/269   train_loss = 3.596\n",
      "Epoch  63 Batch  156/269   train_loss = 4.101\n",
      "Epoch  63 Batch  157/269   train_loss = 3.773\n",
      "Epoch  63 Batch  158/269   train_loss = 3.754\n",
      "Epoch  63 Batch  159/269   train_loss = 3.746\n",
      "Epoch  63 Batch  160/269   train_loss = 3.643\n",
      "Epoch  63 Batch  161/269   train_loss = 3.837\n",
      "Epoch  63 Batch  162/269   train_loss = 3.832\n",
      "Epoch  63 Batch  163/269   train_loss = 3.619\n",
      "Epoch  63 Batch  164/269   train_loss = 3.929\n",
      "Epoch  63 Batch  165/269   train_loss = 3.900\n",
      "Epoch  63 Batch  166/269   train_loss = 3.796\n",
      "Epoch  63 Batch  167/269   train_loss = 3.581\n",
      "Epoch  63 Batch  168/269   train_loss = 3.798\n",
      "Epoch  63 Batch  169/269   train_loss = 4.033\n",
      "Epoch  63 Batch  170/269   train_loss = 3.450\n",
      "Epoch  63 Batch  171/269   train_loss = 3.721\n",
      "Epoch  63 Batch  172/269   train_loss = 3.561\n",
      "Epoch  63 Batch  173/269   train_loss = 3.727\n",
      "Epoch  63 Batch  174/269   train_loss = 3.893\n",
      "Epoch  63 Batch  175/269   train_loss = 3.759\n",
      "Epoch  63 Batch  176/269   train_loss = 3.738\n",
      "Epoch  63 Batch  177/269   train_loss = 3.695\n",
      "Epoch  63 Batch  178/269   train_loss = 3.942\n",
      "Epoch  63 Batch  179/269   train_loss = 3.724\n",
      "Epoch  63 Batch  180/269   train_loss = 3.591\n",
      "Epoch  63 Batch  181/269   train_loss = 3.817\n",
      "Epoch  63 Batch  182/269   train_loss = 3.670\n",
      "Epoch  63 Batch  183/269   train_loss = 3.630\n",
      "Epoch  63 Batch  184/269   train_loss = 3.766\n",
      "Epoch  63 Batch  185/269   train_loss = 3.816\n",
      "Epoch  63 Batch  186/269   train_loss = 3.689\n",
      "Epoch  63 Batch  187/269   train_loss = 3.931\n",
      "Epoch  63 Batch  188/269   train_loss = 3.654\n",
      "Epoch  63 Batch  189/269   train_loss = 3.618\n",
      "Epoch  63 Batch  190/269   train_loss = 4.077\n",
      "Epoch  63 Batch  191/269   train_loss = 3.798\n",
      "Epoch  63 Batch  192/269   train_loss = 3.755\n",
      "Epoch  63 Batch  193/269   train_loss = 3.736\n",
      "Epoch  63 Batch  194/269   train_loss = 3.821\n",
      "Epoch  63 Batch  195/269   train_loss = 3.737\n",
      "Epoch  63 Batch  196/269   train_loss = 3.739\n",
      "Epoch  63 Batch  197/269   train_loss = 3.913\n",
      "Epoch  63 Batch  198/269   train_loss = 3.780\n",
      "Epoch  63 Batch  199/269   train_loss = 3.839\n",
      "Epoch  63 Batch  200/269   train_loss = 3.585\n",
      "Epoch  63 Batch  201/269   train_loss = 3.734\n",
      "Epoch  63 Batch  202/269   train_loss = 3.517\n",
      "Epoch  63 Batch  203/269   train_loss = 3.646\n",
      "Epoch  63 Batch  204/269   train_loss = 3.854\n",
      "Epoch  63 Batch  205/269   train_loss = 3.802\n",
      "Epoch  63 Batch  206/269   train_loss = 3.667\n",
      "Epoch  63 Batch  207/269   train_loss = 3.624\n",
      "Epoch  63 Batch  208/269   train_loss = 3.747\n",
      "Epoch  63 Batch  209/269   train_loss = 3.798\n",
      "Epoch  63 Batch  210/269   train_loss = 3.639\n",
      "Epoch  63 Batch  211/269   train_loss = 3.613\n",
      "Epoch  63 Batch  212/269   train_loss = 4.018\n",
      "Epoch  63 Batch  213/269   train_loss = 3.607\n",
      "Epoch  63 Batch  214/269   train_loss = 3.723\n",
      "Epoch  63 Batch  215/269   train_loss = 3.991\n",
      "Epoch  63 Batch  216/269   train_loss = 3.882\n",
      "Epoch  63 Batch  217/269   train_loss = 3.596\n",
      "Epoch  63 Batch  218/269   train_loss = 3.805\n",
      "Epoch  63 Batch  219/269   train_loss = 3.515\n",
      "Epoch  63 Batch  220/269   train_loss = 3.865\n",
      "Epoch  63 Batch  221/269   train_loss = 3.580\n",
      "Epoch  63 Batch  222/269   train_loss = 3.761\n",
      "Epoch  63 Batch  223/269   train_loss = 3.524\n",
      "Epoch  63 Batch  224/269   train_loss = 3.835\n",
      "Epoch  63 Batch  225/269   train_loss = 3.896\n",
      "Epoch  63 Batch  226/269   train_loss = 3.721\n",
      "Epoch  63 Batch  227/269   train_loss = 3.432\n",
      "Epoch  63 Batch  228/269   train_loss = 3.718\n",
      "Epoch  63 Batch  229/269   train_loss = 3.901\n",
      "Epoch  63 Batch  230/269   train_loss = 3.848\n",
      "Epoch  63 Batch  231/269   train_loss = 3.739\n",
      "Epoch  63 Batch  232/269   train_loss = 3.700\n",
      "Epoch  63 Batch  233/269   train_loss = 3.791\n",
      "Epoch  63 Batch  234/269   train_loss = 3.709\n",
      "Epoch  63 Batch  235/269   train_loss = 4.017\n",
      "Epoch  63 Batch  236/269   train_loss = 3.627\n",
      "Epoch  63 Batch  237/269   train_loss = 3.246\n",
      "Epoch  63 Batch  238/269   train_loss = 3.485\n",
      "Epoch  63 Batch  239/269   train_loss = 3.985\n",
      "Epoch  63 Batch  240/269   train_loss = 3.586\n",
      "Epoch  63 Batch  241/269   train_loss = 3.971\n",
      "Epoch  63 Batch  242/269   train_loss = 3.528\n",
      "Epoch  63 Batch  243/269   train_loss = 3.902\n",
      "Epoch  63 Batch  244/269   train_loss = 3.711\n",
      "Epoch  63 Batch  245/269   train_loss = 3.710\n",
      "Epoch  63 Batch  246/269   train_loss = 3.374\n",
      "Epoch  63 Batch  247/269   train_loss = 3.805\n",
      "Epoch  63 Batch  248/269   train_loss = 3.738\n",
      "Epoch  63 Batch  249/269   train_loss = 3.658\n",
      "Epoch  63 Batch  250/269   train_loss = 3.524\n",
      "Epoch  63 Batch  251/269   train_loss = 3.950\n",
      "Epoch  63 Batch  252/269   train_loss = 3.931\n",
      "Epoch  63 Batch  253/269   train_loss = 3.607\n",
      "Epoch  63 Batch  254/269   train_loss = 3.619\n",
      "Epoch  63 Batch  255/269   train_loss = 3.702\n",
      "Epoch  63 Batch  256/269   train_loss = 3.609\n",
      "Epoch  63 Batch  257/269   train_loss = 3.542\n",
      "Epoch  63 Batch  258/269   train_loss = 3.451\n",
      "Epoch  63 Batch  259/269   train_loss = 3.562\n",
      "Epoch  63 Batch  260/269   train_loss = 3.865\n",
      "Epoch  63 Batch  261/269   train_loss = 3.822\n",
      "Epoch  63 Batch  262/269   train_loss = 3.628\n",
      "Epoch  63 Batch  263/269   train_loss = 3.510\n",
      "Epoch  63 Batch  264/269   train_loss = 3.942\n",
      "Epoch  63 Batch  265/269   train_loss = 3.806\n",
      "Epoch  63 Batch  266/269   train_loss = 3.616\n",
      "Epoch  63 Batch  267/269   train_loss = 3.753\n",
      "Epoch  63 Batch  268/269   train_loss = 3.827\n",
      "Epoch  64 Batch    0/269   train_loss = 3.680\n",
      "Epoch  64 Batch    1/269   train_loss = 3.463\n",
      "Epoch  64 Batch    2/269   train_loss = 3.590\n",
      "Epoch  64 Batch    3/269   train_loss = 3.679\n",
      "Epoch  64 Batch    4/269   train_loss = 4.102\n",
      "Epoch  64 Batch    5/269   train_loss = 3.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  64 Batch    6/269   train_loss = 3.618\n",
      "Epoch  64 Batch    7/269   train_loss = 3.449\n",
      "Epoch  64 Batch    8/269   train_loss = 3.822\n",
      "Epoch  64 Batch    9/269   train_loss = 3.481\n",
      "Epoch  64 Batch   10/269   train_loss = 3.576\n",
      "Epoch  64 Batch   11/269   train_loss = 3.636\n",
      "Epoch  64 Batch   12/269   train_loss = 3.391\n",
      "Epoch  64 Batch   13/269   train_loss = 3.684\n",
      "Epoch  64 Batch   14/269   train_loss = 3.590\n",
      "Epoch  64 Batch   15/269   train_loss = 3.916\n",
      "Epoch  64 Batch   16/269   train_loss = 3.632\n",
      "Epoch  64 Batch   17/269   train_loss = 3.710\n",
      "Epoch  64 Batch   18/269   train_loss = 3.629\n",
      "Epoch  64 Batch   19/269   train_loss = 3.655\n",
      "Epoch  64 Batch   20/269   train_loss = 3.995\n",
      "Epoch  64 Batch   21/269   train_loss = 3.877\n",
      "Epoch  64 Batch   22/269   train_loss = 3.561\n",
      "Epoch  64 Batch   23/269   train_loss = 3.803\n",
      "Epoch  64 Batch   24/269   train_loss = 3.508\n",
      "Epoch  64 Batch   25/269   train_loss = 3.879\n",
      "Epoch  64 Batch   26/269   train_loss = 3.755\n",
      "Epoch  64 Batch   27/269   train_loss = 3.615\n",
      "Epoch  64 Batch   28/269   train_loss = 3.766\n",
      "Epoch  64 Batch   29/269   train_loss = 3.740\n",
      "Epoch  64 Batch   30/269   train_loss = 4.050\n",
      "Epoch  64 Batch   31/269   train_loss = 3.707\n",
      "Epoch  64 Batch   32/269   train_loss = 3.618\n",
      "Epoch  64 Batch   33/269   train_loss = 3.395\n",
      "Epoch  64 Batch   34/269   train_loss = 3.603\n",
      "Epoch  64 Batch   35/269   train_loss = 3.487\n",
      "Epoch  64 Batch   36/269   train_loss = 3.541\n",
      "Epoch  64 Batch   37/269   train_loss = 3.746\n",
      "Epoch  64 Batch   38/269   train_loss = 3.431\n",
      "Epoch  64 Batch   39/269   train_loss = 3.780\n",
      "Epoch  64 Batch   40/269   train_loss = 3.582\n",
      "Epoch  64 Batch   41/269   train_loss = 3.560\n",
      "Epoch  64 Batch   42/269   train_loss = 4.036\n",
      "Epoch  64 Batch   43/269   train_loss = 3.969\n",
      "Epoch  64 Batch   44/269   train_loss = 3.762\n",
      "Epoch  64 Batch   45/269   train_loss = 3.392\n",
      "Epoch  64 Batch   46/269   train_loss = 3.764\n",
      "Epoch  64 Batch   47/269   train_loss = 3.424\n",
      "Epoch  64 Batch   48/269   train_loss = 3.874\n",
      "Epoch  64 Batch   49/269   train_loss = 3.875\n",
      "Epoch  64 Batch   50/269   train_loss = 3.810\n",
      "Epoch  64 Batch   51/269   train_loss = 3.740\n",
      "Epoch  64 Batch   52/269   train_loss = 3.739\n",
      "Epoch  64 Batch   53/269   train_loss = 3.958\n",
      "Epoch  64 Batch   54/269   train_loss = 3.662\n",
      "Epoch  64 Batch   55/269   train_loss = 3.823\n",
      "Epoch  64 Batch   56/269   train_loss = 3.600\n",
      "Epoch  64 Batch   57/269   train_loss = 3.911\n",
      "Epoch  64 Batch   58/269   train_loss = 3.515\n",
      "Epoch  64 Batch   59/269   train_loss = 3.602\n",
      "Epoch  64 Batch   60/269   train_loss = 3.799\n",
      "Epoch  64 Batch   61/269   train_loss = 3.956\n",
      "Epoch  64 Batch   62/269   train_loss = 3.555\n",
      "Epoch  64 Batch   63/269   train_loss = 3.904\n",
      "Epoch  64 Batch   64/269   train_loss = 3.606\n",
      "Epoch  64 Batch   65/269   train_loss = 3.721\n",
      "Epoch  64 Batch   66/269   train_loss = 3.629\n",
      "Epoch  64 Batch   67/269   train_loss = 3.638\n",
      "Epoch  64 Batch   68/269   train_loss = 3.746\n",
      "Epoch  64 Batch   69/269   train_loss = 3.804\n",
      "Epoch  64 Batch   70/269   train_loss = 3.418\n",
      "Epoch  64 Batch   71/269   train_loss = 3.763\n",
      "Epoch  64 Batch   72/269   train_loss = 3.467\n",
      "Epoch  64 Batch   73/269   train_loss = 3.682\n",
      "Epoch  64 Batch   74/269   train_loss = 3.641\n",
      "Epoch  64 Batch   75/269   train_loss = 3.814\n",
      "Epoch  64 Batch   76/269   train_loss = 3.794\n",
      "Epoch  64 Batch   77/269   train_loss = 3.391\n",
      "Epoch  64 Batch   78/269   train_loss = 3.817\n",
      "Epoch  64 Batch   79/269   train_loss = 3.914\n",
      "Epoch  64 Batch   80/269   train_loss = 3.797\n",
      "Epoch  64 Batch   81/269   train_loss = 3.624\n",
      "Epoch  64 Batch   82/269   train_loss = 3.811\n",
      "Epoch  64 Batch   83/269   train_loss = 3.618\n",
      "Epoch  64 Batch   84/269   train_loss = 3.889\n",
      "Epoch  64 Batch   85/269   train_loss = 4.070\n",
      "Epoch  64 Batch   86/269   train_loss = 4.069\n",
      "Epoch  64 Batch   87/269   train_loss = 3.701\n",
      "Epoch  64 Batch   88/269   train_loss = 3.773\n",
      "Epoch  64 Batch   89/269   train_loss = 3.742\n",
      "Epoch  64 Batch   90/269   train_loss = 3.583\n",
      "Epoch  64 Batch   91/269   train_loss = 3.856\n",
      "Epoch  64 Batch   92/269   train_loss = 3.842\n",
      "Epoch  64 Batch   93/269   train_loss = 3.557\n",
      "Epoch  64 Batch   94/269   train_loss = 4.056\n",
      "Epoch  64 Batch   95/269   train_loss = 3.503\n",
      "Epoch  64 Batch   96/269   train_loss = 3.629\n",
      "Epoch  64 Batch   97/269   train_loss = 3.678\n",
      "Epoch  64 Batch   98/269   train_loss = 3.940\n",
      "Epoch  64 Batch   99/269   train_loss = 3.739\n",
      "Epoch  64 Batch  100/269   train_loss = 3.744\n",
      "Epoch  64 Batch  101/269   train_loss = 3.800\n",
      "Epoch  64 Batch  102/269   train_loss = 3.497\n",
      "Epoch  64 Batch  103/269   train_loss = 3.631\n",
      "Epoch  64 Batch  104/269   train_loss = 3.620\n",
      "Epoch  64 Batch  105/269   train_loss = 3.874\n",
      "Epoch  64 Batch  106/269   train_loss = 3.955\n",
      "Epoch  64 Batch  107/269   train_loss = 3.960\n",
      "Epoch  64 Batch  108/269   train_loss = 3.721\n",
      "Epoch  64 Batch  109/269   train_loss = 3.735\n",
      "Epoch  64 Batch  110/269   train_loss = 3.740\n",
      "Epoch  64 Batch  111/269   train_loss = 3.847\n",
      "Epoch  64 Batch  112/269   train_loss = 3.889\n",
      "Epoch  64 Batch  113/269   train_loss = 3.726\n",
      "Epoch  64 Batch  114/269   train_loss = 3.696\n",
      "Epoch  64 Batch  115/269   train_loss = 3.613\n",
      "Epoch  64 Batch  116/269   train_loss = 3.576\n",
      "Epoch  64 Batch  117/269   train_loss = 3.624\n",
      "Epoch  64 Batch  118/269   train_loss = 3.933\n",
      "Epoch  64 Batch  119/269   train_loss = 3.446\n",
      "Epoch  64 Batch  120/269   train_loss = 3.732\n",
      "Epoch  64 Batch  121/269   train_loss = 3.658\n",
      "Epoch  64 Batch  122/269   train_loss = 3.695\n",
      "Epoch  64 Batch  123/269   train_loss = 3.553\n",
      "Epoch  64 Batch  124/269   train_loss = 3.918\n",
      "Epoch  64 Batch  125/269   train_loss = 3.529\n",
      "Epoch  64 Batch  126/269   train_loss = 3.446\n",
      "Epoch  64 Batch  127/269   train_loss = 3.676\n",
      "Epoch  64 Batch  128/269   train_loss = 3.636\n",
      "Epoch  64 Batch  129/269   train_loss = 3.661\n",
      "Epoch  64 Batch  130/269   train_loss = 3.730\n",
      "Epoch  64 Batch  131/269   train_loss = 3.396\n",
      "Epoch  64 Batch  132/269   train_loss = 3.744\n",
      "Epoch  64 Batch  133/269   train_loss = 3.483\n",
      "Epoch  64 Batch  134/269   train_loss = 3.897\n",
      "Epoch  64 Batch  135/269   train_loss = 3.599\n",
      "Epoch  64 Batch  136/269   train_loss = 3.454\n",
      "Epoch  64 Batch  137/269   train_loss = 3.813\n",
      "Epoch  64 Batch  138/269   train_loss = 3.539\n",
      "Epoch  64 Batch  139/269   train_loss = 3.686\n",
      "Epoch  64 Batch  140/269   train_loss = 3.768\n",
      "Epoch  64 Batch  141/269   train_loss = 4.089\n",
      "Epoch  64 Batch  142/269   train_loss = 3.481\n",
      "Epoch  64 Batch  143/269   train_loss = 3.691\n",
      "Epoch  64 Batch  144/269   train_loss = 3.917\n",
      "Epoch  64 Batch  145/269   train_loss = 3.632\n",
      "Epoch  64 Batch  146/269   train_loss = 3.917\n",
      "Epoch  64 Batch  147/269   train_loss = 3.485\n",
      "Epoch  64 Batch  148/269   train_loss = 3.831\n",
      "Epoch  64 Batch  149/269   train_loss = 3.588\n",
      "Epoch  64 Batch  150/269   train_loss = 3.824\n",
      "Epoch  64 Batch  151/269   train_loss = 3.758\n",
      "Epoch  64 Batch  152/269   train_loss = 3.682\n",
      "Epoch  64 Batch  153/269   train_loss = 3.790\n",
      "Epoch  64 Batch  154/269   train_loss = 3.601\n",
      "Epoch  64 Batch  155/269   train_loss = 3.599\n",
      "Epoch  64 Batch  156/269   train_loss = 4.067\n",
      "Epoch  64 Batch  157/269   train_loss = 3.737\n",
      "Epoch  64 Batch  158/269   train_loss = 3.779\n",
      "Epoch  64 Batch  159/269   train_loss = 3.729\n",
      "Epoch  64 Batch  160/269   train_loss = 3.654\n",
      "Epoch  64 Batch  161/269   train_loss = 3.832\n",
      "Epoch  64 Batch  162/269   train_loss = 3.826\n",
      "Epoch  64 Batch  163/269   train_loss = 3.607\n",
      "Epoch  64 Batch  164/269   train_loss = 3.899\n",
      "Epoch  64 Batch  165/269   train_loss = 3.907\n",
      "Epoch  64 Batch  166/269   train_loss = 3.772\n",
      "Epoch  64 Batch  167/269   train_loss = 3.566\n",
      "Epoch  64 Batch  168/269   train_loss = 3.778\n",
      "Epoch  64 Batch  169/269   train_loss = 3.993\n",
      "Epoch  64 Batch  170/269   train_loss = 3.439\n",
      "Epoch  64 Batch  171/269   train_loss = 3.713\n",
      "Epoch  64 Batch  172/269   train_loss = 3.566\n",
      "Epoch  64 Batch  173/269   train_loss = 3.726\n",
      "Epoch  64 Batch  174/269   train_loss = 3.872\n",
      "Epoch  64 Batch  175/269   train_loss = 3.763\n",
      "Epoch  64 Batch  176/269   train_loss = 3.697\n",
      "Epoch  64 Batch  177/269   train_loss = 3.717\n",
      "Epoch  64 Batch  178/269   train_loss = 3.921\n",
      "Epoch  64 Batch  179/269   train_loss = 3.720\n",
      "Epoch  64 Batch  180/269   train_loss = 3.557\n",
      "Epoch  64 Batch  181/269   train_loss = 3.808\n",
      "Epoch  64 Batch  182/269   train_loss = 3.672\n",
      "Epoch  64 Batch  183/269   train_loss = 3.612\n",
      "Epoch  64 Batch  184/269   train_loss = 3.788\n",
      "Epoch  64 Batch  185/269   train_loss = 3.841\n",
      "Epoch  64 Batch  186/269   train_loss = 3.685\n",
      "Epoch  64 Batch  187/269   train_loss = 3.894\n",
      "Epoch  64 Batch  188/269   train_loss = 3.656\n",
      "Epoch  64 Batch  189/269   train_loss = 3.618\n",
      "Epoch  64 Batch  190/269   train_loss = 4.055\n",
      "Epoch  64 Batch  191/269   train_loss = 3.776\n",
      "Epoch  64 Batch  192/269   train_loss = 3.782\n",
      "Epoch  64 Batch  193/269   train_loss = 3.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  64 Batch  194/269   train_loss = 3.789\n",
      "Epoch  64 Batch  195/269   train_loss = 3.725\n",
      "Epoch  64 Batch  196/269   train_loss = 3.744\n",
      "Epoch  64 Batch  197/269   train_loss = 3.920\n",
      "Epoch  64 Batch  198/269   train_loss = 3.771\n",
      "Epoch  64 Batch  199/269   train_loss = 3.830\n",
      "Epoch  64 Batch  200/269   train_loss = 3.591\n",
      "Epoch  64 Batch  201/269   train_loss = 3.743\n",
      "Epoch  64 Batch  202/269   train_loss = 3.498\n",
      "Epoch  64 Batch  203/269   train_loss = 3.653\n",
      "Epoch  64 Batch  204/269   train_loss = 3.805\n",
      "Epoch  64 Batch  205/269   train_loss = 3.773\n",
      "Epoch  64 Batch  206/269   train_loss = 3.663\n",
      "Epoch  64 Batch  207/269   train_loss = 3.610\n",
      "Epoch  64 Batch  208/269   train_loss = 3.739\n",
      "Epoch  64 Batch  209/269   train_loss = 3.800\n",
      "Epoch  64 Batch  210/269   train_loss = 3.669\n",
      "Epoch  64 Batch  211/269   train_loss = 3.616\n",
      "Epoch  64 Batch  212/269   train_loss = 4.015\n",
      "Epoch  64 Batch  213/269   train_loss = 3.646\n",
      "Epoch  64 Batch  214/269   train_loss = 3.737\n",
      "Epoch  64 Batch  215/269   train_loss = 3.984\n",
      "Epoch  64 Batch  216/269   train_loss = 3.866\n",
      "Epoch  64 Batch  217/269   train_loss = 3.549\n",
      "Epoch  64 Batch  218/269   train_loss = 3.797\n",
      "Epoch  64 Batch  219/269   train_loss = 3.479\n",
      "Epoch  64 Batch  220/269   train_loss = 3.875\n",
      "Epoch  64 Batch  221/269   train_loss = 3.587\n",
      "Epoch  64 Batch  222/269   train_loss = 3.746\n",
      "Epoch  64 Batch  223/269   train_loss = 3.529\n",
      "Epoch  64 Batch  224/269   train_loss = 3.796\n",
      "Epoch  64 Batch  225/269   train_loss = 3.878\n",
      "Epoch  64 Batch  226/269   train_loss = 3.698\n",
      "Epoch  64 Batch  227/269   train_loss = 3.424\n",
      "Epoch  64 Batch  228/269   train_loss = 3.724\n",
      "Epoch  64 Batch  229/269   train_loss = 3.901\n",
      "Epoch  64 Batch  230/269   train_loss = 3.839\n",
      "Epoch  64 Batch  231/269   train_loss = 3.697\n",
      "Epoch  64 Batch  232/269   train_loss = 3.719\n",
      "Epoch  64 Batch  233/269   train_loss = 3.781\n",
      "Epoch  64 Batch  234/269   train_loss = 3.706\n",
      "Epoch  64 Batch  235/269   train_loss = 4.004\n",
      "Epoch  64 Batch  236/269   train_loss = 3.589\n",
      "Epoch  64 Batch  237/269   train_loss = 3.234\n",
      "Epoch  64 Batch  238/269   train_loss = 3.510\n",
      "Epoch  64 Batch  239/269   train_loss = 3.974\n",
      "Epoch  64 Batch  240/269   train_loss = 3.615\n",
      "Epoch  64 Batch  241/269   train_loss = 3.969\n",
      "Epoch  64 Batch  242/269   train_loss = 3.524\n",
      "Epoch  64 Batch  243/269   train_loss = 3.903\n",
      "Epoch  64 Batch  244/269   train_loss = 3.722\n",
      "Epoch  64 Batch  245/269   train_loss = 3.683\n",
      "Epoch  64 Batch  246/269   train_loss = 3.364\n",
      "Epoch  64 Batch  247/269   train_loss = 3.790\n",
      "Epoch  64 Batch  248/269   train_loss = 3.746\n",
      "Epoch  64 Batch  249/269   train_loss = 3.625\n",
      "Epoch  64 Batch  250/269   train_loss = 3.514\n",
      "Epoch  64 Batch  251/269   train_loss = 3.940\n",
      "Epoch  64 Batch  252/269   train_loss = 3.858\n",
      "Epoch  64 Batch  253/269   train_loss = 3.588\n",
      "Epoch  64 Batch  254/269   train_loss = 3.635\n",
      "Epoch  64 Batch  255/269   train_loss = 3.713\n",
      "Epoch  64 Batch  256/269   train_loss = 3.600\n",
      "Epoch  64 Batch  257/269   train_loss = 3.556\n",
      "Epoch  64 Batch  258/269   train_loss = 3.436\n",
      "Epoch  64 Batch  259/269   train_loss = 3.561\n",
      "Epoch  64 Batch  260/269   train_loss = 3.824\n",
      "Epoch  64 Batch  261/269   train_loss = 3.813\n",
      "Epoch  64 Batch  262/269   train_loss = 3.639\n",
      "Epoch  64 Batch  263/269   train_loss = 3.497\n",
      "Epoch  64 Batch  264/269   train_loss = 3.933\n",
      "Epoch  64 Batch  265/269   train_loss = 3.775\n",
      "Epoch  64 Batch  266/269   train_loss = 3.635\n",
      "Epoch  64 Batch  267/269   train_loss = 3.721\n",
      "Epoch  64 Batch  268/269   train_loss = 3.820\n",
      "Epoch  65 Batch    0/269   train_loss = 3.701\n",
      "Epoch  65 Batch    1/269   train_loss = 3.480\n",
      "Epoch  65 Batch    2/269   train_loss = 3.589\n",
      "Epoch  65 Batch    3/269   train_loss = 3.670\n",
      "Epoch  65 Batch    4/269   train_loss = 4.117\n",
      "Epoch  65 Batch    5/269   train_loss = 3.679\n",
      "Epoch  65 Batch    6/269   train_loss = 3.616\n",
      "Epoch  65 Batch    7/269   train_loss = 3.445\n",
      "Epoch  65 Batch    8/269   train_loss = 3.782\n",
      "Epoch  65 Batch    9/269   train_loss = 3.451\n",
      "Epoch  65 Batch   10/269   train_loss = 3.552\n",
      "Epoch  65 Batch   11/269   train_loss = 3.659\n",
      "Epoch  65 Batch   12/269   train_loss = 3.386\n",
      "Epoch  65 Batch   13/269   train_loss = 3.721\n",
      "Epoch  65 Batch   14/269   train_loss = 3.598\n",
      "Epoch  65 Batch   15/269   train_loss = 3.915\n",
      "Epoch  65 Batch   16/269   train_loss = 3.627\n",
      "Epoch  65 Batch   17/269   train_loss = 3.707\n",
      "Epoch  65 Batch   18/269   train_loss = 3.646\n",
      "Epoch  65 Batch   19/269   train_loss = 3.667\n",
      "Epoch  65 Batch   20/269   train_loss = 4.013\n",
      "Epoch  65 Batch   21/269   train_loss = 3.856\n",
      "Epoch  65 Batch   22/269   train_loss = 3.558\n",
      "Epoch  65 Batch   23/269   train_loss = 3.775\n",
      "Epoch  65 Batch   24/269   train_loss = 3.527\n",
      "Epoch  65 Batch   25/269   train_loss = 3.889\n",
      "Epoch  65 Batch   26/269   train_loss = 3.757\n",
      "Epoch  65 Batch   27/269   train_loss = 3.588\n",
      "Epoch  65 Batch   28/269   train_loss = 3.759\n",
      "Epoch  65 Batch   29/269   train_loss = 3.718\n",
      "Epoch  65 Batch   30/269   train_loss = 4.054\n",
      "Epoch  65 Batch   31/269   train_loss = 3.695\n",
      "Epoch  65 Batch   32/269   train_loss = 3.596\n",
      "Epoch  65 Batch   33/269   train_loss = 3.418\n",
      "Epoch  65 Batch   34/269   train_loss = 3.588\n",
      "Epoch  65 Batch   35/269   train_loss = 3.468\n",
      "Epoch  65 Batch   36/269   train_loss = 3.493\n",
      "Epoch  65 Batch   37/269   train_loss = 3.748\n",
      "Epoch  65 Batch   38/269   train_loss = 3.431\n",
      "Epoch  65 Batch   39/269   train_loss = 3.786\n",
      "Epoch  65 Batch   40/269   train_loss = 3.562\n",
      "Epoch  65 Batch   41/269   train_loss = 3.549\n",
      "Epoch  65 Batch   42/269   train_loss = 4.022\n",
      "Epoch  65 Batch   43/269   train_loss = 3.932\n",
      "Epoch  65 Batch   44/269   train_loss = 3.721\n",
      "Epoch  65 Batch   45/269   train_loss = 3.388\n",
      "Epoch  65 Batch   46/269   train_loss = 3.738\n",
      "Epoch  65 Batch   47/269   train_loss = 3.420\n",
      "Epoch  65 Batch   48/269   train_loss = 3.877\n",
      "Epoch  65 Batch   49/269   train_loss = 3.896\n",
      "Epoch  65 Batch   50/269   train_loss = 3.784\n",
      "Epoch  65 Batch   51/269   train_loss = 3.720\n",
      "Epoch  65 Batch   52/269   train_loss = 3.729\n",
      "Epoch  65 Batch   53/269   train_loss = 3.938\n",
      "Epoch  65 Batch   54/269   train_loss = 3.649\n",
      "Epoch  65 Batch   55/269   train_loss = 3.810\n",
      "Epoch  65 Batch   56/269   train_loss = 3.582\n",
      "Epoch  65 Batch   57/269   train_loss = 3.880\n",
      "Epoch  65 Batch   58/269   train_loss = 3.498\n",
      "Epoch  65 Batch   59/269   train_loss = 3.586\n",
      "Epoch  65 Batch   60/269   train_loss = 3.786\n",
      "Epoch  65 Batch   61/269   train_loss = 3.904\n",
      "Epoch  65 Batch   62/269   train_loss = 3.537\n",
      "Epoch  65 Batch   63/269   train_loss = 3.879\n",
      "Epoch  65 Batch   64/269   train_loss = 3.677\n",
      "Epoch  65 Batch   65/269   train_loss = 3.725\n",
      "Epoch  65 Batch   66/269   train_loss = 3.630\n",
      "Epoch  65 Batch   67/269   train_loss = 3.624\n",
      "Epoch  65 Batch   68/269   train_loss = 3.756\n",
      "Epoch  65 Batch   69/269   train_loss = 3.787\n",
      "Epoch  65 Batch   70/269   train_loss = 3.431\n",
      "Epoch  65 Batch   71/269   train_loss = 3.766\n",
      "Epoch  65 Batch   72/269   train_loss = 3.460\n",
      "Epoch  65 Batch   73/269   train_loss = 3.659\n",
      "Epoch  65 Batch   74/269   train_loss = 3.625\n",
      "Epoch  65 Batch   75/269   train_loss = 3.802\n",
      "Epoch  65 Batch   76/269   train_loss = 3.802\n",
      "Epoch  65 Batch   77/269   train_loss = 3.404\n",
      "Epoch  65 Batch   78/269   train_loss = 3.818\n",
      "Epoch  65 Batch   79/269   train_loss = 3.905\n",
      "Epoch  65 Batch   80/269   train_loss = 3.804\n",
      "Epoch  65 Batch   81/269   train_loss = 3.619\n",
      "Epoch  65 Batch   82/269   train_loss = 3.789\n",
      "Epoch  65 Batch   83/269   train_loss = 3.631\n",
      "Epoch  65 Batch   84/269   train_loss = 3.881\n",
      "Epoch  65 Batch   85/269   train_loss = 4.075\n",
      "Epoch  65 Batch   86/269   train_loss = 4.032\n",
      "Epoch  65 Batch   87/269   train_loss = 3.658\n",
      "Epoch  65 Batch   88/269   train_loss = 3.791\n",
      "Epoch  65 Batch   89/269   train_loss = 3.740\n",
      "Epoch  65 Batch   90/269   train_loss = 3.604\n",
      "Epoch  65 Batch   91/269   train_loss = 3.835\n",
      "Epoch  65 Batch   92/269   train_loss = 3.834\n",
      "Epoch  65 Batch   93/269   train_loss = 3.574\n",
      "Epoch  65 Batch   94/269   train_loss = 4.058\n",
      "Epoch  65 Batch   95/269   train_loss = 3.496\n",
      "Epoch  65 Batch   96/269   train_loss = 3.621\n",
      "Epoch  65 Batch   97/269   train_loss = 3.676\n",
      "Epoch  65 Batch   98/269   train_loss = 3.954\n",
      "Epoch  65 Batch   99/269   train_loss = 3.759\n",
      "Epoch  65 Batch  100/269   train_loss = 3.744\n",
      "Epoch  65 Batch  101/269   train_loss = 3.785\n",
      "Epoch  65 Batch  102/269   train_loss = 3.494\n",
      "Epoch  65 Batch  103/269   train_loss = 3.613\n",
      "Epoch  65 Batch  104/269   train_loss = 3.676\n",
      "Epoch  65 Batch  105/269   train_loss = 3.878\n",
      "Epoch  65 Batch  106/269   train_loss = 3.945\n",
      "Epoch  65 Batch  107/269   train_loss = 3.971\n",
      "Epoch  65 Batch  108/269   train_loss = 3.726\n",
      "Epoch  65 Batch  109/269   train_loss = 3.726\n",
      "Epoch  65 Batch  110/269   train_loss = 3.738\n",
      "Epoch  65 Batch  111/269   train_loss = 3.865\n",
      "Epoch  65 Batch  112/269   train_loss = 3.889\n",
      "Epoch  65 Batch  113/269   train_loss = 3.734\n",
      "Epoch  65 Batch  114/269   train_loss = 3.694\n",
      "Epoch  65 Batch  115/269   train_loss = 3.576\n",
      "Epoch  65 Batch  116/269   train_loss = 3.590\n",
      "Epoch  65 Batch  117/269   train_loss = 3.622\n",
      "Epoch  65 Batch  118/269   train_loss = 3.914\n",
      "Epoch  65 Batch  119/269   train_loss = 3.464\n",
      "Epoch  65 Batch  120/269   train_loss = 3.714\n",
      "Epoch  65 Batch  121/269   train_loss = 3.656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  65 Batch  122/269   train_loss = 3.712\n",
      "Epoch  65 Batch  123/269   train_loss = 3.530\n",
      "Epoch  65 Batch  124/269   train_loss = 3.891\n",
      "Epoch  65 Batch  125/269   train_loss = 3.498\n",
      "Epoch  65 Batch  126/269   train_loss = 3.448\n",
      "Epoch  65 Batch  127/269   train_loss = 3.686\n",
      "Epoch  65 Batch  128/269   train_loss = 3.642\n",
      "Epoch  65 Batch  129/269   train_loss = 3.642\n",
      "Epoch  65 Batch  130/269   train_loss = 3.739\n",
      "Epoch  65 Batch  131/269   train_loss = 3.389\n",
      "Epoch  65 Batch  132/269   train_loss = 3.780\n",
      "Epoch  65 Batch  133/269   train_loss = 3.516\n",
      "Epoch  65 Batch  134/269   train_loss = 3.894\n",
      "Epoch  65 Batch  135/269   train_loss = 3.598\n",
      "Epoch  65 Batch  136/269   train_loss = 3.439\n",
      "Epoch  65 Batch  137/269   train_loss = 3.794\n",
      "Epoch  65 Batch  138/269   train_loss = 3.539\n",
      "Epoch  65 Batch  139/269   train_loss = 3.667\n",
      "Epoch  65 Batch  140/269   train_loss = 3.744\n",
      "Epoch  65 Batch  141/269   train_loss = 4.110\n",
      "Epoch  65 Batch  142/269   train_loss = 3.468\n",
      "Epoch  65 Batch  143/269   train_loss = 3.705\n",
      "Epoch  65 Batch  144/269   train_loss = 3.935\n",
      "Epoch  65 Batch  145/269   train_loss = 3.659\n",
      "Epoch  65 Batch  146/269   train_loss = 3.940\n",
      "Epoch  65 Batch  147/269   train_loss = 3.532\n",
      "Epoch  65 Batch  148/269   train_loss = 3.819\n",
      "Epoch  65 Batch  149/269   train_loss = 3.580\n",
      "Epoch  65 Batch  150/269   train_loss = 3.825\n",
      "Epoch  65 Batch  151/269   train_loss = 3.752\n",
      "Epoch  65 Batch  152/269   train_loss = 3.684\n",
      "Epoch  65 Batch  153/269   train_loss = 3.715\n",
      "Epoch  65 Batch  154/269   train_loss = 3.641\n",
      "Epoch  65 Batch  155/269   train_loss = 3.614\n",
      "Epoch  65 Batch  156/269   train_loss = 4.051\n",
      "Epoch  65 Batch  157/269   train_loss = 3.745\n",
      "Epoch  65 Batch  158/269   train_loss = 3.771\n",
      "Epoch  65 Batch  159/269   train_loss = 3.728\n",
      "Epoch  65 Batch  160/269   train_loss = 3.641\n",
      "Epoch  65 Batch  161/269   train_loss = 3.809\n",
      "Epoch  65 Batch  162/269   train_loss = 3.821\n",
      "Epoch  65 Batch  163/269   train_loss = 3.603\n",
      "Epoch  65 Batch  164/269   train_loss = 3.918\n",
      "Epoch  65 Batch  165/269   train_loss = 3.914\n",
      "Epoch  65 Batch  166/269   train_loss = 3.773\n",
      "Epoch  65 Batch  167/269   train_loss = 3.576\n",
      "Epoch  65 Batch  168/269   train_loss = 3.754\n",
      "Epoch  65 Batch  169/269   train_loss = 3.999\n",
      "Epoch  65 Batch  170/269   train_loss = 3.486\n",
      "Epoch  65 Batch  171/269   train_loss = 3.725\n",
      "Epoch  65 Batch  172/269   train_loss = 3.588\n",
      "Epoch  65 Batch  173/269   train_loss = 3.722\n",
      "Epoch  65 Batch  174/269   train_loss = 3.867\n",
      "Epoch  65 Batch  175/269   train_loss = 3.754\n",
      "Epoch  65 Batch  176/269   train_loss = 3.718\n",
      "Epoch  65 Batch  177/269   train_loss = 3.759\n",
      "Epoch  65 Batch  178/269   train_loss = 3.950\n",
      "Epoch  65 Batch  179/269   train_loss = 3.728\n",
      "Epoch  65 Batch  180/269   train_loss = 3.587\n",
      "Epoch  65 Batch  181/269   train_loss = 3.815\n",
      "Epoch  65 Batch  182/269   train_loss = 3.670\n",
      "Epoch  65 Batch  183/269   train_loss = 3.617\n",
      "Epoch  65 Batch  184/269   train_loss = 3.735\n",
      "Epoch  65 Batch  185/269   train_loss = 3.794\n",
      "Epoch  65 Batch  186/269   train_loss = 3.685\n",
      "Epoch  65 Batch  187/269   train_loss = 3.883\n",
      "Epoch  65 Batch  188/269   train_loss = 3.631\n",
      "Epoch  65 Batch  189/269   train_loss = 3.606\n",
      "Epoch  65 Batch  190/269   train_loss = 4.068\n",
      "Epoch  65 Batch  191/269   train_loss = 3.774\n",
      "Epoch  65 Batch  192/269   train_loss = 3.758\n",
      "Epoch  65 Batch  193/269   train_loss = 3.717\n",
      "Epoch  65 Batch  194/269   train_loss = 3.817\n",
      "Epoch  65 Batch  195/269   train_loss = 3.705\n",
      "Epoch  65 Batch  196/269   train_loss = 3.790\n",
      "Epoch  65 Batch  197/269   train_loss = 3.886\n",
      "Epoch  65 Batch  198/269   train_loss = 3.790\n",
      "Epoch  65 Batch  199/269   train_loss = 3.841\n",
      "Epoch  65 Batch  200/269   train_loss = 3.593\n",
      "Epoch  65 Batch  201/269   train_loss = 3.752\n",
      "Epoch  65 Batch  202/269   train_loss = 3.514\n",
      "Epoch  65 Batch  203/269   train_loss = 3.651\n",
      "Epoch  65 Batch  204/269   train_loss = 3.818\n",
      "Epoch  65 Batch  205/269   train_loss = 3.752\n",
      "Epoch  65 Batch  206/269   train_loss = 3.696\n",
      "Epoch  65 Batch  207/269   train_loss = 3.629\n",
      "Epoch  65 Batch  208/269   train_loss = 3.749\n",
      "Epoch  65 Batch  209/269   train_loss = 3.798\n",
      "Epoch  65 Batch  210/269   train_loss = 3.629\n",
      "Epoch  65 Batch  211/269   train_loss = 3.619\n",
      "Epoch  65 Batch  212/269   train_loss = 4.011\n",
      "Epoch  65 Batch  213/269   train_loss = 3.625\n",
      "Epoch  65 Batch  214/269   train_loss = 3.745\n",
      "Epoch  65 Batch  215/269   train_loss = 3.946\n",
      "Epoch  65 Batch  216/269   train_loss = 3.884\n",
      "Epoch  65 Batch  217/269   train_loss = 3.559\n",
      "Epoch  65 Batch  218/269   train_loss = 3.789\n",
      "Epoch  65 Batch  219/269   train_loss = 3.510\n",
      "Epoch  65 Batch  220/269   train_loss = 3.859\n",
      "Epoch  65 Batch  221/269   train_loss = 3.573\n",
      "Epoch  65 Batch  222/269   train_loss = 3.787\n",
      "Epoch  65 Batch  223/269   train_loss = 3.524\n",
      "Epoch  65 Batch  224/269   train_loss = 3.825\n",
      "Epoch  65 Batch  225/269   train_loss = 3.860\n",
      "Epoch  65 Batch  226/269   train_loss = 3.683\n",
      "Epoch  65 Batch  227/269   train_loss = 3.445\n",
      "Epoch  65 Batch  228/269   train_loss = 3.731\n",
      "Epoch  65 Batch  229/269   train_loss = 3.886\n",
      "Epoch  65 Batch  230/269   train_loss = 3.848\n",
      "Epoch  65 Batch  231/269   train_loss = 3.720\n",
      "Epoch  65 Batch  232/269   train_loss = 3.701\n",
      "Epoch  65 Batch  233/269   train_loss = 3.784\n",
      "Epoch  65 Batch  234/269   train_loss = 3.695\n",
      "Epoch  65 Batch  235/269   train_loss = 4.035\n",
      "Epoch  65 Batch  236/269   train_loss = 3.586\n",
      "Epoch  65 Batch  237/269   train_loss = 3.239\n",
      "Epoch  65 Batch  238/269   train_loss = 3.488\n",
      "Epoch  65 Batch  239/269   train_loss = 3.987\n",
      "Epoch  65 Batch  240/269   train_loss = 3.601\n",
      "Epoch  65 Batch  241/269   train_loss = 3.964\n",
      "Epoch  65 Batch  242/269   train_loss = 3.517\n",
      "Epoch  65 Batch  243/269   train_loss = 3.903\n",
      "Epoch  65 Batch  244/269   train_loss = 3.720\n",
      "Epoch  65 Batch  245/269   train_loss = 3.695\n",
      "Epoch  65 Batch  246/269   train_loss = 3.361\n",
      "Epoch  65 Batch  247/269   train_loss = 3.791\n",
      "Epoch  65 Batch  248/269   train_loss = 3.791\n",
      "Epoch  65 Batch  249/269   train_loss = 3.621\n",
      "Epoch  65 Batch  250/269   train_loss = 3.522\n",
      "Epoch  65 Batch  251/269   train_loss = 3.961\n",
      "Epoch  65 Batch  252/269   train_loss = 3.850\n",
      "Epoch  65 Batch  253/269   train_loss = 3.640\n",
      "Epoch  65 Batch  254/269   train_loss = 3.634\n",
      "Epoch  65 Batch  255/269   train_loss = 3.724\n",
      "Epoch  65 Batch  256/269   train_loss = 3.621\n",
      "Epoch  65 Batch  257/269   train_loss = 3.577\n",
      "Epoch  65 Batch  258/269   train_loss = 3.453\n",
      "Epoch  65 Batch  259/269   train_loss = 3.571\n",
      "Epoch  65 Batch  260/269   train_loss = 3.797\n",
      "Epoch  65 Batch  261/269   train_loss = 3.828\n",
      "Epoch  65 Batch  262/269   train_loss = 3.620\n",
      "Epoch  65 Batch  263/269   train_loss = 3.474\n",
      "Epoch  65 Batch  264/269   train_loss = 3.908\n",
      "Epoch  65 Batch  265/269   train_loss = 3.769\n",
      "Epoch  65 Batch  266/269   train_loss = 3.625\n",
      "Epoch  65 Batch  267/269   train_loss = 3.726\n",
      "Epoch  65 Batch  268/269   train_loss = 3.840\n",
      "Epoch  66 Batch    0/269   train_loss = 3.658\n",
      "Epoch  66 Batch    1/269   train_loss = 3.465\n",
      "Epoch  66 Batch    2/269   train_loss = 3.579\n",
      "Epoch  66 Batch    3/269   train_loss = 3.678\n",
      "Epoch  66 Batch    4/269   train_loss = 4.122\n",
      "Epoch  66 Batch    5/269   train_loss = 3.682\n",
      "Epoch  66 Batch    6/269   train_loss = 3.605\n",
      "Epoch  66 Batch    7/269   train_loss = 3.447\n",
      "Epoch  66 Batch    8/269   train_loss = 3.777\n",
      "Epoch  66 Batch    9/269   train_loss = 3.464\n",
      "Epoch  66 Batch   10/269   train_loss = 3.586\n",
      "Epoch  66 Batch   11/269   train_loss = 3.625\n",
      "Epoch  66 Batch   12/269   train_loss = 3.415\n",
      "Epoch  66 Batch   13/269   train_loss = 3.708\n",
      "Epoch  66 Batch   14/269   train_loss = 3.585\n",
      "Epoch  66 Batch   15/269   train_loss = 3.921\n",
      "Epoch  66 Batch   16/269   train_loss = 3.629\n",
      "Epoch  66 Batch   17/269   train_loss = 3.703\n",
      "Epoch  66 Batch   18/269   train_loss = 3.640\n",
      "Epoch  66 Batch   19/269   train_loss = 3.672\n",
      "Epoch  66 Batch   20/269   train_loss = 4.017\n",
      "Epoch  66 Batch   21/269   train_loss = 3.860\n",
      "Epoch  66 Batch   22/269   train_loss = 3.630\n",
      "Epoch  66 Batch   23/269   train_loss = 3.796\n",
      "Epoch  66 Batch   24/269   train_loss = 3.513\n",
      "Epoch  66 Batch   25/269   train_loss = 3.906\n",
      "Epoch  66 Batch   26/269   train_loss = 3.745\n",
      "Epoch  66 Batch   27/269   train_loss = 3.592\n",
      "Epoch  66 Batch   28/269   train_loss = 3.798\n",
      "Epoch  66 Batch   29/269   train_loss = 3.731\n",
      "Epoch  66 Batch   30/269   train_loss = 4.070\n",
      "Epoch  66 Batch   31/269   train_loss = 3.732\n",
      "Epoch  66 Batch   32/269   train_loss = 3.603\n",
      "Epoch  66 Batch   33/269   train_loss = 3.401\n",
      "Epoch  66 Batch   34/269   train_loss = 3.562\n",
      "Epoch  66 Batch   35/269   train_loss = 3.473\n",
      "Epoch  66 Batch   36/269   train_loss = 3.501\n",
      "Epoch  66 Batch   37/269   train_loss = 3.747\n",
      "Epoch  66 Batch   38/269   train_loss = 3.416\n",
      "Epoch  66 Batch   39/269   train_loss = 3.748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  66 Batch   40/269   train_loss = 3.591\n",
      "Epoch  66 Batch   41/269   train_loss = 3.560\n",
      "Epoch  66 Batch   42/269   train_loss = 4.031\n",
      "Epoch  66 Batch   43/269   train_loss = 3.979\n",
      "Epoch  66 Batch   44/269   train_loss = 3.728\n",
      "Epoch  66 Batch   45/269   train_loss = 3.383\n",
      "Epoch  66 Batch   46/269   train_loss = 3.744\n",
      "Epoch  66 Batch   47/269   train_loss = 3.446\n",
      "Epoch  66 Batch   48/269   train_loss = 3.872\n",
      "Epoch  66 Batch   49/269   train_loss = 3.896\n",
      "Epoch  66 Batch   50/269   train_loss = 3.826\n",
      "Epoch  66 Batch   51/269   train_loss = 3.746\n",
      "Epoch  66 Batch   52/269   train_loss = 3.751\n",
      "Epoch  66 Batch   53/269   train_loss = 3.958\n",
      "Epoch  66 Batch   54/269   train_loss = 3.636\n",
      "Epoch  66 Batch   55/269   train_loss = 3.822\n",
      "Epoch  66 Batch   56/269   train_loss = 3.601\n",
      "Epoch  66 Batch   57/269   train_loss = 3.956\n",
      "Epoch  66 Batch   58/269   train_loss = 3.523\n",
      "Epoch  66 Batch   59/269   train_loss = 3.608\n",
      "Epoch  66 Batch   60/269   train_loss = 3.788\n",
      "Epoch  66 Batch   61/269   train_loss = 3.906\n",
      "Epoch  66 Batch   62/269   train_loss = 3.561\n",
      "Epoch  66 Batch   63/269   train_loss = 3.883\n",
      "Epoch  66 Batch   64/269   train_loss = 3.619\n",
      "Epoch  66 Batch   65/269   train_loss = 3.739\n",
      "Epoch  66 Batch   66/269   train_loss = 3.633\n",
      "Epoch  66 Batch   67/269   train_loss = 3.623\n",
      "Epoch  66 Batch   68/269   train_loss = 3.745\n",
      "Epoch  66 Batch   69/269   train_loss = 3.749\n",
      "Epoch  66 Batch   70/269   train_loss = 3.433\n",
      "Epoch  66 Batch   71/269   train_loss = 3.746\n",
      "Epoch  66 Batch   72/269   train_loss = 3.455\n",
      "Epoch  66 Batch   73/269   train_loss = 3.682\n",
      "Epoch  66 Batch   74/269   train_loss = 3.652\n",
      "Epoch  66 Batch   75/269   train_loss = 3.826\n",
      "Epoch  66 Batch   76/269   train_loss = 3.796\n",
      "Epoch  66 Batch   77/269   train_loss = 3.378\n",
      "Epoch  66 Batch   78/269   train_loss = 3.821\n",
      "Epoch  66 Batch   79/269   train_loss = 3.873\n",
      "Epoch  66 Batch   80/269   train_loss = 3.791\n",
      "Epoch  66 Batch   81/269   train_loss = 3.624\n",
      "Epoch  66 Batch   82/269   train_loss = 3.781\n",
      "Epoch  66 Batch   83/269   train_loss = 3.634\n",
      "Epoch  66 Batch   84/269   train_loss = 3.863\n",
      "Epoch  66 Batch   85/269   train_loss = 4.075\n",
      "Epoch  66 Batch   86/269   train_loss = 4.023\n",
      "Epoch  66 Batch   87/269   train_loss = 3.664\n",
      "Epoch  66 Batch   88/269   train_loss = 3.792\n",
      "Epoch  66 Batch   89/269   train_loss = 3.722\n",
      "Epoch  66 Batch   90/269   train_loss = 3.584\n",
      "Epoch  66 Batch   91/269   train_loss = 3.845\n",
      "Epoch  66 Batch   92/269   train_loss = 3.865\n",
      "Epoch  66 Batch   93/269   train_loss = 3.564\n",
      "Epoch  66 Batch   94/269   train_loss = 4.074\n",
      "Epoch  66 Batch   95/269   train_loss = 3.521\n",
      "Epoch  66 Batch   96/269   train_loss = 3.640\n",
      "Epoch  66 Batch   97/269   train_loss = 3.685\n",
      "Epoch  66 Batch   98/269   train_loss = 3.947\n",
      "Epoch  66 Batch   99/269   train_loss = 3.759\n",
      "Epoch  66 Batch  100/269   train_loss = 3.743\n",
      "Epoch  66 Batch  101/269   train_loss = 3.801\n",
      "Epoch  66 Batch  102/269   train_loss = 3.486\n",
      "Epoch  66 Batch  103/269   train_loss = 3.611\n",
      "Epoch  66 Batch  104/269   train_loss = 3.621\n",
      "Epoch  66 Batch  105/269   train_loss = 3.897\n",
      "Epoch  66 Batch  106/269   train_loss = 3.925\n",
      "Epoch  66 Batch  107/269   train_loss = 3.982\n",
      "Epoch  66 Batch  108/269   train_loss = 3.760\n",
      "Epoch  66 Batch  109/269   train_loss = 3.754\n",
      "Epoch  66 Batch  110/269   train_loss = 3.751\n",
      "Epoch  66 Batch  111/269   train_loss = 3.851\n",
      "Epoch  66 Batch  112/269   train_loss = 3.882\n",
      "Epoch  66 Batch  113/269   train_loss = 3.750\n",
      "Epoch  66 Batch  114/269   train_loss = 3.687\n",
      "Epoch  66 Batch  115/269   train_loss = 3.566\n",
      "Epoch  66 Batch  116/269   train_loss = 3.560\n",
      "Epoch  66 Batch  117/269   train_loss = 3.599\n",
      "Epoch  66 Batch  118/269   train_loss = 3.931\n",
      "Epoch  66 Batch  119/269   train_loss = 3.464\n",
      "Epoch  66 Batch  120/269   train_loss = 3.713\n",
      "Epoch  66 Batch  121/269   train_loss = 3.669\n",
      "Epoch  66 Batch  122/269   train_loss = 3.681\n",
      "Epoch  66 Batch  123/269   train_loss = 3.519\n",
      "Epoch  66 Batch  124/269   train_loss = 3.884\n",
      "Epoch  66 Batch  125/269   train_loss = 3.515\n",
      "Epoch  66 Batch  126/269   train_loss = 3.433\n",
      "Epoch  66 Batch  127/269   train_loss = 3.683\n",
      "Epoch  66 Batch  128/269   train_loss = 3.661\n",
      "Epoch  66 Batch  129/269   train_loss = 3.626\n",
      "Epoch  66 Batch  130/269   train_loss = 3.743\n",
      "Epoch  66 Batch  131/269   train_loss = 3.375\n",
      "Epoch  66 Batch  132/269   train_loss = 3.776\n",
      "Epoch  66 Batch  133/269   train_loss = 3.514\n",
      "Epoch  66 Batch  134/269   train_loss = 3.900\n",
      "Epoch  66 Batch  135/269   train_loss = 3.600\n",
      "Epoch  66 Batch  136/269   train_loss = 3.424\n",
      "Epoch  66 Batch  137/269   train_loss = 3.798\n",
      "Epoch  66 Batch  138/269   train_loss = 3.553\n",
      "Epoch  66 Batch  139/269   train_loss = 3.699\n",
      "Epoch  66 Batch  140/269   train_loss = 3.712\n",
      "Epoch  66 Batch  141/269   train_loss = 4.033\n",
      "Epoch  66 Batch  142/269   train_loss = 3.486\n",
      "Epoch  66 Batch  143/269   train_loss = 3.713\n",
      "Epoch  66 Batch  144/269   train_loss = 3.941\n",
      "Epoch  66 Batch  145/269   train_loss = 3.650\n",
      "Epoch  66 Batch  146/269   train_loss = 3.904\n",
      "Epoch  66 Batch  147/269   train_loss = 3.538\n",
      "Epoch  66 Batch  148/269   train_loss = 3.824\n",
      "Epoch  66 Batch  149/269   train_loss = 3.600\n",
      "Epoch  66 Batch  150/269   train_loss = 3.812\n",
      "Epoch  66 Batch  151/269   train_loss = 3.758\n",
      "Epoch  66 Batch  152/269   train_loss = 3.692\n",
      "Epoch  66 Batch  153/269   train_loss = 3.726\n",
      "Epoch  66 Batch  154/269   train_loss = 3.630\n",
      "Epoch  66 Batch  155/269   train_loss = 3.602\n",
      "Epoch  66 Batch  156/269   train_loss = 4.060\n",
      "Epoch  66 Batch  157/269   train_loss = 3.756\n",
      "Epoch  66 Batch  158/269   train_loss = 3.748\n",
      "Epoch  66 Batch  159/269   train_loss = 3.741\n",
      "Epoch  66 Batch  160/269   train_loss = 3.645\n",
      "Epoch  66 Batch  161/269   train_loss = 3.812\n",
      "Epoch  66 Batch  162/269   train_loss = 3.827\n",
      "Epoch  66 Batch  163/269   train_loss = 3.641\n",
      "Epoch  66 Batch  164/269   train_loss = 3.930\n",
      "Epoch  66 Batch  165/269   train_loss = 3.916\n",
      "Epoch  66 Batch  166/269   train_loss = 3.756\n",
      "Epoch  66 Batch  167/269   train_loss = 3.578\n",
      "Epoch  66 Batch  168/269   train_loss = 3.761\n",
      "Epoch  66 Batch  169/269   train_loss = 4.003\n",
      "Epoch  66 Batch  170/269   train_loss = 3.484\n",
      "Epoch  66 Batch  171/269   train_loss = 3.758\n",
      "Epoch  66 Batch  172/269   train_loss = 3.549\n",
      "Epoch  66 Batch  173/269   train_loss = 3.750\n",
      "Epoch  66 Batch  174/269   train_loss = 3.869\n",
      "Epoch  66 Batch  175/269   train_loss = 3.768\n",
      "Epoch  66 Batch  176/269   train_loss = 3.718\n",
      "Epoch  66 Batch  177/269   train_loss = 3.754\n",
      "Epoch  66 Batch  178/269   train_loss = 3.923\n",
      "Epoch  66 Batch  179/269   train_loss = 3.703\n",
      "Epoch  66 Batch  180/269   train_loss = 3.574\n",
      "Epoch  66 Batch  181/269   train_loss = 3.812\n",
      "Epoch  66 Batch  182/269   train_loss = 3.662\n",
      "Epoch  66 Batch  183/269   train_loss = 3.596\n",
      "Epoch  66 Batch  184/269   train_loss = 3.751\n",
      "Epoch  66 Batch  185/269   train_loss = 3.800\n",
      "Epoch  66 Batch  186/269   train_loss = 3.684\n",
      "Epoch  66 Batch  187/269   train_loss = 3.888\n",
      "Epoch  66 Batch  188/269   train_loss = 3.639\n",
      "Epoch  66 Batch  189/269   train_loss = 3.602\n",
      "Epoch  66 Batch  190/269   train_loss = 4.050\n",
      "Epoch  66 Batch  191/269   train_loss = 3.782\n",
      "Epoch  66 Batch  192/269   train_loss = 3.750\n",
      "Epoch  66 Batch  193/269   train_loss = 3.732\n",
      "Epoch  66 Batch  194/269   train_loss = 3.796\n",
      "Epoch  66 Batch  195/269   train_loss = 3.715\n",
      "Epoch  66 Batch  196/269   train_loss = 3.764\n",
      "Epoch  66 Batch  197/269   train_loss = 3.876\n",
      "Epoch  66 Batch  198/269   train_loss = 3.768\n",
      "Epoch  66 Batch  199/269   train_loss = 3.827\n",
      "Epoch  66 Batch  200/269   train_loss = 3.577\n",
      "Epoch  66 Batch  201/269   train_loss = 3.749\n",
      "Epoch  66 Batch  202/269   train_loss = 3.492\n",
      "Epoch  66 Batch  203/269   train_loss = 3.635\n",
      "Epoch  66 Batch  204/269   train_loss = 3.811\n",
      "Epoch  66 Batch  205/269   train_loss = 3.769\n",
      "Epoch  66 Batch  206/269   train_loss = 3.698\n",
      "Epoch  66 Batch  207/269   train_loss = 3.625\n",
      "Epoch  66 Batch  208/269   train_loss = 3.736\n",
      "Epoch  66 Batch  209/269   train_loss = 3.800\n",
      "Epoch  66 Batch  210/269   train_loss = 3.643\n",
      "Epoch  66 Batch  211/269   train_loss = 3.625\n",
      "Epoch  66 Batch  212/269   train_loss = 3.998\n",
      "Epoch  66 Batch  213/269   train_loss = 3.613\n",
      "Epoch  66 Batch  214/269   train_loss = 3.775\n",
      "Epoch  66 Batch  215/269   train_loss = 3.945\n",
      "Epoch  66 Batch  216/269   train_loss = 3.892\n",
      "Epoch  66 Batch  217/269   train_loss = 3.579\n",
      "Epoch  66 Batch  218/269   train_loss = 3.774\n",
      "Epoch  66 Batch  219/269   train_loss = 3.483\n",
      "Epoch  66 Batch  220/269   train_loss = 3.870\n",
      "Epoch  66 Batch  221/269   train_loss = 3.581\n",
      "Epoch  66 Batch  222/269   train_loss = 3.742\n",
      "Epoch  66 Batch  223/269   train_loss = 3.541\n",
      "Epoch  66 Batch  224/269   train_loss = 3.813\n",
      "Epoch  66 Batch  225/269   train_loss = 3.875\n",
      "Epoch  66 Batch  226/269   train_loss = 3.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  66 Batch  227/269   train_loss = 3.450\n",
      "Epoch  66 Batch  228/269   train_loss = 3.722\n",
      "Epoch  66 Batch  229/269   train_loss = 3.882\n",
      "Epoch  66 Batch  230/269   train_loss = 3.831\n",
      "Epoch  66 Batch  231/269   train_loss = 3.669\n",
      "Epoch  66 Batch  232/269   train_loss = 3.687\n",
      "Epoch  66 Batch  233/269   train_loss = 3.795\n",
      "Epoch  66 Batch  234/269   train_loss = 3.703\n",
      "Epoch  66 Batch  235/269   train_loss = 4.004\n",
      "Epoch  66 Batch  236/269   train_loss = 3.595\n",
      "Epoch  66 Batch  237/269   train_loss = 3.253\n",
      "Epoch  66 Batch  238/269   train_loss = 3.496\n",
      "Epoch  66 Batch  239/269   train_loss = 3.964\n",
      "Epoch  66 Batch  240/269   train_loss = 3.594\n",
      "Epoch  66 Batch  241/269   train_loss = 3.950\n",
      "Epoch  66 Batch  242/269   train_loss = 3.513\n",
      "Epoch  66 Batch  243/269   train_loss = 3.900\n",
      "Epoch  66 Batch  244/269   train_loss = 3.707\n",
      "Epoch  66 Batch  245/269   train_loss = 3.659\n",
      "Epoch  66 Batch  246/269   train_loss = 3.341\n",
      "Epoch  66 Batch  247/269   train_loss = 3.775\n",
      "Epoch  66 Batch  248/269   train_loss = 3.740\n",
      "Epoch  66 Batch  249/269   train_loss = 3.606\n",
      "Epoch  66 Batch  250/269   train_loss = 3.528\n",
      "Epoch  66 Batch  251/269   train_loss = 3.956\n",
      "Epoch  66 Batch  252/269   train_loss = 3.853\n",
      "Epoch  66 Batch  253/269   train_loss = 3.656\n",
      "Epoch  66 Batch  254/269   train_loss = 3.610\n",
      "Epoch  66 Batch  255/269   train_loss = 3.712\n",
      "Epoch  66 Batch  256/269   train_loss = 3.586\n",
      "Epoch  66 Batch  257/269   train_loss = 3.569\n",
      "Epoch  66 Batch  258/269   train_loss = 3.479\n",
      "Epoch  66 Batch  259/269   train_loss = 3.541\n",
      "Epoch  66 Batch  260/269   train_loss = 3.794\n",
      "Epoch  66 Batch  261/269   train_loss = 3.822\n",
      "Epoch  66 Batch  262/269   train_loss = 3.659\n",
      "Epoch  66 Batch  263/269   train_loss = 3.501\n",
      "Epoch  66 Batch  264/269   train_loss = 3.900\n",
      "Epoch  66 Batch  265/269   train_loss = 3.775\n",
      "Epoch  66 Batch  266/269   train_loss = 3.625\n",
      "Epoch  66 Batch  267/269   train_loss = 3.725\n",
      "Epoch  66 Batch  268/269   train_loss = 3.825\n",
      "Epoch  67 Batch    0/269   train_loss = 3.662\n",
      "Epoch  67 Batch    1/269   train_loss = 3.452\n",
      "Epoch  67 Batch    2/269   train_loss = 3.600\n",
      "Epoch  67 Batch    3/269   train_loss = 3.663\n",
      "Epoch  67 Batch    4/269   train_loss = 4.085\n",
      "Epoch  67 Batch    5/269   train_loss = 3.647\n",
      "Epoch  67 Batch    6/269   train_loss = 3.603\n",
      "Epoch  67 Batch    7/269   train_loss = 3.437\n",
      "Epoch  67 Batch    8/269   train_loss = 3.760\n",
      "Epoch  67 Batch    9/269   train_loss = 3.446\n",
      "Epoch  67 Batch   10/269   train_loss = 3.609\n",
      "Epoch  67 Batch   11/269   train_loss = 3.646\n",
      "Epoch  67 Batch   12/269   train_loss = 3.407\n",
      "Epoch  67 Batch   13/269   train_loss = 3.712\n",
      "Epoch  67 Batch   14/269   train_loss = 3.570\n",
      "Epoch  67 Batch   15/269   train_loss = 3.897\n",
      "Epoch  67 Batch   16/269   train_loss = 3.604\n",
      "Epoch  67 Batch   17/269   train_loss = 3.679\n",
      "Epoch  67 Batch   18/269   train_loss = 3.639\n",
      "Epoch  67 Batch   19/269   train_loss = 3.650\n",
      "Epoch  67 Batch   20/269   train_loss = 4.024\n",
      "Epoch  67 Batch   21/269   train_loss = 3.847\n",
      "Epoch  67 Batch   22/269   train_loss = 3.622\n",
      "Epoch  67 Batch   23/269   train_loss = 3.799\n",
      "Epoch  67 Batch   24/269   train_loss = 3.509\n",
      "Epoch  67 Batch   25/269   train_loss = 3.914\n",
      "Epoch  67 Batch   26/269   train_loss = 3.746\n",
      "Epoch  67 Batch   27/269   train_loss = 3.586\n",
      "Epoch  67 Batch   28/269   train_loss = 3.750\n",
      "Epoch  67 Batch   29/269   train_loss = 3.738\n",
      "Epoch  67 Batch   30/269   train_loss = 4.073\n",
      "Epoch  67 Batch   31/269   train_loss = 3.721\n",
      "Epoch  67 Batch   32/269   train_loss = 3.587\n",
      "Epoch  67 Batch   33/269   train_loss = 3.388\n",
      "Epoch  67 Batch   34/269   train_loss = 3.598\n",
      "Epoch  67 Batch   35/269   train_loss = 3.471\n",
      "Epoch  67 Batch   36/269   train_loss = 3.508\n",
      "Epoch  67 Batch   37/269   train_loss = 3.739\n",
      "Epoch  67 Batch   38/269   train_loss = 3.426\n",
      "Epoch  67 Batch   39/269   train_loss = 3.742\n",
      "Epoch  67 Batch   40/269   train_loss = 3.587\n",
      "Epoch  67 Batch   41/269   train_loss = 3.552\n",
      "Epoch  67 Batch   42/269   train_loss = 4.031\n",
      "Epoch  67 Batch   43/269   train_loss = 3.933\n",
      "Epoch  67 Batch   44/269   train_loss = 3.733\n",
      "Epoch  67 Batch   45/269   train_loss = 3.391\n",
      "Epoch  67 Batch   46/269   train_loss = 3.754\n",
      "Epoch  67 Batch   47/269   train_loss = 3.401\n",
      "Epoch  67 Batch   48/269   train_loss = 3.871\n",
      "Epoch  67 Batch   49/269   train_loss = 3.885\n",
      "Epoch  67 Batch   50/269   train_loss = 3.820\n",
      "Epoch  67 Batch   51/269   train_loss = 3.728\n",
      "Epoch  67 Batch   52/269   train_loss = 3.737\n",
      "Epoch  67 Batch   53/269   train_loss = 3.941\n",
      "Epoch  67 Batch   54/269   train_loss = 3.646\n",
      "Epoch  67 Batch   55/269   train_loss = 3.784\n",
      "Epoch  67 Batch   56/269   train_loss = 3.560\n",
      "Epoch  67 Batch   57/269   train_loss = 3.900\n",
      "Epoch  67 Batch   58/269   train_loss = 3.506\n",
      "Epoch  67 Batch   59/269   train_loss = 3.583\n",
      "Epoch  67 Batch   60/269   train_loss = 3.775\n",
      "Epoch  67 Batch   61/269   train_loss = 3.914\n",
      "Epoch  67 Batch   62/269   train_loss = 3.535\n",
      "Epoch  67 Batch   63/269   train_loss = 3.884\n",
      "Epoch  67 Batch   64/269   train_loss = 3.609\n",
      "Epoch  67 Batch   65/269   train_loss = 3.742\n",
      "Epoch  67 Batch   66/269   train_loss = 3.603\n",
      "Epoch  67 Batch   67/269   train_loss = 3.612\n",
      "Epoch  67 Batch   68/269   train_loss = 3.742\n",
      "Epoch  67 Batch   69/269   train_loss = 3.745\n",
      "Epoch  67 Batch   70/269   train_loss = 3.436\n",
      "Epoch  67 Batch   71/269   train_loss = 3.744\n",
      "Epoch  67 Batch   72/269   train_loss = 3.463\n",
      "Epoch  67 Batch   73/269   train_loss = 3.691\n",
      "Epoch  67 Batch   74/269   train_loss = 3.645\n",
      "Epoch  67 Batch   75/269   train_loss = 3.833\n",
      "Epoch  67 Batch   76/269   train_loss = 3.787\n",
      "Epoch  67 Batch   77/269   train_loss = 3.373\n",
      "Epoch  67 Batch   78/269   train_loss = 3.836\n",
      "Epoch  67 Batch   79/269   train_loss = 3.896\n",
      "Epoch  67 Batch   80/269   train_loss = 3.786\n",
      "Epoch  67 Batch   81/269   train_loss = 3.630\n",
      "Epoch  67 Batch   82/269   train_loss = 3.774\n",
      "Epoch  67 Batch   83/269   train_loss = 3.626\n",
      "Epoch  67 Batch   84/269   train_loss = 3.864\n",
      "Epoch  67 Batch   85/269   train_loss = 4.083\n",
      "Epoch  67 Batch   86/269   train_loss = 4.045\n",
      "Epoch  67 Batch   87/269   train_loss = 3.640\n",
      "Epoch  67 Batch   88/269   train_loss = 3.805\n",
      "Epoch  67 Batch   89/269   train_loss = 3.724\n",
      "Epoch  67 Batch   90/269   train_loss = 3.559\n",
      "Epoch  67 Batch   91/269   train_loss = 3.811\n",
      "Epoch  67 Batch   92/269   train_loss = 3.818\n",
      "Epoch  67 Batch   93/269   train_loss = 3.594\n",
      "Epoch  67 Batch   94/269   train_loss = 4.057\n",
      "Epoch  67 Batch   95/269   train_loss = 3.494\n",
      "Epoch  67 Batch   96/269   train_loss = 3.632\n",
      "Epoch  67 Batch   97/269   train_loss = 3.684\n",
      "Epoch  67 Batch   98/269   train_loss = 3.939\n",
      "Epoch  67 Batch   99/269   train_loss = 3.757\n",
      "Epoch  67 Batch  100/269   train_loss = 3.728\n",
      "Epoch  67 Batch  101/269   train_loss = 3.764\n",
      "Epoch  67 Batch  102/269   train_loss = 3.508\n",
      "Epoch  67 Batch  103/269   train_loss = 3.615\n",
      "Epoch  67 Batch  104/269   train_loss = 3.613\n",
      "Epoch  67 Batch  105/269   train_loss = 3.904\n",
      "Epoch  67 Batch  106/269   train_loss = 3.921\n",
      "Epoch  67 Batch  107/269   train_loss = 3.948\n",
      "Epoch  67 Batch  108/269   train_loss = 3.715\n",
      "Epoch  67 Batch  109/269   train_loss = 3.736\n",
      "Epoch  67 Batch  110/269   train_loss = 3.749\n",
      "Epoch  67 Batch  111/269   train_loss = 3.869\n",
      "Epoch  67 Batch  112/269   train_loss = 3.915\n",
      "Epoch  67 Batch  113/269   train_loss = 3.740\n",
      "Epoch  67 Batch  114/269   train_loss = 3.696\n",
      "Epoch  67 Batch  115/269   train_loss = 3.574\n",
      "Epoch  67 Batch  116/269   train_loss = 3.559\n",
      "Epoch  67 Batch  117/269   train_loss = 3.613\n",
      "Epoch  67 Batch  118/269   train_loss = 3.913\n",
      "Epoch  67 Batch  119/269   train_loss = 3.494\n",
      "Epoch  67 Batch  120/269   train_loss = 3.710\n",
      "Epoch  67 Batch  121/269   train_loss = 3.670\n",
      "Epoch  67 Batch  122/269   train_loss = 3.702\n",
      "Epoch  67 Batch  123/269   train_loss = 3.517\n",
      "Epoch  67 Batch  124/269   train_loss = 3.866\n",
      "Epoch  67 Batch  125/269   train_loss = 3.507\n",
      "Epoch  67 Batch  126/269   train_loss = 3.433\n",
      "Epoch  67 Batch  127/269   train_loss = 3.672\n",
      "Epoch  67 Batch  128/269   train_loss = 3.640\n",
      "Epoch  67 Batch  129/269   train_loss = 3.635\n",
      "Epoch  67 Batch  130/269   train_loss = 3.775\n",
      "Epoch  67 Batch  131/269   train_loss = 3.383\n",
      "Epoch  67 Batch  132/269   train_loss = 3.732\n",
      "Epoch  67 Batch  133/269   train_loss = 3.509\n",
      "Epoch  67 Batch  134/269   train_loss = 3.877\n",
      "Epoch  67 Batch  135/269   train_loss = 3.594\n",
      "Epoch  67 Batch  136/269   train_loss = 3.420\n",
      "Epoch  67 Batch  137/269   train_loss = 3.810\n",
      "Epoch  67 Batch  138/269   train_loss = 3.550\n",
      "Epoch  67 Batch  139/269   train_loss = 3.692\n",
      "Epoch  67 Batch  140/269   train_loss = 3.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  67 Batch  141/269   train_loss = 4.045\n",
      "Epoch  67 Batch  142/269   train_loss = 3.479\n",
      "Epoch  67 Batch  143/269   train_loss = 3.703\n",
      "Epoch  67 Batch  144/269   train_loss = 3.926\n",
      "Epoch  67 Batch  145/269   train_loss = 3.659\n",
      "Epoch  67 Batch  146/269   train_loss = 3.901\n",
      "Epoch  67 Batch  147/269   train_loss = 3.507\n",
      "Epoch  67 Batch  148/269   train_loss = 3.820\n",
      "Epoch  67 Batch  149/269   train_loss = 3.605\n",
      "Epoch  67 Batch  150/269   train_loss = 3.823\n",
      "Epoch  67 Batch  151/269   train_loss = 3.762\n",
      "Epoch  67 Batch  152/269   train_loss = 3.712\n",
      "Epoch  67 Batch  153/269   train_loss = 3.734\n",
      "Epoch  67 Batch  154/269   train_loss = 3.609\n",
      "Epoch  67 Batch  155/269   train_loss = 3.596\n",
      "Epoch  67 Batch  156/269   train_loss = 4.044\n",
      "Epoch  67 Batch  157/269   train_loss = 3.742\n",
      "Epoch  67 Batch  158/269   train_loss = 3.738\n",
      "Epoch  67 Batch  159/269   train_loss = 3.730\n",
      "Epoch  67 Batch  160/269   train_loss = 3.633\n",
      "Epoch  67 Batch  161/269   train_loss = 3.778\n",
      "Epoch  67 Batch  162/269   train_loss = 3.844\n",
      "Epoch  67 Batch  163/269   train_loss = 3.602\n",
      "Epoch  67 Batch  164/269   train_loss = 3.894\n",
      "Epoch  67 Batch  165/269   train_loss = 3.931\n",
      "Epoch  67 Batch  166/269   train_loss = 3.747\n",
      "Epoch  67 Batch  167/269   train_loss = 3.548\n",
      "Epoch  67 Batch  168/269   train_loss = 3.757\n",
      "Epoch  67 Batch  169/269   train_loss = 3.979\n",
      "Epoch  67 Batch  170/269   train_loss = 3.445\n",
      "Epoch  67 Batch  171/269   train_loss = 3.740\n",
      "Epoch  67 Batch  172/269   train_loss = 3.550\n",
      "Epoch  67 Batch  173/269   train_loss = 3.762\n",
      "Epoch  67 Batch  174/269   train_loss = 3.865\n",
      "Epoch  67 Batch  175/269   train_loss = 3.759\n",
      "Epoch  67 Batch  176/269   train_loss = 3.697\n",
      "Epoch  67 Batch  177/269   train_loss = 3.738\n",
      "Epoch  67 Batch  178/269   train_loss = 3.925\n",
      "Epoch  67 Batch  179/269   train_loss = 3.722\n",
      "Epoch  67 Batch  180/269   train_loss = 3.578\n",
      "Epoch  67 Batch  181/269   train_loss = 3.806\n",
      "Epoch  67 Batch  182/269   train_loss = 3.666\n",
      "Epoch  67 Batch  183/269   train_loss = 3.597\n",
      "Epoch  67 Batch  184/269   train_loss = 3.729\n",
      "Epoch  67 Batch  185/269   train_loss = 3.778\n",
      "Epoch  67 Batch  186/269   train_loss = 3.712\n",
      "Epoch  67 Batch  187/269   train_loss = 3.865\n",
      "Epoch  67 Batch  188/269   train_loss = 3.642\n",
      "Epoch  67 Batch  189/269   train_loss = 3.600\n",
      "Epoch  67 Batch  190/269   train_loss = 4.053\n",
      "Epoch  67 Batch  191/269   train_loss = 3.775\n",
      "Epoch  67 Batch  192/269   train_loss = 3.750\n",
      "Epoch  67 Batch  193/269   train_loss = 3.720\n",
      "Epoch  67 Batch  194/269   train_loss = 3.771\n",
      "Epoch  67 Batch  195/269   train_loss = 3.702\n",
      "Epoch  67 Batch  196/269   train_loss = 3.780\n",
      "Epoch  67 Batch  197/269   train_loss = 3.901\n",
      "Epoch  67 Batch  198/269   train_loss = 3.790\n",
      "Epoch  67 Batch  199/269   train_loss = 3.822\n",
      "Epoch  67 Batch  200/269   train_loss = 3.585\n",
      "Epoch  67 Batch  201/269   train_loss = 3.733\n",
      "Epoch  67 Batch  202/269   train_loss = 3.516\n",
      "Epoch  67 Batch  203/269   train_loss = 3.637\n",
      "Epoch  67 Batch  204/269   train_loss = 3.863\n",
      "Epoch  67 Batch  205/269   train_loss = 3.775\n",
      "Epoch  67 Batch  206/269   train_loss = 3.738\n",
      "Epoch  67 Batch  207/269   train_loss = 3.607\n",
      "Epoch  67 Batch  208/269   train_loss = 3.752\n",
      "Epoch  67 Batch  209/269   train_loss = 3.809\n",
      "Epoch  67 Batch  210/269   train_loss = 3.668\n",
      "Epoch  67 Batch  211/269   train_loss = 3.617\n",
      "Epoch  67 Batch  212/269   train_loss = 4.015\n",
      "Epoch  67 Batch  213/269   train_loss = 3.589\n",
      "Epoch  67 Batch  214/269   train_loss = 3.707\n",
      "Epoch  67 Batch  215/269   train_loss = 3.899\n",
      "Epoch  67 Batch  216/269   train_loss = 3.866\n",
      "Epoch  67 Batch  217/269   train_loss = 3.549\n",
      "Epoch  67 Batch  218/269   train_loss = 3.790\n",
      "Epoch  67 Batch  219/269   train_loss = 3.472\n",
      "Epoch  67 Batch  220/269   train_loss = 3.859\n",
      "Epoch  67 Batch  221/269   train_loss = 3.582\n",
      "Epoch  67 Batch  222/269   train_loss = 3.736\n",
      "Epoch  67 Batch  223/269   train_loss = 3.524\n",
      "Epoch  67 Batch  224/269   train_loss = 3.884\n",
      "Epoch  67 Batch  225/269   train_loss = 3.868\n",
      "Epoch  67 Batch  226/269   train_loss = 3.692\n",
      "Epoch  67 Batch  227/269   train_loss = 3.430\n",
      "Epoch  67 Batch  228/269   train_loss = 3.684\n",
      "Epoch  67 Batch  229/269   train_loss = 3.887\n",
      "Epoch  67 Batch  230/269   train_loss = 3.805\n",
      "Epoch  67 Batch  231/269   train_loss = 3.690\n",
      "Epoch  67 Batch  232/269   train_loss = 3.696\n",
      "Epoch  67 Batch  233/269   train_loss = 3.781\n",
      "Epoch  67 Batch  234/269   train_loss = 3.682\n",
      "Epoch  67 Batch  235/269   train_loss = 3.978\n",
      "Epoch  67 Batch  236/269   train_loss = 3.588\n",
      "Epoch  67 Batch  237/269   train_loss = 3.253\n",
      "Epoch  67 Batch  238/269   train_loss = 3.458\n",
      "Epoch  67 Batch  239/269   train_loss = 3.974\n",
      "Epoch  67 Batch  240/269   train_loss = 3.580\n",
      "Epoch  67 Batch  241/269   train_loss = 3.923\n",
      "Epoch  67 Batch  242/269   train_loss = 3.510\n",
      "Epoch  67 Batch  243/269   train_loss = 3.864\n",
      "Epoch  67 Batch  244/269   train_loss = 3.707\n",
      "Epoch  67 Batch  245/269   train_loss = 3.659\n",
      "Epoch  67 Batch  246/269   train_loss = 3.324\n",
      "Epoch  67 Batch  247/269   train_loss = 3.759\n",
      "Epoch  67 Batch  248/269   train_loss = 3.752\n",
      "Epoch  67 Batch  249/269   train_loss = 3.579\n",
      "Epoch  67 Batch  250/269   train_loss = 3.499\n",
      "Epoch  67 Batch  251/269   train_loss = 3.971\n",
      "Epoch  67 Batch  252/269   train_loss = 3.828\n",
      "Epoch  67 Batch  253/269   train_loss = 3.624\n",
      "Epoch  67 Batch  254/269   train_loss = 3.586\n",
      "Epoch  67 Batch  255/269   train_loss = 3.701\n",
      "Epoch  67 Batch  256/269   train_loss = 3.600\n",
      "Epoch  67 Batch  257/269   train_loss = 3.573\n",
      "Epoch  67 Batch  258/269   train_loss = 3.471\n",
      "Epoch  67 Batch  259/269   train_loss = 3.541\n",
      "Epoch  67 Batch  260/269   train_loss = 3.805\n",
      "Epoch  67 Batch  261/269   train_loss = 3.806\n",
      "Epoch  67 Batch  262/269   train_loss = 3.614\n",
      "Epoch  67 Batch  263/269   train_loss = 3.517\n",
      "Epoch  67 Batch  264/269   train_loss = 3.881\n",
      "Epoch  67 Batch  265/269   train_loss = 3.759\n",
      "Epoch  67 Batch  266/269   train_loss = 3.624\n",
      "Epoch  67 Batch  267/269   train_loss = 3.731\n",
      "Epoch  67 Batch  268/269   train_loss = 3.805\n",
      "Epoch  68 Batch    0/269   train_loss = 3.626\n",
      "Epoch  68 Batch    1/269   train_loss = 3.458\n",
      "Epoch  68 Batch    2/269   train_loss = 3.577\n",
      "Epoch  68 Batch    3/269   train_loss = 3.676\n",
      "Epoch  68 Batch    4/269   train_loss = 4.109\n",
      "Epoch  68 Batch    5/269   train_loss = 3.662\n",
      "Epoch  68 Batch    6/269   train_loss = 3.602\n",
      "Epoch  68 Batch    7/269   train_loss = 3.440\n",
      "Epoch  68 Batch    8/269   train_loss = 3.765\n",
      "Epoch  68 Batch    9/269   train_loss = 3.459\n",
      "Epoch  68 Batch   10/269   train_loss = 3.622\n",
      "Epoch  68 Batch   11/269   train_loss = 3.623\n",
      "Epoch  68 Batch   12/269   train_loss = 3.405\n",
      "Epoch  68 Batch   13/269   train_loss = 3.729\n",
      "Epoch  68 Batch   14/269   train_loss = 3.601\n",
      "Epoch  68 Batch   15/269   train_loss = 3.908\n",
      "Epoch  68 Batch   16/269   train_loss = 3.629\n",
      "Epoch  68 Batch   17/269   train_loss = 3.701\n",
      "Epoch  68 Batch   18/269   train_loss = 3.643\n",
      "Epoch  68 Batch   19/269   train_loss = 3.666\n",
      "Epoch  68 Batch   20/269   train_loss = 4.001\n",
      "Epoch  68 Batch   21/269   train_loss = 3.841\n",
      "Epoch  68 Batch   22/269   train_loss = 3.607\n",
      "Epoch  68 Batch   23/269   train_loss = 3.784\n",
      "Epoch  68 Batch   24/269   train_loss = 3.476\n",
      "Epoch  68 Batch   25/269   train_loss = 3.917\n",
      "Epoch  68 Batch   26/269   train_loss = 3.763\n",
      "Epoch  68 Batch   27/269   train_loss = 3.578\n",
      "Epoch  68 Batch   28/269   train_loss = 3.753\n",
      "Epoch  68 Batch   29/269   train_loss = 3.732\n",
      "Epoch  68 Batch   30/269   train_loss = 4.063\n",
      "Epoch  68 Batch   31/269   train_loss = 3.690\n",
      "Epoch  68 Batch   32/269   train_loss = 3.600\n",
      "Epoch  68 Batch   33/269   train_loss = 3.370\n",
      "Epoch  68 Batch   34/269   train_loss = 3.617\n",
      "Epoch  68 Batch   35/269   train_loss = 3.481\n",
      "Epoch  68 Batch   36/269   train_loss = 3.514\n",
      "Epoch  68 Batch   37/269   train_loss = 3.722\n",
      "Epoch  68 Batch   38/269   train_loss = 3.437\n",
      "Epoch  68 Batch   39/269   train_loss = 3.768\n",
      "Epoch  68 Batch   40/269   train_loss = 3.578\n",
      "Epoch  68 Batch   41/269   train_loss = 3.537\n",
      "Epoch  68 Batch   42/269   train_loss = 4.033\n",
      "Epoch  68 Batch   43/269   train_loss = 3.977\n",
      "Epoch  68 Batch   44/269   train_loss = 3.730\n",
      "Epoch  68 Batch   45/269   train_loss = 3.415\n",
      "Epoch  68 Batch   46/269   train_loss = 3.768\n",
      "Epoch  68 Batch   47/269   train_loss = 3.430\n",
      "Epoch  68 Batch   48/269   train_loss = 3.869\n",
      "Epoch  68 Batch   49/269   train_loss = 3.899\n",
      "Epoch  68 Batch   50/269   train_loss = 3.808\n",
      "Epoch  68 Batch   51/269   train_loss = 3.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  68 Batch   52/269   train_loss = 3.734\n",
      "Epoch  68 Batch   53/269   train_loss = 3.948\n",
      "Epoch  68 Batch   54/269   train_loss = 3.653\n",
      "Epoch  68 Batch   55/269   train_loss = 3.794\n",
      "Epoch  68 Batch   56/269   train_loss = 3.585\n",
      "Epoch  68 Batch   57/269   train_loss = 3.906\n",
      "Epoch  68 Batch   58/269   train_loss = 3.502\n",
      "Epoch  68 Batch   59/269   train_loss = 3.569\n",
      "Epoch  68 Batch   60/269   train_loss = 3.789\n",
      "Epoch  68 Batch   61/269   train_loss = 3.891\n",
      "Epoch  68 Batch   62/269   train_loss = 3.536\n",
      "Epoch  68 Batch   63/269   train_loss = 3.872\n",
      "Epoch  68 Batch   64/269   train_loss = 3.604\n",
      "Epoch  68 Batch   65/269   train_loss = 3.727\n",
      "Epoch  68 Batch   66/269   train_loss = 3.623\n",
      "Epoch  68 Batch   67/269   train_loss = 3.638\n",
      "Epoch  68 Batch   68/269   train_loss = 3.733\n",
      "Epoch  68 Batch   69/269   train_loss = 3.728\n",
      "Epoch  68 Batch   70/269   train_loss = 3.445\n",
      "Epoch  68 Batch   71/269   train_loss = 3.783\n",
      "Epoch  68 Batch   72/269   train_loss = 3.435\n",
      "Epoch  68 Batch   73/269   train_loss = 3.812\n",
      "Epoch  68 Batch   74/269   train_loss = 3.645\n",
      "Epoch  68 Batch   75/269   train_loss = 3.814\n",
      "Epoch  68 Batch   76/269   train_loss = 3.778\n",
      "Epoch  68 Batch   77/269   train_loss = 3.400\n",
      "Epoch  68 Batch   78/269   train_loss = 3.836\n",
      "Epoch  68 Batch   79/269   train_loss = 3.905\n",
      "Epoch  68 Batch   80/269   train_loss = 3.770\n",
      "Epoch  68 Batch   81/269   train_loss = 3.589\n",
      "Epoch  68 Batch   82/269   train_loss = 3.776\n",
      "Epoch  68 Batch   83/269   train_loss = 3.630\n",
      "Epoch  68 Batch   84/269   train_loss = 3.888\n",
      "Epoch  68 Batch   85/269   train_loss = 4.081\n",
      "Epoch  68 Batch   86/269   train_loss = 4.047\n",
      "Epoch  68 Batch   87/269   train_loss = 3.682\n",
      "Epoch  68 Batch   88/269   train_loss = 3.813\n",
      "Epoch  68 Batch   89/269   train_loss = 3.725\n",
      "Epoch  68 Batch   90/269   train_loss = 3.545\n",
      "Epoch  68 Batch   91/269   train_loss = 3.809\n",
      "Epoch  68 Batch   92/269   train_loss = 3.855\n",
      "Epoch  68 Batch   93/269   train_loss = 3.589\n",
      "Epoch  68 Batch   94/269   train_loss = 4.070\n",
      "Epoch  68 Batch   95/269   train_loss = 3.496\n",
      "Epoch  68 Batch   96/269   train_loss = 3.617\n",
      "Epoch  68 Batch   97/269   train_loss = 3.663\n",
      "Epoch  68 Batch   98/269   train_loss = 3.935\n",
      "Epoch  68 Batch   99/269   train_loss = 3.721\n",
      "Epoch  68 Batch  100/269   train_loss = 3.705\n",
      "Epoch  68 Batch  101/269   train_loss = 3.786\n",
      "Epoch  68 Batch  102/269   train_loss = 3.497\n",
      "Epoch  68 Batch  103/269   train_loss = 3.585\n",
      "Epoch  68 Batch  104/269   train_loss = 3.613\n",
      "Epoch  68 Batch  105/269   train_loss = 3.874\n",
      "Epoch  68 Batch  106/269   train_loss = 3.944\n",
      "Epoch  68 Batch  107/269   train_loss = 3.960\n",
      "Epoch  68 Batch  108/269   train_loss = 3.713\n",
      "Epoch  68 Batch  109/269   train_loss = 3.756\n",
      "Epoch  68 Batch  110/269   train_loss = 3.712\n",
      "Epoch  68 Batch  111/269   train_loss = 3.873\n",
      "Epoch  68 Batch  112/269   train_loss = 3.914\n",
      "Epoch  68 Batch  113/269   train_loss = 3.745\n",
      "Epoch  68 Batch  114/269   train_loss = 3.718\n",
      "Epoch  68 Batch  115/269   train_loss = 3.625\n",
      "Epoch  68 Batch  116/269   train_loss = 3.567\n",
      "Epoch  68 Batch  117/269   train_loss = 3.608\n",
      "Epoch  68 Batch  118/269   train_loss = 3.926\n",
      "Epoch  68 Batch  119/269   train_loss = 3.482\n",
      "Epoch  68 Batch  120/269   train_loss = 3.712\n",
      "Epoch  68 Batch  121/269   train_loss = 3.691\n",
      "Epoch  68 Batch  122/269   train_loss = 3.700\n",
      "Epoch  68 Batch  123/269   train_loss = 3.525\n",
      "Epoch  68 Batch  124/269   train_loss = 3.860\n",
      "Epoch  68 Batch  125/269   train_loss = 3.500\n",
      "Epoch  68 Batch  126/269   train_loss = 3.425\n",
      "Epoch  68 Batch  127/269   train_loss = 3.682\n",
      "Epoch  68 Batch  128/269   train_loss = 3.639\n",
      "Epoch  68 Batch  129/269   train_loss = 3.629\n",
      "Epoch  68 Batch  130/269   train_loss = 3.779\n",
      "Epoch  68 Batch  131/269   train_loss = 3.384\n",
      "Epoch  68 Batch  132/269   train_loss = 3.741\n",
      "Epoch  68 Batch  133/269   train_loss = 3.491\n",
      "Epoch  68 Batch  134/269   train_loss = 3.880\n",
      "Epoch  68 Batch  135/269   train_loss = 3.583\n",
      "Epoch  68 Batch  136/269   train_loss = 3.417\n",
      "Epoch  68 Batch  137/269   train_loss = 3.807\n",
      "Epoch  68 Batch  138/269   train_loss = 3.541\n",
      "Epoch  68 Batch  139/269   train_loss = 3.678\n",
      "Epoch  68 Batch  140/269   train_loss = 3.705\n",
      "Epoch  68 Batch  141/269   train_loss = 4.016\n",
      "Epoch  68 Batch  142/269   train_loss = 3.468\n",
      "Epoch  68 Batch  143/269   train_loss = 3.699\n",
      "Epoch  68 Batch  144/269   train_loss = 3.940\n",
      "Epoch  68 Batch  145/269   train_loss = 3.639\n",
      "Epoch  68 Batch  146/269   train_loss = 3.879\n",
      "Epoch  68 Batch  147/269   train_loss = 3.496\n",
      "Epoch  68 Batch  148/269   train_loss = 3.829\n",
      "Epoch  68 Batch  149/269   train_loss = 3.589\n",
      "Epoch  68 Batch  150/269   train_loss = 3.806\n",
      "Epoch  68 Batch  151/269   train_loss = 3.744\n",
      "Epoch  68 Batch  152/269   train_loss = 3.715\n",
      "Epoch  68 Batch  153/269   train_loss = 3.741\n",
      "Epoch  68 Batch  154/269   train_loss = 3.636\n",
      "Epoch  68 Batch  155/269   train_loss = 3.579\n",
      "Epoch  68 Batch  156/269   train_loss = 4.030\n",
      "Epoch  68 Batch  157/269   train_loss = 3.760\n",
      "Epoch  68 Batch  158/269   train_loss = 3.749\n",
      "Epoch  68 Batch  159/269   train_loss = 3.726\n",
      "Epoch  68 Batch  160/269   train_loss = 3.618\n",
      "Epoch  68 Batch  161/269   train_loss = 3.796\n",
      "Epoch  68 Batch  162/269   train_loss = 3.841\n",
      "Epoch  68 Batch  163/269   train_loss = 3.593\n",
      "Epoch  68 Batch  164/269   train_loss = 3.896\n",
      "Epoch  68 Batch  165/269   train_loss = 3.920\n",
      "Epoch  68 Batch  166/269   train_loss = 3.769\n",
      "Epoch  68 Batch  167/269   train_loss = 3.551\n",
      "Epoch  68 Batch  168/269   train_loss = 3.793\n",
      "Epoch  68 Batch  169/269   train_loss = 4.003\n",
      "Epoch  68 Batch  170/269   train_loss = 3.424\n",
      "Epoch  68 Batch  171/269   train_loss = 3.740\n",
      "Epoch  68 Batch  172/269   train_loss = 3.534\n",
      "Epoch  68 Batch  173/269   train_loss = 3.748\n",
      "Epoch  68 Batch  174/269   train_loss = 3.875\n",
      "Epoch  68 Batch  175/269   train_loss = 3.768\n",
      "Epoch  68 Batch  176/269   train_loss = 3.685\n",
      "Epoch  68 Batch  177/269   train_loss = 3.762\n",
      "Epoch  68 Batch  178/269   train_loss = 3.949\n",
      "Epoch  68 Batch  179/269   train_loss = 3.757\n",
      "Epoch  68 Batch  180/269   train_loss = 3.621\n",
      "Epoch  68 Batch  181/269   train_loss = 3.799\n",
      "Epoch  68 Batch  182/269   train_loss = 3.651\n",
      "Epoch  68 Batch  183/269   train_loss = 3.615\n",
      "Epoch  68 Batch  184/269   train_loss = 3.718\n",
      "Epoch  68 Batch  185/269   train_loss = 3.779\n",
      "Epoch  68 Batch  186/269   train_loss = 3.673\n",
      "Epoch  68 Batch  187/269   train_loss = 3.881\n",
      "Epoch  68 Batch  188/269   train_loss = 3.614\n",
      "Epoch  68 Batch  189/269   train_loss = 3.620\n",
      "Epoch  68 Batch  190/269   train_loss = 4.057\n",
      "Epoch  68 Batch  191/269   train_loss = 3.777\n",
      "Epoch  68 Batch  192/269   train_loss = 3.738\n",
      "Epoch  68 Batch  193/269   train_loss = 3.697\n",
      "Epoch  68 Batch  194/269   train_loss = 3.783\n",
      "Epoch  68 Batch  195/269   train_loss = 3.704\n",
      "Epoch  68 Batch  196/269   train_loss = 3.755\n",
      "Epoch  68 Batch  197/269   train_loss = 3.894\n",
      "Epoch  68 Batch  198/269   train_loss = 3.764\n",
      "Epoch  68 Batch  199/269   train_loss = 3.851\n",
      "Epoch  68 Batch  200/269   train_loss = 3.586\n",
      "Epoch  68 Batch  201/269   train_loss = 3.730\n",
      "Epoch  68 Batch  202/269   train_loss = 3.517\n",
      "Epoch  68 Batch  203/269   train_loss = 3.651\n",
      "Epoch  68 Batch  204/269   train_loss = 3.833\n",
      "Epoch  68 Batch  205/269   train_loss = 3.749\n",
      "Epoch  68 Batch  206/269   train_loss = 3.731\n",
      "Epoch  68 Batch  207/269   train_loss = 3.600\n",
      "Epoch  68 Batch  208/269   train_loss = 3.738\n",
      "Epoch  68 Batch  209/269   train_loss = 3.798\n",
      "Epoch  68 Batch  210/269   train_loss = 3.688\n",
      "Epoch  68 Batch  211/269   train_loss = 3.619\n",
      "Epoch  68 Batch  212/269   train_loss = 4.001\n",
      "Epoch  68 Batch  213/269   train_loss = 3.608\n",
      "Epoch  68 Batch  214/269   train_loss = 3.699\n",
      "Epoch  68 Batch  215/269   train_loss = 3.902\n",
      "Epoch  68 Batch  216/269   train_loss = 3.880\n",
      "Epoch  68 Batch  217/269   train_loss = 3.518\n",
      "Epoch  68 Batch  218/269   train_loss = 3.787\n",
      "Epoch  68 Batch  219/269   train_loss = 3.501\n",
      "Epoch  68 Batch  220/269   train_loss = 3.851\n",
      "Epoch  68 Batch  221/269   train_loss = 3.597\n",
      "Epoch  68 Batch  222/269   train_loss = 3.733\n",
      "Epoch  68 Batch  223/269   train_loss = 3.510\n",
      "Epoch  68 Batch  224/269   train_loss = 3.880\n",
      "Epoch  68 Batch  225/269   train_loss = 3.865\n",
      "Epoch  68 Batch  226/269   train_loss = 3.685\n",
      "Epoch  68 Batch  227/269   train_loss = 3.405\n",
      "Epoch  68 Batch  228/269   train_loss = 3.715\n",
      "Epoch  68 Batch  229/269   train_loss = 3.871\n",
      "Epoch  68 Batch  230/269   train_loss = 3.848\n",
      "Epoch  68 Batch  231/269   train_loss = 3.668\n",
      "Epoch  68 Batch  232/269   train_loss = 3.660\n",
      "Epoch  68 Batch  233/269   train_loss = 3.765\n",
      "Epoch  68 Batch  234/269   train_loss = 3.701\n",
      "Epoch  68 Batch  235/269   train_loss = 3.988\n",
      "Epoch  68 Batch  236/269   train_loss = 3.615\n",
      "Epoch  68 Batch  237/269   train_loss = 3.230\n",
      "Epoch  68 Batch  238/269   train_loss = 3.466\n",
      "Epoch  68 Batch  239/269   train_loss = 3.978\n",
      "Epoch  68 Batch  240/269   train_loss = 3.576\n",
      "Epoch  68 Batch  241/269   train_loss = 3.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  68 Batch  242/269   train_loss = 3.496\n",
      "Epoch  68 Batch  243/269   train_loss = 3.858\n",
      "Epoch  68 Batch  244/269   train_loss = 3.699\n",
      "Epoch  68 Batch  245/269   train_loss = 3.674\n",
      "Epoch  68 Batch  246/269   train_loss = 3.336\n",
      "Epoch  68 Batch  247/269   train_loss = 3.756\n",
      "Epoch  68 Batch  248/269   train_loss = 3.748\n",
      "Epoch  68 Batch  249/269   train_loss = 3.556\n",
      "Epoch  68 Batch  250/269   train_loss = 3.531\n",
      "Epoch  68 Batch  251/269   train_loss = 3.958\n",
      "Epoch  68 Batch  252/269   train_loss = 3.801\n",
      "Epoch  68 Batch  253/269   train_loss = 3.630\n",
      "Epoch  68 Batch  254/269   train_loss = 3.580\n",
      "Epoch  68 Batch  255/269   train_loss = 3.697\n",
      "Epoch  68 Batch  256/269   train_loss = 3.597\n",
      "Epoch  68 Batch  257/269   train_loss = 3.556\n",
      "Epoch  68 Batch  258/269   train_loss = 3.476\n",
      "Epoch  68 Batch  259/269   train_loss = 3.541\n",
      "Epoch  68 Batch  260/269   train_loss = 3.820\n",
      "Epoch  68 Batch  261/269   train_loss = 3.796\n",
      "Epoch  68 Batch  262/269   train_loss = 3.621\n",
      "Epoch  68 Batch  263/269   train_loss = 3.508\n",
      "Epoch  68 Batch  264/269   train_loss = 3.883\n",
      "Epoch  68 Batch  265/269   train_loss = 3.760\n",
      "Epoch  68 Batch  266/269   train_loss = 3.647\n",
      "Epoch  68 Batch  267/269   train_loss = 3.704\n",
      "Epoch  68 Batch  268/269   train_loss = 3.809\n",
      "Epoch  69 Batch    0/269   train_loss = 3.626\n",
      "Epoch  69 Batch    1/269   train_loss = 3.442\n",
      "Epoch  69 Batch    2/269   train_loss = 3.561\n",
      "Epoch  69 Batch    3/269   train_loss = 3.706\n",
      "Epoch  69 Batch    4/269   train_loss = 4.160\n",
      "Epoch  69 Batch    5/269   train_loss = 3.694\n",
      "Epoch  69 Batch    6/269   train_loss = 3.595\n",
      "Epoch  69 Batch    7/269   train_loss = 3.473\n",
      "Epoch  69 Batch    8/269   train_loss = 3.767\n",
      "Epoch  69 Batch    9/269   train_loss = 3.438\n",
      "Epoch  69 Batch   10/269   train_loss = 3.628\n",
      "Epoch  69 Batch   11/269   train_loss = 3.636\n",
      "Epoch  69 Batch   12/269   train_loss = 3.400\n",
      "Epoch  69 Batch   13/269   train_loss = 3.710\n",
      "Epoch  69 Batch   14/269   train_loss = 3.605\n",
      "Epoch  69 Batch   15/269   train_loss = 3.889\n",
      "Epoch  69 Batch   16/269   train_loss = 3.603\n",
      "Epoch  69 Batch   17/269   train_loss = 3.698\n",
      "Epoch  69 Batch   18/269   train_loss = 3.630\n",
      "Epoch  69 Batch   19/269   train_loss = 3.657\n",
      "Epoch  69 Batch   20/269   train_loss = 4.045\n",
      "Epoch  69 Batch   21/269   train_loss = 3.840\n",
      "Epoch  69 Batch   22/269   train_loss = 3.617\n",
      "Epoch  69 Batch   23/269   train_loss = 3.848\n",
      "Epoch  69 Batch   24/269   train_loss = 3.490\n",
      "Epoch  69 Batch   25/269   train_loss = 3.898\n",
      "Epoch  69 Batch   26/269   train_loss = 3.772\n",
      "Epoch  69 Batch   27/269   train_loss = 3.609\n",
      "Epoch  69 Batch   28/269   train_loss = 3.750\n",
      "Epoch  69 Batch   29/269   train_loss = 3.743\n",
      "Epoch  69 Batch   30/269   train_loss = 4.090\n",
      "Epoch  69 Batch   31/269   train_loss = 3.724\n",
      "Epoch  69 Batch   32/269   train_loss = 3.607\n",
      "Epoch  69 Batch   33/269   train_loss = 3.391\n",
      "Epoch  69 Batch   34/269   train_loss = 3.617\n",
      "Epoch  69 Batch   35/269   train_loss = 3.492\n",
      "Epoch  69 Batch   36/269   train_loss = 3.519\n",
      "Epoch  69 Batch   37/269   train_loss = 3.714\n",
      "Epoch  69 Batch   38/269   train_loss = 3.405\n",
      "Epoch  69 Batch   39/269   train_loss = 3.791\n",
      "Epoch  69 Batch   40/269   train_loss = 3.616\n",
      "Epoch  69 Batch   41/269   train_loss = 3.521\n",
      "Epoch  69 Batch   42/269   train_loss = 4.029\n",
      "Epoch  69 Batch   43/269   train_loss = 3.984\n",
      "Epoch  69 Batch   44/269   train_loss = 3.706\n",
      "Epoch  69 Batch   45/269   train_loss = 3.370\n",
      "Epoch  69 Batch   46/269   train_loss = 3.761\n",
      "Epoch  69 Batch   47/269   train_loss = 3.432\n",
      "Epoch  69 Batch   48/269   train_loss = 3.908\n",
      "Epoch  69 Batch   49/269   train_loss = 3.888\n",
      "Epoch  69 Batch   50/269   train_loss = 3.825\n",
      "Epoch  69 Batch   51/269   train_loss = 3.761\n",
      "Epoch  69 Batch   52/269   train_loss = 3.736\n",
      "Epoch  69 Batch   53/269   train_loss = 3.960\n",
      "Epoch  69 Batch   54/269   train_loss = 3.655\n",
      "Epoch  69 Batch   55/269   train_loss = 3.803\n",
      "Epoch  69 Batch   56/269   train_loss = 3.591\n",
      "Epoch  69 Batch   57/269   train_loss = 3.898\n",
      "Epoch  69 Batch   58/269   train_loss = 3.536\n",
      "Epoch  69 Batch   59/269   train_loss = 3.602\n",
      "Epoch  69 Batch   60/269   train_loss = 3.755\n",
      "Epoch  69 Batch   61/269   train_loss = 3.924\n",
      "Epoch  69 Batch   62/269   train_loss = 3.538\n",
      "Epoch  69 Batch   63/269   train_loss = 3.866\n",
      "Epoch  69 Batch   64/269   train_loss = 3.598\n",
      "Epoch  69 Batch   65/269   train_loss = 3.717\n",
      "Epoch  69 Batch   66/269   train_loss = 3.605\n",
      "Epoch  69 Batch   67/269   train_loss = 3.641\n",
      "Epoch  69 Batch   68/269   train_loss = 3.725\n",
      "Epoch  69 Batch   69/269   train_loss = 3.715\n",
      "Epoch  69 Batch   70/269   train_loss = 3.425\n",
      "Epoch  69 Batch   71/269   train_loss = 3.806\n",
      "Epoch  69 Batch   72/269   train_loss = 3.454\n",
      "Epoch  69 Batch   73/269   train_loss = 3.704\n",
      "Epoch  69 Batch   74/269   train_loss = 3.693\n",
      "Epoch  69 Batch   75/269   train_loss = 3.817\n",
      "Epoch  69 Batch   76/269   train_loss = 3.788\n",
      "Epoch  69 Batch   77/269   train_loss = 3.390\n",
      "Epoch  69 Batch   78/269   train_loss = 3.818\n",
      "Epoch  69 Batch   79/269   train_loss = 3.897\n",
      "Epoch  69 Batch   80/269   train_loss = 3.789\n",
      "Epoch  69 Batch   81/269   train_loss = 3.590\n",
      "Epoch  69 Batch   82/269   train_loss = 3.790\n",
      "Epoch  69 Batch   83/269   train_loss = 3.582\n",
      "Epoch  69 Batch   84/269   train_loss = 3.879\n",
      "Epoch  69 Batch   85/269   train_loss = 4.078\n",
      "Epoch  69 Batch   86/269   train_loss = 4.048\n",
      "Epoch  69 Batch   87/269   train_loss = 3.684\n",
      "Epoch  69 Batch   88/269   train_loss = 3.831\n",
      "Epoch  69 Batch   89/269   train_loss = 3.696\n",
      "Epoch  69 Batch   90/269   train_loss = 3.554\n",
      "Epoch  69 Batch   91/269   train_loss = 3.795\n",
      "Epoch  69 Batch   92/269   train_loss = 3.844\n",
      "Epoch  69 Batch   93/269   train_loss = 3.596\n",
      "Epoch  69 Batch   94/269   train_loss = 4.084\n",
      "Epoch  69 Batch   95/269   train_loss = 3.501\n",
      "Epoch  69 Batch   96/269   train_loss = 3.591\n",
      "Epoch  69 Batch   97/269   train_loss = 3.658\n",
      "Epoch  69 Batch   98/269   train_loss = 3.920\n",
      "Epoch  69 Batch   99/269   train_loss = 3.795\n",
      "Epoch  69 Batch  100/269   train_loss = 3.734\n",
      "Epoch  69 Batch  101/269   train_loss = 3.760\n",
      "Epoch  69 Batch  102/269   train_loss = 3.485\n",
      "Epoch  69 Batch  103/269   train_loss = 3.605\n",
      "Epoch  69 Batch  104/269   train_loss = 3.635\n",
      "Epoch  69 Batch  105/269   train_loss = 3.883\n",
      "Epoch  69 Batch  106/269   train_loss = 3.959\n",
      "Epoch  69 Batch  107/269   train_loss = 3.967\n",
      "Epoch  69 Batch  108/269   train_loss = 3.716\n",
      "Epoch  69 Batch  109/269   train_loss = 3.739\n",
      "Epoch  69 Batch  110/269   train_loss = 3.724\n",
      "Epoch  69 Batch  111/269   train_loss = 3.868\n",
      "Epoch  69 Batch  112/269   train_loss = 3.932\n",
      "Epoch  69 Batch  113/269   train_loss = 3.745\n",
      "Epoch  69 Batch  114/269   train_loss = 3.694\n",
      "Epoch  69 Batch  115/269   train_loss = 3.600\n",
      "Epoch  69 Batch  116/269   train_loss = 3.538\n",
      "Epoch  69 Batch  117/269   train_loss = 3.608\n",
      "Epoch  69 Batch  118/269   train_loss = 3.902\n",
      "Epoch  69 Batch  119/269   train_loss = 3.471\n",
      "Epoch  69 Batch  120/269   train_loss = 3.695\n",
      "Epoch  69 Batch  121/269   train_loss = 3.724\n",
      "Epoch  69 Batch  122/269   train_loss = 3.731\n",
      "Epoch  69 Batch  123/269   train_loss = 3.524\n",
      "Epoch  69 Batch  124/269   train_loss = 3.882\n",
      "Epoch  69 Batch  125/269   train_loss = 3.529\n",
      "Epoch  69 Batch  126/269   train_loss = 3.414\n",
      "Epoch  69 Batch  127/269   train_loss = 3.712\n",
      "Epoch  69 Batch  128/269   train_loss = 3.641\n",
      "Epoch  69 Batch  129/269   train_loss = 3.622\n",
      "Epoch  69 Batch  130/269   train_loss = 3.764\n",
      "Epoch  69 Batch  131/269   train_loss = 3.401\n",
      "Epoch  69 Batch  132/269   train_loss = 3.765\n",
      "Epoch  69 Batch  133/269   train_loss = 3.483\n",
      "Epoch  69 Batch  134/269   train_loss = 3.900\n",
      "Epoch  69 Batch  135/269   train_loss = 3.572\n",
      "Epoch  69 Batch  136/269   train_loss = 3.428\n",
      "Epoch  69 Batch  137/269   train_loss = 3.824\n",
      "Epoch  69 Batch  138/269   train_loss = 3.549\n",
      "Epoch  69 Batch  139/269   train_loss = 3.707\n",
      "Epoch  69 Batch  140/269   train_loss = 3.718\n",
      "Epoch  69 Batch  141/269   train_loss = 4.018\n",
      "Epoch  69 Batch  142/269   train_loss = 3.485\n",
      "Epoch  69 Batch  143/269   train_loss = 3.698\n",
      "Epoch  69 Batch  144/269   train_loss = 3.910\n",
      "Epoch  69 Batch  145/269   train_loss = 3.648\n",
      "Epoch  69 Batch  146/269   train_loss = 3.877\n",
      "Epoch  69 Batch  147/269   train_loss = 3.506\n",
      "Epoch  69 Batch  148/269   train_loss = 3.801\n",
      "Epoch  69 Batch  149/269   train_loss = 3.585\n",
      "Epoch  69 Batch  150/269   train_loss = 3.812\n",
      "Epoch  69 Batch  151/269   train_loss = 3.770\n",
      "Epoch  69 Batch  152/269   train_loss = 3.730\n",
      "Epoch  69 Batch  153/269   train_loss = 3.727\n",
      "Epoch  69 Batch  154/269   train_loss = 3.634\n",
      "Epoch  69 Batch  155/269   train_loss = 3.608\n",
      "Epoch  69 Batch  156/269   train_loss = 4.044\n",
      "Epoch  69 Batch  157/269   train_loss = 3.759\n",
      "Epoch  69 Batch  158/269   train_loss = 3.743\n",
      "Epoch  69 Batch  159/269   train_loss = 3.719\n",
      "Epoch  69 Batch  160/269   train_loss = 3.632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  69 Batch  161/269   train_loss = 3.850\n",
      "Epoch  69 Batch  162/269   train_loss = 3.832\n",
      "Epoch  69 Batch  163/269   train_loss = 3.607\n",
      "Epoch  69 Batch  164/269   train_loss = 3.900\n",
      "Epoch  69 Batch  165/269   train_loss = 3.883\n",
      "Epoch  69 Batch  166/269   train_loss = 3.773\n",
      "Epoch  69 Batch  167/269   train_loss = 3.525\n",
      "Epoch  69 Batch  168/269   train_loss = 3.789\n",
      "Epoch  69 Batch  169/269   train_loss = 3.987\n",
      "Epoch  69 Batch  170/269   train_loss = 3.409\n",
      "Epoch  69 Batch  171/269   train_loss = 3.743\n",
      "Epoch  69 Batch  172/269   train_loss = 3.542\n",
      "Epoch  69 Batch  173/269   train_loss = 3.744\n",
      "Epoch  69 Batch  174/269   train_loss = 3.869\n",
      "Epoch  69 Batch  175/269   train_loss = 3.737\n",
      "Epoch  69 Batch  176/269   train_loss = 3.708\n",
      "Epoch  69 Batch  177/269   train_loss = 3.741\n",
      "Epoch  69 Batch  178/269   train_loss = 3.975\n",
      "Epoch  69 Batch  179/269   train_loss = 3.740\n",
      "Epoch  69 Batch  180/269   train_loss = 3.608\n",
      "Epoch  69 Batch  181/269   train_loss = 3.807\n",
      "Epoch  69 Batch  182/269   train_loss = 3.663\n",
      "Epoch  69 Batch  183/269   train_loss = 3.620\n",
      "Epoch  69 Batch  184/269   train_loss = 3.699\n",
      "Epoch  69 Batch  185/269   train_loss = 3.777\n",
      "Epoch  69 Batch  186/269   train_loss = 3.697\n",
      "Epoch  69 Batch  187/269   train_loss = 3.913\n",
      "Epoch  69 Batch  188/269   train_loss = 3.629\n",
      "Epoch  69 Batch  189/269   train_loss = 3.622\n",
      "Epoch  69 Batch  190/269   train_loss = 4.046\n",
      "Epoch  69 Batch  191/269   train_loss = 3.787\n",
      "Epoch  69 Batch  192/269   train_loss = 3.732\n",
      "Epoch  69 Batch  193/269   train_loss = 3.697\n",
      "Epoch  69 Batch  194/269   train_loss = 3.795\n",
      "Epoch  69 Batch  195/269   train_loss = 3.695\n",
      "Epoch  69 Batch  196/269   train_loss = 3.721\n",
      "Epoch  69 Batch  197/269   train_loss = 3.929\n",
      "Epoch  69 Batch  198/269   train_loss = 3.765\n",
      "Epoch  69 Batch  199/269   train_loss = 3.837\n",
      "Epoch  69 Batch  200/269   train_loss = 3.601\n",
      "Epoch  69 Batch  201/269   train_loss = 3.750\n",
      "Epoch  69 Batch  202/269   train_loss = 3.535\n",
      "Epoch  69 Batch  203/269   train_loss = 3.635\n",
      "Epoch  69 Batch  204/269   train_loss = 3.864\n",
      "Epoch  69 Batch  205/269   train_loss = 3.786\n",
      "Epoch  69 Batch  206/269   train_loss = 3.719\n",
      "Epoch  69 Batch  207/269   train_loss = 3.632\n",
      "Epoch  69 Batch  208/269   train_loss = 3.785\n",
      "Epoch  69 Batch  209/269   train_loss = 3.787\n",
      "Epoch  69 Batch  210/269   train_loss = 3.661\n",
      "Epoch  69 Batch  211/269   train_loss = 3.622\n",
      "Epoch  69 Batch  212/269   train_loss = 4.041\n",
      "Epoch  69 Batch  213/269   train_loss = 3.628\n",
      "Epoch  69 Batch  214/269   train_loss = 3.736\n",
      "Epoch  69 Batch  215/269   train_loss = 3.936\n",
      "Epoch  69 Batch  216/269   train_loss = 3.909\n",
      "Epoch  69 Batch  217/269   train_loss = 3.564\n",
      "Epoch  69 Batch  218/269   train_loss = 3.830\n",
      "Epoch  69 Batch  219/269   train_loss = 3.496\n",
      "Epoch  69 Batch  220/269   train_loss = 3.859\n",
      "Epoch  69 Batch  221/269   train_loss = 3.636\n",
      "Epoch  69 Batch  222/269   train_loss = 3.754\n",
      "Epoch  69 Batch  223/269   train_loss = 3.528\n",
      "Epoch  69 Batch  224/269   train_loss = 3.892\n",
      "Epoch  69 Batch  225/269   train_loss = 3.928\n",
      "Epoch  69 Batch  226/269   train_loss = 3.680\n",
      "Epoch  69 Batch  227/269   train_loss = 3.423\n",
      "Epoch  69 Batch  228/269   train_loss = 3.705\n",
      "Epoch  69 Batch  229/269   train_loss = 3.895\n",
      "Epoch  69 Batch  230/269   train_loss = 3.834\n",
      "Epoch  69 Batch  231/269   train_loss = 3.682\n",
      "Epoch  69 Batch  232/269   train_loss = 3.694\n",
      "Epoch  69 Batch  233/269   train_loss = 3.787\n",
      "Epoch  69 Batch  234/269   train_loss = 3.709\n",
      "Epoch  69 Batch  235/269   train_loss = 3.984\n",
      "Epoch  69 Batch  236/269   train_loss = 3.626\n",
      "Epoch  69 Batch  237/269   train_loss = 3.248\n",
      "Epoch  69 Batch  238/269   train_loss = 3.471\n",
      "Epoch  69 Batch  239/269   train_loss = 3.997\n",
      "Epoch  69 Batch  240/269   train_loss = 3.542\n",
      "Epoch  69 Batch  241/269   train_loss = 3.953\n",
      "Epoch  69 Batch  242/269   train_loss = 3.492\n",
      "Epoch  69 Batch  243/269   train_loss = 3.892\n",
      "Epoch  69 Batch  244/269   train_loss = 3.705\n",
      "Epoch  69 Batch  245/269   train_loss = 3.656\n",
      "Epoch  69 Batch  246/269   train_loss = 3.357\n",
      "Epoch  69 Batch  247/269   train_loss = 3.742\n",
      "Epoch  69 Batch  248/269   train_loss = 3.776\n",
      "Epoch  69 Batch  249/269   train_loss = 3.603\n",
      "Epoch  69 Batch  250/269   train_loss = 3.520\n",
      "Epoch  69 Batch  251/269   train_loss = 3.933\n",
      "Epoch  69 Batch  252/269   train_loss = 3.849\n",
      "Epoch  69 Batch  253/269   train_loss = 3.669\n",
      "Epoch  69 Batch  254/269   train_loss = 3.602\n",
      "Epoch  69 Batch  255/269   train_loss = 3.687\n",
      "Epoch  69 Batch  256/269   train_loss = 3.605\n",
      "Epoch  69 Batch  257/269   train_loss = 3.583\n",
      "Epoch  69 Batch  258/269   train_loss = 3.491\n",
      "Epoch  69 Batch  259/269   train_loss = 3.534\n",
      "Epoch  69 Batch  260/269   train_loss = 3.816\n",
      "Epoch  69 Batch  261/269   train_loss = 3.790\n",
      "Epoch  69 Batch  262/269   train_loss = 3.621\n",
      "Epoch  69 Batch  263/269   train_loss = 3.535\n",
      "Epoch  69 Batch  264/269   train_loss = 3.919\n",
      "Epoch  69 Batch  265/269   train_loss = 3.773\n",
      "Epoch  69 Batch  266/269   train_loss = 3.615\n",
      "Epoch  69 Batch  267/269   train_loss = 3.725\n",
      "Epoch  69 Batch  268/269   train_loss = 3.865\n",
      "Epoch  70 Batch    0/269   train_loss = 3.642\n",
      "Epoch  70 Batch    1/269   train_loss = 3.445\n",
      "Epoch  70 Batch    2/269   train_loss = 3.557\n",
      "Epoch  70 Batch    3/269   train_loss = 3.694\n",
      "Epoch  70 Batch    4/269   train_loss = 4.130\n",
      "Epoch  70 Batch    5/269   train_loss = 3.668\n",
      "Epoch  70 Batch    6/269   train_loss = 3.615\n",
      "Epoch  70 Batch    7/269   train_loss = 3.441\n",
      "Epoch  70 Batch    8/269   train_loss = 3.767\n",
      "Epoch  70 Batch    9/269   train_loss = 3.429\n",
      "Epoch  70 Batch   10/269   train_loss = 3.618\n",
      "Epoch  70 Batch   11/269   train_loss = 3.679\n",
      "Epoch  70 Batch   12/269   train_loss = 3.411\n",
      "Epoch  70 Batch   13/269   train_loss = 3.711\n",
      "Epoch  70 Batch   14/269   train_loss = 3.586\n",
      "Epoch  70 Batch   15/269   train_loss = 3.891\n",
      "Epoch  70 Batch   16/269   train_loss = 3.631\n",
      "Epoch  70 Batch   17/269   train_loss = 3.687\n",
      "Epoch  70 Batch   18/269   train_loss = 3.611\n",
      "Epoch  70 Batch   19/269   train_loss = 3.664\n",
      "Epoch  70 Batch   20/269   train_loss = 4.068\n",
      "Epoch  70 Batch   21/269   train_loss = 3.874\n",
      "Epoch  70 Batch   22/269   train_loss = 3.587\n",
      "Epoch  70 Batch   23/269   train_loss = 3.807\n",
      "Epoch  70 Batch   24/269   train_loss = 3.477\n",
      "Epoch  70 Batch   25/269   train_loss = 3.916\n",
      "Epoch  70 Batch   26/269   train_loss = 3.773\n",
      "Epoch  70 Batch   27/269   train_loss = 3.594\n",
      "Epoch  70 Batch   28/269   train_loss = 3.757\n",
      "Epoch  70 Batch   29/269   train_loss = 3.792\n",
      "Epoch  70 Batch   30/269   train_loss = 4.032\n",
      "Epoch  70 Batch   31/269   train_loss = 3.715\n",
      "Epoch  70 Batch   32/269   train_loss = 3.609\n",
      "Epoch  70 Batch   33/269   train_loss = 3.397\n",
      "Epoch  70 Batch   34/269   train_loss = 3.625\n",
      "Epoch  70 Batch   35/269   train_loss = 3.462\n",
      "Epoch  70 Batch   36/269   train_loss = 3.612\n",
      "Epoch  70 Batch   37/269   train_loss = 3.721\n",
      "Epoch  70 Batch   38/269   train_loss = 3.384\n",
      "Epoch  70 Batch   39/269   train_loss = 3.814\n",
      "Epoch  70 Batch   40/269   train_loss = 3.633\n",
      "Epoch  70 Batch   41/269   train_loss = 3.529\n",
      "Epoch  70 Batch   42/269   train_loss = 4.016\n",
      "Epoch  70 Batch   43/269   train_loss = 4.014\n",
      "Epoch  70 Batch   44/269   train_loss = 3.723\n",
      "Epoch  70 Batch   45/269   train_loss = 3.424\n",
      "Epoch  70 Batch   46/269   train_loss = 3.751\n",
      "Epoch  70 Batch   47/269   train_loss = 3.468\n",
      "Epoch  70 Batch   48/269   train_loss = 3.922\n",
      "Epoch  70 Batch   49/269   train_loss = 3.904\n",
      "Epoch  70 Batch   50/269   train_loss = 3.801\n",
      "Epoch  70 Batch   51/269   train_loss = 3.732\n",
      "Epoch  70 Batch   52/269   train_loss = 3.744\n",
      "Epoch  70 Batch   53/269   train_loss = 3.986\n",
      "Epoch  70 Batch   54/269   train_loss = 3.658\n",
      "Epoch  70 Batch   55/269   train_loss = 3.799\n",
      "Epoch  70 Batch   56/269   train_loss = 3.555\n",
      "Epoch  70 Batch   57/269   train_loss = 3.885\n",
      "Epoch  70 Batch   58/269   train_loss = 3.551\n",
      "Epoch  70 Batch   59/269   train_loss = 3.589\n",
      "Epoch  70 Batch   60/269   train_loss = 3.740\n",
      "Epoch  70 Batch   61/269   train_loss = 3.945\n",
      "Epoch  70 Batch   62/269   train_loss = 3.574\n",
      "Epoch  70 Batch   63/269   train_loss = 3.859\n",
      "Epoch  70 Batch   64/269   train_loss = 3.586\n",
      "Epoch  70 Batch   65/269   train_loss = 3.715\n",
      "Epoch  70 Batch   66/269   train_loss = 3.600\n",
      "Epoch  70 Batch   67/269   train_loss = 3.633\n",
      "Epoch  70 Batch   68/269   train_loss = 3.724\n",
      "Epoch  70 Batch   69/269   train_loss = 3.693\n",
      "Epoch  70 Batch   70/269   train_loss = 3.427\n",
      "Epoch  70 Batch   71/269   train_loss = 3.800\n",
      "Epoch  70 Batch   72/269   train_loss = 3.453\n",
      "Epoch  70 Batch   73/269   train_loss = 3.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  70 Batch   74/269   train_loss = 3.689\n",
      "Epoch  70 Batch   75/269   train_loss = 3.851\n",
      "Epoch  70 Batch   76/269   train_loss = 3.785\n",
      "Epoch  70 Batch   77/269   train_loss = 3.405\n",
      "Epoch  70 Batch   78/269   train_loss = 3.819\n",
      "Epoch  70 Batch   79/269   train_loss = 3.869\n",
      "Epoch  70 Batch   80/269   train_loss = 3.793\n",
      "Epoch  70 Batch   81/269   train_loss = 3.599\n",
      "Epoch  70 Batch   82/269   train_loss = 3.787\n",
      "Epoch  70 Batch   83/269   train_loss = 3.598\n",
      "Epoch  70 Batch   84/269   train_loss = 3.928\n",
      "Epoch  70 Batch   85/269   train_loss = 4.070\n",
      "Epoch  70 Batch   86/269   train_loss = 4.069\n",
      "Epoch  70 Batch   87/269   train_loss = 3.660\n",
      "Epoch  70 Batch   88/269   train_loss = 3.788\n",
      "Epoch  70 Batch   89/269   train_loss = 3.703\n",
      "Epoch  70 Batch   90/269   train_loss = 3.590\n",
      "Epoch  70 Batch   91/269   train_loss = 3.770\n",
      "Epoch  70 Batch   92/269   train_loss = 3.868\n",
      "Epoch  70 Batch   93/269   train_loss = 3.606\n",
      "Epoch  70 Batch   94/269   train_loss = 4.067\n",
      "Epoch  70 Batch   95/269   train_loss = 3.502\n",
      "Epoch  70 Batch   96/269   train_loss = 3.588\n",
      "Epoch  70 Batch   97/269   train_loss = 3.677\n",
      "Epoch  70 Batch   98/269   train_loss = 3.938\n",
      "Epoch  70 Batch   99/269   train_loss = 3.770\n",
      "Epoch  70 Batch  100/269   train_loss = 3.703\n",
      "Epoch  70 Batch  101/269   train_loss = 3.764\n",
      "Epoch  70 Batch  102/269   train_loss = 3.520\n",
      "Epoch  70 Batch  103/269   train_loss = 3.614\n",
      "Epoch  70 Batch  104/269   train_loss = 3.667\n",
      "Epoch  70 Batch  105/269   train_loss = 3.873\n",
      "Epoch  70 Batch  106/269   train_loss = 3.926\n",
      "Epoch  70 Batch  107/269   train_loss = 3.961\n",
      "Epoch  70 Batch  108/269   train_loss = 3.744\n",
      "Epoch  70 Batch  109/269   train_loss = 3.785\n",
      "Epoch  70 Batch  110/269   train_loss = 3.725\n",
      "Epoch  70 Batch  111/269   train_loss = 3.860\n",
      "Epoch  70 Batch  112/269   train_loss = 3.932\n",
      "Epoch  70 Batch  113/269   train_loss = 3.734\n",
      "Epoch  70 Batch  114/269   train_loss = 3.674\n",
      "Epoch  70 Batch  115/269   train_loss = 3.606\n",
      "Epoch  70 Batch  116/269   train_loss = 3.577\n",
      "Epoch  70 Batch  117/269   train_loss = 3.600\n",
      "Epoch  70 Batch  118/269   train_loss = 3.895\n",
      "Epoch  70 Batch  119/269   train_loss = 3.499\n",
      "Epoch  70 Batch  120/269   train_loss = 3.671\n",
      "Epoch  70 Batch  121/269   train_loss = 3.746\n",
      "Epoch  70 Batch  122/269   train_loss = 3.746\n",
      "Epoch  70 Batch  123/269   train_loss = 3.510\n",
      "Epoch  70 Batch  124/269   train_loss = 3.912\n",
      "Epoch  70 Batch  125/269   train_loss = 3.553\n",
      "Epoch  70 Batch  126/269   train_loss = 3.417\n",
      "Epoch  70 Batch  127/269   train_loss = 3.764\n",
      "Epoch  70 Batch  128/269   train_loss = 3.640\n",
      "Epoch  70 Batch  129/269   train_loss = 3.631\n",
      "Epoch  70 Batch  130/269   train_loss = 3.769\n",
      "Epoch  70 Batch  131/269   train_loss = 3.408\n",
      "Epoch  70 Batch  132/269   train_loss = 3.762\n",
      "Epoch  70 Batch  133/269   train_loss = 3.473\n",
      "Epoch  70 Batch  134/269   train_loss = 3.852\n",
      "Epoch  70 Batch  135/269   train_loss = 3.587\n",
      "Epoch  70 Batch  136/269   train_loss = 3.420\n",
      "Epoch  70 Batch  137/269   train_loss = 3.818\n",
      "Epoch  70 Batch  138/269   train_loss = 3.541\n",
      "Epoch  70 Batch  139/269   train_loss = 3.677\n",
      "Epoch  70 Batch  140/269   train_loss = 3.757\n",
      "Epoch  70 Batch  141/269   train_loss = 4.018\n",
      "Epoch  70 Batch  142/269   train_loss = 3.506\n",
      "Epoch  70 Batch  143/269   train_loss = 3.680\n",
      "Epoch  70 Batch  144/269   train_loss = 3.884\n",
      "Epoch  70 Batch  145/269   train_loss = 3.677\n",
      "Epoch  70 Batch  146/269   train_loss = 3.873\n",
      "Epoch  70 Batch  147/269   train_loss = 3.487\n",
      "Epoch  70 Batch  148/269   train_loss = 3.799\n",
      "Epoch  70 Batch  149/269   train_loss = 3.597\n",
      "Epoch  70 Batch  150/269   train_loss = 3.821\n",
      "Epoch  70 Batch  151/269   train_loss = 3.772\n",
      "Epoch  70 Batch  152/269   train_loss = 3.705\n",
      "Epoch  70 Batch  153/269   train_loss = 3.697\n",
      "Epoch  70 Batch  154/269   train_loss = 3.621\n",
      "Epoch  70 Batch  155/269   train_loss = 3.666\n",
      "Epoch  70 Batch  156/269   train_loss = 4.072\n",
      "Epoch  70 Batch  157/269   train_loss = 3.781\n",
      "Epoch  70 Batch  158/269   train_loss = 3.746\n",
      "Epoch  70 Batch  159/269   train_loss = 3.737\n",
      "Epoch  70 Batch  160/269   train_loss = 3.645\n",
      "Epoch  70 Batch  161/269   train_loss = 3.867\n",
      "Epoch  70 Batch  162/269   train_loss = 3.882\n",
      "Epoch  70 Batch  163/269   train_loss = 3.628\n",
      "Epoch  70 Batch  164/269   train_loss = 3.925\n",
      "Epoch  70 Batch  165/269   train_loss = 3.911\n",
      "Epoch  70 Batch  166/269   train_loss = 3.812\n",
      "Epoch  70 Batch  167/269   train_loss = 3.539\n",
      "Epoch  70 Batch  168/269   train_loss = 3.793\n",
      "Epoch  70 Batch  169/269   train_loss = 3.984\n",
      "Epoch  70 Batch  170/269   train_loss = 3.427\n",
      "Epoch  70 Batch  171/269   train_loss = 3.758\n",
      "Epoch  70 Batch  172/269   train_loss = 3.556\n",
      "Epoch  70 Batch  173/269   train_loss = 3.753\n",
      "Epoch  70 Batch  174/269   train_loss = 3.869\n",
      "Epoch  70 Batch  175/269   train_loss = 3.768\n",
      "Epoch  70 Batch  176/269   train_loss = 3.689\n",
      "Epoch  70 Batch  177/269   train_loss = 3.772\n",
      "Epoch  70 Batch  178/269   train_loss = 3.977\n",
      "Epoch  70 Batch  179/269   train_loss = 3.746\n",
      "Epoch  70 Batch  180/269   train_loss = 3.672\n",
      "Epoch  70 Batch  181/269   train_loss = 3.832\n",
      "Epoch  70 Batch  182/269   train_loss = 3.644\n",
      "Epoch  70 Batch  183/269   train_loss = 3.624\n",
      "Epoch  70 Batch  184/269   train_loss = 3.727\n",
      "Epoch  70 Batch  185/269   train_loss = 3.835\n",
      "Epoch  70 Batch  186/269   train_loss = 3.726\n",
      "Epoch  70 Batch  187/269   train_loss = 3.920\n",
      "Epoch  70 Batch  188/269   train_loss = 3.630\n",
      "Epoch  70 Batch  189/269   train_loss = 3.611\n",
      "Epoch  70 Batch  190/269   train_loss = 4.103\n",
      "Epoch  70 Batch  191/269   train_loss = 3.799\n",
      "Epoch  70 Batch  192/269   train_loss = 3.739\n",
      "Epoch  70 Batch  193/269   train_loss = 3.707\n",
      "Epoch  70 Batch  194/269   train_loss = 3.787\n",
      "Epoch  70 Batch  195/269   train_loss = 3.698\n",
      "Epoch  70 Batch  196/269   train_loss = 3.724\n",
      "Epoch  70 Batch  197/269   train_loss = 3.921\n",
      "Epoch  70 Batch  198/269   train_loss = 3.735\n",
      "Epoch  70 Batch  199/269   train_loss = 3.854\n",
      "Epoch  70 Batch  200/269   train_loss = 3.637\n",
      "Epoch  70 Batch  201/269   train_loss = 3.742\n",
      "Epoch  70 Batch  202/269   train_loss = 3.521\n",
      "Epoch  70 Batch  203/269   train_loss = 3.650\n",
      "Epoch  70 Batch  204/269   train_loss = 3.872\n",
      "Epoch  70 Batch  205/269   train_loss = 3.774\n",
      "Epoch  70 Batch  206/269   train_loss = 3.700\n",
      "Epoch  70 Batch  207/269   train_loss = 3.637\n",
      "Epoch  70 Batch  208/269   train_loss = 3.751\n",
      "Epoch  70 Batch  209/269   train_loss = 3.793\n",
      "Epoch  70 Batch  210/269   train_loss = 3.682\n",
      "Epoch  70 Batch  211/269   train_loss = 3.621\n",
      "Epoch  70 Batch  212/269   train_loss = 4.015\n",
      "Epoch  70 Batch  213/269   train_loss = 3.661\n",
      "Epoch  70 Batch  214/269   train_loss = 3.724\n",
      "Epoch  70 Batch  215/269   train_loss = 3.927\n",
      "Epoch  70 Batch  216/269   train_loss = 3.917\n",
      "Epoch  70 Batch  217/269   train_loss = 3.587\n",
      "Epoch  70 Batch  218/269   train_loss = 3.830\n",
      "Epoch  70 Batch  219/269   train_loss = 3.486\n",
      "Epoch  70 Batch  220/269   train_loss = 3.883\n",
      "Epoch  70 Batch  221/269   train_loss = 3.609\n",
      "Epoch  70 Batch  222/269   train_loss = 3.756\n",
      "Epoch  70 Batch  223/269   train_loss = 3.528\n",
      "Epoch  70 Batch  224/269   train_loss = 3.877\n",
      "Epoch  70 Batch  225/269   train_loss = 3.918\n",
      "Epoch  70 Batch  226/269   train_loss = 3.691\n",
      "Epoch  70 Batch  227/269   train_loss = 3.493\n",
      "Epoch  70 Batch  228/269   train_loss = 3.719\n",
      "Epoch  70 Batch  229/269   train_loss = 3.919\n",
      "Epoch  70 Batch  230/269   train_loss = 3.843\n",
      "Epoch  70 Batch  231/269   train_loss = 3.665\n",
      "Epoch  70 Batch  232/269   train_loss = 3.711\n",
      "Epoch  70 Batch  233/269   train_loss = 3.835\n",
      "Epoch  70 Batch  234/269   train_loss = 3.703\n",
      "Epoch  70 Batch  235/269   train_loss = 3.968\n",
      "Epoch  70 Batch  236/269   train_loss = 3.659\n",
      "Epoch  70 Batch  237/269   train_loss = 3.242\n",
      "Epoch  70 Batch  238/269   train_loss = 3.498\n",
      "Epoch  70 Batch  239/269   train_loss = 3.994\n",
      "Epoch  70 Batch  240/269   train_loss = 3.535\n",
      "Epoch  70 Batch  241/269   train_loss = 3.950\n",
      "Epoch  70 Batch  242/269   train_loss = 3.509\n",
      "Epoch  70 Batch  243/269   train_loss = 3.881\n",
      "Epoch  70 Batch  244/269   train_loss = 3.727\n",
      "Epoch  70 Batch  245/269   train_loss = 3.695\n",
      "Epoch  70 Batch  246/269   train_loss = 3.366\n",
      "Epoch  70 Batch  247/269   train_loss = 3.755\n",
      "Epoch  70 Batch  248/269   train_loss = 3.795\n",
      "Epoch  70 Batch  249/269   train_loss = 3.612\n",
      "Epoch  70 Batch  250/269   train_loss = 3.556\n",
      "Epoch  70 Batch  251/269   train_loss = 3.945\n",
      "Epoch  70 Batch  252/269   train_loss = 3.854\n",
      "Epoch  70 Batch  253/269   train_loss = 3.685\n",
      "Epoch  70 Batch  254/269   train_loss = 3.602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  70 Batch  255/269   train_loss = 3.744\n",
      "Epoch  70 Batch  256/269   train_loss = 3.659\n",
      "Epoch  70 Batch  257/269   train_loss = 3.607\n",
      "Epoch  70 Batch  258/269   train_loss = 3.479\n",
      "Epoch  70 Batch  259/269   train_loss = 3.533\n",
      "Epoch  70 Batch  260/269   train_loss = 3.818\n",
      "Epoch  70 Batch  261/269   train_loss = 3.770\n",
      "Epoch  70 Batch  262/269   train_loss = 3.625\n",
      "Epoch  70 Batch  263/269   train_loss = 3.507\n",
      "Epoch  70 Batch  264/269   train_loss = 3.956\n",
      "Epoch  70 Batch  265/269   train_loss = 3.773\n",
      "Epoch  70 Batch  266/269   train_loss = 3.653\n",
      "Epoch  70 Batch  267/269   train_loss = 3.714\n",
      "Epoch  70 Batch  268/269   train_loss = 3.877\n",
      "Epoch  71 Batch    0/269   train_loss = 3.625\n",
      "Epoch  71 Batch    1/269   train_loss = 3.442\n",
      "Epoch  71 Batch    2/269   train_loss = 3.547\n",
      "Epoch  71 Batch    3/269   train_loss = 3.673\n",
      "Epoch  71 Batch    4/269   train_loss = 4.131\n",
      "Epoch  71 Batch    5/269   train_loss = 3.649\n",
      "Epoch  71 Batch    6/269   train_loss = 3.586\n",
      "Epoch  71 Batch    7/269   train_loss = 3.444\n",
      "Epoch  71 Batch    8/269   train_loss = 3.774\n",
      "Epoch  71 Batch    9/269   train_loss = 3.424\n",
      "Epoch  71 Batch   10/269   train_loss = 3.603\n",
      "Epoch  71 Batch   11/269   train_loss = 3.682\n",
      "Epoch  71 Batch   12/269   train_loss = 3.423\n",
      "Epoch  71 Batch   13/269   train_loss = 3.689\n",
      "Epoch  71 Batch   14/269   train_loss = 3.592\n",
      "Epoch  71 Batch   15/269   train_loss = 3.881\n",
      "Epoch  71 Batch   16/269   train_loss = 3.672\n",
      "Epoch  71 Batch   17/269   train_loss = 3.675\n",
      "Epoch  71 Batch   18/269   train_loss = 3.624\n",
      "Epoch  71 Batch   19/269   train_loss = 3.673\n",
      "Epoch  71 Batch   20/269   train_loss = 4.073\n",
      "Epoch  71 Batch   21/269   train_loss = 3.929\n",
      "Epoch  71 Batch   22/269   train_loss = 3.631\n",
      "Epoch  71 Batch   23/269   train_loss = 3.803\n",
      "Epoch  71 Batch   24/269   train_loss = 3.508\n",
      "Epoch  71 Batch   25/269   train_loss = 3.917\n",
      "Epoch  71 Batch   26/269   train_loss = 3.782\n",
      "Epoch  71 Batch   27/269   train_loss = 3.599\n",
      "Epoch  71 Batch   28/269   train_loss = 3.753\n",
      "Epoch  71 Batch   29/269   train_loss = 3.782\n",
      "Epoch  71 Batch   30/269   train_loss = 4.086\n",
      "Epoch  71 Batch   31/269   train_loss = 3.736\n",
      "Epoch  71 Batch   32/269   train_loss = 3.646\n",
      "Epoch  71 Batch   33/269   train_loss = 3.425\n",
      "Epoch  71 Batch   34/269   train_loss = 3.596\n",
      "Epoch  71 Batch   35/269   train_loss = 3.465\n",
      "Epoch  71 Batch   36/269   train_loss = 3.558\n",
      "Epoch  71 Batch   37/269   train_loss = 3.754\n",
      "Epoch  71 Batch   38/269   train_loss = 3.416\n",
      "Epoch  71 Batch   39/269   train_loss = 3.850\n",
      "Epoch  71 Batch   40/269   train_loss = 3.621\n",
      "Epoch  71 Batch   41/269   train_loss = 3.518\n",
      "Epoch  71 Batch   42/269   train_loss = 4.029\n",
      "Epoch  71 Batch   43/269   train_loss = 3.993\n",
      "Epoch  71 Batch   44/269   train_loss = 3.735\n",
      "Epoch  71 Batch   45/269   train_loss = 3.401\n",
      "Epoch  71 Batch   46/269   train_loss = 3.735\n",
      "Epoch  71 Batch   47/269   train_loss = 3.457\n",
      "Epoch  71 Batch   48/269   train_loss = 3.894\n",
      "Epoch  71 Batch   49/269   train_loss = 3.882\n",
      "Epoch  71 Batch   50/269   train_loss = 3.785\n",
      "Epoch  71 Batch   51/269   train_loss = 3.798\n",
      "Epoch  71 Batch   52/269   train_loss = 3.718\n",
      "Epoch  71 Batch   53/269   train_loss = 3.999\n",
      "Epoch  71 Batch   54/269   train_loss = 3.665\n",
      "Epoch  71 Batch   55/269   train_loss = 3.818\n",
      "Epoch  71 Batch   56/269   train_loss = 3.575\n",
      "Epoch  71 Batch   57/269   train_loss = 3.942\n",
      "Epoch  71 Batch   58/269   train_loss = 3.539\n",
      "Epoch  71 Batch   59/269   train_loss = 3.646\n",
      "Epoch  71 Batch   60/269   train_loss = 3.789\n",
      "Epoch  71 Batch   61/269   train_loss = 3.950\n",
      "Epoch  71 Batch   62/269   train_loss = 3.565\n",
      "Epoch  71 Batch   63/269   train_loss = 3.874\n",
      "Epoch  71 Batch   64/269   train_loss = 3.609\n",
      "Epoch  71 Batch   65/269   train_loss = 3.751\n",
      "Epoch  71 Batch   66/269   train_loss = 3.611\n",
      "Epoch  71 Batch   67/269   train_loss = 3.643\n",
      "Epoch  71 Batch   68/269   train_loss = 3.716\n",
      "Epoch  71 Batch   69/269   train_loss = 3.765\n",
      "Epoch  71 Batch   70/269   train_loss = 3.434\n",
      "Epoch  71 Batch   71/269   train_loss = 3.825\n",
      "Epoch  71 Batch   72/269   train_loss = 3.437\n",
      "Epoch  71 Batch   73/269   train_loss = 3.718\n",
      "Epoch  71 Batch   74/269   train_loss = 3.697\n",
      "Epoch  71 Batch   75/269   train_loss = 3.855\n",
      "Epoch  71 Batch   76/269   train_loss = 3.810\n",
      "Epoch  71 Batch   77/269   train_loss = 3.429\n",
      "Epoch  71 Batch   78/269   train_loss = 3.854\n",
      "Epoch  71 Batch   79/269   train_loss = 3.912\n",
      "Epoch  71 Batch   80/269   train_loss = 3.793\n",
      "Epoch  71 Batch   81/269   train_loss = 3.636\n",
      "Epoch  71 Batch   82/269   train_loss = 3.784\n",
      "Epoch  71 Batch   83/269   train_loss = 3.615\n",
      "Epoch  71 Batch   84/269   train_loss = 3.912\n",
      "Epoch  71 Batch   85/269   train_loss = 4.081\n",
      "Epoch  71 Batch   86/269   train_loss = 4.079\n",
      "Epoch  71 Batch   87/269   train_loss = 3.722\n",
      "Epoch  71 Batch   88/269   train_loss = 3.804\n",
      "Epoch  71 Batch   89/269   train_loss = 3.694\n",
      "Epoch  71 Batch   90/269   train_loss = 3.648\n",
      "Epoch  71 Batch   91/269   train_loss = 3.776\n",
      "Epoch  71 Batch   92/269   train_loss = 3.854\n",
      "Epoch  71 Batch   93/269   train_loss = 3.612\n",
      "Epoch  71 Batch   94/269   train_loss = 4.093\n",
      "Epoch  71 Batch   95/269   train_loss = 3.499\n",
      "Epoch  71 Batch   96/269   train_loss = 3.591\n",
      "Epoch  71 Batch   97/269   train_loss = 3.682\n",
      "Epoch  71 Batch   98/269   train_loss = 3.916\n",
      "Epoch  71 Batch   99/269   train_loss = 3.785\n",
      "Epoch  71 Batch  100/269   train_loss = 3.754\n",
      "Epoch  71 Batch  101/269   train_loss = 3.785\n",
      "Epoch  71 Batch  102/269   train_loss = 3.575\n",
      "Epoch  71 Batch  103/269   train_loss = 3.642\n",
      "Epoch  71 Batch  104/269   train_loss = 3.665\n",
      "Epoch  71 Batch  105/269   train_loss = 3.897\n",
      "Epoch  71 Batch  106/269   train_loss = 3.914\n",
      "Epoch  71 Batch  107/269   train_loss = 3.986\n",
      "Epoch  71 Batch  108/269   train_loss = 3.727\n",
      "Epoch  71 Batch  109/269   train_loss = 3.749\n",
      "Epoch  71 Batch  110/269   train_loss = 3.736\n",
      "Epoch  71 Batch  111/269   train_loss = 3.865\n",
      "Epoch  71 Batch  112/269   train_loss = 3.909\n",
      "Epoch  71 Batch  113/269   train_loss = 3.755\n",
      "Epoch  71 Batch  114/269   train_loss = 3.667\n",
      "Epoch  71 Batch  115/269   train_loss = 3.640\n",
      "Epoch  71 Batch  116/269   train_loss = 3.542\n",
      "Epoch  71 Batch  117/269   train_loss = 3.589\n",
      "Epoch  71 Batch  118/269   train_loss = 3.924\n",
      "Epoch  71 Batch  119/269   train_loss = 3.537\n",
      "Epoch  71 Batch  120/269   train_loss = 3.672\n",
      "Epoch  71 Batch  121/269   train_loss = 3.741\n",
      "Epoch  71 Batch  122/269   train_loss = 3.753\n",
      "Epoch  71 Batch  123/269   train_loss = 3.517\n",
      "Epoch  71 Batch  124/269   train_loss = 3.888\n",
      "Epoch  71 Batch  125/269   train_loss = 3.559\n",
      "Epoch  71 Batch  126/269   train_loss = 3.429\n",
      "Epoch  71 Batch  127/269   train_loss = 3.769\n",
      "Epoch  71 Batch  128/269   train_loss = 3.651\n",
      "Epoch  71 Batch  129/269   train_loss = 3.635\n",
      "Epoch  71 Batch  130/269   train_loss = 3.810\n",
      "Epoch  71 Batch  131/269   train_loss = 3.427\n",
      "Epoch  71 Batch  132/269   train_loss = 3.837\n",
      "Epoch  71 Batch  133/269   train_loss = 3.504\n",
      "Epoch  71 Batch  134/269   train_loss = 3.908\n",
      "Epoch  71 Batch  135/269   train_loss = 3.576\n",
      "Epoch  71 Batch  136/269   train_loss = 3.444\n",
      "Epoch  71 Batch  137/269   train_loss = 3.812\n",
      "Epoch  71 Batch  138/269   train_loss = 3.542\n",
      "Epoch  71 Batch  139/269   train_loss = 3.679\n",
      "Epoch  71 Batch  140/269   train_loss = 3.753\n",
      "Epoch  71 Batch  141/269   train_loss = 4.069\n",
      "Epoch  71 Batch  142/269   train_loss = 3.468\n",
      "Epoch  71 Batch  143/269   train_loss = 3.691\n",
      "Epoch  71 Batch  144/269   train_loss = 3.925\n",
      "Epoch  71 Batch  145/269   train_loss = 3.650\n",
      "Epoch  71 Batch  146/269   train_loss = 3.881\n",
      "Epoch  71 Batch  147/269   train_loss = 3.488\n",
      "Epoch  71 Batch  148/269   train_loss = 3.790\n",
      "Epoch  71 Batch  149/269   train_loss = 3.592\n",
      "Epoch  71 Batch  150/269   train_loss = 3.836\n",
      "Epoch  71 Batch  151/269   train_loss = 3.814\n",
      "Epoch  71 Batch  152/269   train_loss = 3.706\n",
      "Epoch  71 Batch  153/269   train_loss = 3.736\n",
      "Epoch  71 Batch  154/269   train_loss = 3.628\n",
      "Epoch  71 Batch  155/269   train_loss = 3.674\n",
      "Epoch  71 Batch  156/269   train_loss = 4.140\n",
      "Epoch  71 Batch  157/269   train_loss = 3.783\n",
      "Epoch  71 Batch  158/269   train_loss = 3.740\n",
      "Epoch  71 Batch  159/269   train_loss = 3.786\n",
      "Epoch  71 Batch  160/269   train_loss = 3.625\n",
      "Epoch  71 Batch  161/269   train_loss = 3.822\n",
      "Epoch  71 Batch  162/269   train_loss = 3.842\n",
      "Epoch  71 Batch  163/269   train_loss = 3.623\n",
      "Epoch  71 Batch  164/269   train_loss = 3.908\n",
      "Epoch  71 Batch  165/269   train_loss = 3.888\n",
      "Epoch  71 Batch  166/269   train_loss = 3.789\n",
      "Epoch  71 Batch  167/269   train_loss = 3.555\n",
      "Epoch  71 Batch  168/269   train_loss = 3.722\n",
      "Epoch  71 Batch  169/269   train_loss = 3.972\n",
      "Epoch  71 Batch  170/269   train_loss = 3.450\n",
      "Epoch  71 Batch  171/269   train_loss = 3.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  71 Batch  172/269   train_loss = 3.541\n",
      "Epoch  71 Batch  173/269   train_loss = 3.726\n",
      "Epoch  71 Batch  174/269   train_loss = 3.855\n",
      "Epoch  71 Batch  175/269   train_loss = 3.750\n",
      "Epoch  71 Batch  176/269   train_loss = 3.674\n",
      "Epoch  71 Batch  177/269   train_loss = 3.776\n",
      "Epoch  71 Batch  178/269   train_loss = 3.984\n",
      "Epoch  71 Batch  179/269   train_loss = 3.741\n",
      "Epoch  71 Batch  180/269   train_loss = 3.660\n",
      "Epoch  71 Batch  181/269   train_loss = 3.837\n",
      "Epoch  71 Batch  182/269   train_loss = 3.660\n",
      "Epoch  71 Batch  183/269   train_loss = 3.633\n",
      "Epoch  71 Batch  184/269   train_loss = 3.756\n",
      "Epoch  71 Batch  185/269   train_loss = 3.787\n",
      "Epoch  71 Batch  186/269   train_loss = 3.693\n",
      "Epoch  71 Batch  187/269   train_loss = 3.884\n",
      "Epoch  71 Batch  188/269   train_loss = 3.613\n",
      "Epoch  71 Batch  189/269   train_loss = 3.676\n",
      "Epoch  71 Batch  190/269   train_loss = 4.073\n",
      "Epoch  71 Batch  191/269   train_loss = 3.791\n",
      "Epoch  71 Batch  192/269   train_loss = 3.744\n",
      "Epoch  71 Batch  193/269   train_loss = 3.711\n",
      "Epoch  71 Batch  194/269   train_loss = 3.783\n",
      "Epoch  71 Batch  195/269   train_loss = 3.762\n",
      "Epoch  71 Batch  196/269   train_loss = 3.727\n",
      "Epoch  71 Batch  197/269   train_loss = 3.964\n",
      "Epoch  71 Batch  198/269   train_loss = 3.744\n",
      "Epoch  71 Batch  199/269   train_loss = 3.829\n",
      "Epoch  71 Batch  200/269   train_loss = 3.593\n",
      "Epoch  71 Batch  201/269   train_loss = 3.737\n",
      "Epoch  71 Batch  202/269   train_loss = 3.516\n",
      "Epoch  71 Batch  203/269   train_loss = 3.638\n",
      "Epoch  71 Batch  204/269   train_loss = 3.855\n",
      "Epoch  71 Batch  205/269   train_loss = 3.758\n",
      "Epoch  71 Batch  206/269   train_loss = 3.672\n",
      "Epoch  71 Batch  207/269   train_loss = 3.630\n",
      "Epoch  71 Batch  208/269   train_loss = 3.755\n",
      "Epoch  71 Batch  209/269   train_loss = 3.839\n",
      "Epoch  71 Batch  210/269   train_loss = 3.685\n",
      "Epoch  71 Batch  211/269   train_loss = 3.597\n",
      "Epoch  71 Batch  212/269   train_loss = 4.051\n",
      "Epoch  71 Batch  213/269   train_loss = 3.646\n",
      "Epoch  71 Batch  214/269   train_loss = 3.720\n",
      "Epoch  71 Batch  215/269   train_loss = 3.905\n",
      "Epoch  71 Batch  216/269   train_loss = 3.889\n",
      "Epoch  71 Batch  217/269   train_loss = 3.562\n",
      "Epoch  71 Batch  218/269   train_loss = 3.817\n",
      "Epoch  71 Batch  219/269   train_loss = 3.494\n",
      "Epoch  71 Batch  220/269   train_loss = 3.881\n",
      "Epoch  71 Batch  221/269   train_loss = 3.595\n",
      "Epoch  71 Batch  222/269   train_loss = 3.795\n",
      "Epoch  71 Batch  223/269   train_loss = 3.513\n",
      "Epoch  71 Batch  224/269   train_loss = 3.907\n",
      "Epoch  71 Batch  225/269   train_loss = 3.896\n",
      "Epoch  71 Batch  226/269   train_loss = 3.731\n",
      "Epoch  71 Batch  227/269   train_loss = 3.461\n",
      "Epoch  71 Batch  228/269   train_loss = 3.744\n",
      "Epoch  71 Batch  229/269   train_loss = 3.902\n",
      "Epoch  71 Batch  230/269   train_loss = 3.834\n",
      "Epoch  71 Batch  231/269   train_loss = 3.703\n",
      "Epoch  71 Batch  232/269   train_loss = 3.722\n",
      "Epoch  71 Batch  233/269   train_loss = 3.814\n",
      "Epoch  71 Batch  234/269   train_loss = 3.701\n",
      "Epoch  71 Batch  235/269   train_loss = 3.969\n",
      "Epoch  71 Batch  236/269   train_loss = 3.612\n",
      "Epoch  71 Batch  237/269   train_loss = 3.263\n",
      "Epoch  71 Batch  238/269   train_loss = 3.460\n",
      "Epoch  71 Batch  239/269   train_loss = 4.026\n",
      "Epoch  71 Batch  240/269   train_loss = 3.541\n",
      "Epoch  71 Batch  241/269   train_loss = 3.924\n",
      "Epoch  71 Batch  242/269   train_loss = 3.488\n",
      "Epoch  71 Batch  243/269   train_loss = 3.879\n",
      "Epoch  71 Batch  244/269   train_loss = 3.725\n",
      "Epoch  71 Batch  245/269   train_loss = 3.684\n",
      "Epoch  71 Batch  246/269   train_loss = 3.354\n",
      "Epoch  71 Batch  247/269   train_loss = 3.744\n",
      "Epoch  71 Batch  248/269   train_loss = 3.785\n",
      "Epoch  71 Batch  249/269   train_loss = 3.660\n",
      "Epoch  71 Batch  250/269   train_loss = 3.553\n",
      "Epoch  71 Batch  251/269   train_loss = 3.946\n",
      "Epoch  71 Batch  252/269   train_loss = 3.824\n",
      "Epoch  71 Batch  253/269   train_loss = 3.631\n",
      "Epoch  71 Batch  254/269   train_loss = 3.625\n",
      "Epoch  71 Batch  255/269   train_loss = 3.730\n",
      "Epoch  71 Batch  256/269   train_loss = 3.663\n",
      "Epoch  71 Batch  257/269   train_loss = 3.611\n",
      "Epoch  71 Batch  258/269   train_loss = 3.459\n",
      "Epoch  71 Batch  259/269   train_loss = 3.536\n",
      "Epoch  71 Batch  260/269   train_loss = 3.826\n",
      "Epoch  71 Batch  261/269   train_loss = 3.763\n",
      "Epoch  71 Batch  262/269   train_loss = 3.687\n",
      "Epoch  71 Batch  263/269   train_loss = 3.545\n",
      "Epoch  71 Batch  264/269   train_loss = 3.909\n",
      "Epoch  71 Batch  265/269   train_loss = 3.757\n",
      "Epoch  71 Batch  266/269   train_loss = 3.638\n",
      "Epoch  71 Batch  267/269   train_loss = 3.732\n",
      "Epoch  71 Batch  268/269   train_loss = 3.860\n",
      "Epoch  72 Batch    0/269   train_loss = 3.616\n",
      "Epoch  72 Batch    1/269   train_loss = 3.417\n",
      "Epoch  72 Batch    2/269   train_loss = 3.558\n",
      "Epoch  72 Batch    3/269   train_loss = 3.642\n",
      "Epoch  72 Batch    4/269   train_loss = 4.135\n",
      "Epoch  72 Batch    5/269   train_loss = 3.671\n",
      "Epoch  72 Batch    6/269   train_loss = 3.604\n",
      "Epoch  72 Batch    7/269   train_loss = 3.433\n",
      "Epoch  72 Batch    8/269   train_loss = 3.752\n",
      "Epoch  72 Batch    9/269   train_loss = 3.436\n",
      "Epoch  72 Batch   10/269   train_loss = 3.609\n",
      "Epoch  72 Batch   11/269   train_loss = 3.632\n",
      "Epoch  72 Batch   12/269   train_loss = 3.418\n",
      "Epoch  72 Batch   13/269   train_loss = 3.672\n",
      "Epoch  72 Batch   14/269   train_loss = 3.601\n",
      "Epoch  72 Batch   15/269   train_loss = 3.889\n",
      "Epoch  72 Batch   16/269   train_loss = 3.666\n",
      "Epoch  72 Batch   17/269   train_loss = 3.694\n",
      "Epoch  72 Batch   18/269   train_loss = 3.632\n",
      "Epoch  72 Batch   19/269   train_loss = 3.645\n",
      "Epoch  72 Batch   20/269   train_loss = 4.055\n",
      "Epoch  72 Batch   21/269   train_loss = 3.891\n",
      "Epoch  72 Batch   22/269   train_loss = 3.587\n",
      "Epoch  72 Batch   23/269   train_loss = 3.770\n",
      "Epoch  72 Batch   24/269   train_loss = 3.481\n",
      "Epoch  72 Batch   25/269   train_loss = 3.901\n",
      "Epoch  72 Batch   26/269   train_loss = 3.774\n",
      "Epoch  72 Batch   27/269   train_loss = 3.599\n",
      "Epoch  72 Batch   28/269   train_loss = 3.742\n",
      "Epoch  72 Batch   29/269   train_loss = 3.762\n",
      "Epoch  72 Batch   30/269   train_loss = 4.060\n",
      "Epoch  72 Batch   31/269   train_loss = 3.732\n",
      "Epoch  72 Batch   32/269   train_loss = 3.602\n",
      "Epoch  72 Batch   33/269   train_loss = 3.401\n",
      "Epoch  72 Batch   34/269   train_loss = 3.593\n",
      "Epoch  72 Batch   35/269   train_loss = 3.481\n",
      "Epoch  72 Batch   36/269   train_loss = 3.566\n",
      "Epoch  72 Batch   37/269   train_loss = 3.730\n",
      "Epoch  72 Batch   38/269   train_loss = 3.416\n",
      "Epoch  72 Batch   39/269   train_loss = 3.819\n",
      "Epoch  72 Batch   40/269   train_loss = 3.631\n",
      "Epoch  72 Batch   41/269   train_loss = 3.513\n",
      "Epoch  72 Batch   42/269   train_loss = 4.003\n",
      "Epoch  72 Batch   43/269   train_loss = 3.965\n",
      "Epoch  72 Batch   44/269   train_loss = 3.736\n",
      "Epoch  72 Batch   45/269   train_loss = 3.391\n",
      "Epoch  72 Batch   46/269   train_loss = 3.782\n",
      "Epoch  72 Batch   47/269   train_loss = 3.449\n",
      "Epoch  72 Batch   48/269   train_loss = 3.863\n",
      "Epoch  72 Batch   49/269   train_loss = 3.865\n",
      "Epoch  72 Batch   50/269   train_loss = 3.785\n",
      "Epoch  72 Batch   51/269   train_loss = 3.763\n",
      "Epoch  72 Batch   52/269   train_loss = 3.707\n",
      "Epoch  72 Batch   53/269   train_loss = 3.952\n",
      "Epoch  72 Batch   54/269   train_loss = 3.651\n",
      "Epoch  72 Batch   55/269   train_loss = 3.783\n",
      "Epoch  72 Batch   56/269   train_loss = 3.539\n",
      "Epoch  72 Batch   57/269   train_loss = 3.903\n",
      "Epoch  72 Batch   58/269   train_loss = 3.519\n",
      "Epoch  72 Batch   59/269   train_loss = 3.597\n",
      "Epoch  72 Batch   60/269   train_loss = 3.739\n",
      "Epoch  72 Batch   61/269   train_loss = 3.937\n",
      "Epoch  72 Batch   62/269   train_loss = 3.592\n",
      "Epoch  72 Batch   63/269   train_loss = 3.876\n",
      "Epoch  72 Batch   64/269   train_loss = 3.580\n",
      "Epoch  72 Batch   65/269   train_loss = 3.719\n",
      "Epoch  72 Batch   66/269   train_loss = 3.664\n",
      "Epoch  72 Batch   67/269   train_loss = 3.643\n",
      "Epoch  72 Batch   68/269   train_loss = 3.726\n",
      "Epoch  72 Batch   69/269   train_loss = 3.722\n",
      "Epoch  72 Batch   70/269   train_loss = 3.449\n",
      "Epoch  72 Batch   71/269   train_loss = 3.791\n",
      "Epoch  72 Batch   72/269   train_loss = 3.434\n",
      "Epoch  72 Batch   73/269   train_loss = 3.701\n",
      "Epoch  72 Batch   74/269   train_loss = 3.673\n",
      "Epoch  72 Batch   75/269   train_loss = 3.840\n",
      "Epoch  72 Batch   76/269   train_loss = 3.783\n",
      "Epoch  72 Batch   77/269   train_loss = 3.444\n",
      "Epoch  72 Batch   78/269   train_loss = 3.826\n",
      "Epoch  72 Batch   79/269   train_loss = 3.904\n",
      "Epoch  72 Batch   80/269   train_loss = 3.792\n",
      "Epoch  72 Batch   81/269   train_loss = 3.626\n",
      "Epoch  72 Batch   82/269   train_loss = 3.794\n",
      "Epoch  72 Batch   83/269   train_loss = 3.647\n",
      "Epoch  72 Batch   84/269   train_loss = 3.956\n",
      "Epoch  72 Batch   85/269   train_loss = 4.075\n",
      "Epoch  72 Batch   86/269   train_loss = 4.127\n",
      "Epoch  72 Batch   87/269   train_loss = 3.708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  72 Batch   88/269   train_loss = 3.832\n",
      "Epoch  72 Batch   89/269   train_loss = 3.702\n",
      "Epoch  72 Batch   90/269   train_loss = 3.622\n",
      "Epoch  72 Batch   91/269   train_loss = 3.774\n",
      "Epoch  72 Batch   92/269   train_loss = 3.842\n",
      "Epoch  72 Batch   93/269   train_loss = 3.588\n",
      "Epoch  72 Batch   94/269   train_loss = 4.075\n",
      "Epoch  72 Batch   95/269   train_loss = 3.472\n",
      "Epoch  72 Batch   96/269   train_loss = 3.572\n",
      "Epoch  72 Batch   97/269   train_loss = 3.662\n",
      "Epoch  72 Batch   98/269   train_loss = 3.945\n",
      "Epoch  72 Batch   99/269   train_loss = 3.765\n",
      "Epoch  72 Batch  100/269   train_loss = 3.744\n",
      "Epoch  72 Batch  101/269   train_loss = 3.764\n",
      "Epoch  72 Batch  102/269   train_loss = 3.551\n",
      "Epoch  72 Batch  103/269   train_loss = 3.638\n",
      "Epoch  72 Batch  104/269   train_loss = 3.645\n",
      "Epoch  72 Batch  105/269   train_loss = 3.883\n",
      "Epoch  72 Batch  106/269   train_loss = 3.944\n",
      "Epoch  72 Batch  107/269   train_loss = 3.959\n",
      "Epoch  72 Batch  108/269   train_loss = 3.708\n",
      "Epoch  72 Batch  109/269   train_loss = 3.752\n",
      "Epoch  72 Batch  110/269   train_loss = 3.742\n",
      "Epoch  72 Batch  111/269   train_loss = 3.868\n",
      "Epoch  72 Batch  112/269   train_loss = 3.950\n",
      "Epoch  72 Batch  113/269   train_loss = 3.745\n",
      "Epoch  72 Batch  114/269   train_loss = 3.653\n",
      "Epoch  72 Batch  115/269   train_loss = 3.623\n",
      "Epoch  72 Batch  116/269   train_loss = 3.560\n",
      "Epoch  72 Batch  117/269   train_loss = 3.610\n",
      "Epoch  72 Batch  118/269   train_loss = 3.905\n",
      "Epoch  72 Batch  119/269   train_loss = 3.569\n",
      "Epoch  72 Batch  120/269   train_loss = 3.679\n",
      "Epoch  72 Batch  121/269   train_loss = 3.743\n",
      "Epoch  72 Batch  122/269   train_loss = 3.740\n",
      "Epoch  72 Batch  123/269   train_loss = 3.515\n",
      "Epoch  72 Batch  124/269   train_loss = 3.870\n",
      "Epoch  72 Batch  125/269   train_loss = 3.552\n",
      "Epoch  72 Batch  126/269   train_loss = 3.423\n",
      "Epoch  72 Batch  127/269   train_loss = 3.762\n",
      "Epoch  72 Batch  128/269   train_loss = 3.632\n",
      "Epoch  72 Batch  129/269   train_loss = 3.600\n",
      "Epoch  72 Batch  130/269   train_loss = 3.747\n",
      "Epoch  72 Batch  131/269   train_loss = 3.411\n",
      "Epoch  72 Batch  132/269   train_loss = 3.745\n",
      "Epoch  72 Batch  133/269   train_loss = 3.465\n",
      "Epoch  72 Batch  134/269   train_loss = 3.886\n",
      "Epoch  72 Batch  135/269   train_loss = 3.602\n",
      "Epoch  72 Batch  136/269   train_loss = 3.399\n",
      "Epoch  72 Batch  137/269   train_loss = 3.818\n",
      "Epoch  72 Batch  138/269   train_loss = 3.538\n",
      "Epoch  72 Batch  139/269   train_loss = 3.664\n",
      "Epoch  72 Batch  140/269   train_loss = 3.744\n",
      "Epoch  72 Batch  141/269   train_loss = 4.077\n",
      "Epoch  72 Batch  142/269   train_loss = 3.463\n",
      "Epoch  72 Batch  143/269   train_loss = 3.682\n",
      "Epoch  72 Batch  144/269   train_loss = 3.932\n",
      "Epoch  72 Batch  145/269   train_loss = 3.652\n",
      "Epoch  72 Batch  146/269   train_loss = 3.867\n",
      "Epoch  72 Batch  147/269   train_loss = 3.476\n",
      "Epoch  72 Batch  148/269   train_loss = 3.779\n",
      "Epoch  72 Batch  149/269   train_loss = 3.584\n",
      "Epoch  72 Batch  150/269   train_loss = 3.827\n",
      "Epoch  72 Batch  151/269   train_loss = 3.796\n",
      "Epoch  72 Batch  152/269   train_loss = 3.699\n",
      "Epoch  72 Batch  153/269   train_loss = 3.733\n",
      "Epoch  72 Batch  154/269   train_loss = 3.625\n",
      "Epoch  72 Batch  155/269   train_loss = 3.656\n",
      "Epoch  72 Batch  156/269   train_loss = 4.099\n",
      "Epoch  72 Batch  157/269   train_loss = 3.772\n",
      "Epoch  72 Batch  158/269   train_loss = 3.726\n",
      "Epoch  72 Batch  159/269   train_loss = 3.836\n",
      "Epoch  72 Batch  160/269   train_loss = 3.630\n",
      "Epoch  72 Batch  161/269   train_loss = 3.816\n",
      "Epoch  72 Batch  162/269   train_loss = 3.846\n",
      "Epoch  72 Batch  163/269   train_loss = 3.622\n",
      "Epoch  72 Batch  164/269   train_loss = 3.903\n",
      "Epoch  72 Batch  165/269   train_loss = 3.903\n",
      "Epoch  72 Batch  166/269   train_loss = 3.788\n",
      "Epoch  72 Batch  167/269   train_loss = 3.610\n",
      "Epoch  72 Batch  168/269   train_loss = 3.738\n",
      "Epoch  72 Batch  169/269   train_loss = 3.981\n",
      "Epoch  72 Batch  170/269   train_loss = 3.446\n",
      "Epoch  72 Batch  171/269   train_loss = 3.738\n",
      "Epoch  72 Batch  172/269   train_loss = 3.567\n",
      "Epoch  72 Batch  173/269   train_loss = 3.728\n",
      "Epoch  72 Batch  174/269   train_loss = 3.833\n",
      "Epoch  72 Batch  175/269   train_loss = 3.740\n",
      "Epoch  72 Batch  176/269   train_loss = 3.691\n",
      "Epoch  72 Batch  177/269   train_loss = 3.747\n",
      "Epoch  72 Batch  178/269   train_loss = 4.008\n",
      "Epoch  72 Batch  179/269   train_loss = 3.720\n",
      "Epoch  72 Batch  180/269   train_loss = 3.679\n",
      "Epoch  72 Batch  181/269   train_loss = 3.836\n",
      "Epoch  72 Batch  182/269   train_loss = 3.679\n",
      "Epoch  72 Batch  183/269   train_loss = 3.646\n",
      "Epoch  72 Batch  184/269   train_loss = 3.709\n",
      "Epoch  72 Batch  185/269   train_loss = 3.785\n",
      "Epoch  72 Batch  186/269   train_loss = 3.687\n",
      "Epoch  72 Batch  187/269   train_loss = 3.839\n",
      "Epoch  72 Batch  188/269   train_loss = 3.604\n",
      "Epoch  72 Batch  189/269   train_loss = 3.682\n",
      "Epoch  72 Batch  190/269   train_loss = 4.082\n",
      "Epoch  72 Batch  191/269   train_loss = 3.785\n",
      "Epoch  72 Batch  192/269   train_loss = 3.708\n",
      "Epoch  72 Batch  193/269   train_loss = 3.716\n",
      "Epoch  72 Batch  194/269   train_loss = 3.783\n",
      "Epoch  72 Batch  195/269   train_loss = 3.753\n",
      "Epoch  72 Batch  196/269   train_loss = 3.730\n",
      "Epoch  72 Batch  197/269   train_loss = 3.951\n",
      "Epoch  72 Batch  198/269   train_loss = 3.747\n",
      "Epoch  72 Batch  199/269   train_loss = 3.842\n",
      "Epoch  72 Batch  200/269   train_loss = 3.588\n",
      "Epoch  72 Batch  201/269   train_loss = 3.742\n",
      "Epoch  72 Batch  202/269   train_loss = 3.534\n",
      "Epoch  72 Batch  203/269   train_loss = 3.630\n",
      "Epoch  72 Batch  204/269   train_loss = 3.849\n",
      "Epoch  72 Batch  205/269   train_loss = 3.759\n",
      "Epoch  72 Batch  206/269   train_loss = 3.686\n",
      "Epoch  72 Batch  207/269   train_loss = 3.629\n",
      "Epoch  72 Batch  208/269   train_loss = 3.733\n",
      "Epoch  72 Batch  209/269   train_loss = 3.813\n",
      "Epoch  72 Batch  210/269   train_loss = 3.693\n",
      "Epoch  72 Batch  211/269   train_loss = 3.596\n",
      "Epoch  72 Batch  212/269   train_loss = 4.024\n",
      "Epoch  72 Batch  213/269   train_loss = 3.625\n",
      "Epoch  72 Batch  214/269   train_loss = 3.702\n",
      "Epoch  72 Batch  215/269   train_loss = 3.937\n",
      "Epoch  72 Batch  216/269   train_loss = 3.890\n",
      "Epoch  72 Batch  217/269   train_loss = 3.567\n",
      "Epoch  72 Batch  218/269   train_loss = 3.812\n",
      "Epoch  72 Batch  219/269   train_loss = 3.484\n",
      "Epoch  72 Batch  220/269   train_loss = 3.871\n",
      "Epoch  72 Batch  221/269   train_loss = 3.599\n",
      "Epoch  72 Batch  222/269   train_loss = 3.749\n",
      "Epoch  72 Batch  223/269   train_loss = 3.484\n",
      "Epoch  72 Batch  224/269   train_loss = 3.915\n",
      "Epoch  72 Batch  225/269   train_loss = 3.898\n",
      "Epoch  72 Batch  226/269   train_loss = 3.698\n",
      "Epoch  72 Batch  227/269   train_loss = 3.469\n",
      "Epoch  72 Batch  228/269   train_loss = 3.731\n",
      "Epoch  72 Batch  229/269   train_loss = 3.883\n",
      "Epoch  72 Batch  230/269   train_loss = 3.825\n",
      "Epoch  72 Batch  231/269   train_loss = 3.693\n",
      "Epoch  72 Batch  232/269   train_loss = 3.684\n",
      "Epoch  72 Batch  233/269   train_loss = 3.788\n",
      "Epoch  72 Batch  234/269   train_loss = 3.690\n",
      "Epoch  72 Batch  235/269   train_loss = 3.972\n",
      "Epoch  72 Batch  236/269   train_loss = 3.603\n",
      "Epoch  72 Batch  237/269   train_loss = 3.264\n",
      "Epoch  72 Batch  238/269   train_loss = 3.456\n",
      "Epoch  72 Batch  239/269   train_loss = 3.996\n",
      "Epoch  72 Batch  240/269   train_loss = 3.539\n",
      "Epoch  72 Batch  241/269   train_loss = 3.920\n",
      "Epoch  72 Batch  242/269   train_loss = 3.496\n",
      "Epoch  72 Batch  243/269   train_loss = 3.876\n",
      "Epoch  72 Batch  244/269   train_loss = 3.725\n",
      "Epoch  72 Batch  245/269   train_loss = 3.693\n",
      "Epoch  72 Batch  246/269   train_loss = 3.366\n",
      "Epoch  72 Batch  247/269   train_loss = 3.739\n",
      "Epoch  72 Batch  248/269   train_loss = 3.795\n",
      "Epoch  72 Batch  249/269   train_loss = 3.634\n",
      "Epoch  72 Batch  250/269   train_loss = 3.519\n",
      "Epoch  72 Batch  251/269   train_loss = 3.938\n",
      "Epoch  72 Batch  252/269   train_loss = 3.819\n",
      "Epoch  72 Batch  253/269   train_loss = 3.624\n",
      "Epoch  72 Batch  254/269   train_loss = 3.628\n",
      "Epoch  72 Batch  255/269   train_loss = 3.725\n",
      "Epoch  72 Batch  256/269   train_loss = 3.669\n",
      "Epoch  72 Batch  257/269   train_loss = 3.588\n",
      "Epoch  72 Batch  258/269   train_loss = 3.459\n",
      "Epoch  72 Batch  259/269   train_loss = 3.527\n",
      "Epoch  72 Batch  260/269   train_loss = 3.816\n",
      "Epoch  72 Batch  261/269   train_loss = 3.760\n",
      "Epoch  72 Batch  262/269   train_loss = 3.675\n",
      "Epoch  72 Batch  263/269   train_loss = 3.555\n",
      "Epoch  72 Batch  264/269   train_loss = 3.902\n",
      "Epoch  72 Batch  265/269   train_loss = 3.753\n",
      "Epoch  72 Batch  266/269   train_loss = 3.609\n",
      "Epoch  72 Batch  267/269   train_loss = 3.731\n",
      "Epoch  72 Batch  268/269   train_loss = 3.879\n",
      "Epoch  73 Batch    0/269   train_loss = 3.610\n",
      "Epoch  73 Batch    1/269   train_loss = 3.405\n",
      "Epoch  73 Batch    2/269   train_loss = 3.565\n",
      "Epoch  73 Batch    3/269   train_loss = 3.645\n",
      "Epoch  73 Batch    4/269   train_loss = 4.141\n",
      "Epoch  73 Batch    5/269   train_loss = 3.652\n",
      "Epoch  73 Batch    6/269   train_loss = 3.592\n",
      "Epoch  73 Batch    7/269   train_loss = 3.444\n",
      "Epoch  73 Batch    8/269   train_loss = 3.748\n",
      "Epoch  73 Batch    9/269   train_loss = 3.453\n",
      "Epoch  73 Batch   10/269   train_loss = 3.594\n",
      "Epoch  73 Batch   11/269   train_loss = 3.638\n",
      "Epoch  73 Batch   12/269   train_loss = 3.383\n",
      "Epoch  73 Batch   13/269   train_loss = 3.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  73 Batch   14/269   train_loss = 3.584\n",
      "Epoch  73 Batch   15/269   train_loss = 3.908\n",
      "Epoch  73 Batch   16/269   train_loss = 3.680\n",
      "Epoch  73 Batch   17/269   train_loss = 3.659\n",
      "Epoch  73 Batch   18/269   train_loss = 3.641\n",
      "Epoch  73 Batch   19/269   train_loss = 3.665\n",
      "Epoch  73 Batch   20/269   train_loss = 4.062\n",
      "Epoch  73 Batch   21/269   train_loss = 3.897\n",
      "Epoch  73 Batch   22/269   train_loss = 3.615\n",
      "Epoch  73 Batch   23/269   train_loss = 3.795\n",
      "Epoch  73 Batch   24/269   train_loss = 3.476\n",
      "Epoch  73 Batch   25/269   train_loss = 3.901\n",
      "Epoch  73 Batch   26/269   train_loss = 3.762\n",
      "Epoch  73 Batch   27/269   train_loss = 3.598\n",
      "Epoch  73 Batch   28/269   train_loss = 3.734\n",
      "Epoch  73 Batch   29/269   train_loss = 3.785\n",
      "Epoch  73 Batch   30/269   train_loss = 4.095\n",
      "Epoch  73 Batch   31/269   train_loss = 3.712\n",
      "Epoch  73 Batch   32/269   train_loss = 3.592\n",
      "Epoch  73 Batch   33/269   train_loss = 3.398\n",
      "Epoch  73 Batch   34/269   train_loss = 3.605\n",
      "Epoch  73 Batch   35/269   train_loss = 3.467\n",
      "Epoch  73 Batch   36/269   train_loss = 3.562\n",
      "Epoch  73 Batch   37/269   train_loss = 3.775\n",
      "Epoch  73 Batch   38/269   train_loss = 3.425\n",
      "Epoch  73 Batch   39/269   train_loss = 3.807\n",
      "Epoch  73 Batch   40/269   train_loss = 3.609\n",
      "Epoch  73 Batch   41/269   train_loss = 3.520\n",
      "Epoch  73 Batch   42/269   train_loss = 3.989\n",
      "Epoch  73 Batch   43/269   train_loss = 3.983\n",
      "Epoch  73 Batch   44/269   train_loss = 3.745\n",
      "Epoch  73 Batch   45/269   train_loss = 3.396\n",
      "Epoch  73 Batch   46/269   train_loss = 3.763\n",
      "Epoch  73 Batch   47/269   train_loss = 3.441\n",
      "Epoch  73 Batch   48/269   train_loss = 3.874\n",
      "Epoch  73 Batch   49/269   train_loss = 3.896\n",
      "Epoch  73 Batch   50/269   train_loss = 3.799\n",
      "Epoch  73 Batch   51/269   train_loss = 3.753\n",
      "Epoch  73 Batch   52/269   train_loss = 3.729\n",
      "Epoch  73 Batch   53/269   train_loss = 3.933\n",
      "Epoch  73 Batch   54/269   train_loss = 3.645\n",
      "Epoch  73 Batch   55/269   train_loss = 3.784\n",
      "Epoch  73 Batch   56/269   train_loss = 3.534\n",
      "Epoch  73 Batch   57/269   train_loss = 3.914\n",
      "Epoch  73 Batch   58/269   train_loss = 3.526\n",
      "Epoch  73 Batch   59/269   train_loss = 3.611\n",
      "Epoch  73 Batch   60/269   train_loss = 3.749\n",
      "Epoch  73 Batch   61/269   train_loss = 3.935\n",
      "Epoch  73 Batch   62/269   train_loss = 3.547\n",
      "Epoch  73 Batch   63/269   train_loss = 3.880\n",
      "Epoch  73 Batch   64/269   train_loss = 3.606\n",
      "Epoch  73 Batch   65/269   train_loss = 3.743\n",
      "Epoch  73 Batch   66/269   train_loss = 3.637\n",
      "Epoch  73 Batch   67/269   train_loss = 3.636\n",
      "Epoch  73 Batch   68/269   train_loss = 3.740\n",
      "Epoch  73 Batch   69/269   train_loss = 3.749\n",
      "Epoch  73 Batch   70/269   train_loss = 3.517\n",
      "Epoch  73 Batch   71/269   train_loss = 3.797\n",
      "Epoch  73 Batch   72/269   train_loss = 3.462\n",
      "Epoch  73 Batch   73/269   train_loss = 3.738\n",
      "Epoch  73 Batch   74/269   train_loss = 3.669\n",
      "Epoch  73 Batch   75/269   train_loss = 3.867\n",
      "Epoch  73 Batch   76/269   train_loss = 3.793\n",
      "Epoch  73 Batch   77/269   train_loss = 3.433\n",
      "Epoch  73 Batch   78/269   train_loss = 3.856\n",
      "Epoch  73 Batch   79/269   train_loss = 3.882\n",
      "Epoch  73 Batch   80/269   train_loss = 3.811\n",
      "Epoch  73 Batch   81/269   train_loss = 3.624\n",
      "Epoch  73 Batch   82/269   train_loss = 3.817\n",
      "Epoch  73 Batch   83/269   train_loss = 3.585\n",
      "Epoch  73 Batch   84/269   train_loss = 3.953\n",
      "Epoch  73 Batch   85/269   train_loss = 4.092\n",
      "Epoch  73 Batch   86/269   train_loss = 4.133\n",
      "Epoch  73 Batch   87/269   train_loss = 3.697\n",
      "Epoch  73 Batch   88/269   train_loss = 3.802\n",
      "Epoch  73 Batch   89/269   train_loss = 3.684\n",
      "Epoch  73 Batch   90/269   train_loss = 3.593\n",
      "Epoch  73 Batch   91/269   train_loss = 3.760\n",
      "Epoch  73 Batch   92/269   train_loss = 3.900\n",
      "Epoch  73 Batch   93/269   train_loss = 3.596\n",
      "Epoch  73 Batch   94/269   train_loss = 4.116\n",
      "Epoch  73 Batch   95/269   train_loss = 3.465\n",
      "Epoch  73 Batch   96/269   train_loss = 3.607\n",
      "Epoch  73 Batch   97/269   train_loss = 3.712\n",
      "Epoch  73 Batch   98/269   train_loss = 3.964\n",
      "Epoch  73 Batch   99/269   train_loss = 3.795\n",
      "Epoch  73 Batch  100/269   train_loss = 3.759\n",
      "Epoch  73 Batch  101/269   train_loss = 3.797\n",
      "Epoch  73 Batch  102/269   train_loss = 3.616\n",
      "Epoch  73 Batch  103/269   train_loss = 3.712\n",
      "Epoch  73 Batch  104/269   train_loss = 3.696\n",
      "Epoch  73 Batch  105/269   train_loss = 3.887\n",
      "Epoch  73 Batch  106/269   train_loss = 3.972\n",
      "Epoch  73 Batch  107/269   train_loss = 3.944\n",
      "Epoch  73 Batch  108/269   train_loss = 3.731\n",
      "Epoch  73 Batch  109/269   train_loss = 3.743\n",
      "Epoch  73 Batch  110/269   train_loss = 3.755\n",
      "Epoch  73 Batch  111/269   train_loss = 3.923\n",
      "Epoch  73 Batch  112/269   train_loss = 3.941\n",
      "Epoch  73 Batch  113/269   train_loss = 3.741\n",
      "Epoch  73 Batch  114/269   train_loss = 3.660\n",
      "Epoch  73 Batch  115/269   train_loss = 3.639\n",
      "Epoch  73 Batch  116/269   train_loss = 3.557\n",
      "Epoch  73 Batch  117/269   train_loss = 3.616\n",
      "Epoch  73 Batch  118/269   train_loss = 3.964\n",
      "Epoch  73 Batch  119/269   train_loss = 3.562\n",
      "Epoch  73 Batch  120/269   train_loss = 3.688\n",
      "Epoch  73 Batch  121/269   train_loss = 3.755\n",
      "Epoch  73 Batch  122/269   train_loss = 3.749\n",
      "Epoch  73 Batch  123/269   train_loss = 3.498\n",
      "Epoch  73 Batch  124/269   train_loss = 3.893\n",
      "Epoch  73 Batch  125/269   train_loss = 3.522\n",
      "Epoch  73 Batch  126/269   train_loss = 3.450\n",
      "Epoch  73 Batch  127/269   train_loss = 3.768\n",
      "Epoch  73 Batch  128/269   train_loss = 3.641\n",
      "Epoch  73 Batch  129/269   train_loss = 3.627\n",
      "Epoch  73 Batch  130/269   train_loss = 3.743\n",
      "Epoch  73 Batch  131/269   train_loss = 3.419\n",
      "Epoch  73 Batch  132/269   train_loss = 3.757\n",
      "Epoch  73 Batch  133/269   train_loss = 3.477\n",
      "Epoch  73 Batch  134/269   train_loss = 3.918\n",
      "Epoch  73 Batch  135/269   train_loss = 3.587\n",
      "Epoch  73 Batch  136/269   train_loss = 3.406\n",
      "Epoch  73 Batch  137/269   train_loss = 3.825\n",
      "Epoch  73 Batch  138/269   train_loss = 3.548\n",
      "Epoch  73 Batch  139/269   train_loss = 3.665\n",
      "Epoch  73 Batch  140/269   train_loss = 3.762\n",
      "Epoch  73 Batch  141/269   train_loss = 4.046\n",
      "Epoch  73 Batch  142/269   train_loss = 3.492\n",
      "Epoch  73 Batch  143/269   train_loss = 3.696\n",
      "Epoch  73 Batch  144/269   train_loss = 3.957\n",
      "Epoch  73 Batch  145/269   train_loss = 3.666\n",
      "Epoch  73 Batch  146/269   train_loss = 3.878\n",
      "Epoch  73 Batch  147/269   train_loss = 3.506\n",
      "Epoch  73 Batch  148/269   train_loss = 3.771\n",
      "Epoch  73 Batch  149/269   train_loss = 3.579\n",
      "Epoch  73 Batch  150/269   train_loss = 3.803\n",
      "Epoch  73 Batch  151/269   train_loss = 3.787\n",
      "Epoch  73 Batch  152/269   train_loss = 3.740\n",
      "Epoch  73 Batch  153/269   train_loss = 3.745\n",
      "Epoch  73 Batch  154/269   train_loss = 3.641\n",
      "Epoch  73 Batch  155/269   train_loss = 3.665\n",
      "Epoch  73 Batch  156/269   train_loss = 4.135\n",
      "Epoch  73 Batch  157/269   train_loss = 3.820\n",
      "Epoch  73 Batch  158/269   train_loss = 3.761\n",
      "Epoch  73 Batch  159/269   train_loss = 3.807\n",
      "Epoch  73 Batch  160/269   train_loss = 3.642\n",
      "Epoch  73 Batch  161/269   train_loss = 3.873\n",
      "Epoch  73 Batch  162/269   train_loss = 3.878\n",
      "Epoch  73 Batch  163/269   train_loss = 3.630\n",
      "Epoch  73 Batch  164/269   train_loss = 3.920\n",
      "Epoch  73 Batch  165/269   train_loss = 3.973\n",
      "Epoch  73 Batch  166/269   train_loss = 3.839\n",
      "Epoch  73 Batch  167/269   train_loss = 3.577\n",
      "Epoch  73 Batch  168/269   train_loss = 3.737\n",
      "Epoch  73 Batch  169/269   train_loss = 3.972\n",
      "Epoch  73 Batch  170/269   train_loss = 3.465\n",
      "Epoch  73 Batch  171/269   train_loss = 3.743\n",
      "Epoch  73 Batch  172/269   train_loss = 3.604\n",
      "Epoch  73 Batch  173/269   train_loss = 3.745\n",
      "Epoch  73 Batch  174/269   train_loss = 3.887\n",
      "Epoch  73 Batch  175/269   train_loss = 3.730\n",
      "Epoch  73 Batch  176/269   train_loss = 3.658\n",
      "Epoch  73 Batch  177/269   train_loss = 3.793\n",
      "Epoch  73 Batch  178/269   train_loss = 4.044\n",
      "Epoch  73 Batch  179/269   train_loss = 3.797\n",
      "Epoch  73 Batch  180/269   train_loss = 3.663\n",
      "Epoch  73 Batch  181/269   train_loss = 3.873\n",
      "Epoch  73 Batch  182/269   train_loss = 3.641\n",
      "Epoch  73 Batch  183/269   train_loss = 3.612\n",
      "Epoch  73 Batch  184/269   train_loss = 3.731\n",
      "Epoch  73 Batch  185/269   train_loss = 3.792\n",
      "Epoch  73 Batch  186/269   train_loss = 3.659\n",
      "Epoch  73 Batch  187/269   train_loss = 3.846\n",
      "Epoch  73 Batch  188/269   train_loss = 3.630\n",
      "Epoch  73 Batch  189/269   train_loss = 3.637\n",
      "Epoch  73 Batch  190/269   train_loss = 4.081\n",
      "Epoch  73 Batch  191/269   train_loss = 3.815\n",
      "Epoch  73 Batch  192/269   train_loss = 3.751\n",
      "Epoch  73 Batch  193/269   train_loss = 3.713\n",
      "Epoch  73 Batch  194/269   train_loss = 3.844\n",
      "Epoch  73 Batch  195/269   train_loss = 3.757\n",
      "Epoch  73 Batch  196/269   train_loss = 3.762\n",
      "Epoch  73 Batch  197/269   train_loss = 3.943\n",
      "Epoch  73 Batch  198/269   train_loss = 3.747\n",
      "Epoch  73 Batch  199/269   train_loss = 3.851\n",
      "Epoch  73 Batch  200/269   train_loss = 3.594\n",
      "Epoch  73 Batch  201/269   train_loss = 3.779\n",
      "Epoch  73 Batch  202/269   train_loss = 3.531\n",
      "Epoch  73 Batch  203/269   train_loss = 3.681\n",
      "Epoch  73 Batch  204/269   train_loss = 3.842\n",
      "Epoch  73 Batch  205/269   train_loss = 3.760\n",
      "Epoch  73 Batch  206/269   train_loss = 3.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  73 Batch  207/269   train_loss = 3.645\n",
      "Epoch  73 Batch  208/269   train_loss = 3.761\n",
      "Epoch  73 Batch  209/269   train_loss = 3.815\n",
      "Epoch  73 Batch  210/269   train_loss = 3.699\n",
      "Epoch  73 Batch  211/269   train_loss = 3.618\n",
      "Epoch  73 Batch  212/269   train_loss = 4.048\n",
      "Epoch  73 Batch  213/269   train_loss = 3.681\n",
      "Epoch  73 Batch  214/269   train_loss = 3.739\n",
      "Epoch  73 Batch  215/269   train_loss = 3.964\n",
      "Epoch  73 Batch  216/269   train_loss = 3.893\n",
      "Epoch  73 Batch  217/269   train_loss = 3.566\n",
      "Epoch  73 Batch  218/269   train_loss = 3.835\n",
      "Epoch  73 Batch  219/269   train_loss = 3.528\n",
      "Epoch  73 Batch  220/269   train_loss = 3.895\n",
      "Epoch  73 Batch  221/269   train_loss = 3.606\n",
      "Epoch  73 Batch  222/269   train_loss = 3.762\n",
      "Epoch  73 Batch  223/269   train_loss = 3.494\n",
      "Epoch  73 Batch  224/269   train_loss = 3.898\n",
      "Epoch  73 Batch  225/269   train_loss = 3.895\n",
      "Epoch  73 Batch  226/269   train_loss = 3.707\n",
      "Epoch  73 Batch  227/269   train_loss = 3.426\n",
      "Epoch  73 Batch  228/269   train_loss = 3.731\n",
      "Epoch  73 Batch  229/269   train_loss = 3.905\n",
      "Epoch  73 Batch  230/269   train_loss = 3.845\n",
      "Epoch  73 Batch  231/269   train_loss = 3.699\n",
      "Epoch  73 Batch  232/269   train_loss = 3.715\n",
      "Epoch  73 Batch  233/269   train_loss = 3.823\n",
      "Epoch  73 Batch  234/269   train_loss = 3.697\n",
      "Epoch  73 Batch  235/269   train_loss = 3.975\n",
      "Epoch  73 Batch  236/269   train_loss = 3.615\n",
      "Epoch  73 Batch  237/269   train_loss = 3.296\n",
      "Epoch  73 Batch  238/269   train_loss = 3.523\n",
      "Epoch  73 Batch  239/269   train_loss = 4.016\n",
      "Epoch  73 Batch  240/269   train_loss = 3.546\n",
      "Epoch  73 Batch  241/269   train_loss = 3.917\n",
      "Epoch  73 Batch  242/269   train_loss = 3.498\n",
      "Epoch  73 Batch  243/269   train_loss = 3.912\n",
      "Epoch  73 Batch  244/269   train_loss = 3.747\n",
      "Epoch  73 Batch  245/269   train_loss = 3.733\n",
      "Epoch  73 Batch  246/269   train_loss = 3.369\n",
      "Epoch  73 Batch  247/269   train_loss = 3.709\n",
      "Epoch  73 Batch  248/269   train_loss = 3.783\n",
      "Epoch  73 Batch  249/269   train_loss = 3.682\n",
      "Epoch  73 Batch  250/269   train_loss = 3.576\n",
      "Epoch  73 Batch  251/269   train_loss = 3.898\n",
      "Epoch  73 Batch  252/269   train_loss = 3.831\n",
      "Epoch  73 Batch  253/269   train_loss = 3.643\n",
      "Epoch  73 Batch  254/269   train_loss = 3.599\n",
      "Epoch  73 Batch  255/269   train_loss = 3.728\n",
      "Epoch  73 Batch  256/269   train_loss = 3.684\n",
      "Epoch  73 Batch  257/269   train_loss = 3.640\n",
      "Epoch  73 Batch  258/269   train_loss = 3.480\n",
      "Epoch  73 Batch  259/269   train_loss = 3.550\n",
      "Epoch  73 Batch  260/269   train_loss = 3.816\n",
      "Epoch  73 Batch  261/269   train_loss = 3.773\n",
      "Epoch  73 Batch  262/269   train_loss = 3.661\n",
      "Epoch  73 Batch  263/269   train_loss = 3.516\n",
      "Epoch  73 Batch  264/269   train_loss = 3.881\n",
      "Epoch  73 Batch  265/269   train_loss = 3.756\n",
      "Epoch  73 Batch  266/269   train_loss = 3.688\n",
      "Epoch  73 Batch  267/269   train_loss = 3.724\n",
      "Epoch  73 Batch  268/269   train_loss = 3.828\n",
      "Epoch  74 Batch    0/269   train_loss = 3.647\n",
      "Epoch  74 Batch    1/269   train_loss = 3.418\n",
      "Epoch  74 Batch    2/269   train_loss = 3.559\n",
      "Epoch  74 Batch    3/269   train_loss = 3.687\n",
      "Epoch  74 Batch    4/269   train_loss = 4.129\n",
      "Epoch  74 Batch    5/269   train_loss = 3.662\n",
      "Epoch  74 Batch    6/269   train_loss = 3.597\n",
      "Epoch  74 Batch    7/269   train_loss = 3.478\n",
      "Epoch  74 Batch    8/269   train_loss = 3.756\n",
      "Epoch  74 Batch    9/269   train_loss = 3.424\n",
      "Epoch  74 Batch   10/269   train_loss = 3.601\n",
      "Epoch  74 Batch   11/269   train_loss = 3.664\n",
      "Epoch  74 Batch   12/269   train_loss = 3.391\n",
      "Epoch  74 Batch   13/269   train_loss = 3.740\n",
      "Epoch  74 Batch   14/269   train_loss = 3.609\n",
      "Epoch  74 Batch   15/269   train_loss = 3.899\n",
      "Epoch  74 Batch   16/269   train_loss = 3.693\n",
      "Epoch  74 Batch   17/269   train_loss = 3.688\n",
      "Epoch  74 Batch   18/269   train_loss = 3.644\n",
      "Epoch  74 Batch   19/269   train_loss = 3.659\n",
      "Epoch  74 Batch   20/269   train_loss = 4.061\n",
      "Epoch  74 Batch   21/269   train_loss = 3.908\n",
      "Epoch  74 Batch   22/269   train_loss = 3.599\n",
      "Epoch  74 Batch   23/269   train_loss = 3.784\n",
      "Epoch  74 Batch   24/269   train_loss = 3.482\n",
      "Epoch  74 Batch   25/269   train_loss = 3.914\n",
      "Epoch  74 Batch   26/269   train_loss = 3.748\n",
      "Epoch  74 Batch   27/269   train_loss = 3.600\n",
      "Epoch  74 Batch   28/269   train_loss = 3.748\n",
      "Epoch  74 Batch   29/269   train_loss = 3.791\n",
      "Epoch  74 Batch   30/269   train_loss = 4.067\n",
      "Epoch  74 Batch   31/269   train_loss = 3.707\n",
      "Epoch  74 Batch   32/269   train_loss = 3.607\n",
      "Epoch  74 Batch   33/269   train_loss = 3.410\n",
      "Epoch  74 Batch   34/269   train_loss = 3.617\n",
      "Epoch  74 Batch   35/269   train_loss = 3.479\n",
      "Epoch  74 Batch   36/269   train_loss = 3.554\n",
      "Epoch  74 Batch   37/269   train_loss = 3.750\n",
      "Epoch  74 Batch   38/269   train_loss = 3.431\n",
      "Epoch  74 Batch   39/269   train_loss = 3.849\n",
      "Epoch  74 Batch   40/269   train_loss = 3.630\n",
      "Epoch  74 Batch   41/269   train_loss = 3.505\n",
      "Epoch  74 Batch   42/269   train_loss = 4.007\n",
      "Epoch  74 Batch   43/269   train_loss = 3.982\n",
      "Epoch  74 Batch   44/269   train_loss = 3.739\n",
      "Epoch  74 Batch   45/269   train_loss = 3.428\n",
      "Epoch  74 Batch   46/269   train_loss = 3.747\n",
      "Epoch  74 Batch   47/269   train_loss = 3.434\n",
      "Epoch  74 Batch   48/269   train_loss = 3.885\n",
      "Epoch  74 Batch   49/269   train_loss = 3.871\n",
      "Epoch  74 Batch   50/269   train_loss = 3.803\n",
      "Epoch  74 Batch   51/269   train_loss = 3.767\n",
      "Epoch  74 Batch   52/269   train_loss = 3.752\n",
      "Epoch  74 Batch   53/269   train_loss = 3.931\n",
      "Epoch  74 Batch   54/269   train_loss = 3.646\n",
      "Epoch  74 Batch   55/269   train_loss = 3.768\n",
      "Epoch  74 Batch   56/269   train_loss = 3.545\n",
      "Epoch  74 Batch   57/269   train_loss = 3.882\n",
      "Epoch  74 Batch   58/269   train_loss = 3.524\n",
      "Epoch  74 Batch   59/269   train_loss = 3.612\n",
      "Epoch  74 Batch   60/269   train_loss = 3.717\n",
      "Epoch  74 Batch   61/269   train_loss = 3.929\n",
      "Epoch  74 Batch   62/269   train_loss = 3.581\n",
      "Epoch  74 Batch   63/269   train_loss = 3.871\n",
      "Epoch  74 Batch   64/269   train_loss = 3.606\n",
      "Epoch  74 Batch   65/269   train_loss = 3.707\n",
      "Epoch  74 Batch   66/269   train_loss = 3.603\n",
      "Epoch  74 Batch   67/269   train_loss = 3.641\n",
      "Epoch  74 Batch   68/269   train_loss = 3.753\n",
      "Epoch  74 Batch   69/269   train_loss = 3.777\n",
      "Epoch  74 Batch   70/269   train_loss = 3.480\n",
      "Epoch  74 Batch   71/269   train_loss = 3.788\n",
      "Epoch  74 Batch   72/269   train_loss = 3.474\n",
      "Epoch  74 Batch   73/269   train_loss = 3.692\n",
      "Epoch  74 Batch   74/269   train_loss = 3.688\n",
      "Epoch  74 Batch   75/269   train_loss = 3.856\n",
      "Epoch  74 Batch   76/269   train_loss = 3.832\n",
      "Epoch  74 Batch   77/269   train_loss = 3.459\n",
      "Epoch  74 Batch   78/269   train_loss = 3.872\n",
      "Epoch  74 Batch   79/269   train_loss = 3.919\n",
      "Epoch  74 Batch   80/269   train_loss = 3.798\n",
      "Epoch  74 Batch   81/269   train_loss = 3.699\n",
      "Epoch  74 Batch   82/269   train_loss = 3.840\n",
      "Epoch  74 Batch   83/269   train_loss = 3.600\n",
      "Epoch  74 Batch   84/269   train_loss = 3.940\n",
      "Epoch  74 Batch   85/269   train_loss = 4.122\n",
      "Epoch  74 Batch   86/269   train_loss = 4.140\n",
      "Epoch  74 Batch   87/269   train_loss = 3.733\n",
      "Epoch  74 Batch   88/269   train_loss = 3.832\n",
      "Epoch  74 Batch   89/269   train_loss = 3.700\n",
      "Epoch  74 Batch   90/269   train_loss = 3.636\n",
      "Epoch  74 Batch   91/269   train_loss = 3.775\n",
      "Epoch  74 Batch   92/269   train_loss = 3.919\n",
      "Epoch  74 Batch   93/269   train_loss = 3.651\n",
      "Epoch  74 Batch   94/269   train_loss = 4.098\n",
      "Epoch  74 Batch   95/269   train_loss = 3.498\n",
      "Epoch  74 Batch   96/269   train_loss = 3.582\n",
      "Epoch  74 Batch   97/269   train_loss = 3.688\n",
      "Epoch  74 Batch   98/269   train_loss = 3.922\n",
      "Epoch  74 Batch   99/269   train_loss = 3.784\n",
      "Epoch  74 Batch  100/269   train_loss = 3.782\n",
      "Epoch  74 Batch  101/269   train_loss = 3.784\n",
      "Epoch  74 Batch  102/269   train_loss = 3.623\n",
      "Epoch  74 Batch  103/269   train_loss = 3.741\n",
      "Epoch  74 Batch  104/269   train_loss = 3.783\n",
      "Epoch  74 Batch  105/269   train_loss = 3.922\n",
      "Epoch  74 Batch  106/269   train_loss = 3.988\n",
      "Epoch  74 Batch  107/269   train_loss = 3.979\n",
      "Epoch  74 Batch  108/269   train_loss = 3.779\n",
      "Epoch  74 Batch  109/269   train_loss = 3.798\n",
      "Epoch  74 Batch  110/269   train_loss = 3.819\n",
      "Epoch  74 Batch  111/269   train_loss = 3.929\n",
      "Epoch  74 Batch  112/269   train_loss = 3.934\n",
      "Epoch  74 Batch  113/269   train_loss = 3.748\n",
      "Epoch  74 Batch  114/269   train_loss = 3.714\n",
      "Epoch  74 Batch  115/269   train_loss = 3.626\n",
      "Epoch  74 Batch  116/269   train_loss = 3.567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  74 Batch  117/269   train_loss = 3.602\n",
      "Epoch  74 Batch  118/269   train_loss = 3.959\n",
      "Epoch  74 Batch  119/269   train_loss = 3.589\n",
      "Epoch  74 Batch  120/269   train_loss = 3.704\n",
      "Epoch  74 Batch  121/269   train_loss = 3.762\n",
      "Epoch  74 Batch  122/269   train_loss = 3.735\n",
      "Epoch  74 Batch  123/269   train_loss = 3.520\n",
      "Epoch  74 Batch  124/269   train_loss = 3.872\n",
      "Epoch  74 Batch  125/269   train_loss = 3.546\n",
      "Epoch  74 Batch  126/269   train_loss = 3.429\n",
      "Epoch  74 Batch  127/269   train_loss = 3.763\n",
      "Epoch  74 Batch  128/269   train_loss = 3.635\n",
      "Epoch  74 Batch  129/269   train_loss = 3.613\n",
      "Epoch  74 Batch  130/269   train_loss = 3.787\n",
      "Epoch  74 Batch  131/269   train_loss = 3.427\n",
      "Epoch  74 Batch  132/269   train_loss = 3.733\n",
      "Epoch  74 Batch  133/269   train_loss = 3.486\n",
      "Epoch  74 Batch  134/269   train_loss = 3.907\n",
      "Epoch  74 Batch  135/269   train_loss = 3.581\n",
      "Epoch  74 Batch  136/269   train_loss = 3.431\n",
      "Epoch  74 Batch  137/269   train_loss = 3.811\n",
      "Epoch  74 Batch  138/269   train_loss = 3.567\n",
      "Epoch  74 Batch  139/269   train_loss = 3.674\n",
      "Epoch  74 Batch  140/269   train_loss = 3.763\n",
      "Epoch  74 Batch  141/269   train_loss = 4.075\n",
      "Epoch  74 Batch  142/269   train_loss = 3.483\n",
      "Epoch  74 Batch  143/269   train_loss = 3.680\n",
      "Epoch  74 Batch  144/269   train_loss = 3.942\n",
      "Epoch  74 Batch  145/269   train_loss = 3.740\n",
      "Epoch  74 Batch  146/269   train_loss = 3.855\n",
      "Epoch  74 Batch  147/269   train_loss = 3.487\n",
      "Epoch  74 Batch  148/269   train_loss = 3.780\n",
      "Epoch  74 Batch  149/269   train_loss = 3.585\n",
      "Epoch  74 Batch  150/269   train_loss = 3.821\n",
      "Epoch  74 Batch  151/269   train_loss = 3.786\n",
      "Epoch  74 Batch  152/269   train_loss = 3.725\n",
      "Epoch  74 Batch  153/269   train_loss = 3.743\n",
      "Epoch  74 Batch  154/269   train_loss = 3.639\n",
      "Epoch  74 Batch  155/269   train_loss = 3.677\n",
      "Epoch  74 Batch  156/269   train_loss = 4.120\n",
      "Epoch  74 Batch  157/269   train_loss = 3.821\n",
      "Epoch  74 Batch  158/269   train_loss = 3.770\n",
      "Epoch  74 Batch  159/269   train_loss = 3.814\n",
      "Epoch  74 Batch  160/269   train_loss = 3.595\n",
      "Epoch  74 Batch  161/269   train_loss = 3.818\n",
      "Epoch  74 Batch  162/269   train_loss = 3.852\n",
      "Epoch  74 Batch  163/269   train_loss = 3.628\n",
      "Epoch  74 Batch  164/269   train_loss = 3.931\n",
      "Epoch  74 Batch  165/269   train_loss = 3.962\n",
      "Epoch  74 Batch  166/269   train_loss = 3.830\n",
      "Epoch  74 Batch  167/269   train_loss = 3.627\n",
      "Epoch  74 Batch  168/269   train_loss = 3.740\n",
      "Epoch  74 Batch  169/269   train_loss = 3.989\n",
      "Epoch  74 Batch  170/269   train_loss = 3.447\n",
      "Epoch  74 Batch  171/269   train_loss = 3.738\n",
      "Epoch  74 Batch  172/269   train_loss = 3.620\n",
      "Epoch  74 Batch  173/269   train_loss = 3.733\n",
      "Epoch  74 Batch  174/269   train_loss = 3.863\n",
      "Epoch  74 Batch  175/269   train_loss = 3.744\n",
      "Epoch  74 Batch  176/269   train_loss = 3.699\n",
      "Epoch  74 Batch  177/269   train_loss = 3.759\n",
      "Epoch  74 Batch  178/269   train_loss = 4.010\n",
      "Epoch  74 Batch  179/269   train_loss = 3.782\n",
      "Epoch  74 Batch  180/269   train_loss = 3.654\n",
      "Epoch  74 Batch  181/269   train_loss = 3.847\n",
      "Epoch  74 Batch  182/269   train_loss = 3.677\n",
      "Epoch  74 Batch  183/269   train_loss = 3.643\n",
      "Epoch  74 Batch  184/269   train_loss = 3.693\n",
      "Epoch  74 Batch  185/269   train_loss = 3.805\n",
      "Epoch  74 Batch  186/269   train_loss = 3.676\n",
      "Epoch  74 Batch  187/269   train_loss = 3.879\n",
      "Epoch  74 Batch  188/269   train_loss = 3.671\n",
      "Epoch  74 Batch  189/269   train_loss = 3.649\n",
      "Epoch  74 Batch  190/269   train_loss = 4.063\n",
      "Epoch  74 Batch  191/269   train_loss = 3.815\n",
      "Epoch  74 Batch  192/269   train_loss = 3.720\n",
      "Epoch  74 Batch  193/269   train_loss = 3.779\n",
      "Epoch  74 Batch  194/269   train_loss = 3.833\n",
      "Epoch  74 Batch  195/269   train_loss = 3.746\n",
      "Epoch  74 Batch  196/269   train_loss = 3.734\n",
      "Epoch  74 Batch  197/269   train_loss = 3.949\n",
      "Epoch  74 Batch  198/269   train_loss = 3.779\n",
      "Epoch  74 Batch  199/269   train_loss = 3.905\n",
      "Epoch  74 Batch  200/269   train_loss = 3.580\n",
      "Epoch  74 Batch  201/269   train_loss = 3.755\n",
      "Epoch  74 Batch  202/269   train_loss = 3.505\n",
      "Epoch  74 Batch  203/269   train_loss = 3.670\n",
      "Epoch  74 Batch  204/269   train_loss = 3.900\n",
      "Epoch  74 Batch  205/269   train_loss = 3.803\n",
      "Epoch  74 Batch  206/269   train_loss = 3.676\n",
      "Epoch  74 Batch  207/269   train_loss = 3.658\n",
      "Epoch  74 Batch  208/269   train_loss = 3.774\n",
      "Epoch  74 Batch  209/269   train_loss = 3.824\n",
      "Epoch  74 Batch  210/269   train_loss = 3.686\n",
      "Epoch  74 Batch  211/269   train_loss = 3.660\n",
      "Epoch  74 Batch  212/269   train_loss = 4.036\n",
      "Epoch  74 Batch  213/269   train_loss = 3.669\n",
      "Epoch  74 Batch  214/269   train_loss = 3.738\n",
      "Epoch  74 Batch  215/269   train_loss = 3.949\n",
      "Epoch  74 Batch  216/269   train_loss = 3.908\n",
      "Epoch  74 Batch  217/269   train_loss = 3.599\n",
      "Epoch  74 Batch  218/269   train_loss = 3.863\n",
      "Epoch  74 Batch  219/269   train_loss = 3.528\n",
      "Epoch  74 Batch  220/269   train_loss = 3.889\n",
      "Epoch  74 Batch  221/269   train_loss = 3.612\n",
      "Epoch  74 Batch  222/269   train_loss = 3.799\n",
      "Epoch  74 Batch  223/269   train_loss = 3.523\n",
      "Epoch  74 Batch  224/269   train_loss = 3.897\n",
      "Epoch  74 Batch  225/269   train_loss = 3.900\n",
      "Epoch  74 Batch  226/269   train_loss = 3.715\n",
      "Epoch  74 Batch  227/269   train_loss = 3.427\n",
      "Epoch  74 Batch  228/269   train_loss = 3.776\n",
      "Epoch  74 Batch  229/269   train_loss = 3.874\n",
      "Epoch  74 Batch  230/269   train_loss = 3.821\n",
      "Epoch  74 Batch  231/269   train_loss = 3.675\n",
      "Epoch  74 Batch  232/269   train_loss = 3.712\n",
      "Epoch  74 Batch  233/269   train_loss = 3.796\n",
      "Epoch  74 Batch  234/269   train_loss = 3.715\n",
      "Epoch  74 Batch  235/269   train_loss = 4.015\n",
      "Epoch  74 Batch  236/269   train_loss = 3.634\n",
      "Epoch  74 Batch  237/269   train_loss = 3.291\n",
      "Epoch  74 Batch  238/269   train_loss = 3.473\n",
      "Epoch  74 Batch  239/269   train_loss = 3.992\n",
      "Epoch  74 Batch  240/269   train_loss = 3.535\n",
      "Epoch  74 Batch  241/269   train_loss = 3.951\n",
      "Epoch  74 Batch  242/269   train_loss = 3.487\n",
      "Epoch  74 Batch  243/269   train_loss = 3.908\n",
      "Epoch  74 Batch  244/269   train_loss = 3.723\n",
      "Epoch  74 Batch  245/269   train_loss = 3.685\n",
      "Epoch  74 Batch  246/269   train_loss = 3.359\n",
      "Epoch  74 Batch  247/269   train_loss = 3.735\n",
      "Epoch  74 Batch  248/269   train_loss = 3.795\n",
      "Epoch  74 Batch  249/269   train_loss = 3.664\n",
      "Epoch  74 Batch  250/269   train_loss = 3.537\n",
      "Epoch  74 Batch  251/269   train_loss = 3.891\n",
      "Epoch  74 Batch  252/269   train_loss = 3.854\n",
      "Epoch  74 Batch  253/269   train_loss = 3.599\n",
      "Epoch  74 Batch  254/269   train_loss = 3.608\n",
      "Epoch  74 Batch  255/269   train_loss = 3.720\n",
      "Epoch  74 Batch  256/269   train_loss = 3.650\n",
      "Epoch  74 Batch  257/269   train_loss = 3.577\n",
      "Epoch  74 Batch  258/269   train_loss = 3.470\n",
      "Epoch  74 Batch  259/269   train_loss = 3.535\n",
      "Epoch  74 Batch  260/269   train_loss = 3.826\n",
      "Epoch  74 Batch  261/269   train_loss = 3.747\n",
      "Epoch  74 Batch  262/269   train_loss = 3.670\n",
      "Epoch  74 Batch  263/269   train_loss = 3.504\n",
      "Epoch  74 Batch  264/269   train_loss = 3.869\n",
      "Epoch  74 Batch  265/269   train_loss = 3.726\n",
      "Epoch  74 Batch  266/269   train_loss = 3.622\n",
      "Epoch  74 Batch  267/269   train_loss = 3.713\n",
      "Epoch  74 Batch  268/269   train_loss = 3.881\n",
      "Epoch  75 Batch    0/269   train_loss = 3.658\n",
      "Epoch  75 Batch    1/269   train_loss = 3.418\n",
      "Epoch  75 Batch    2/269   train_loss = 3.559\n",
      "Epoch  75 Batch    3/269   train_loss = 3.634\n",
      "Epoch  75 Batch    4/269   train_loss = 4.153\n",
      "Epoch  75 Batch    5/269   train_loss = 3.692\n",
      "Epoch  75 Batch    6/269   train_loss = 3.623\n",
      "Epoch  75 Batch    7/269   train_loss = 3.483\n",
      "Epoch  75 Batch    8/269   train_loss = 3.749\n",
      "Epoch  75 Batch    9/269   train_loss = 3.435\n",
      "Epoch  75 Batch   10/269   train_loss = 3.716\n",
      "Epoch  75 Batch   11/269   train_loss = 3.669\n",
      "Epoch  75 Batch   12/269   train_loss = 3.414\n",
      "Epoch  75 Batch   13/269   train_loss = 3.751\n",
      "Epoch  75 Batch   14/269   train_loss = 3.680\n",
      "Epoch  75 Batch   15/269   train_loss = 3.934\n",
      "Epoch  75 Batch   16/269   train_loss = 3.698\n",
      "Epoch  75 Batch   17/269   train_loss = 3.773\n",
      "Epoch  75 Batch   18/269   train_loss = 3.652\n",
      "Epoch  75 Batch   19/269   train_loss = 3.704\n",
      "Epoch  75 Batch   20/269   train_loss = 4.066\n",
      "Epoch  75 Batch   21/269   train_loss = 3.910\n",
      "Epoch  75 Batch   22/269   train_loss = 3.634\n",
      "Epoch  75 Batch   23/269   train_loss = 3.770\n",
      "Epoch  75 Batch   24/269   train_loss = 3.505\n",
      "Epoch  75 Batch   25/269   train_loss = 3.914\n",
      "Epoch  75 Batch   26/269   train_loss = 3.806\n",
      "Epoch  75 Batch   27/269   train_loss = 3.579\n",
      "Epoch  75 Batch   28/269   train_loss = 3.773\n",
      "Epoch  75 Batch   29/269   train_loss = 3.765\n",
      "Epoch  75 Batch   30/269   train_loss = 4.075\n",
      "Epoch  75 Batch   31/269   train_loss = 3.749\n",
      "Epoch  75 Batch   32/269   train_loss = 3.586\n",
      "Epoch  75 Batch   33/269   train_loss = 3.381\n",
      "Epoch  75 Batch   34/269   train_loss = 3.634\n",
      "Epoch  75 Batch   35/269   train_loss = 3.482\n",
      "Epoch  75 Batch   36/269   train_loss = 3.595\n",
      "Epoch  75 Batch   37/269   train_loss = 3.776\n",
      "Epoch  75 Batch   38/269   train_loss = 3.432\n",
      "Epoch  75 Batch   39/269   train_loss = 3.863\n",
      "Epoch  75 Batch   40/269   train_loss = 3.631\n",
      "Epoch  75 Batch   41/269   train_loss = 3.616\n",
      "Epoch  75 Batch   42/269   train_loss = 4.063\n",
      "Epoch  75 Batch   43/269   train_loss = 3.995\n",
      "Epoch  75 Batch   44/269   train_loss = 3.777\n",
      "Epoch  75 Batch   45/269   train_loss = 3.418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  75 Batch   46/269   train_loss = 3.824\n",
      "Epoch  75 Batch   47/269   train_loss = 3.431\n",
      "Epoch  75 Batch   48/269   train_loss = 3.905\n",
      "Epoch  75 Batch   49/269   train_loss = 3.899\n",
      "Epoch  75 Batch   50/269   train_loss = 3.800\n",
      "Epoch  75 Batch   51/269   train_loss = 3.787\n",
      "Epoch  75 Batch   52/269   train_loss = 3.784\n",
      "Epoch  75 Batch   53/269   train_loss = 3.943\n",
      "Epoch  75 Batch   54/269   train_loss = 3.663\n",
      "Epoch  75 Batch   55/269   train_loss = 3.775\n",
      "Epoch  75 Batch   56/269   train_loss = 3.516\n",
      "Epoch  75 Batch   57/269   train_loss = 3.904\n",
      "Epoch  75 Batch   58/269   train_loss = 3.579\n",
      "Epoch  75 Batch   59/269   train_loss = 3.628\n",
      "Epoch  75 Batch   60/269   train_loss = 3.750\n",
      "Epoch  75 Batch   61/269   train_loss = 3.919\n",
      "Epoch  75 Batch   62/269   train_loss = 3.551\n",
      "Epoch  75 Batch   63/269   train_loss = 3.838\n",
      "Epoch  75 Batch   64/269   train_loss = 3.590\n",
      "Epoch  75 Batch   65/269   train_loss = 3.702\n",
      "Epoch  75 Batch   66/269   train_loss = 3.613\n",
      "Epoch  75 Batch   67/269   train_loss = 3.673\n",
      "Epoch  75 Batch   68/269   train_loss = 3.733\n",
      "Epoch  75 Batch   69/269   train_loss = 3.730\n",
      "Epoch  75 Batch   70/269   train_loss = 3.454\n",
      "Epoch  75 Batch   71/269   train_loss = 3.802\n",
      "Epoch  75 Batch   72/269   train_loss = 3.492\n",
      "Epoch  75 Batch   73/269   train_loss = 3.703\n",
      "Epoch  75 Batch   74/269   train_loss = 3.669\n",
      "Epoch  75 Batch   75/269   train_loss = 3.844\n",
      "Epoch  75 Batch   76/269   train_loss = 3.816\n",
      "Epoch  75 Batch   77/269   train_loss = 3.482\n",
      "Epoch  75 Batch   78/269   train_loss = 3.842\n",
      "Epoch  75 Batch   79/269   train_loss = 3.891\n",
      "Epoch  75 Batch   80/269   train_loss = 3.770\n",
      "Epoch  75 Batch   81/269   train_loss = 3.603\n",
      "Epoch  75 Batch   82/269   train_loss = 3.825\n",
      "Epoch  75 Batch   83/269   train_loss = 3.585\n",
      "Epoch  75 Batch   84/269   train_loss = 3.923\n",
      "Epoch  75 Batch   85/269   train_loss = 4.062\n",
      "Epoch  75 Batch   86/269   train_loss = 4.129\n",
      "Epoch  75 Batch   87/269   train_loss = 3.737\n",
      "Epoch  75 Batch   88/269   train_loss = 3.828\n",
      "Epoch  75 Batch   89/269   train_loss = 3.669\n",
      "Epoch  75 Batch   90/269   train_loss = 3.633\n",
      "Epoch  75 Batch   91/269   train_loss = 3.804\n",
      "Epoch  75 Batch   92/269   train_loss = 3.898\n",
      "Epoch  75 Batch   93/269   train_loss = 3.615\n",
      "Epoch  75 Batch   94/269   train_loss = 4.096\n",
      "Epoch  75 Batch   95/269   train_loss = 3.452\n",
      "Epoch  75 Batch   96/269   train_loss = 3.615\n",
      "Epoch  75 Batch   97/269   train_loss = 3.658\n",
      "Epoch  75 Batch   98/269   train_loss = 3.916\n",
      "Epoch  75 Batch   99/269   train_loss = 3.754\n",
      "Epoch  75 Batch  100/269   train_loss = 3.780\n",
      "Epoch  75 Batch  101/269   train_loss = 3.771\n",
      "Epoch  75 Batch  102/269   train_loss = 3.533\n",
      "Epoch  75 Batch  103/269   train_loss = 3.662\n",
      "Epoch  75 Batch  104/269   train_loss = 3.720\n",
      "Epoch  75 Batch  105/269   train_loss = 3.888\n",
      "Epoch  75 Batch  106/269   train_loss = 3.931\n",
      "Epoch  75 Batch  107/269   train_loss = 3.984\n",
      "Epoch  75 Batch  108/269   train_loss = 3.749\n",
      "Epoch  75 Batch  109/269   train_loss = 3.774\n",
      "Epoch  75 Batch  110/269   train_loss = 3.773\n",
      "Epoch  75 Batch  111/269   train_loss = 3.899\n",
      "Epoch  75 Batch  112/269   train_loss = 3.948\n",
      "Epoch  75 Batch  113/269   train_loss = 3.722\n",
      "Epoch  75 Batch  114/269   train_loss = 3.672\n",
      "Epoch  75 Batch  115/269   train_loss = 3.595\n",
      "Epoch  75 Batch  116/269   train_loss = 3.571\n",
      "Epoch  75 Batch  117/269   train_loss = 3.586\n",
      "Epoch  75 Batch  118/269   train_loss = 3.894\n",
      "Epoch  75 Batch  119/269   train_loss = 3.544\n",
      "Epoch  75 Batch  120/269   train_loss = 3.682\n",
      "Epoch  75 Batch  121/269   train_loss = 3.719\n",
      "Epoch  75 Batch  122/269   train_loss = 3.715\n",
      "Epoch  75 Batch  123/269   train_loss = 3.508\n",
      "Epoch  75 Batch  124/269   train_loss = 3.892\n",
      "Epoch  75 Batch  125/269   train_loss = 3.536\n",
      "Epoch  75 Batch  126/269   train_loss = 3.433\n",
      "Epoch  75 Batch  127/269   train_loss = 3.752\n",
      "Epoch  75 Batch  128/269   train_loss = 3.642\n",
      "Epoch  75 Batch  129/269   train_loss = 3.627\n",
      "Epoch  75 Batch  130/269   train_loss = 3.726\n",
      "Epoch  75 Batch  131/269   train_loss = 3.413\n",
      "Epoch  75 Batch  132/269   train_loss = 3.739\n",
      "Epoch  75 Batch  133/269   train_loss = 3.517\n",
      "Epoch  75 Batch  134/269   train_loss = 3.876\n",
      "Epoch  75 Batch  135/269   train_loss = 3.617\n",
      "Epoch  75 Batch  136/269   train_loss = 3.420\n",
      "Epoch  75 Batch  137/269   train_loss = 3.808\n",
      "Epoch  75 Batch  138/269   train_loss = 3.525\n",
      "Epoch  75 Batch  139/269   train_loss = 3.653\n",
      "Epoch  75 Batch  140/269   train_loss = 3.750\n",
      "Epoch  75 Batch  141/269   train_loss = 4.056\n",
      "Epoch  75 Batch  142/269   train_loss = 3.491\n",
      "Epoch  75 Batch  143/269   train_loss = 3.658\n",
      "Epoch  75 Batch  144/269   train_loss = 3.913\n",
      "Epoch  75 Batch  145/269   train_loss = 3.707\n",
      "Epoch  75 Batch  146/269   train_loss = 3.864\n",
      "Epoch  75 Batch  147/269   train_loss = 3.477\n",
      "Epoch  75 Batch  148/269   train_loss = 3.850\n",
      "Epoch  75 Batch  149/269   train_loss = 3.583\n",
      "Epoch  75 Batch  150/269   train_loss = 3.818\n",
      "Epoch  75 Batch  151/269   train_loss = 3.786\n",
      "Epoch  75 Batch  152/269   train_loss = 3.707\n",
      "Epoch  75 Batch  153/269   train_loss = 3.700\n",
      "Epoch  75 Batch  154/269   train_loss = 3.647\n",
      "Epoch  75 Batch  155/269   train_loss = 3.637\n",
      "Epoch  75 Batch  156/269   train_loss = 4.106\n",
      "Epoch  75 Batch  157/269   train_loss = 3.824\n",
      "Epoch  75 Batch  158/269   train_loss = 3.748\n",
      "Epoch  75 Batch  159/269   train_loss = 3.801\n",
      "Epoch  75 Batch  160/269   train_loss = 3.632\n",
      "Epoch  75 Batch  161/269   train_loss = 3.806\n",
      "Epoch  75 Batch  162/269   train_loss = 3.849\n",
      "Epoch  75 Batch  163/269   train_loss = 3.614\n",
      "Epoch  75 Batch  164/269   train_loss = 3.881\n",
      "Epoch  75 Batch  165/269   train_loss = 3.949\n",
      "Epoch  75 Batch  166/269   train_loss = 3.786\n",
      "Epoch  75 Batch  167/269   train_loss = 3.586\n",
      "Epoch  75 Batch  168/269   train_loss = 3.743\n",
      "Epoch  75 Batch  169/269   train_loss = 3.968\n",
      "Epoch  75 Batch  170/269   train_loss = 3.455\n",
      "Epoch  75 Batch  171/269   train_loss = 3.726\n",
      "Epoch  75 Batch  172/269   train_loss = 3.566\n",
      "Epoch  75 Batch  173/269   train_loss = 3.741\n",
      "Epoch  75 Batch  174/269   train_loss = 3.889\n",
      "Epoch  75 Batch  175/269   train_loss = 3.743\n",
      "Epoch  75 Batch  176/269   train_loss = 3.691\n",
      "Epoch  75 Batch  177/269   train_loss = 3.757\n",
      "Epoch  75 Batch  178/269   train_loss = 3.993\n",
      "Epoch  75 Batch  179/269   train_loss = 3.767\n",
      "Epoch  75 Batch  180/269   train_loss = 3.612\n",
      "Epoch  75 Batch  181/269   train_loss = 3.841\n",
      "Epoch  75 Batch  182/269   train_loss = 3.685\n",
      "Epoch  75 Batch  183/269   train_loss = 3.634\n",
      "Epoch  75 Batch  184/269   train_loss = 3.748\n",
      "Epoch  75 Batch  185/269   train_loss = 3.801\n",
      "Epoch  75 Batch  186/269   train_loss = 3.665\n",
      "Epoch  75 Batch  187/269   train_loss = 3.835\n",
      "Epoch  75 Batch  188/269   train_loss = 3.636\n",
      "Epoch  75 Batch  189/269   train_loss = 3.639\n",
      "Epoch  75 Batch  190/269   train_loss = 4.055\n",
      "Epoch  75 Batch  191/269   train_loss = 3.794\n",
      "Epoch  75 Batch  192/269   train_loss = 3.707\n",
      "Epoch  75 Batch  193/269   train_loss = 3.740\n",
      "Epoch  75 Batch  194/269   train_loss = 3.845\n",
      "Epoch  75 Batch  195/269   train_loss = 3.755\n",
      "Epoch  75 Batch  196/269   train_loss = 3.718\n",
      "Epoch  75 Batch  197/269   train_loss = 3.937\n",
      "Epoch  75 Batch  198/269   train_loss = 3.807\n",
      "Epoch  75 Batch  199/269   train_loss = 3.830\n",
      "Epoch  75 Batch  200/269   train_loss = 3.613\n",
      "Epoch  75 Batch  201/269   train_loss = 3.737\n",
      "Epoch  75 Batch  202/269   train_loss = 3.528\n",
      "Epoch  75 Batch  203/269   train_loss = 3.651\n",
      "Epoch  75 Batch  204/269   train_loss = 3.896\n",
      "Epoch  75 Batch  205/269   train_loss = 3.764\n",
      "Epoch  75 Batch  206/269   train_loss = 3.662\n",
      "Epoch  75 Batch  207/269   train_loss = 3.707\n",
      "Epoch  75 Batch  208/269   train_loss = 3.803\n",
      "Epoch  75 Batch  209/269   train_loss = 3.800\n",
      "Epoch  75 Batch  210/269   train_loss = 3.658\n",
      "Epoch  75 Batch  211/269   train_loss = 3.632\n",
      "Epoch  75 Batch  212/269   train_loss = 4.010\n",
      "Epoch  75 Batch  213/269   train_loss = 3.622\n",
      "Epoch  75 Batch  214/269   train_loss = 3.716\n",
      "Epoch  75 Batch  215/269   train_loss = 3.954\n",
      "Epoch  75 Batch  216/269   train_loss = 3.884\n",
      "Epoch  75 Batch  217/269   train_loss = 3.569\n",
      "Epoch  75 Batch  218/269   train_loss = 3.835\n",
      "Epoch  75 Batch  219/269   train_loss = 3.505\n",
      "Epoch  75 Batch  220/269   train_loss = 3.877\n",
      "Epoch  75 Batch  221/269   train_loss = 3.609\n",
      "Epoch  75 Batch  222/269   train_loss = 3.781\n",
      "Epoch  75 Batch  223/269   train_loss = 3.547\n",
      "Epoch  75 Batch  224/269   train_loss = 3.910\n",
      "Epoch  75 Batch  225/269   train_loss = 3.938\n",
      "Epoch  75 Batch  226/269   train_loss = 3.692\n",
      "Epoch  75 Batch  227/269   train_loss = 3.463\n",
      "Epoch  75 Batch  228/269   train_loss = 3.755\n",
      "Epoch  75 Batch  229/269   train_loss = 3.868\n",
      "Epoch  75 Batch  230/269   train_loss = 3.855\n",
      "Epoch  75 Batch  231/269   train_loss = 3.658\n",
      "Epoch  75 Batch  232/269   train_loss = 3.714\n",
      "Epoch  75 Batch  233/269   train_loss = 3.827\n",
      "Epoch  75 Batch  234/269   train_loss = 3.746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  75 Batch  235/269   train_loss = 4.007\n",
      "Epoch  75 Batch  236/269   train_loss = 3.637\n",
      "Epoch  75 Batch  237/269   train_loss = 3.281\n",
      "Epoch  75 Batch  238/269   train_loss = 3.498\n",
      "Epoch  75 Batch  239/269   train_loss = 4.014\n",
      "Epoch  75 Batch  240/269   train_loss = 3.538\n",
      "Epoch  75 Batch  241/269   train_loss = 3.940\n",
      "Epoch  75 Batch  242/269   train_loss = 3.509\n",
      "Epoch  75 Batch  243/269   train_loss = 3.908\n",
      "Epoch  75 Batch  244/269   train_loss = 3.733\n",
      "Epoch  75 Batch  245/269   train_loss = 3.692\n",
      "Epoch  75 Batch  246/269   train_loss = 3.369\n",
      "Epoch  75 Batch  247/269   train_loss = 3.747\n",
      "Epoch  75 Batch  248/269   train_loss = 3.804\n",
      "Epoch  75 Batch  249/269   train_loss = 3.675\n",
      "Epoch  75 Batch  250/269   train_loss = 3.505\n",
      "Epoch  75 Batch  251/269   train_loss = 3.907\n",
      "Epoch  75 Batch  252/269   train_loss = 3.865\n",
      "Epoch  75 Batch  253/269   train_loss = 3.606\n",
      "Epoch  75 Batch  254/269   train_loss = 3.608\n",
      "Epoch  75 Batch  255/269   train_loss = 3.736\n",
      "Epoch  75 Batch  256/269   train_loss = 3.678\n",
      "Epoch  75 Batch  257/269   train_loss = 3.576\n",
      "Epoch  75 Batch  258/269   train_loss = 3.477\n",
      "Epoch  75 Batch  259/269   train_loss = 3.532\n",
      "Epoch  75 Batch  260/269   train_loss = 3.812\n",
      "Epoch  75 Batch  261/269   train_loss = 3.749\n",
      "Epoch  75 Batch  262/269   train_loss = 3.681\n",
      "Epoch  75 Batch  263/269   train_loss = 3.516\n",
      "Epoch  75 Batch  264/269   train_loss = 3.893\n",
      "Epoch  75 Batch  265/269   train_loss = 3.712\n",
      "Epoch  75 Batch  266/269   train_loss = 3.608\n",
      "Epoch  75 Batch  267/269   train_loss = 3.708\n",
      "Epoch  75 Batch  268/269   train_loss = 3.838\n",
      "Epoch  76 Batch    0/269   train_loss = 3.658\n",
      "Epoch  76 Batch    1/269   train_loss = 3.414\n",
      "Epoch  76 Batch    2/269   train_loss = 3.564\n",
      "Epoch  76 Batch    3/269   train_loss = 3.637\n",
      "Epoch  76 Batch    4/269   train_loss = 4.130\n",
      "Epoch  76 Batch    5/269   train_loss = 3.706\n",
      "Epoch  76 Batch    6/269   train_loss = 3.643\n",
      "Epoch  76 Batch    7/269   train_loss = 3.476\n",
      "Epoch  76 Batch    8/269   train_loss = 3.744\n",
      "Epoch  76 Batch    9/269   train_loss = 3.455\n",
      "Epoch  76 Batch   10/269   train_loss = 3.641\n",
      "Epoch  76 Batch   11/269   train_loss = 3.628\n",
      "Epoch  76 Batch   12/269   train_loss = 3.403\n",
      "Epoch  76 Batch   13/269   train_loss = 3.740\n",
      "Epoch  76 Batch   14/269   train_loss = 3.635\n",
      "Epoch  76 Batch   15/269   train_loss = 3.961\n",
      "Epoch  76 Batch   16/269   train_loss = 3.734\n",
      "Epoch  76 Batch   17/269   train_loss = 3.734\n",
      "Epoch  76 Batch   18/269   train_loss = 3.653\n",
      "Epoch  76 Batch   19/269   train_loss = 3.682\n",
      "Epoch  76 Batch   20/269   train_loss = 4.060\n",
      "Epoch  76 Batch   21/269   train_loss = 3.909\n",
      "Epoch  76 Batch   22/269   train_loss = 3.615\n",
      "Epoch  76 Batch   23/269   train_loss = 3.740\n",
      "Epoch  76 Batch   24/269   train_loss = 3.483\n",
      "Epoch  76 Batch   25/269   train_loss = 3.893\n",
      "Epoch  76 Batch   26/269   train_loss = 3.781\n",
      "Epoch  76 Batch   27/269   train_loss = 3.573\n",
      "Epoch  76 Batch   28/269   train_loss = 3.780\n",
      "Epoch  76 Batch   29/269   train_loss = 3.767\n",
      "Epoch  76 Batch   30/269   train_loss = 4.070\n",
      "Epoch  76 Batch   31/269   train_loss = 3.751\n",
      "Epoch  76 Batch   32/269   train_loss = 3.558\n",
      "Epoch  76 Batch   33/269   train_loss = 3.374\n",
      "Epoch  76 Batch   34/269   train_loss = 3.703\n",
      "Epoch  76 Batch   35/269   train_loss = 3.463\n",
      "Epoch  76 Batch   36/269   train_loss = 3.572\n",
      "Epoch  76 Batch   37/269   train_loss = 3.724\n",
      "Epoch  76 Batch   38/269   train_loss = 3.418\n",
      "Epoch  76 Batch   39/269   train_loss = 3.848\n",
      "Epoch  76 Batch   40/269   train_loss = 3.609\n",
      "Epoch  76 Batch   41/269   train_loss = 3.547\n",
      "Epoch  76 Batch   42/269   train_loss = 4.050\n",
      "Epoch  76 Batch   43/269   train_loss = 3.990\n",
      "Epoch  76 Batch   44/269   train_loss = 3.719\n",
      "Epoch  76 Batch   45/269   train_loss = 3.406\n",
      "Epoch  76 Batch   46/269   train_loss = 3.735\n",
      "Epoch  76 Batch   47/269   train_loss = 3.406\n",
      "Epoch  76 Batch   48/269   train_loss = 3.922\n",
      "Epoch  76 Batch   49/269   train_loss = 3.880\n",
      "Epoch  76 Batch   50/269   train_loss = 3.815\n",
      "Epoch  76 Batch   51/269   train_loss = 3.761\n",
      "Epoch  76 Batch   52/269   train_loss = 3.744\n",
      "Epoch  76 Batch   53/269   train_loss = 3.956\n",
      "Epoch  76 Batch   54/269   train_loss = 3.643\n",
      "Epoch  76 Batch   55/269   train_loss = 3.765\n",
      "Epoch  76 Batch   56/269   train_loss = 3.569\n",
      "Epoch  76 Batch   57/269   train_loss = 3.949\n",
      "Epoch  76 Batch   58/269   train_loss = 3.608\n",
      "Epoch  76 Batch   59/269   train_loss = 3.652\n",
      "Epoch  76 Batch   60/269   train_loss = 3.746\n",
      "Epoch  76 Batch   61/269   train_loss = 3.888\n",
      "Epoch  76 Batch   62/269   train_loss = 3.539\n",
      "Epoch  76 Batch   63/269   train_loss = 3.861\n",
      "Epoch  76 Batch   64/269   train_loss = 3.611\n",
      "Epoch  76 Batch   65/269   train_loss = 3.709\n",
      "Epoch  76 Batch   66/269   train_loss = 3.621\n",
      "Epoch  76 Batch   67/269   train_loss = 3.694\n",
      "Epoch  76 Batch   68/269   train_loss = 3.704\n",
      "Epoch  76 Batch   69/269   train_loss = 3.706\n",
      "Epoch  76 Batch   70/269   train_loss = 3.469\n",
      "Epoch  76 Batch   71/269   train_loss = 3.786\n",
      "Epoch  76 Batch   72/269   train_loss = 3.501\n",
      "Epoch  76 Batch   73/269   train_loss = 3.689\n",
      "Epoch  76 Batch   74/269   train_loss = 3.652\n",
      "Epoch  76 Batch   75/269   train_loss = 3.852\n",
      "Epoch  76 Batch   76/269   train_loss = 3.839\n",
      "Epoch  76 Batch   77/269   train_loss = 3.468\n",
      "Epoch  76 Batch   78/269   train_loss = 3.845\n",
      "Epoch  76 Batch   79/269   train_loss = 3.877\n",
      "Epoch  76 Batch   80/269   train_loss = 3.788\n",
      "Epoch  76 Batch   81/269   train_loss = 3.593\n",
      "Epoch  76 Batch   82/269   train_loss = 3.819\n",
      "Epoch  76 Batch   83/269   train_loss = 3.572\n",
      "Epoch  76 Batch   84/269   train_loss = 3.904\n",
      "Epoch  76 Batch   85/269   train_loss = 4.068\n",
      "Epoch  76 Batch   86/269   train_loss = 4.084\n",
      "Epoch  76 Batch   87/269   train_loss = 3.837\n",
      "Epoch  76 Batch   88/269   train_loss = 3.914\n",
      "Epoch  76 Batch   89/269   train_loss = 3.865\n",
      "Epoch  76 Batch   90/269   train_loss = 3.719\n",
      "Epoch  76 Batch   91/269   train_loss = 3.897\n",
      "Epoch  76 Batch   92/269   train_loss = 3.895\n",
      "Epoch  76 Batch   93/269   train_loss = 3.631\n",
      "Epoch  76 Batch   94/269   train_loss = 4.113\n",
      "Epoch  76 Batch   95/269   train_loss = 3.534\n",
      "Epoch  76 Batch   96/269   train_loss = 3.726\n",
      "Epoch  76 Batch   97/269   train_loss = 3.676\n",
      "Epoch  76 Batch   98/269   train_loss = 3.983\n",
      "Epoch  76 Batch   99/269   train_loss = 3.763\n",
      "Epoch  76 Batch  100/269   train_loss = 3.860\n",
      "Epoch  76 Batch  101/269   train_loss = 3.828\n",
      "Epoch  76 Batch  102/269   train_loss = 3.576\n",
      "Epoch  76 Batch  103/269   train_loss = 3.783\n",
      "Epoch  76 Batch  104/269   train_loss = 3.768\n",
      "Epoch  76 Batch  105/269   train_loss = 3.894\n",
      "Epoch  76 Batch  106/269   train_loss = 4.046\n",
      "Epoch  76 Batch  107/269   train_loss = 3.978\n",
      "Epoch  76 Batch  108/269   train_loss = 3.806\n",
      "Epoch  76 Batch  109/269   train_loss = 3.767\n",
      "Epoch  76 Batch  110/269   train_loss = 3.784\n",
      "Epoch  76 Batch  111/269   train_loss = 3.922\n",
      "Epoch  76 Batch  112/269   train_loss = 3.970\n",
      "Epoch  76 Batch  113/269   train_loss = 3.752\n",
      "Epoch  76 Batch  114/269   train_loss = 3.661\n",
      "Epoch  76 Batch  115/269   train_loss = 3.703\n",
      "Epoch  76 Batch  116/269   train_loss = 3.637\n",
      "Epoch  76 Batch  117/269   train_loss = 3.590\n",
      "Epoch  76 Batch  118/269   train_loss = 3.894\n",
      "Epoch  76 Batch  119/269   train_loss = 3.556\n",
      "Epoch  76 Batch  120/269   train_loss = 3.754\n",
      "Epoch  76 Batch  121/269   train_loss = 3.799\n",
      "Epoch  76 Batch  122/269   train_loss = 3.734\n",
      "Epoch  76 Batch  123/269   train_loss = 3.561\n",
      "Epoch  76 Batch  124/269   train_loss = 3.959\n",
      "Epoch  76 Batch  125/269   train_loss = 3.593\n",
      "Epoch  76 Batch  126/269   train_loss = 3.481\n",
      "Epoch  76 Batch  127/269   train_loss = 3.786\n",
      "Epoch  76 Batch  128/269   train_loss = 3.657\n",
      "Epoch  76 Batch  129/269   train_loss = 3.610\n",
      "Epoch  76 Batch  130/269   train_loss = 3.754\n",
      "Epoch  76 Batch  131/269   train_loss = 3.500\n",
      "Epoch  76 Batch  132/269   train_loss = 3.801\n",
      "Epoch  76 Batch  133/269   train_loss = 3.521\n",
      "Epoch  76 Batch  134/269   train_loss = 3.901\n",
      "Epoch  76 Batch  135/269   train_loss = 3.663\n",
      "Epoch  76 Batch  136/269   train_loss = 3.455\n",
      "Epoch  76 Batch  137/269   train_loss = 3.834\n",
      "Epoch  76 Batch  138/269   train_loss = 3.557\n",
      "Epoch  76 Batch  139/269   train_loss = 3.685\n",
      "Epoch  76 Batch  140/269   train_loss = 3.803\n",
      "Epoch  76 Batch  141/269   train_loss = 4.033\n",
      "Epoch  76 Batch  142/269   train_loss = 3.537\n",
      "Epoch  76 Batch  143/269   train_loss = 3.663\n",
      "Epoch  76 Batch  144/269   train_loss = 3.908\n",
      "Epoch  76 Batch  145/269   train_loss = 3.782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  76 Batch  146/269   train_loss = 3.896\n",
      "Epoch  76 Batch  147/269   train_loss = 3.502\n",
      "Epoch  76 Batch  148/269   train_loss = 3.907\n",
      "Epoch  76 Batch  149/269   train_loss = 3.626\n",
      "Epoch  76 Batch  150/269   train_loss = 3.817\n",
      "Epoch  76 Batch  151/269   train_loss = 3.816\n",
      "Epoch  76 Batch  152/269   train_loss = 3.728\n",
      "Epoch  76 Batch  153/269   train_loss = 3.758\n",
      "Epoch  76 Batch  154/269   train_loss = 3.705\n",
      "Epoch  76 Batch  155/269   train_loss = 3.639\n",
      "Epoch  76 Batch  156/269   train_loss = 4.098\n",
      "Epoch  76 Batch  157/269   train_loss = 3.806\n",
      "Epoch  76 Batch  158/269   train_loss = 3.772\n",
      "Epoch  76 Batch  159/269   train_loss = 3.814\n",
      "Epoch  76 Batch  160/269   train_loss = 3.658\n",
      "Epoch  76 Batch  161/269   train_loss = 3.837\n",
      "Epoch  76 Batch  162/269   train_loss = 3.828\n",
      "Epoch  76 Batch  163/269   train_loss = 3.609\n",
      "Epoch  76 Batch  164/269   train_loss = 3.925\n",
      "Epoch  76 Batch  165/269   train_loss = 3.972\n",
      "Epoch  76 Batch  166/269   train_loss = 3.810\n",
      "Epoch  76 Batch  167/269   train_loss = 3.591\n",
      "Epoch  76 Batch  168/269   train_loss = 3.755\n",
      "Epoch  76 Batch  169/269   train_loss = 3.959\n",
      "Epoch  76 Batch  170/269   train_loss = 3.587\n",
      "Epoch  76 Batch  171/269   train_loss = 3.759\n",
      "Epoch  76 Batch  172/269   train_loss = 3.596\n",
      "Epoch  76 Batch  173/269   train_loss = 3.741\n",
      "Epoch  76 Batch  174/269   train_loss = 3.940\n",
      "Epoch  76 Batch  175/269   train_loss = 3.811\n",
      "Epoch  76 Batch  176/269   train_loss = 3.719\n",
      "Epoch  76 Batch  177/269   train_loss = 3.832\n",
      "Epoch  76 Batch  178/269   train_loss = 4.050\n",
      "Epoch  76 Batch  179/269   train_loss = 3.784\n",
      "Epoch  76 Batch  180/269   train_loss = 3.617\n",
      "Epoch  76 Batch  181/269   train_loss = 3.878\n",
      "Epoch  76 Batch  182/269   train_loss = 3.718\n",
      "Epoch  76 Batch  183/269   train_loss = 3.662\n",
      "Epoch  76 Batch  184/269   train_loss = 3.766\n",
      "Epoch  76 Batch  185/269   train_loss = 3.842\n",
      "Epoch  76 Batch  186/269   train_loss = 3.681\n",
      "Epoch  76 Batch  187/269   train_loss = 3.839\n",
      "Epoch  76 Batch  188/269   train_loss = 3.645\n",
      "Epoch  76 Batch  189/269   train_loss = 3.700\n",
      "Epoch  76 Batch  190/269   train_loss = 4.057\n",
      "Epoch  76 Batch  191/269   train_loss = 3.854\n",
      "Epoch  76 Batch  192/269   train_loss = 3.741\n",
      "Epoch  76 Batch  193/269   train_loss = 3.781\n",
      "Epoch  76 Batch  194/269   train_loss = 3.839\n",
      "Epoch  76 Batch  195/269   train_loss = 3.780\n",
      "Epoch  76 Batch  196/269   train_loss = 3.756\n",
      "Epoch  76 Batch  197/269   train_loss = 3.930\n",
      "Epoch  76 Batch  198/269   train_loss = 3.767\n",
      "Epoch  76 Batch  199/269   train_loss = 3.890\n",
      "Epoch  76 Batch  200/269   train_loss = 3.634\n",
      "Epoch  76 Batch  201/269   train_loss = 3.754\n",
      "Epoch  76 Batch  202/269   train_loss = 3.530\n",
      "Epoch  76 Batch  203/269   train_loss = 3.674\n",
      "Epoch  76 Batch  204/269   train_loss = 3.868\n",
      "Epoch  76 Batch  205/269   train_loss = 3.910\n",
      "Epoch  76 Batch  206/269   train_loss = 3.680\n",
      "Epoch  76 Batch  207/269   train_loss = 3.697\n",
      "Epoch  76 Batch  208/269   train_loss = 3.921\n",
      "Epoch  76 Batch  209/269   train_loss = 3.875\n",
      "Epoch  76 Batch  210/269   train_loss = 3.688\n",
      "Epoch  76 Batch  211/269   train_loss = 3.659\n",
      "Epoch  76 Batch  212/269   train_loss = 4.070\n",
      "Epoch  76 Batch  213/269   train_loss = 3.636\n",
      "Epoch  76 Batch  214/269   train_loss = 3.747\n",
      "Epoch  76 Batch  215/269   train_loss = 3.961\n",
      "Epoch  76 Batch  216/269   train_loss = 3.956\n",
      "Epoch  76 Batch  217/269   train_loss = 3.612\n",
      "Epoch  76 Batch  218/269   train_loss = 3.861\n",
      "Epoch  76 Batch  219/269   train_loss = 3.544\n",
      "Epoch  76 Batch  220/269   train_loss = 3.932\n",
      "Epoch  76 Batch  221/269   train_loss = 3.632\n",
      "Epoch  76 Batch  222/269   train_loss = 3.836\n",
      "Epoch  76 Batch  223/269   train_loss = 3.570\n",
      "Epoch  76 Batch  224/269   train_loss = 3.892\n",
      "Epoch  76 Batch  225/269   train_loss = 3.936\n",
      "Epoch  76 Batch  226/269   train_loss = 3.752\n",
      "Epoch  76 Batch  227/269   train_loss = 3.491\n",
      "Epoch  76 Batch  228/269   train_loss = 3.781\n",
      "Epoch  76 Batch  229/269   train_loss = 3.893\n",
      "Epoch  76 Batch  230/269   train_loss = 3.904\n",
      "Epoch  76 Batch  231/269   train_loss = 3.650\n",
      "Epoch  76 Batch  232/269   train_loss = 3.723\n",
      "Epoch  76 Batch  233/269   train_loss = 3.867\n",
      "Epoch  76 Batch  234/269   train_loss = 3.791\n",
      "Epoch  76 Batch  235/269   train_loss = 4.006\n",
      "Epoch  76 Batch  236/269   train_loss = 3.613\n",
      "Epoch  76 Batch  237/269   train_loss = 3.360\n",
      "Epoch  76 Batch  238/269   train_loss = 3.522\n",
      "Epoch  76 Batch  239/269   train_loss = 4.037\n",
      "Epoch  76 Batch  240/269   train_loss = 3.578\n",
      "Epoch  76 Batch  241/269   train_loss = 3.943\n",
      "Epoch  76 Batch  242/269   train_loss = 3.530\n",
      "Epoch  76 Batch  243/269   train_loss = 3.894\n",
      "Epoch  76 Batch  244/269   train_loss = 3.752\n",
      "Epoch  76 Batch  245/269   train_loss = 3.708\n",
      "Epoch  76 Batch  246/269   train_loss = 3.398\n",
      "Epoch  76 Batch  247/269   train_loss = 3.739\n",
      "Epoch  76 Batch  248/269   train_loss = 3.801\n",
      "Epoch  76 Batch  249/269   train_loss = 3.685\n",
      "Epoch  76 Batch  250/269   train_loss = 3.545\n",
      "Epoch  76 Batch  251/269   train_loss = 3.903\n",
      "Epoch  76 Batch  252/269   train_loss = 3.858\n",
      "Epoch  76 Batch  253/269   train_loss = 3.631\n",
      "Epoch  76 Batch  254/269   train_loss = 3.655\n",
      "Epoch  76 Batch  255/269   train_loss = 3.761\n",
      "Epoch  76 Batch  256/269   train_loss = 3.656\n",
      "Epoch  76 Batch  257/269   train_loss = 3.624\n",
      "Epoch  76 Batch  258/269   train_loss = 3.517\n",
      "Epoch  76 Batch  259/269   train_loss = 3.545\n",
      "Epoch  76 Batch  260/269   train_loss = 3.840\n",
      "Epoch  76 Batch  261/269   train_loss = 3.773\n",
      "Epoch  76 Batch  262/269   train_loss = 3.689\n",
      "Epoch  76 Batch  263/269   train_loss = 3.547\n",
      "Epoch  76 Batch  264/269   train_loss = 3.893\n",
      "Epoch  76 Batch  265/269   train_loss = 3.725\n",
      "Epoch  76 Batch  266/269   train_loss = 3.602\n",
      "Epoch  76 Batch  267/269   train_loss = 3.742\n",
      "Epoch  76 Batch  268/269   train_loss = 3.894\n",
      "Epoch  77 Batch    0/269   train_loss = 3.666\n",
      "Epoch  77 Batch    1/269   train_loss = 3.458\n",
      "Epoch  77 Batch    2/269   train_loss = 3.602\n",
      "Epoch  77 Batch    3/269   train_loss = 3.703\n",
      "Epoch  77 Batch    4/269   train_loss = 4.132\n",
      "Epoch  77 Batch    5/269   train_loss = 3.764\n",
      "Epoch  77 Batch    6/269   train_loss = 3.625\n",
      "Epoch  77 Batch    7/269   train_loss = 3.480\n",
      "Epoch  77 Batch    8/269   train_loss = 3.782\n",
      "Epoch  77 Batch    9/269   train_loss = 3.472\n",
      "Epoch  77 Batch   10/269   train_loss = 3.639\n",
      "Epoch  77 Batch   11/269   train_loss = 3.656\n",
      "Epoch  77 Batch   12/269   train_loss = 3.453\n",
      "Epoch  77 Batch   13/269   train_loss = 3.781\n",
      "Epoch  77 Batch   14/269   train_loss = 3.678\n",
      "Epoch  77 Batch   15/269   train_loss = 3.981\n",
      "Epoch  77 Batch   16/269   train_loss = 3.687\n",
      "Epoch  77 Batch   17/269   train_loss = 3.795\n",
      "Epoch  77 Batch   18/269   train_loss = 3.675\n",
      "Epoch  77 Batch   19/269   train_loss = 3.768\n",
      "Epoch  77 Batch   20/269   train_loss = 4.089\n",
      "Epoch  77 Batch   21/269   train_loss = 3.914\n",
      "Epoch  77 Batch   22/269   train_loss = 3.627\n",
      "Epoch  77 Batch   23/269   train_loss = 3.742\n",
      "Epoch  77 Batch   24/269   train_loss = 3.495\n",
      "Epoch  77 Batch   25/269   train_loss = 3.917\n",
      "Epoch  77 Batch   26/269   train_loss = 3.812\n",
      "Epoch  77 Batch   27/269   train_loss = 3.621\n",
      "Epoch  77 Batch   28/269   train_loss = 3.783\n",
      "Epoch  77 Batch   29/269   train_loss = 3.822\n",
      "Epoch  77 Batch   30/269   train_loss = 4.076\n",
      "Epoch  77 Batch   31/269   train_loss = 3.721\n",
      "Epoch  77 Batch   32/269   train_loss = 3.589\n",
      "Epoch  77 Batch   33/269   train_loss = 3.391\n",
      "Epoch  77 Batch   34/269   train_loss = 3.680\n",
      "Epoch  77 Batch   35/269   train_loss = 3.467\n",
      "Epoch  77 Batch   36/269   train_loss = 3.559\n",
      "Epoch  77 Batch   37/269   train_loss = 3.747\n",
      "Epoch  77 Batch   38/269   train_loss = 3.475\n",
      "Epoch  77 Batch   39/269   train_loss = 3.843\n",
      "Epoch  77 Batch   40/269   train_loss = 3.585\n",
      "Epoch  77 Batch   41/269   train_loss = 3.612\n",
      "Epoch  77 Batch   42/269   train_loss = 4.045\n",
      "Epoch  77 Batch   43/269   train_loss = 4.020\n",
      "Epoch  77 Batch   44/269   train_loss = 3.762\n",
      "Epoch  77 Batch   45/269   train_loss = 3.496\n",
      "Epoch  77 Batch   46/269   train_loss = 3.773\n",
      "Epoch  77 Batch   47/269   train_loss = 3.457\n",
      "Epoch  77 Batch   48/269   train_loss = 3.952\n",
      "Epoch  77 Batch   49/269   train_loss = 3.885\n",
      "Epoch  77 Batch   50/269   train_loss = 3.873\n",
      "Epoch  77 Batch   51/269   train_loss = 3.820\n",
      "Epoch  77 Batch   52/269   train_loss = 3.815\n",
      "Epoch  77 Batch   53/269   train_loss = 3.999\n",
      "Epoch  77 Batch   54/269   train_loss = 3.638\n",
      "Epoch  77 Batch   55/269   train_loss = 3.812\n",
      "Epoch  77 Batch   56/269   train_loss = 3.554\n",
      "Epoch  77 Batch   57/269   train_loss = 3.939\n",
      "Epoch  77 Batch   58/269   train_loss = 3.641\n",
      "Epoch  77 Batch   59/269   train_loss = 3.662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  77 Batch   60/269   train_loss = 3.759\n",
      "Epoch  77 Batch   61/269   train_loss = 3.914\n",
      "Epoch  77 Batch   62/269   train_loss = 3.559\n",
      "Epoch  77 Batch   63/269   train_loss = 3.925\n",
      "Epoch  77 Batch   64/269   train_loss = 3.628\n",
      "Epoch  77 Batch   65/269   train_loss = 3.759\n",
      "Epoch  77 Batch   66/269   train_loss = 3.683\n",
      "Epoch  77 Batch   67/269   train_loss = 3.707\n",
      "Epoch  77 Batch   68/269   train_loss = 3.769\n",
      "Epoch  77 Batch   69/269   train_loss = 3.724\n",
      "Epoch  77 Batch   70/269   train_loss = 3.512\n",
      "Epoch  77 Batch   71/269   train_loss = 3.791\n",
      "Epoch  77 Batch   72/269   train_loss = 3.518\n",
      "Epoch  77 Batch   73/269   train_loss = 3.709\n",
      "Epoch  77 Batch   74/269   train_loss = 3.677\n",
      "Epoch  77 Batch   75/269   train_loss = 3.848\n",
      "Epoch  77 Batch   76/269   train_loss = 3.928\n",
      "Epoch  77 Batch   77/269   train_loss = 3.527\n",
      "Epoch  77 Batch   78/269   train_loss = 3.849\n",
      "Epoch  77 Batch   79/269   train_loss = 3.886\n",
      "Epoch  77 Batch   80/269   train_loss = 3.856\n",
      "Epoch  77 Batch   81/269   train_loss = 3.645\n",
      "Epoch  77 Batch   82/269   train_loss = 3.819\n",
      "Epoch  77 Batch   83/269   train_loss = 3.613\n",
      "Epoch  77 Batch   84/269   train_loss = 3.966\n",
      "Epoch  77 Batch   85/269   train_loss = 4.107\n",
      "Epoch  77 Batch   86/269   train_loss = 4.078\n",
      "Epoch  77 Batch   87/269   train_loss = 3.736\n",
      "Epoch  77 Batch   88/269   train_loss = 3.848\n",
      "Epoch  77 Batch   89/269   train_loss = 3.732\n",
      "Epoch  77 Batch   90/269   train_loss = 3.694\n",
      "Epoch  77 Batch   91/269   train_loss = 3.802\n",
      "Epoch  77 Batch   92/269   train_loss = 3.915\n",
      "Epoch  77 Batch   93/269   train_loss = 3.600\n",
      "Epoch  77 Batch   94/269   train_loss = 4.140\n",
      "Epoch  77 Batch   95/269   train_loss = 3.488\n",
      "Epoch  77 Batch   96/269   train_loss = 3.628\n",
      "Epoch  77 Batch   97/269   train_loss = 3.647\n",
      "Epoch  77 Batch   98/269   train_loss = 3.922\n",
      "Epoch  77 Batch   99/269   train_loss = 3.746\n",
      "Epoch  77 Batch  100/269   train_loss = 3.754\n",
      "Epoch  77 Batch  101/269   train_loss = 3.799\n",
      "Epoch  77 Batch  102/269   train_loss = 3.575\n",
      "Epoch  77 Batch  103/269   train_loss = 3.683\n",
      "Epoch  77 Batch  104/269   train_loss = 3.715\n",
      "Epoch  77 Batch  105/269   train_loss = 3.903\n",
      "Epoch  77 Batch  106/269   train_loss = 3.998\n",
      "Epoch  77 Batch  107/269   train_loss = 3.993\n",
      "Epoch  77 Batch  108/269   train_loss = 3.821\n",
      "Epoch  77 Batch  109/269   train_loss = 3.776\n",
      "Epoch  77 Batch  110/269   train_loss = 3.811\n",
      "Epoch  77 Batch  111/269   train_loss = 3.890\n",
      "Epoch  77 Batch  112/269   train_loss = 3.947\n",
      "Epoch  77 Batch  113/269   train_loss = 3.733\n",
      "Epoch  77 Batch  114/269   train_loss = 3.664\n",
      "Epoch  77 Batch  115/269   train_loss = 3.659\n",
      "Epoch  77 Batch  116/269   train_loss = 3.559\n",
      "Epoch  77 Batch  117/269   train_loss = 3.641\n",
      "Epoch  77 Batch  118/269   train_loss = 3.898\n",
      "Epoch  77 Batch  119/269   train_loss = 3.582\n",
      "Epoch  77 Batch  120/269   train_loss = 3.706\n",
      "Epoch  77 Batch  121/269   train_loss = 3.746\n",
      "Epoch  77 Batch  122/269   train_loss = 3.719\n",
      "Epoch  77 Batch  123/269   train_loss = 3.532\n",
      "Epoch  77 Batch  124/269   train_loss = 3.894\n",
      "Epoch  77 Batch  125/269   train_loss = 3.563\n",
      "Epoch  77 Batch  126/269   train_loss = 3.418\n",
      "Epoch  77 Batch  127/269   train_loss = 3.784\n",
      "Epoch  77 Batch  128/269   train_loss = 3.647\n",
      "Epoch  77 Batch  129/269   train_loss = 3.578\n",
      "Epoch  77 Batch  130/269   train_loss = 3.728\n",
      "Epoch  77 Batch  131/269   train_loss = 3.442\n",
      "Epoch  77 Batch  132/269   train_loss = 3.732\n",
      "Epoch  77 Batch  133/269   train_loss = 3.491\n",
      "Epoch  77 Batch  134/269   train_loss = 3.873\n",
      "Epoch  77 Batch  135/269   train_loss = 3.600\n",
      "Epoch  77 Batch  136/269   train_loss = 3.425\n",
      "Epoch  77 Batch  137/269   train_loss = 3.843\n",
      "Epoch  77 Batch  138/269   train_loss = 3.558\n",
      "Epoch  77 Batch  139/269   train_loss = 3.642\n",
      "Epoch  77 Batch  140/269   train_loss = 3.815\n",
      "Epoch  77 Batch  141/269   train_loss = 4.031\n",
      "Epoch  77 Batch  142/269   train_loss = 3.521\n",
      "Epoch  77 Batch  143/269   train_loss = 3.722\n",
      "Epoch  77 Batch  144/269   train_loss = 3.918\n",
      "Epoch  77 Batch  145/269   train_loss = 3.752\n",
      "Epoch  77 Batch  146/269   train_loss = 3.879\n",
      "Epoch  77 Batch  147/269   train_loss = 3.559\n",
      "Epoch  77 Batch  148/269   train_loss = 3.864\n",
      "Epoch  77 Batch  149/269   train_loss = 3.604\n",
      "Epoch  77 Batch  150/269   train_loss = 3.821\n",
      "Epoch  77 Batch  151/269   train_loss = 3.826\n",
      "Epoch  77 Batch  152/269   train_loss = 3.710\n",
      "Epoch  77 Batch  153/269   train_loss = 3.763\n",
      "Epoch  77 Batch  154/269   train_loss = 3.687\n",
      "Epoch  77 Batch  155/269   train_loss = 3.642\n",
      "Epoch  77 Batch  156/269   train_loss = 4.146\n",
      "Epoch  77 Batch  157/269   train_loss = 3.877\n",
      "Epoch  77 Batch  158/269   train_loss = 3.786\n",
      "Epoch  77 Batch  159/269   train_loss = 3.815\n",
      "Epoch  77 Batch  160/269   train_loss = 3.638\n",
      "Epoch  77 Batch  161/269   train_loss = 3.864\n",
      "Epoch  77 Batch  162/269   train_loss = 3.883\n",
      "Epoch  77 Batch  163/269   train_loss = 3.641\n",
      "Epoch  77 Batch  164/269   train_loss = 3.914\n",
      "Epoch  77 Batch  165/269   train_loss = 3.919\n",
      "Epoch  77 Batch  166/269   train_loss = 3.820\n",
      "Epoch  77 Batch  167/269   train_loss = 3.585\n",
      "Epoch  77 Batch  168/269   train_loss = 3.774\n",
      "Epoch  77 Batch  169/269   train_loss = 3.992\n",
      "Epoch  77 Batch  170/269   train_loss = 3.535\n",
      "Epoch  77 Batch  171/269   train_loss = 3.744\n",
      "Epoch  77 Batch  172/269   train_loss = 3.584\n",
      "Epoch  77 Batch  173/269   train_loss = 3.775\n",
      "Epoch  77 Batch  174/269   train_loss = 3.912\n",
      "Epoch  77 Batch  175/269   train_loss = 3.793\n",
      "Epoch  77 Batch  176/269   train_loss = 3.722\n",
      "Epoch  77 Batch  177/269   train_loss = 3.823\n",
      "Epoch  77 Batch  178/269   train_loss = 4.061\n",
      "Epoch  77 Batch  179/269   train_loss = 3.763\n",
      "Epoch  77 Batch  180/269   train_loss = 3.637\n",
      "Epoch  77 Batch  181/269   train_loss = 3.875\n",
      "Epoch  77 Batch  182/269   train_loss = 3.724\n",
      "Epoch  77 Batch  183/269   train_loss = 3.686\n",
      "Epoch  77 Batch  184/269   train_loss = 3.756\n",
      "Epoch  77 Batch  185/269   train_loss = 3.782\n",
      "Epoch  77 Batch  186/269   train_loss = 3.705\n",
      "Epoch  77 Batch  187/269   train_loss = 3.839\n",
      "Epoch  77 Batch  188/269   train_loss = 3.648\n",
      "Epoch  77 Batch  189/269   train_loss = 3.723\n",
      "Epoch  77 Batch  190/269   train_loss = 4.075\n",
      "Epoch  77 Batch  191/269   train_loss = 3.817\n",
      "Epoch  77 Batch  192/269   train_loss = 3.713\n",
      "Epoch  77 Batch  193/269   train_loss = 3.761\n",
      "Epoch  77 Batch  194/269   train_loss = 3.858\n",
      "Epoch  77 Batch  195/269   train_loss = 3.773\n",
      "Epoch  77 Batch  196/269   train_loss = 3.760\n",
      "Epoch  77 Batch  197/269   train_loss = 3.929\n",
      "Epoch  77 Batch  198/269   train_loss = 3.748\n",
      "Epoch  77 Batch  199/269   train_loss = 3.877\n",
      "Epoch  77 Batch  200/269   train_loss = 3.636\n",
      "Epoch  77 Batch  201/269   train_loss = 3.771\n",
      "Epoch  77 Batch  202/269   train_loss = 3.508\n",
      "Epoch  77 Batch  203/269   train_loss = 3.622\n",
      "Epoch  77 Batch  204/269   train_loss = 3.845\n",
      "Epoch  77 Batch  205/269   train_loss = 3.878\n",
      "Epoch  77 Batch  206/269   train_loss = 3.678\n",
      "Epoch  77 Batch  207/269   train_loss = 3.666\n",
      "Epoch  77 Batch  208/269   train_loss = 3.866\n",
      "Epoch  77 Batch  209/269   train_loss = 3.895\n",
      "Epoch  77 Batch  210/269   train_loss = 3.680\n",
      "Epoch  77 Batch  211/269   train_loss = 3.659\n",
      "Epoch  77 Batch  212/269   train_loss = 4.066\n",
      "Epoch  77 Batch  213/269   train_loss = 3.637\n",
      "Epoch  77 Batch  214/269   train_loss = 3.751\n",
      "Epoch  77 Batch  215/269   train_loss = 3.941\n",
      "Epoch  77 Batch  216/269   train_loss = 3.941\n",
      "Epoch  77 Batch  217/269   train_loss = 3.609\n",
      "Epoch  77 Batch  218/269   train_loss = 3.848\n",
      "Epoch  77 Batch  219/269   train_loss = 3.516\n",
      "Epoch  77 Batch  220/269   train_loss = 3.906\n",
      "Epoch  77 Batch  221/269   train_loss = 3.651\n",
      "Epoch  77 Batch  222/269   train_loss = 3.837\n",
      "Epoch  77 Batch  223/269   train_loss = 3.519\n",
      "Epoch  77 Batch  224/269   train_loss = 3.900\n",
      "Epoch  77 Batch  225/269   train_loss = 3.942\n",
      "Epoch  77 Batch  226/269   train_loss = 3.701\n",
      "Epoch  77 Batch  227/269   train_loss = 3.484\n",
      "Epoch  77 Batch  228/269   train_loss = 3.803\n",
      "Epoch  77 Batch  229/269   train_loss = 3.932\n",
      "Epoch  77 Batch  230/269   train_loss = 3.820\n",
      "Epoch  77 Batch  231/269   train_loss = 3.649\n",
      "Epoch  77 Batch  232/269   train_loss = 3.706\n",
      "Epoch  77 Batch  233/269   train_loss = 3.861\n",
      "Epoch  77 Batch  234/269   train_loss = 3.758\n",
      "Epoch  77 Batch  235/269   train_loss = 4.053\n",
      "Epoch  77 Batch  236/269   train_loss = 3.633\n",
      "Epoch  77 Batch  237/269   train_loss = 3.282\n",
      "Epoch  77 Batch  238/269   train_loss = 3.555\n",
      "Epoch  77 Batch  239/269   train_loss = 4.044\n",
      "Epoch  77 Batch  240/269   train_loss = 3.584\n",
      "Epoch  77 Batch  241/269   train_loss = 3.923\n",
      "Epoch  77 Batch  242/269   train_loss = 3.550\n",
      "Epoch  77 Batch  243/269   train_loss = 3.927\n",
      "Epoch  77 Batch  244/269   train_loss = 3.717\n",
      "Epoch  77 Batch  245/269   train_loss = 3.712\n",
      "Epoch  77 Batch  246/269   train_loss = 3.371\n",
      "Epoch  77 Batch  247/269   train_loss = 3.759\n",
      "Epoch  77 Batch  248/269   train_loss = 3.805\n",
      "Epoch  77 Batch  249/269   train_loss = 3.713\n",
      "Epoch  77 Batch  250/269   train_loss = 3.553\n",
      "Epoch  77 Batch  251/269   train_loss = 3.932\n",
      "Epoch  77 Batch  252/269   train_loss = 3.874\n",
      "Epoch  77 Batch  253/269   train_loss = 3.616\n",
      "Epoch  77 Batch  254/269   train_loss = 3.629\n",
      "Epoch  77 Batch  255/269   train_loss = 3.734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  77 Batch  256/269   train_loss = 3.672\n",
      "Epoch  77 Batch  257/269   train_loss = 3.608\n",
      "Epoch  77 Batch  258/269   train_loss = 3.512\n",
      "Epoch  77 Batch  259/269   train_loss = 3.541\n",
      "Epoch  77 Batch  260/269   train_loss = 3.831\n",
      "Epoch  77 Batch  261/269   train_loss = 3.791\n",
      "Epoch  77 Batch  262/269   train_loss = 3.687\n",
      "Epoch  77 Batch  263/269   train_loss = 3.546\n",
      "Epoch  77 Batch  264/269   train_loss = 3.909\n",
      "Epoch  77 Batch  265/269   train_loss = 3.757\n",
      "Epoch  77 Batch  266/269   train_loss = 3.601\n",
      "Epoch  77 Batch  267/269   train_loss = 3.752\n",
      "Epoch  77 Batch  268/269   train_loss = 3.898\n",
      "Epoch  78 Batch    0/269   train_loss = 3.686\n",
      "Epoch  78 Batch    1/269   train_loss = 3.448\n",
      "Epoch  78 Batch    2/269   train_loss = 3.613\n",
      "Epoch  78 Batch    3/269   train_loss = 3.680\n",
      "Epoch  78 Batch    4/269   train_loss = 4.105\n",
      "Epoch  78 Batch    5/269   train_loss = 3.737\n",
      "Epoch  78 Batch    6/269   train_loss = 3.626\n",
      "Epoch  78 Batch    7/269   train_loss = 3.471\n",
      "Epoch  78 Batch    8/269   train_loss = 3.794\n",
      "Epoch  78 Batch    9/269   train_loss = 3.461\n",
      "Epoch  78 Batch   10/269   train_loss = 3.639\n",
      "Epoch  78 Batch   11/269   train_loss = 3.681\n",
      "Epoch  78 Batch   12/269   train_loss = 3.445\n",
      "Epoch  78 Batch   13/269   train_loss = 3.755\n",
      "Epoch  78 Batch   14/269   train_loss = 3.673\n",
      "Epoch  78 Batch   15/269   train_loss = 3.941\n",
      "Epoch  78 Batch   16/269   train_loss = 3.685\n",
      "Epoch  78 Batch   17/269   train_loss = 3.800\n",
      "Epoch  78 Batch   18/269   train_loss = 3.684\n",
      "Epoch  78 Batch   19/269   train_loss = 3.733\n",
      "Epoch  78 Batch   20/269   train_loss = 4.096\n",
      "Epoch  78 Batch   21/269   train_loss = 3.907\n",
      "Epoch  78 Batch   22/269   train_loss = 3.639\n",
      "Epoch  78 Batch   23/269   train_loss = 3.750\n",
      "Epoch  78 Batch   24/269   train_loss = 3.538\n",
      "Epoch  78 Batch   25/269   train_loss = 3.891\n",
      "Epoch  78 Batch   26/269   train_loss = 3.767\n",
      "Epoch  78 Batch   27/269   train_loss = 3.631\n",
      "Epoch  78 Batch   28/269   train_loss = 3.774\n",
      "Epoch  78 Batch   29/269   train_loss = 3.835\n",
      "Epoch  78 Batch   30/269   train_loss = 4.083\n",
      "Epoch  78 Batch   31/269   train_loss = 3.752\n",
      "Epoch  78 Batch   32/269   train_loss = 3.591\n",
      "Epoch  78 Batch   33/269   train_loss = 3.412\n",
      "Epoch  78 Batch   34/269   train_loss = 3.685\n",
      "Epoch  78 Batch   35/269   train_loss = 3.482\n",
      "Epoch  78 Batch   36/269   train_loss = 3.552\n",
      "Epoch  78 Batch   37/269   train_loss = 3.723\n",
      "Epoch  78 Batch   38/269   train_loss = 3.471\n",
      "Epoch  78 Batch   39/269   train_loss = 3.860\n",
      "Epoch  78 Batch   40/269   train_loss = 3.612\n",
      "Epoch  78 Batch   41/269   train_loss = 3.597\n",
      "Epoch  78 Batch   42/269   train_loss = 4.032\n",
      "Epoch  78 Batch   43/269   train_loss = 4.040\n",
      "Epoch  78 Batch   44/269   train_loss = 3.750\n",
      "Epoch  78 Batch   45/269   train_loss = 3.441\n",
      "Epoch  78 Batch   46/269   train_loss = 3.739\n",
      "Epoch  78 Batch   47/269   train_loss = 3.432\n",
      "Epoch  78 Batch   48/269   train_loss = 3.939\n",
      "Epoch  78 Batch   49/269   train_loss = 3.872\n",
      "Epoch  78 Batch   50/269   train_loss = 3.846\n",
      "Epoch  78 Batch   51/269   train_loss = 3.804\n",
      "Epoch  78 Batch   52/269   train_loss = 3.776\n",
      "Epoch  78 Batch   53/269   train_loss = 3.977\n",
      "Epoch  78 Batch   54/269   train_loss = 3.605\n",
      "Epoch  78 Batch   55/269   train_loss = 3.790\n",
      "Epoch  78 Batch   56/269   train_loss = 3.576\n",
      "Epoch  78 Batch   57/269   train_loss = 3.943\n",
      "Epoch  78 Batch   58/269   train_loss = 3.600\n",
      "Epoch  78 Batch   59/269   train_loss = 3.717\n",
      "Epoch  78 Batch   60/269   train_loss = 3.778\n",
      "Epoch  78 Batch   61/269   train_loss = 3.929\n",
      "Epoch  78 Batch   62/269   train_loss = 3.550\n",
      "Epoch  78 Batch   63/269   train_loss = 3.912\n",
      "Epoch  78 Batch   64/269   train_loss = 3.637\n",
      "Epoch  78 Batch   65/269   train_loss = 3.758\n",
      "Epoch  78 Batch   66/269   train_loss = 3.661\n",
      "Epoch  78 Batch   67/269   train_loss = 3.682\n",
      "Epoch  78 Batch   68/269   train_loss = 3.795\n",
      "Epoch  78 Batch   69/269   train_loss = 3.724\n",
      "Epoch  78 Batch   70/269   train_loss = 3.487\n",
      "Epoch  78 Batch   71/269   train_loss = 3.806\n",
      "Epoch  78 Batch   72/269   train_loss = 3.511\n",
      "Epoch  78 Batch   73/269   train_loss = 3.693\n",
      "Epoch  78 Batch   74/269   train_loss = 3.699\n",
      "Epoch  78 Batch   75/269   train_loss = 3.877\n",
      "Epoch  78 Batch   76/269   train_loss = 3.893\n",
      "Epoch  78 Batch   77/269   train_loss = 3.501\n",
      "Epoch  78 Batch   78/269   train_loss = 3.889\n",
      "Epoch  78 Batch   79/269   train_loss = 3.884\n",
      "Epoch  78 Batch   80/269   train_loss = 3.843\n",
      "Epoch  78 Batch   81/269   train_loss = 3.666\n",
      "Epoch  78 Batch   82/269   train_loss = 3.808\n",
      "Epoch  78 Batch   83/269   train_loss = 3.614\n",
      "Epoch  78 Batch   84/269   train_loss = 3.936\n",
      "Epoch  78 Batch   85/269   train_loss = 4.079\n",
      "Epoch  78 Batch   86/269   train_loss = 4.113\n",
      "Epoch  78 Batch   87/269   train_loss = 3.748\n",
      "Epoch  78 Batch   88/269   train_loss = 3.863\n",
      "Epoch  78 Batch   89/269   train_loss = 3.703\n",
      "Epoch  78 Batch   90/269   train_loss = 3.661\n",
      "Epoch  78 Batch   91/269   train_loss = 3.805\n",
      "Epoch  78 Batch   92/269   train_loss = 3.901\n",
      "Epoch  78 Batch   93/269   train_loss = 3.594\n",
      "Epoch  78 Batch   94/269   train_loss = 4.099\n",
      "Epoch  78 Batch   95/269   train_loss = 3.489\n",
      "Epoch  78 Batch   96/269   train_loss = 3.642\n",
      "Epoch  78 Batch   97/269   train_loss = 3.647\n",
      "Epoch  78 Batch   98/269   train_loss = 3.902\n",
      "Epoch  78 Batch   99/269   train_loss = 3.763\n",
      "Epoch  78 Batch  100/269   train_loss = 3.746\n",
      "Epoch  78 Batch  101/269   train_loss = 3.800\n",
      "Epoch  78 Batch  102/269   train_loss = 3.561\n",
      "Epoch  78 Batch  103/269   train_loss = 3.698\n",
      "Epoch  78 Batch  104/269   train_loss = 3.720\n",
      "Epoch  78 Batch  105/269   train_loss = 3.903\n",
      "Epoch  78 Batch  106/269   train_loss = 4.018\n",
      "Epoch  78 Batch  107/269   train_loss = 3.977\n",
      "Epoch  78 Batch  108/269   train_loss = 3.775\n",
      "Epoch  78 Batch  109/269   train_loss = 3.765\n",
      "Epoch  78 Batch  110/269   train_loss = 3.801\n",
      "Epoch  78 Batch  111/269   train_loss = 3.913\n",
      "Epoch  78 Batch  112/269   train_loss = 3.910\n",
      "Epoch  78 Batch  113/269   train_loss = 3.713\n",
      "Epoch  78 Batch  114/269   train_loss = 3.630\n",
      "Epoch  78 Batch  115/269   train_loss = 3.656\n",
      "Epoch  78 Batch  116/269   train_loss = 3.536\n",
      "Epoch  78 Batch  117/269   train_loss = 3.582\n",
      "Epoch  78 Batch  118/269   train_loss = 3.894\n",
      "Epoch  78 Batch  119/269   train_loss = 3.550\n",
      "Epoch  78 Batch  120/269   train_loss = 3.691\n",
      "Epoch  78 Batch  121/269   train_loss = 3.726\n",
      "Epoch  78 Batch  122/269   train_loss = 3.718\n",
      "Epoch  78 Batch  123/269   train_loss = 3.553\n",
      "Epoch  78 Batch  124/269   train_loss = 3.885\n",
      "Epoch  78 Batch  125/269   train_loss = 3.544\n",
      "Epoch  78 Batch  126/269   train_loss = 3.415\n",
      "Epoch  78 Batch  127/269   train_loss = 3.762\n",
      "Epoch  78 Batch  128/269   train_loss = 3.659\n",
      "Epoch  78 Batch  129/269   train_loss = 3.582\n",
      "Epoch  78 Batch  130/269   train_loss = 3.747\n",
      "Epoch  78 Batch  131/269   train_loss = 3.431\n",
      "Epoch  78 Batch  132/269   train_loss = 3.719\n",
      "Epoch  78 Batch  133/269   train_loss = 3.467\n",
      "Epoch  78 Batch  134/269   train_loss = 3.885\n",
      "Epoch  78 Batch  135/269   train_loss = 3.614\n",
      "Epoch  78 Batch  136/269   train_loss = 3.408\n",
      "Epoch  78 Batch  137/269   train_loss = 3.819\n",
      "Epoch  78 Batch  138/269   train_loss = 3.558\n",
      "Epoch  78 Batch  139/269   train_loss = 3.630\n",
      "Epoch  78 Batch  140/269   train_loss = 3.791\n",
      "Epoch  78 Batch  141/269   train_loss = 4.016\n",
      "Epoch  78 Batch  142/269   train_loss = 3.527\n",
      "Epoch  78 Batch  143/269   train_loss = 3.702\n",
      "Epoch  78 Batch  144/269   train_loss = 3.915\n",
      "Epoch  78 Batch  145/269   train_loss = 3.738\n",
      "Epoch  78 Batch  146/269   train_loss = 3.884\n",
      "Epoch  78 Batch  147/269   train_loss = 3.547\n",
      "Epoch  78 Batch  148/269   train_loss = 3.846\n",
      "Epoch  78 Batch  149/269   train_loss = 3.602\n",
      "Epoch  78 Batch  150/269   train_loss = 3.838\n",
      "Epoch  78 Batch  151/269   train_loss = 3.797\n",
      "Epoch  78 Batch  152/269   train_loss = 3.719\n",
      "Epoch  78 Batch  153/269   train_loss = 3.811\n",
      "Epoch  78 Batch  154/269   train_loss = 3.653\n",
      "Epoch  78 Batch  155/269   train_loss = 3.630\n",
      "Epoch  78 Batch  156/269   train_loss = 4.150\n",
      "Epoch  78 Batch  157/269   train_loss = 3.828\n",
      "Epoch  78 Batch  158/269   train_loss = 3.775\n",
      "Epoch  78 Batch  159/269   train_loss = 3.812\n",
      "Epoch  78 Batch  160/269   train_loss = 3.662\n",
      "Epoch  78 Batch  161/269   train_loss = 3.859\n",
      "Epoch  78 Batch  162/269   train_loss = 3.887\n",
      "Epoch  78 Batch  163/269   train_loss = 3.643\n",
      "Epoch  78 Batch  164/269   train_loss = 3.909\n",
      "Epoch  78 Batch  165/269   train_loss = 3.888\n",
      "Epoch  78 Batch  166/269   train_loss = 3.858\n",
      "Epoch  78 Batch  167/269   train_loss = 3.564\n",
      "Epoch  78 Batch  168/269   train_loss = 3.742\n",
      "Epoch  78 Batch  169/269   train_loss = 3.993\n",
      "Epoch  78 Batch  170/269   train_loss = 3.546\n",
      "Epoch  78 Batch  171/269   train_loss = 3.760\n",
      "Epoch  78 Batch  172/269   train_loss = 3.567\n",
      "Epoch  78 Batch  173/269   train_loss = 3.711\n",
      "Epoch  78 Batch  174/269   train_loss = 3.902\n",
      "Epoch  78 Batch  175/269   train_loss = 3.796\n",
      "Epoch  78 Batch  176/269   train_loss = 3.717\n",
      "Epoch  78 Batch  177/269   train_loss = 3.806\n",
      "Epoch  78 Batch  178/269   train_loss = 4.025\n",
      "Epoch  78 Batch  179/269   train_loss = 3.736\n",
      "Epoch  78 Batch  180/269   train_loss = 3.627\n",
      "Epoch  78 Batch  181/269   train_loss = 3.892\n",
      "Epoch  78 Batch  182/269   train_loss = 3.696\n",
      "Epoch  78 Batch  183/269   train_loss = 3.658\n",
      "Epoch  78 Batch  184/269   train_loss = 3.780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  78 Batch  185/269   train_loss = 3.758\n",
      "Epoch  78 Batch  186/269   train_loss = 3.711\n",
      "Epoch  78 Batch  187/269   train_loss = 3.802\n",
      "Epoch  78 Batch  188/269   train_loss = 3.657\n",
      "Epoch  78 Batch  189/269   train_loss = 3.679\n",
      "Epoch  78 Batch  190/269   train_loss = 4.053\n",
      "Epoch  78 Batch  191/269   train_loss = 3.825\n",
      "Epoch  78 Batch  192/269   train_loss = 3.725\n",
      "Epoch  78 Batch  193/269   train_loss = 3.736\n",
      "Epoch  78 Batch  194/269   train_loss = 3.862\n",
      "Epoch  78 Batch  195/269   train_loss = 3.739\n",
      "Epoch  78 Batch  196/269   train_loss = 3.743\n",
      "Epoch  78 Batch  197/269   train_loss = 3.947\n",
      "Epoch  78 Batch  198/269   train_loss = 3.740\n",
      "Epoch  78 Batch  199/269   train_loss = 3.877\n",
      "Epoch  78 Batch  200/269   train_loss = 3.672\n",
      "Epoch  78 Batch  201/269   train_loss = 3.732\n",
      "Epoch  78 Batch  202/269   train_loss = 3.519\n",
      "Epoch  78 Batch  203/269   train_loss = 3.612\n",
      "Epoch  78 Batch  204/269   train_loss = 3.830\n",
      "Epoch  78 Batch  205/269   train_loss = 3.786\n",
      "Epoch  78 Batch  206/269   train_loss = 3.694\n",
      "Epoch  78 Batch  207/269   train_loss = 3.651\n",
      "Epoch  78 Batch  208/269   train_loss = 3.850\n",
      "Epoch  78 Batch  209/269   train_loss = 3.887\n",
      "Epoch  78 Batch  210/269   train_loss = 3.691\n",
      "Epoch  78 Batch  211/269   train_loss = 3.682\n",
      "Epoch  78 Batch  212/269   train_loss = 4.040\n",
      "Epoch  78 Batch  213/269   train_loss = 3.652\n",
      "Epoch  78 Batch  214/269   train_loss = 3.698\n",
      "Epoch  78 Batch  215/269   train_loss = 3.936\n",
      "Epoch  78 Batch  216/269   train_loss = 3.958\n",
      "Epoch  78 Batch  217/269   train_loss = 3.614\n",
      "Epoch  78 Batch  218/269   train_loss = 3.848\n",
      "Epoch  78 Batch  219/269   train_loss = 3.550\n",
      "Epoch  78 Batch  220/269   train_loss = 3.929\n",
      "Epoch  78 Batch  221/269   train_loss = 3.627\n",
      "Epoch  78 Batch  222/269   train_loss = 3.822\n",
      "Epoch  78 Batch  223/269   train_loss = 3.529\n",
      "Epoch  78 Batch  224/269   train_loss = 3.884\n",
      "Epoch  78 Batch  225/269   train_loss = 3.910\n",
      "Epoch  78 Batch  226/269   train_loss = 3.703\n",
      "Epoch  78 Batch  227/269   train_loss = 3.463\n",
      "Epoch  78 Batch  228/269   train_loss = 3.806\n",
      "Epoch  78 Batch  229/269   train_loss = 3.927\n",
      "Epoch  78 Batch  230/269   train_loss = 3.824\n",
      "Epoch  78 Batch  231/269   train_loss = 3.637\n",
      "Epoch  78 Batch  232/269   train_loss = 3.720\n",
      "Epoch  78 Batch  233/269   train_loss = 3.838\n",
      "Epoch  78 Batch  234/269   train_loss = 3.745\n",
      "Epoch  78 Batch  235/269   train_loss = 3.996\n",
      "Epoch  78 Batch  236/269   train_loss = 3.590\n",
      "Epoch  78 Batch  237/269   train_loss = 3.286\n",
      "Epoch  78 Batch  238/269   train_loss = 3.520\n",
      "Epoch  78 Batch  239/269   train_loss = 4.011\n",
      "Epoch  78 Batch  240/269   train_loss = 3.569\n",
      "Epoch  78 Batch  241/269   train_loss = 3.936\n",
      "Epoch  78 Batch  242/269   train_loss = 3.543\n",
      "Epoch  78 Batch  243/269   train_loss = 3.897\n",
      "Epoch  78 Batch  244/269   train_loss = 3.719\n",
      "Epoch  78 Batch  245/269   train_loss = 3.701\n",
      "Epoch  78 Batch  246/269   train_loss = 3.363\n",
      "Epoch  78 Batch  247/269   train_loss = 3.751\n",
      "Epoch  78 Batch  248/269   train_loss = 3.801\n",
      "Epoch  78 Batch  249/269   train_loss = 3.687\n",
      "Epoch  78 Batch  250/269   train_loss = 3.543\n",
      "Epoch  78 Batch  251/269   train_loss = 3.908\n",
      "Epoch  78 Batch  252/269   train_loss = 3.855\n",
      "Epoch  78 Batch  253/269   train_loss = 3.599\n",
      "Epoch  78 Batch  254/269   train_loss = 3.629\n",
      "Epoch  78 Batch  255/269   train_loss = 3.720\n",
      "Epoch  78 Batch  256/269   train_loss = 3.677\n",
      "Epoch  78 Batch  257/269   train_loss = 3.604\n",
      "Epoch  78 Batch  258/269   train_loss = 3.490\n",
      "Epoch  78 Batch  259/269   train_loss = 3.534\n",
      "Epoch  78 Batch  260/269   train_loss = 3.828\n",
      "Epoch  78 Batch  261/269   train_loss = 3.804\n",
      "Epoch  78 Batch  262/269   train_loss = 3.676\n",
      "Epoch  78 Batch  263/269   train_loss = 3.554\n",
      "Epoch  78 Batch  264/269   train_loss = 3.920\n",
      "Epoch  78 Batch  265/269   train_loss = 3.744\n",
      "Epoch  78 Batch  266/269   train_loss = 3.616\n",
      "Epoch  78 Batch  267/269   train_loss = 3.742\n",
      "Epoch  78 Batch  268/269   train_loss = 3.895\n",
      "Epoch  79 Batch    0/269   train_loss = 3.652\n",
      "Epoch  79 Batch    1/269   train_loss = 3.410\n",
      "Epoch  79 Batch    2/269   train_loss = 3.606\n",
      "Epoch  79 Batch    3/269   train_loss = 3.669\n",
      "Epoch  79 Batch    4/269   train_loss = 4.109\n",
      "Epoch  79 Batch    5/269   train_loss = 3.709\n",
      "Epoch  79 Batch    6/269   train_loss = 3.605\n",
      "Epoch  79 Batch    7/269   train_loss = 3.450\n",
      "Epoch  79 Batch    8/269   train_loss = 3.808\n",
      "Epoch  79 Batch    9/269   train_loss = 3.453\n",
      "Epoch  79 Batch   10/269   train_loss = 3.619\n",
      "Epoch  79 Batch   11/269   train_loss = 3.673\n",
      "Epoch  79 Batch   12/269   train_loss = 3.438\n",
      "Epoch  79 Batch   13/269   train_loss = 3.718\n",
      "Epoch  79 Batch   14/269   train_loss = 3.703\n",
      "Epoch  79 Batch   15/269   train_loss = 3.979\n",
      "Epoch  79 Batch   16/269   train_loss = 3.670\n",
      "Epoch  79 Batch   17/269   train_loss = 3.781\n",
      "Epoch  79 Batch   18/269   train_loss = 3.669\n",
      "Epoch  79 Batch   19/269   train_loss = 3.687\n",
      "Epoch  79 Batch   20/269   train_loss = 4.087\n",
      "Epoch  79 Batch   21/269   train_loss = 3.900\n",
      "Epoch  79 Batch   22/269   train_loss = 3.639\n",
      "Epoch  79 Batch   23/269   train_loss = 3.706\n",
      "Epoch  79 Batch   24/269   train_loss = 3.522\n",
      "Epoch  79 Batch   25/269   train_loss = 3.882\n",
      "Epoch  79 Batch   26/269   train_loss = 3.773\n",
      "Epoch  79 Batch   27/269   train_loss = 3.616\n",
      "Epoch  79 Batch   28/269   train_loss = 3.775\n",
      "Epoch  79 Batch   29/269   train_loss = 3.802\n",
      "Epoch  79 Batch   30/269   train_loss = 4.053\n",
      "Epoch  79 Batch   31/269   train_loss = 3.750\n",
      "Epoch  79 Batch   32/269   train_loss = 3.574\n",
      "Epoch  79 Batch   33/269   train_loss = 3.460\n",
      "Epoch  79 Batch   34/269   train_loss = 3.647\n",
      "Epoch  79 Batch   35/269   train_loss = 3.462\n",
      "Epoch  79 Batch   36/269   train_loss = 3.525\n",
      "Epoch  79 Batch   37/269   train_loss = 3.705\n",
      "Epoch  79 Batch   38/269   train_loss = 3.425\n",
      "Epoch  79 Batch   39/269   train_loss = 3.833\n",
      "Epoch  79 Batch   40/269   train_loss = 3.610\n",
      "Epoch  79 Batch   41/269   train_loss = 3.594\n",
      "Epoch  79 Batch   42/269   train_loss = 4.010\n",
      "Epoch  79 Batch   43/269   train_loss = 4.005\n",
      "Epoch  79 Batch   44/269   train_loss = 3.736\n",
      "Epoch  79 Batch   45/269   train_loss = 3.440\n",
      "Epoch  79 Batch   46/269   train_loss = 3.739\n",
      "Epoch  79 Batch   47/269   train_loss = 3.426\n",
      "Epoch  79 Batch   48/269   train_loss = 3.910\n",
      "Epoch  79 Batch   49/269   train_loss = 3.873\n",
      "Epoch  79 Batch   50/269   train_loss = 3.830\n",
      "Epoch  79 Batch   51/269   train_loss = 3.783\n",
      "Epoch  79 Batch   52/269   train_loss = 3.782\n",
      "Epoch  79 Batch   53/269   train_loss = 3.960\n",
      "Epoch  79 Batch   54/269   train_loss = 3.592\n",
      "Epoch  79 Batch   55/269   train_loss = 3.791\n",
      "Epoch  79 Batch   56/269   train_loss = 3.546\n",
      "Epoch  79 Batch   57/269   train_loss = 3.921\n",
      "Epoch  79 Batch   58/269   train_loss = 3.579\n",
      "Epoch  79 Batch   59/269   train_loss = 3.632\n",
      "Epoch  79 Batch   60/269   train_loss = 3.750\n",
      "Epoch  79 Batch   61/269   train_loss = 3.898\n",
      "Epoch  79 Batch   62/269   train_loss = 3.538\n",
      "Epoch  79 Batch   63/269   train_loss = 3.929\n",
      "Epoch  79 Batch   64/269   train_loss = 3.626\n",
      "Epoch  79 Batch   65/269   train_loss = 3.778\n",
      "Epoch  79 Batch   66/269   train_loss = 3.635\n",
      "Epoch  79 Batch   67/269   train_loss = 3.657\n",
      "Epoch  79 Batch   68/269   train_loss = 3.824\n",
      "Epoch  79 Batch   69/269   train_loss = 3.693\n",
      "Epoch  79 Batch   70/269   train_loss = 3.481\n",
      "Epoch  79 Batch   71/269   train_loss = 3.748\n",
      "Epoch  79 Batch   72/269   train_loss = 3.502\n",
      "Epoch  79 Batch   73/269   train_loss = 3.717\n",
      "Epoch  79 Batch   74/269   train_loss = 3.697\n",
      "Epoch  79 Batch   75/269   train_loss = 3.871\n",
      "Epoch  79 Batch   76/269   train_loss = 3.881\n",
      "Epoch  79 Batch   77/269   train_loss = 3.477\n",
      "Epoch  79 Batch   78/269   train_loss = 3.871\n",
      "Epoch  79 Batch   79/269   train_loss = 3.874\n",
      "Epoch  79 Batch   80/269   train_loss = 3.829\n",
      "Epoch  79 Batch   81/269   train_loss = 3.677\n",
      "Epoch  79 Batch   82/269   train_loss = 3.850\n",
      "Epoch  79 Batch   83/269   train_loss = 3.597\n",
      "Epoch  79 Batch   84/269   train_loss = 3.951\n",
      "Epoch  79 Batch   85/269   train_loss = 4.062\n",
      "Epoch  79 Batch   86/269   train_loss = 4.132\n",
      "Epoch  79 Batch   87/269   train_loss = 3.698\n",
      "Epoch  79 Batch   88/269   train_loss = 3.873\n",
      "Epoch  79 Batch   89/269   train_loss = 3.699\n",
      "Epoch  79 Batch   90/269   train_loss = 3.617\n",
      "Epoch  79 Batch   91/269   train_loss = 3.815\n",
      "Epoch  79 Batch   92/269   train_loss = 3.905\n",
      "Epoch  79 Batch   93/269   train_loss = 3.599\n",
      "Epoch  79 Batch   94/269   train_loss = 4.102\n",
      "Epoch  79 Batch   95/269   train_loss = 3.510\n",
      "Epoch  79 Batch   96/269   train_loss = 3.621\n",
      "Epoch  79 Batch   97/269   train_loss = 3.638\n",
      "Epoch  79 Batch   98/269   train_loss = 3.892\n",
      "Epoch  79 Batch   99/269   train_loss = 3.774\n",
      "Epoch  79 Batch  100/269   train_loss = 3.732\n",
      "Epoch  79 Batch  101/269   train_loss = 3.800\n",
      "Epoch  79 Batch  102/269   train_loss = 3.563\n",
      "Epoch  79 Batch  103/269   train_loss = 3.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  79 Batch  104/269   train_loss = 3.709\n",
      "Epoch  79 Batch  105/269   train_loss = 3.884\n",
      "Epoch  79 Batch  106/269   train_loss = 4.011\n",
      "Epoch  79 Batch  107/269   train_loss = 3.967\n",
      "Epoch  79 Batch  108/269   train_loss = 3.758\n",
      "Epoch  79 Batch  109/269   train_loss = 3.781\n",
      "Epoch  79 Batch  110/269   train_loss = 3.799\n",
      "Epoch  79 Batch  111/269   train_loss = 3.892\n",
      "Epoch  79 Batch  112/269   train_loss = 3.889\n",
      "Epoch  79 Batch  113/269   train_loss = 3.710\n",
      "Epoch  79 Batch  114/269   train_loss = 3.677\n",
      "Epoch  79 Batch  115/269   train_loss = 3.660\n",
      "Epoch  79 Batch  116/269   train_loss = 3.600\n",
      "Epoch  79 Batch  117/269   train_loss = 3.580\n",
      "Epoch  79 Batch  118/269   train_loss = 3.875\n",
      "Epoch  79 Batch  119/269   train_loss = 3.589\n",
      "Epoch  79 Batch  120/269   train_loss = 3.692\n",
      "Epoch  79 Batch  121/269   train_loss = 3.725\n",
      "Epoch  79 Batch  122/269   train_loss = 3.713\n",
      "Epoch  79 Batch  123/269   train_loss = 3.507\n",
      "Epoch  79 Batch  124/269   train_loss = 3.876\n",
      "Epoch  79 Batch  125/269   train_loss = 3.505\n",
      "Epoch  79 Batch  126/269   train_loss = 3.417\n",
      "Epoch  79 Batch  127/269   train_loss = 3.741\n",
      "Epoch  79 Batch  128/269   train_loss = 3.673\n",
      "Epoch  79 Batch  129/269   train_loss = 3.590\n",
      "Epoch  79 Batch  130/269   train_loss = 3.709\n",
      "Epoch  79 Batch  131/269   train_loss = 3.418\n",
      "Epoch  79 Batch  132/269   train_loss = 3.708\n",
      "Epoch  79 Batch  133/269   train_loss = 3.471\n",
      "Epoch  79 Batch  134/269   train_loss = 3.898\n",
      "Epoch  79 Batch  135/269   train_loss = 3.589\n",
      "Epoch  79 Batch  136/269   train_loss = 3.412\n",
      "Epoch  79 Batch  137/269   train_loss = 3.811\n",
      "Epoch  79 Batch  138/269   train_loss = 3.571\n",
      "Epoch  79 Batch  139/269   train_loss = 3.623\n",
      "Epoch  79 Batch  140/269   train_loss = 3.775\n",
      "Epoch  79 Batch  141/269   train_loss = 4.016\n",
      "Epoch  79 Batch  142/269   train_loss = 3.556\n",
      "Epoch  79 Batch  143/269   train_loss = 3.685\n",
      "Epoch  79 Batch  144/269   train_loss = 3.887\n",
      "Epoch  79 Batch  145/269   train_loss = 3.765\n",
      "Epoch  79 Batch  146/269   train_loss = 3.874\n",
      "Epoch  79 Batch  147/269   train_loss = 3.545\n",
      "Epoch  79 Batch  148/269   train_loss = 3.862\n",
      "Epoch  79 Batch  149/269   train_loss = 3.628\n",
      "Epoch  79 Batch  150/269   train_loss = 3.822\n",
      "Epoch  79 Batch  151/269   train_loss = 3.778\n",
      "Epoch  79 Batch  152/269   train_loss = 3.721\n",
      "Epoch  79 Batch  153/269   train_loss = 3.810\n",
      "Epoch  79 Batch  154/269   train_loss = 3.656\n",
      "Epoch  79 Batch  155/269   train_loss = 3.629\n",
      "Epoch  79 Batch  156/269   train_loss = 4.155\n",
      "Epoch  79 Batch  157/269   train_loss = 3.795\n",
      "Epoch  79 Batch  158/269   train_loss = 3.778\n",
      "Epoch  79 Batch  159/269   train_loss = 3.804\n",
      "Epoch  79 Batch  160/269   train_loss = 3.654\n",
      "Epoch  79 Batch  161/269   train_loss = 3.825\n",
      "Epoch  79 Batch  162/269   train_loss = 3.842\n",
      "Epoch  79 Batch  163/269   train_loss = 3.627\n",
      "Epoch  79 Batch  164/269   train_loss = 3.881\n",
      "Epoch  79 Batch  165/269   train_loss = 3.945\n",
      "Epoch  79 Batch  166/269   train_loss = 3.800\n",
      "Epoch  79 Batch  167/269   train_loss = 3.573\n",
      "Epoch  79 Batch  168/269   train_loss = 3.712\n",
      "Epoch  79 Batch  169/269   train_loss = 3.977\n",
      "Epoch  79 Batch  170/269   train_loss = 3.532\n",
      "Epoch  79 Batch  171/269   train_loss = 3.726\n",
      "Epoch  79 Batch  172/269   train_loss = 3.551\n",
      "Epoch  79 Batch  173/269   train_loss = 3.701\n",
      "Epoch  79 Batch  174/269   train_loss = 3.912\n",
      "Epoch  79 Batch  175/269   train_loss = 3.763\n",
      "Epoch  79 Batch  176/269   train_loss = 3.706\n",
      "Epoch  79 Batch  177/269   train_loss = 3.842\n",
      "Epoch  79 Batch  178/269   train_loss = 4.007\n",
      "Epoch  79 Batch  179/269   train_loss = 3.741\n",
      "Epoch  79 Batch  180/269   train_loss = 3.632\n",
      "Epoch  79 Batch  181/269   train_loss = 3.882\n",
      "Epoch  79 Batch  182/269   train_loss = 3.676\n",
      "Epoch  79 Batch  183/269   train_loss = 3.647\n",
      "Epoch  79 Batch  184/269   train_loss = 3.723\n",
      "Epoch  79 Batch  185/269   train_loss = 3.773\n",
      "Epoch  79 Batch  186/269   train_loss = 3.719\n",
      "Epoch  79 Batch  187/269   train_loss = 3.776\n",
      "Epoch  79 Batch  188/269   train_loss = 3.618\n",
      "Epoch  79 Batch  189/269   train_loss = 3.682\n",
      "Epoch  79 Batch  190/269   train_loss = 4.041\n",
      "Epoch  79 Batch  191/269   train_loss = 3.773\n",
      "Epoch  79 Batch  192/269   train_loss = 3.737\n",
      "Epoch  79 Batch  193/269   train_loss = 3.717\n",
      "Epoch  79 Batch  194/269   train_loss = 3.834\n",
      "Epoch  79 Batch  195/269   train_loss = 3.743\n",
      "Epoch  79 Batch  196/269   train_loss = 3.726\n",
      "Epoch  79 Batch  197/269   train_loss = 3.929\n",
      "Epoch  79 Batch  198/269   train_loss = 3.757\n",
      "Epoch  79 Batch  199/269   train_loss = 3.876\n",
      "Epoch  79 Batch  200/269   train_loss = 3.600\n",
      "Epoch  79 Batch  201/269   train_loss = 3.711\n",
      "Epoch  79 Batch  202/269   train_loss = 3.507\n",
      "Epoch  79 Batch  203/269   train_loss = 3.599\n",
      "Epoch  79 Batch  204/269   train_loss = 3.842\n",
      "Epoch  79 Batch  205/269   train_loss = 3.768\n",
      "Epoch  79 Batch  206/269   train_loss = 3.726\n",
      "Epoch  79 Batch  207/269   train_loss = 3.653\n",
      "Epoch  79 Batch  208/269   train_loss = 3.824\n",
      "Epoch  79 Batch  209/269   train_loss = 3.869\n",
      "Epoch  79 Batch  210/269   train_loss = 3.639\n",
      "Epoch  79 Batch  211/269   train_loss = 3.675\n",
      "Epoch  79 Batch  212/269   train_loss = 4.018\n",
      "Epoch  79 Batch  213/269   train_loss = 3.619\n",
      "Epoch  79 Batch  214/269   train_loss = 3.689\n",
      "Epoch  79 Batch  215/269   train_loss = 3.953\n",
      "Epoch  79 Batch  216/269   train_loss = 3.918\n",
      "Epoch  79 Batch  217/269   train_loss = 3.613\n",
      "Epoch  79 Batch  218/269   train_loss = 3.834\n",
      "Epoch  79 Batch  219/269   train_loss = 3.501\n",
      "Epoch  79 Batch  220/269   train_loss = 3.916\n",
      "Epoch  79 Batch  221/269   train_loss = 3.619\n",
      "Epoch  79 Batch  222/269   train_loss = 3.809\n",
      "Epoch  79 Batch  223/269   train_loss = 3.620\n",
      "Epoch  79 Batch  224/269   train_loss = 3.880\n",
      "Epoch  79 Batch  225/269   train_loss = 3.892\n",
      "Epoch  79 Batch  226/269   train_loss = 3.688\n",
      "Epoch  79 Batch  227/269   train_loss = 3.456\n",
      "Epoch  79 Batch  228/269   train_loss = 3.807\n",
      "Epoch  79 Batch  229/269   train_loss = 3.897\n",
      "Epoch  79 Batch  230/269   train_loss = 3.826\n",
      "Epoch  79 Batch  231/269   train_loss = 3.647\n",
      "Epoch  79 Batch  232/269   train_loss = 3.730\n",
      "Epoch  79 Batch  233/269   train_loss = 3.812\n",
      "Epoch  79 Batch  234/269   train_loss = 3.752\n",
      "Epoch  79 Batch  235/269   train_loss = 3.984\n",
      "Epoch  79 Batch  236/269   train_loss = 3.574\n",
      "Epoch  79 Batch  237/269   train_loss = 3.263\n",
      "Epoch  79 Batch  238/269   train_loss = 3.536\n",
      "Epoch  79 Batch  239/269   train_loss = 4.006\n",
      "Epoch  79 Batch  240/269   train_loss = 3.564\n",
      "Epoch  79 Batch  241/269   train_loss = 3.913\n",
      "Epoch  79 Batch  242/269   train_loss = 3.538\n",
      "Epoch  79 Batch  243/269   train_loss = 3.912\n",
      "Epoch  79 Batch  244/269   train_loss = 3.688\n",
      "Epoch  79 Batch  245/269   train_loss = 3.706\n",
      "Epoch  79 Batch  246/269   train_loss = 3.353\n",
      "Epoch  79 Batch  247/269   train_loss = 3.749\n",
      "Epoch  79 Batch  248/269   train_loss = 3.798\n",
      "Epoch  79 Batch  249/269   train_loss = 3.664\n",
      "Epoch  79 Batch  250/269   train_loss = 3.537\n",
      "Epoch  79 Batch  251/269   train_loss = 3.890\n",
      "Epoch  79 Batch  252/269   train_loss = 3.858\n",
      "Epoch  79 Batch  253/269   train_loss = 3.607\n",
      "Epoch  79 Batch  254/269   train_loss = 3.616\n",
      "Epoch  79 Batch  255/269   train_loss = 3.733\n",
      "Epoch  79 Batch  256/269   train_loss = 3.663\n",
      "Epoch  79 Batch  257/269   train_loss = 3.596\n",
      "Epoch  79 Batch  258/269   train_loss = 3.480\n",
      "Epoch  79 Batch  259/269   train_loss = 3.557\n",
      "Epoch  79 Batch  260/269   train_loss = 3.830\n",
      "Epoch  79 Batch  261/269   train_loss = 3.853\n",
      "Epoch  79 Batch  262/269   train_loss = 3.623\n",
      "Epoch  79 Batch  263/269   train_loss = 3.588\n",
      "Epoch  79 Batch  264/269   train_loss = 3.902\n",
      "Epoch  79 Batch  265/269   train_loss = 3.725\n",
      "Epoch  79 Batch  266/269   train_loss = 3.599\n",
      "Epoch  79 Batch  267/269   train_loss = 3.732\n",
      "Epoch  79 Batch  268/269   train_loss = 3.906\n",
      "Epoch  80 Batch    0/269   train_loss = 3.635\n",
      "Epoch  80 Batch    1/269   train_loss = 3.402\n",
      "Epoch  80 Batch    2/269   train_loss = 3.596\n",
      "Epoch  80 Batch    3/269   train_loss = 3.668\n",
      "Epoch  80 Batch    4/269   train_loss = 4.095\n",
      "Epoch  80 Batch    5/269   train_loss = 3.722\n",
      "Epoch  80 Batch    6/269   train_loss = 3.633\n",
      "Epoch  80 Batch    7/269   train_loss = 3.457\n",
      "Epoch  80 Batch    8/269   train_loss = 3.808\n",
      "Epoch  80 Batch    9/269   train_loss = 3.444\n",
      "Epoch  80 Batch   10/269   train_loss = 3.575\n",
      "Epoch  80 Batch   11/269   train_loss = 3.673\n",
      "Epoch  80 Batch   12/269   train_loss = 3.421\n",
      "Epoch  80 Batch   13/269   train_loss = 3.727\n",
      "Epoch  80 Batch   14/269   train_loss = 3.712\n",
      "Epoch  80 Batch   15/269   train_loss = 3.946\n",
      "Epoch  80 Batch   16/269   train_loss = 3.728\n",
      "Epoch  80 Batch   17/269   train_loss = 3.789\n",
      "Epoch  80 Batch   18/269   train_loss = 3.653\n",
      "Epoch  80 Batch   19/269   train_loss = 3.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  80 Batch   20/269   train_loss = 4.080\n",
      "Epoch  80 Batch   21/269   train_loss = 3.903\n",
      "Epoch  80 Batch   22/269   train_loss = 3.643\n",
      "Epoch  80 Batch   23/269   train_loss = 3.717\n",
      "Epoch  80 Batch   24/269   train_loss = 3.513\n",
      "Epoch  80 Batch   25/269   train_loss = 3.881\n",
      "Epoch  80 Batch   26/269   train_loss = 3.763\n",
      "Epoch  80 Batch   27/269   train_loss = 3.717\n",
      "Epoch  80 Batch   28/269   train_loss = 3.759\n",
      "Epoch  80 Batch   29/269   train_loss = 3.763\n",
      "Epoch  80 Batch   30/269   train_loss = 4.051\n",
      "Epoch  80 Batch   31/269   train_loss = 3.764\n",
      "Epoch  80 Batch   32/269   train_loss = 3.600\n",
      "Epoch  80 Batch   33/269   train_loss = 3.389\n",
      "Epoch  80 Batch   34/269   train_loss = 3.659\n",
      "Epoch  80 Batch   35/269   train_loss = 3.443\n",
      "Epoch  80 Batch   36/269   train_loss = 3.531\n",
      "Epoch  80 Batch   37/269   train_loss = 3.720\n",
      "Epoch  80 Batch   38/269   train_loss = 3.487\n",
      "Epoch  80 Batch   39/269   train_loss = 3.844\n",
      "Epoch  80 Batch   40/269   train_loss = 3.636\n",
      "Epoch  80 Batch   41/269   train_loss = 3.595\n",
      "Epoch  80 Batch   42/269   train_loss = 4.020\n",
      "Epoch  80 Batch   43/269   train_loss = 3.991\n",
      "Epoch  80 Batch   44/269   train_loss = 3.742\n",
      "Epoch  80 Batch   45/269   train_loss = 3.433\n",
      "Epoch  80 Batch   46/269   train_loss = 3.725\n",
      "Epoch  80 Batch   47/269   train_loss = 3.458\n",
      "Epoch  80 Batch   48/269   train_loss = 3.894\n",
      "Epoch  80 Batch   49/269   train_loss = 3.851\n",
      "Epoch  80 Batch   50/269   train_loss = 3.858\n",
      "Epoch  80 Batch   51/269   train_loss = 3.756\n",
      "Epoch  80 Batch   52/269   train_loss = 3.789\n",
      "Epoch  80 Batch   53/269   train_loss = 3.950\n",
      "Epoch  80 Batch   54/269   train_loss = 3.601\n",
      "Epoch  80 Batch   55/269   train_loss = 3.764\n",
      "Epoch  80 Batch   56/269   train_loss = 3.523\n",
      "Epoch  80 Batch   57/269   train_loss = 3.918\n",
      "Epoch  80 Batch   58/269   train_loss = 3.584\n",
      "Epoch  80 Batch   59/269   train_loss = 3.687\n",
      "Epoch  80 Batch   60/269   train_loss = 3.770\n",
      "Epoch  80 Batch   61/269   train_loss = 3.880\n",
      "Epoch  80 Batch   62/269   train_loss = 3.554\n",
      "Epoch  80 Batch   63/269   train_loss = 3.876\n",
      "Epoch  80 Batch   64/269   train_loss = 3.655\n",
      "Epoch  80 Batch   65/269   train_loss = 3.749\n",
      "Epoch  80 Batch   66/269   train_loss = 3.626\n",
      "Epoch  80 Batch   67/269   train_loss = 3.632\n",
      "Epoch  80 Batch   68/269   train_loss = 3.784\n",
      "Epoch  80 Batch   69/269   train_loss = 3.689\n",
      "Epoch  80 Batch   70/269   train_loss = 3.478\n",
      "Epoch  80 Batch   71/269   train_loss = 3.735\n",
      "Epoch  80 Batch   72/269   train_loss = 3.492\n",
      "Epoch  80 Batch   73/269   train_loss = 3.717\n",
      "Epoch  80 Batch   74/269   train_loss = 3.719\n",
      "Epoch  80 Batch   75/269   train_loss = 3.903\n",
      "Epoch  80 Batch   76/269   train_loss = 3.864\n",
      "Epoch  80 Batch   77/269   train_loss = 3.474\n",
      "Epoch  80 Batch   78/269   train_loss = 3.877\n",
      "Epoch  80 Batch   79/269   train_loss = 3.906\n",
      "Epoch  80 Batch   80/269   train_loss = 3.861\n",
      "Epoch  80 Batch   81/269   train_loss = 3.676\n",
      "Epoch  80 Batch   82/269   train_loss = 3.868\n",
      "Epoch  80 Batch   83/269   train_loss = 3.611\n",
      "Epoch  80 Batch   84/269   train_loss = 3.951\n",
      "Epoch  80 Batch   85/269   train_loss = 4.077\n",
      "Epoch  80 Batch   86/269   train_loss = 4.132\n",
      "Epoch  80 Batch   87/269   train_loss = 3.720\n",
      "Epoch  80 Batch   88/269   train_loss = 3.851\n",
      "Epoch  80 Batch   89/269   train_loss = 3.694\n",
      "Epoch  80 Batch   90/269   train_loss = 3.682\n",
      "Epoch  80 Batch   91/269   train_loss = 3.799\n",
      "Epoch  80 Batch   92/269   train_loss = 3.897\n",
      "Epoch  80 Batch   93/269   train_loss = 3.595\n",
      "Epoch  80 Batch   94/269   train_loss = 4.091\n",
      "Epoch  80 Batch   95/269   train_loss = 3.486\n",
      "Epoch  80 Batch   96/269   train_loss = 3.633\n",
      "Epoch  80 Batch   97/269   train_loss = 3.624\n",
      "Epoch  80 Batch   98/269   train_loss = 3.888\n",
      "Epoch  80 Batch   99/269   train_loss = 3.735\n",
      "Epoch  80 Batch  100/269   train_loss = 3.753\n",
      "Epoch  80 Batch  101/269   train_loss = 3.790\n",
      "Epoch  80 Batch  102/269   train_loss = 3.559\n",
      "Epoch  80 Batch  103/269   train_loss = 3.684\n",
      "Epoch  80 Batch  104/269   train_loss = 3.709\n",
      "Epoch  80 Batch  105/269   train_loss = 3.874\n",
      "Epoch  80 Batch  106/269   train_loss = 3.988\n",
      "Epoch  80 Batch  107/269   train_loss = 3.979\n",
      "Epoch  80 Batch  108/269   train_loss = 3.760\n",
      "Epoch  80 Batch  109/269   train_loss = 3.792\n",
      "Epoch  80 Batch  110/269   train_loss = 3.806\n",
      "Epoch  80 Batch  111/269   train_loss = 3.907\n",
      "Epoch  80 Batch  112/269   train_loss = 3.909\n",
      "Epoch  80 Batch  113/269   train_loss = 3.722\n",
      "Epoch  80 Batch  114/269   train_loss = 3.672\n",
      "Epoch  80 Batch  115/269   train_loss = 3.633\n",
      "Epoch  80 Batch  116/269   train_loss = 3.563\n",
      "Epoch  80 Batch  117/269   train_loss = 3.590\n",
      "Epoch  80 Batch  118/269   train_loss = 3.850\n",
      "Epoch  80 Batch  119/269   train_loss = 3.599\n",
      "Epoch  80 Batch  120/269   train_loss = 3.684\n",
      "Epoch  80 Batch  121/269   train_loss = 3.728\n",
      "Epoch  80 Batch  122/269   train_loss = 3.714\n",
      "Epoch  80 Batch  123/269   train_loss = 3.508\n",
      "Epoch  80 Batch  124/269   train_loss = 3.849\n",
      "Epoch  80 Batch  125/269   train_loss = 3.492\n",
      "Epoch  80 Batch  126/269   train_loss = 3.408\n",
      "Epoch  80 Batch  127/269   train_loss = 3.729\n",
      "Epoch  80 Batch  128/269   train_loss = 3.658\n",
      "Epoch  80 Batch  129/269   train_loss = 3.578\n",
      "Epoch  80 Batch  130/269   train_loss = 3.739\n",
      "Epoch  80 Batch  131/269   train_loss = 3.391\n",
      "Epoch  80 Batch  132/269   train_loss = 3.735\n",
      "Epoch  80 Batch  133/269   train_loss = 3.449\n",
      "Epoch  80 Batch  134/269   train_loss = 3.901\n",
      "Epoch  80 Batch  135/269   train_loss = 3.587\n",
      "Epoch  80 Batch  136/269   train_loss = 3.430\n",
      "Epoch  80 Batch  137/269   train_loss = 3.790\n",
      "Epoch  80 Batch  138/269   train_loss = 3.542\n",
      "Epoch  80 Batch  139/269   train_loss = 3.648\n",
      "Epoch  80 Batch  140/269   train_loss = 3.760\n",
      "Epoch  80 Batch  141/269   train_loss = 4.007\n",
      "Epoch  80 Batch  142/269   train_loss = 3.523\n",
      "Epoch  80 Batch  143/269   train_loss = 3.671\n",
      "Epoch  80 Batch  144/269   train_loss = 3.876\n",
      "Epoch  80 Batch  145/269   train_loss = 3.750\n",
      "Epoch  80 Batch  146/269   train_loss = 3.875\n",
      "Epoch  80 Batch  147/269   train_loss = 3.545\n",
      "Epoch  80 Batch  148/269   train_loss = 3.842\n",
      "Epoch  80 Batch  149/269   train_loss = 3.604\n",
      "Epoch  80 Batch  150/269   train_loss = 3.830\n",
      "Epoch  80 Batch  151/269   train_loss = 3.786\n",
      "Epoch  80 Batch  152/269   train_loss = 3.714\n",
      "Epoch  80 Batch  153/269   train_loss = 3.802\n",
      "Epoch  80 Batch  154/269   train_loss = 3.629\n",
      "Epoch  80 Batch  155/269   train_loss = 3.635\n",
      "Epoch  80 Batch  156/269   train_loss = 4.137\n",
      "Epoch  80 Batch  157/269   train_loss = 3.788\n",
      "Epoch  80 Batch  158/269   train_loss = 3.759\n",
      "Epoch  80 Batch  159/269   train_loss = 3.787\n",
      "Epoch  80 Batch  160/269   train_loss = 3.648\n",
      "Epoch  80 Batch  161/269   train_loss = 3.807\n",
      "Epoch  80 Batch  162/269   train_loss = 3.820\n",
      "Epoch  80 Batch  163/269   train_loss = 3.607\n",
      "Epoch  80 Batch  164/269   train_loss = 3.874\n",
      "Epoch  80 Batch  165/269   train_loss = 3.947\n",
      "Epoch  80 Batch  166/269   train_loss = 3.801\n",
      "Epoch  80 Batch  167/269   train_loss = 3.531\n",
      "Epoch  80 Batch  168/269   train_loss = 3.731\n",
      "Epoch  80 Batch  169/269   train_loss = 3.973\n",
      "Epoch  80 Batch  170/269   train_loss = 3.552\n",
      "Epoch  80 Batch  171/269   train_loss = 3.724\n",
      "Epoch  80 Batch  172/269   train_loss = 3.552\n",
      "Epoch  80 Batch  173/269   train_loss = 3.704\n",
      "Epoch  80 Batch  174/269   train_loss = 3.920\n",
      "Epoch  80 Batch  175/269   train_loss = 3.783\n",
      "Epoch  80 Batch  176/269   train_loss = 3.687\n",
      "Epoch  80 Batch  177/269   train_loss = 3.846\n",
      "Epoch  80 Batch  178/269   train_loss = 4.002\n",
      "Epoch  80 Batch  179/269   train_loss = 3.732\n",
      "Epoch  80 Batch  180/269   train_loss = 3.631\n",
      "Epoch  80 Batch  181/269   train_loss = 3.865\n",
      "Epoch  80 Batch  182/269   train_loss = 3.677\n",
      "Epoch  80 Batch  183/269   train_loss = 3.639\n",
      "Epoch  80 Batch  184/269   train_loss = 3.721\n",
      "Epoch  80 Batch  185/269   train_loss = 3.740\n",
      "Epoch  80 Batch  186/269   train_loss = 3.713\n",
      "Epoch  80 Batch  187/269   train_loss = 3.779\n",
      "Epoch  80 Batch  188/269   train_loss = 3.603\n",
      "Epoch  80 Batch  189/269   train_loss = 3.661\n",
      "Epoch  80 Batch  190/269   train_loss = 4.034\n",
      "Epoch  80 Batch  191/269   train_loss = 3.796\n",
      "Epoch  80 Batch  192/269   train_loss = 3.761\n",
      "Epoch  80 Batch  193/269   train_loss = 3.708\n",
      "Epoch  80 Batch  194/269   train_loss = 3.813\n",
      "Epoch  80 Batch  195/269   train_loss = 3.722\n",
      "Epoch  80 Batch  196/269   train_loss = 3.723\n",
      "Epoch  80 Batch  197/269   train_loss = 3.927\n",
      "Epoch  80 Batch  198/269   train_loss = 3.749\n",
      "Epoch  80 Batch  199/269   train_loss = 3.880\n",
      "Epoch  80 Batch  200/269   train_loss = 3.621\n",
      "Epoch  80 Batch  201/269   train_loss = 3.703\n",
      "Epoch  80 Batch  202/269   train_loss = 3.500\n",
      "Epoch  80 Batch  203/269   train_loss = 3.595\n",
      "Epoch  80 Batch  204/269   train_loss = 3.859\n",
      "Epoch  80 Batch  205/269   train_loss = 3.728\n",
      "Epoch  80 Batch  206/269   train_loss = 3.697\n",
      "Epoch  80 Batch  207/269   train_loss = 3.669\n",
      "Epoch  80 Batch  208/269   train_loss = 3.804\n",
      "Epoch  80 Batch  209/269   train_loss = 3.855\n",
      "Epoch  80 Batch  210/269   train_loss = 3.629\n",
      "Epoch  80 Batch  211/269   train_loss = 3.660\n",
      "Epoch  80 Batch  212/269   train_loss = 4.044\n",
      "Epoch  80 Batch  213/269   train_loss = 3.616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  80 Batch  214/269   train_loss = 3.663\n",
      "Epoch  80 Batch  215/269   train_loss = 3.949\n",
      "Epoch  80 Batch  216/269   train_loss = 3.941\n",
      "Epoch  80 Batch  217/269   train_loss = 3.590\n",
      "Epoch  80 Batch  218/269   train_loss = 3.825\n",
      "Epoch  80 Batch  219/269   train_loss = 3.488\n",
      "Epoch  80 Batch  220/269   train_loss = 3.937\n",
      "Epoch  80 Batch  221/269   train_loss = 3.610\n",
      "Epoch  80 Batch  222/269   train_loss = 3.793\n",
      "Epoch  80 Batch  223/269   train_loss = 3.595\n",
      "Epoch  80 Batch  224/269   train_loss = 3.872\n",
      "Epoch  80 Batch  225/269   train_loss = 3.888\n",
      "Epoch  80 Batch  226/269   train_loss = 3.692\n",
      "Epoch  80 Batch  227/269   train_loss = 3.460\n",
      "Epoch  80 Batch  228/269   train_loss = 3.795\n",
      "Epoch  80 Batch  229/269   train_loss = 3.920\n",
      "Epoch  80 Batch  230/269   train_loss = 3.823\n",
      "Epoch  80 Batch  231/269   train_loss = 3.616\n",
      "Epoch  80 Batch  232/269   train_loss = 3.725\n",
      "Epoch  80 Batch  233/269   train_loss = 3.819\n",
      "Epoch  80 Batch  234/269   train_loss = 3.751\n",
      "Epoch  80 Batch  235/269   train_loss = 3.989\n",
      "Epoch  80 Batch  236/269   train_loss = 3.571\n",
      "Epoch  80 Batch  237/269   train_loss = 3.290\n",
      "Epoch  80 Batch  238/269   train_loss = 3.505\n",
      "Epoch  80 Batch  239/269   train_loss = 3.982\n",
      "Epoch  80 Batch  240/269   train_loss = 3.541\n",
      "Epoch  80 Batch  241/269   train_loss = 3.919\n",
      "Epoch  80 Batch  242/269   train_loss = 3.517\n",
      "Epoch  80 Batch  243/269   train_loss = 3.864\n",
      "Epoch  80 Batch  244/269   train_loss = 3.677\n",
      "Epoch  80 Batch  245/269   train_loss = 3.709\n",
      "Epoch  80 Batch  246/269   train_loss = 3.358\n",
      "Epoch  80 Batch  247/269   train_loss = 3.733\n",
      "Epoch  80 Batch  248/269   train_loss = 3.799\n",
      "Epoch  80 Batch  249/269   train_loss = 3.657\n",
      "Epoch  80 Batch  250/269   train_loss = 3.513\n",
      "Epoch  80 Batch  251/269   train_loss = 3.902\n",
      "Epoch  80 Batch  252/269   train_loss = 3.848\n",
      "Epoch  80 Batch  253/269   train_loss = 3.617\n",
      "Epoch  80 Batch  254/269   train_loss = 3.619\n",
      "Epoch  80 Batch  255/269   train_loss = 3.730\n",
      "Epoch  80 Batch  256/269   train_loss = 3.653\n",
      "Epoch  80 Batch  257/269   train_loss = 3.586\n",
      "Epoch  80 Batch  258/269   train_loss = 3.451\n",
      "Epoch  80 Batch  259/269   train_loss = 3.532\n",
      "Epoch  80 Batch  260/269   train_loss = 3.829\n",
      "Epoch  80 Batch  261/269   train_loss = 3.804\n",
      "Epoch  80 Batch  262/269   train_loss = 3.634\n",
      "Epoch  80 Batch  263/269   train_loss = 3.577\n",
      "Epoch  80 Batch  264/269   train_loss = 3.875\n",
      "Epoch  80 Batch  265/269   train_loss = 3.729\n",
      "Epoch  80 Batch  266/269   train_loss = 3.589\n",
      "Epoch  80 Batch  267/269   train_loss = 3.726\n",
      "Epoch  80 Batch  268/269   train_loss = 3.862\n",
      "Epoch  81 Batch    0/269   train_loss = 3.618\n",
      "Epoch  81 Batch    1/269   train_loss = 3.382\n",
      "Epoch  81 Batch    2/269   train_loss = 3.603\n",
      "Epoch  81 Batch    3/269   train_loss = 3.646\n",
      "Epoch  81 Batch    4/269   train_loss = 4.091\n",
      "Epoch  81 Batch    5/269   train_loss = 3.696\n",
      "Epoch  81 Batch    6/269   train_loss = 3.602\n",
      "Epoch  81 Batch    7/269   train_loss = 3.449\n",
      "Epoch  81 Batch    8/269   train_loss = 3.795\n",
      "Epoch  81 Batch    9/269   train_loss = 3.447\n",
      "Epoch  81 Batch   10/269   train_loss = 3.582\n",
      "Epoch  81 Batch   11/269   train_loss = 3.671\n",
      "Epoch  81 Batch   12/269   train_loss = 3.422\n",
      "Epoch  81 Batch   13/269   train_loss = 3.693\n",
      "Epoch  81 Batch   14/269   train_loss = 3.646\n",
      "Epoch  81 Batch   15/269   train_loss = 3.930\n",
      "Epoch  81 Batch   16/269   train_loss = 3.704\n",
      "Epoch  81 Batch   17/269   train_loss = 3.788\n",
      "Epoch  81 Batch   18/269   train_loss = 3.630\n",
      "Epoch  81 Batch   19/269   train_loss = 3.705\n",
      "Epoch  81 Batch   20/269   train_loss = 4.044\n",
      "Epoch  81 Batch   21/269   train_loss = 3.897\n",
      "Epoch  81 Batch   22/269   train_loss = 3.637\n",
      "Epoch  81 Batch   23/269   train_loss = 3.724\n",
      "Epoch  81 Batch   24/269   train_loss = 3.507\n",
      "Epoch  81 Batch   25/269   train_loss = 3.907\n",
      "Epoch  81 Batch   26/269   train_loss = 3.742\n",
      "Epoch  81 Batch   27/269   train_loss = 3.648\n",
      "Epoch  81 Batch   28/269   train_loss = 3.750\n",
      "Epoch  81 Batch   29/269   train_loss = 3.760\n",
      "Epoch  81 Batch   30/269   train_loss = 4.058\n",
      "Epoch  81 Batch   31/269   train_loss = 3.737\n",
      "Epoch  81 Batch   32/269   train_loss = 3.581\n",
      "Epoch  81 Batch   33/269   train_loss = 3.380\n",
      "Epoch  81 Batch   34/269   train_loss = 3.651\n",
      "Epoch  81 Batch   35/269   train_loss = 3.461\n",
      "Epoch  81 Batch   36/269   train_loss = 3.517\n",
      "Epoch  81 Batch   37/269   train_loss = 3.718\n",
      "Epoch  81 Batch   38/269   train_loss = 3.421\n",
      "Epoch  81 Batch   39/269   train_loss = 3.844\n",
      "Epoch  81 Batch   40/269   train_loss = 3.627\n",
      "Epoch  81 Batch   41/269   train_loss = 3.587\n",
      "Epoch  81 Batch   42/269   train_loss = 3.994\n",
      "Epoch  81 Batch   43/269   train_loss = 3.989\n",
      "Epoch  81 Batch   44/269   train_loss = 3.716\n",
      "Epoch  81 Batch   45/269   train_loss = 3.416\n",
      "Epoch  81 Batch   46/269   train_loss = 3.735\n",
      "Epoch  81 Batch   47/269   train_loss = 3.430\n",
      "Epoch  81 Batch   48/269   train_loss = 3.919\n",
      "Epoch  81 Batch   49/269   train_loss = 3.843\n",
      "Epoch  81 Batch   50/269   train_loss = 3.812\n",
      "Epoch  81 Batch   51/269   train_loss = 3.754\n",
      "Epoch  81 Batch   52/269   train_loss = 3.757\n",
      "Epoch  81 Batch   53/269   train_loss = 3.949\n",
      "Epoch  81 Batch   54/269   train_loss = 3.585\n",
      "Epoch  81 Batch   55/269   train_loss = 3.770\n",
      "Epoch  81 Batch   56/269   train_loss = 3.521\n",
      "Epoch  81 Batch   57/269   train_loss = 3.918\n",
      "Epoch  81 Batch   58/269   train_loss = 3.587\n",
      "Epoch  81 Batch   59/269   train_loss = 3.661\n",
      "Epoch  81 Batch   60/269   train_loss = 3.754\n",
      "Epoch  81 Batch   61/269   train_loss = 3.882\n",
      "Epoch  81 Batch   62/269   train_loss = 3.559\n",
      "Epoch  81 Batch   63/269   train_loss = 3.873\n",
      "Epoch  81 Batch   64/269   train_loss = 3.633\n",
      "Epoch  81 Batch   65/269   train_loss = 3.750\n",
      "Epoch  81 Batch   66/269   train_loss = 3.620\n",
      "Epoch  81 Batch   67/269   train_loss = 3.618\n",
      "Epoch  81 Batch   68/269   train_loss = 3.774\n",
      "Epoch  81 Batch   69/269   train_loss = 3.664\n",
      "Epoch  81 Batch   70/269   train_loss = 3.498\n",
      "Epoch  81 Batch   71/269   train_loss = 3.732\n",
      "Epoch  81 Batch   72/269   train_loss = 3.496\n",
      "Epoch  81 Batch   73/269   train_loss = 3.709\n",
      "Epoch  81 Batch   74/269   train_loss = 3.674\n",
      "Epoch  81 Batch   75/269   train_loss = 3.881\n",
      "Epoch  81 Batch   76/269   train_loss = 3.864\n",
      "Epoch  81 Batch   77/269   train_loss = 3.457\n",
      "Epoch  81 Batch   78/269   train_loss = 3.869\n",
      "Epoch  81 Batch   79/269   train_loss = 3.878\n",
      "Epoch  81 Batch   80/269   train_loss = 3.828\n",
      "Epoch  81 Batch   81/269   train_loss = 3.634\n",
      "Epoch  81 Batch   82/269   train_loss = 3.819\n",
      "Epoch  81 Batch   83/269   train_loss = 3.593\n",
      "Epoch  81 Batch   84/269   train_loss = 3.917\n",
      "Epoch  81 Batch   85/269   train_loss = 4.055\n",
      "Epoch  81 Batch   86/269   train_loss = 4.116\n",
      "Epoch  81 Batch   87/269   train_loss = 3.705\n",
      "Epoch  81 Batch   88/269   train_loss = 3.848\n",
      "Epoch  81 Batch   89/269   train_loss = 3.693\n",
      "Epoch  81 Batch   90/269   train_loss = 3.700\n",
      "Epoch  81 Batch   91/269   train_loss = 3.791\n",
      "Epoch  81 Batch   92/269   train_loss = 3.909\n",
      "Epoch  81 Batch   93/269   train_loss = 3.594\n",
      "Epoch  81 Batch   94/269   train_loss = 4.057\n",
      "Epoch  81 Batch   95/269   train_loss = 3.466\n",
      "Epoch  81 Batch   96/269   train_loss = 3.645\n",
      "Epoch  81 Batch   97/269   train_loss = 3.633\n",
      "Epoch  81 Batch   98/269   train_loss = 3.888\n",
      "Epoch  81 Batch   99/269   train_loss = 3.720\n",
      "Epoch  81 Batch  100/269   train_loss = 3.733\n",
      "Epoch  81 Batch  101/269   train_loss = 3.798\n",
      "Epoch  81 Batch  102/269   train_loss = 3.578\n",
      "Epoch  81 Batch  103/269   train_loss = 3.665\n",
      "Epoch  81 Batch  104/269   train_loss = 3.715\n",
      "Epoch  81 Batch  105/269   train_loss = 3.899\n",
      "Epoch  81 Batch  106/269   train_loss = 4.018\n",
      "Epoch  81 Batch  107/269   train_loss = 3.955\n",
      "Epoch  81 Batch  108/269   train_loss = 3.742\n",
      "Epoch  81 Batch  109/269   train_loss = 3.785\n",
      "Epoch  81 Batch  110/269   train_loss = 3.787\n",
      "Epoch  81 Batch  111/269   train_loss = 3.882\n",
      "Epoch  81 Batch  112/269   train_loss = 3.874\n",
      "Epoch  81 Batch  113/269   train_loss = 3.719\n",
      "Epoch  81 Batch  114/269   train_loss = 3.661\n",
      "Epoch  81 Batch  115/269   train_loss = 3.652\n",
      "Epoch  81 Batch  116/269   train_loss = 3.559\n",
      "Epoch  81 Batch  117/269   train_loss = 3.580\n",
      "Epoch  81 Batch  118/269   train_loss = 3.850\n",
      "Epoch  81 Batch  119/269   train_loss = 3.596\n",
      "Epoch  81 Batch  120/269   train_loss = 3.659\n",
      "Epoch  81 Batch  121/269   train_loss = 3.722\n",
      "Epoch  81 Batch  122/269   train_loss = 3.696\n",
      "Epoch  81 Batch  123/269   train_loss = 3.503\n",
      "Epoch  81 Batch  124/269   train_loss = 3.846\n",
      "Epoch  81 Batch  125/269   train_loss = 3.473\n",
      "Epoch  81 Batch  126/269   train_loss = 3.398\n",
      "Epoch  81 Batch  127/269   train_loss = 3.731\n",
      "Epoch  81 Batch  128/269   train_loss = 3.645\n",
      "Epoch  81 Batch  129/269   train_loss = 3.567\n",
      "Epoch  81 Batch  130/269   train_loss = 3.717\n",
      "Epoch  81 Batch  131/269   train_loss = 3.417\n",
      "Epoch  81 Batch  132/269   train_loss = 3.717\n",
      "Epoch  81 Batch  133/269   train_loss = 3.441\n",
      "Epoch  81 Batch  134/269   train_loss = 3.918\n",
      "Epoch  81 Batch  135/269   train_loss = 3.581\n",
      "Epoch  81 Batch  136/269   train_loss = 3.410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  81 Batch  137/269   train_loss = 3.788\n",
      "Epoch  81 Batch  138/269   train_loss = 3.560\n",
      "Epoch  81 Batch  139/269   train_loss = 3.622\n",
      "Epoch  81 Batch  140/269   train_loss = 3.753\n",
      "Epoch  81 Batch  141/269   train_loss = 3.990\n",
      "Epoch  81 Batch  142/269   train_loss = 3.533\n",
      "Epoch  81 Batch  143/269   train_loss = 3.661\n",
      "Epoch  81 Batch  144/269   train_loss = 3.882\n",
      "Epoch  81 Batch  145/269   train_loss = 3.748\n",
      "Epoch  81 Batch  146/269   train_loss = 3.884\n",
      "Epoch  81 Batch  147/269   train_loss = 3.527\n",
      "Epoch  81 Batch  148/269   train_loss = 3.852\n",
      "Epoch  81 Batch  149/269   train_loss = 3.603\n",
      "Epoch  81 Batch  150/269   train_loss = 3.791\n",
      "Epoch  81 Batch  151/269   train_loss = 3.778\n",
      "Epoch  81 Batch  152/269   train_loss = 3.702\n",
      "Epoch  81 Batch  153/269   train_loss = 3.763\n",
      "Epoch  81 Batch  154/269   train_loss = 3.645\n",
      "Epoch  81 Batch  155/269   train_loss = 3.631\n",
      "Epoch  81 Batch  156/269   train_loss = 4.149\n",
      "Epoch  81 Batch  157/269   train_loss = 3.814\n",
      "Epoch  81 Batch  158/269   train_loss = 3.750\n",
      "Epoch  81 Batch  159/269   train_loss = 3.798\n",
      "Epoch  81 Batch  160/269   train_loss = 3.643\n",
      "Epoch  81 Batch  161/269   train_loss = 3.787\n",
      "Epoch  81 Batch  162/269   train_loss = 3.842\n",
      "Epoch  81 Batch  163/269   train_loss = 3.608\n",
      "Epoch  81 Batch  164/269   train_loss = 3.887\n",
      "Epoch  81 Batch  165/269   train_loss = 3.917\n",
      "Epoch  81 Batch  166/269   train_loss = 3.794\n",
      "Epoch  81 Batch  167/269   train_loss = 3.529\n",
      "Epoch  81 Batch  168/269   train_loss = 3.726\n",
      "Epoch  81 Batch  169/269   train_loss = 3.971\n",
      "Epoch  81 Batch  170/269   train_loss = 3.547\n",
      "Epoch  81 Batch  171/269   train_loss = 3.711\n",
      "Epoch  81 Batch  172/269   train_loss = 3.561\n",
      "Epoch  81 Batch  173/269   train_loss = 3.692\n",
      "Epoch  81 Batch  174/269   train_loss = 3.904\n",
      "Epoch  81 Batch  175/269   train_loss = 3.803\n",
      "Epoch  81 Batch  176/269   train_loss = 3.695\n",
      "Epoch  81 Batch  177/269   train_loss = 3.859\n",
      "Epoch  81 Batch  178/269   train_loss = 4.004\n",
      "Epoch  81 Batch  179/269   train_loss = 3.735\n",
      "Epoch  81 Batch  180/269   train_loss = 3.613\n",
      "Epoch  81 Batch  181/269   train_loss = 3.847\n",
      "Epoch  81 Batch  182/269   train_loss = 3.686\n",
      "Epoch  81 Batch  183/269   train_loss = 3.651\n",
      "Epoch  81 Batch  184/269   train_loss = 3.714\n",
      "Epoch  81 Batch  185/269   train_loss = 3.762\n",
      "Epoch  81 Batch  186/269   train_loss = 3.708\n",
      "Epoch  81 Batch  187/269   train_loss = 3.782\n",
      "Epoch  81 Batch  188/269   train_loss = 3.622\n",
      "Epoch  81 Batch  189/269   train_loss = 3.674\n",
      "Epoch  81 Batch  190/269   train_loss = 4.040\n",
      "Epoch  81 Batch  191/269   train_loss = 3.812\n",
      "Epoch  81 Batch  192/269   train_loss = 3.728\n",
      "Epoch  81 Batch  193/269   train_loss = 3.691\n",
      "Epoch  81 Batch  194/269   train_loss = 3.823\n",
      "Epoch  81 Batch  195/269   train_loss = 3.718\n",
      "Epoch  81 Batch  196/269   train_loss = 3.727\n",
      "Epoch  81 Batch  197/269   train_loss = 3.935\n",
      "Epoch  81 Batch  198/269   train_loss = 3.732\n",
      "Epoch  81 Batch  199/269   train_loss = 3.870\n",
      "Epoch  81 Batch  200/269   train_loss = 3.631\n",
      "Epoch  81 Batch  201/269   train_loss = 3.697\n",
      "Epoch  81 Batch  202/269   train_loss = 3.476\n",
      "Epoch  81 Batch  203/269   train_loss = 3.605\n",
      "Epoch  81 Batch  204/269   train_loss = 3.850\n",
      "Epoch  81 Batch  205/269   train_loss = 3.734\n",
      "Epoch  81 Batch  206/269   train_loss = 3.721\n",
      "Epoch  81 Batch  207/269   train_loss = 3.666\n",
      "Epoch  81 Batch  208/269   train_loss = 3.782\n",
      "Epoch  81 Batch  209/269   train_loss = 3.843\n",
      "Epoch  81 Batch  210/269   train_loss = 3.653\n",
      "Epoch  81 Batch  211/269   train_loss = 3.667\n",
      "Epoch  81 Batch  212/269   train_loss = 4.013\n",
      "Epoch  81 Batch  213/269   train_loss = 3.602\n",
      "Epoch  81 Batch  214/269   train_loss = 3.664\n",
      "Epoch  81 Batch  215/269   train_loss = 3.950\n",
      "Epoch  81 Batch  216/269   train_loss = 3.939\n",
      "Epoch  81 Batch  217/269   train_loss = 3.604\n",
      "Epoch  81 Batch  218/269   train_loss = 3.853\n",
      "Epoch  81 Batch  219/269   train_loss = 3.509\n",
      "Epoch  81 Batch  220/269   train_loss = 3.914\n",
      "Epoch  81 Batch  221/269   train_loss = 3.618\n",
      "Epoch  81 Batch  222/269   train_loss = 3.794\n",
      "Epoch  81 Batch  223/269   train_loss = 3.598\n",
      "Epoch  81 Batch  224/269   train_loss = 3.891\n",
      "Epoch  81 Batch  225/269   train_loss = 3.936\n",
      "Epoch  81 Batch  226/269   train_loss = 3.681\n",
      "Epoch  81 Batch  227/269   train_loss = 3.456\n",
      "Epoch  81 Batch  228/269   train_loss = 3.778\n",
      "Epoch  81 Batch  229/269   train_loss = 3.900\n",
      "Epoch  81 Batch  230/269   train_loss = 3.825\n",
      "Epoch  81 Batch  231/269   train_loss = 3.632\n",
      "Epoch  81 Batch  232/269   train_loss = 3.739\n",
      "Epoch  81 Batch  233/269   train_loss = 3.813\n",
      "Epoch  81 Batch  234/269   train_loss = 3.758\n",
      "Epoch  81 Batch  235/269   train_loss = 3.981\n",
      "Epoch  81 Batch  236/269   train_loss = 3.570\n",
      "Epoch  81 Batch  237/269   train_loss = 3.283\n",
      "Epoch  81 Batch  238/269   train_loss = 3.513\n",
      "Epoch  81 Batch  239/269   train_loss = 3.995\n",
      "Epoch  81 Batch  240/269   train_loss = 3.561\n",
      "Epoch  81 Batch  241/269   train_loss = 3.920\n",
      "Epoch  81 Batch  242/269   train_loss = 3.524\n",
      "Epoch  81 Batch  243/269   train_loss = 3.868\n",
      "Epoch  81 Batch  244/269   train_loss = 3.689\n",
      "Epoch  81 Batch  245/269   train_loss = 3.695\n",
      "Epoch  81 Batch  246/269   train_loss = 3.343\n",
      "Epoch  81 Batch  247/269   train_loss = 3.747\n",
      "Epoch  81 Batch  248/269   train_loss = 3.791\n",
      "Epoch  81 Batch  249/269   train_loss = 3.661\n",
      "Epoch  81 Batch  250/269   train_loss = 3.515\n",
      "Epoch  81 Batch  251/269   train_loss = 3.892\n",
      "Epoch  81 Batch  252/269   train_loss = 3.855\n",
      "Epoch  81 Batch  253/269   train_loss = 3.598\n",
      "Epoch  81 Batch  254/269   train_loss = 3.610\n",
      "Epoch  81 Batch  255/269   train_loss = 3.747\n",
      "Epoch  81 Batch  256/269   train_loss = 3.634\n",
      "Epoch  81 Batch  257/269   train_loss = 3.593\n",
      "Epoch  81 Batch  258/269   train_loss = 3.496\n",
      "Epoch  81 Batch  259/269   train_loss = 3.528\n",
      "Epoch  81 Batch  260/269   train_loss = 3.841\n",
      "Epoch  81 Batch  261/269   train_loss = 3.814\n",
      "Epoch  81 Batch  262/269   train_loss = 3.631\n",
      "Epoch  81 Batch  263/269   train_loss = 3.571\n",
      "Epoch  81 Batch  264/269   train_loss = 3.893\n",
      "Epoch  81 Batch  265/269   train_loss = 3.736\n",
      "Epoch  81 Batch  266/269   train_loss = 3.590\n",
      "Epoch  81 Batch  267/269   train_loss = 3.723\n",
      "Epoch  81 Batch  268/269   train_loss = 3.891\n",
      "Epoch  82 Batch    0/269   train_loss = 3.636\n",
      "Epoch  82 Batch    1/269   train_loss = 3.393\n",
      "Epoch  82 Batch    2/269   train_loss = 3.590\n",
      "Epoch  82 Batch    3/269   train_loss = 3.657\n",
      "Epoch  82 Batch    4/269   train_loss = 4.080\n",
      "Epoch  82 Batch    5/269   train_loss = 3.690\n",
      "Epoch  82 Batch    6/269   train_loss = 3.608\n",
      "Epoch  82 Batch    7/269   train_loss = 3.449\n",
      "Epoch  82 Batch    8/269   train_loss = 3.797\n",
      "Epoch  82 Batch    9/269   train_loss = 3.434\n",
      "Epoch  82 Batch   10/269   train_loss = 3.568\n",
      "Epoch  82 Batch   11/269   train_loss = 3.659\n",
      "Epoch  82 Batch   12/269   train_loss = 3.381\n",
      "Epoch  82 Batch   13/269   train_loss = 3.689\n",
      "Epoch  82 Batch   14/269   train_loss = 3.658\n",
      "Epoch  82 Batch   15/269   train_loss = 3.943\n",
      "Epoch  82 Batch   16/269   train_loss = 3.690\n",
      "Epoch  82 Batch   17/269   train_loss = 3.783\n",
      "Epoch  82 Batch   18/269   train_loss = 3.653\n",
      "Epoch  82 Batch   19/269   train_loss = 3.704\n",
      "Epoch  82 Batch   20/269   train_loss = 4.052\n",
      "Epoch  82 Batch   21/269   train_loss = 3.886\n",
      "Epoch  82 Batch   22/269   train_loss = 3.645\n",
      "Epoch  82 Batch   23/269   train_loss = 3.739\n",
      "Epoch  82 Batch   24/269   train_loss = 3.494\n",
      "Epoch  82 Batch   25/269   train_loss = 3.895\n",
      "Epoch  82 Batch   26/269   train_loss = 3.735\n",
      "Epoch  82 Batch   27/269   train_loss = 3.614\n",
      "Epoch  82 Batch   28/269   train_loss = 3.805\n",
      "Epoch  82 Batch   29/269   train_loss = 3.767\n",
      "Epoch  82 Batch   30/269   train_loss = 4.088\n",
      "Epoch  82 Batch   31/269   train_loss = 3.738\n",
      "Epoch  82 Batch   32/269   train_loss = 3.555\n",
      "Epoch  82 Batch   33/269   train_loss = 3.393\n",
      "Epoch  82 Batch   34/269   train_loss = 3.660\n",
      "Epoch  82 Batch   35/269   train_loss = 3.456\n",
      "Epoch  82 Batch   36/269   train_loss = 3.510\n",
      "Epoch  82 Batch   37/269   train_loss = 3.709\n",
      "Epoch  82 Batch   38/269   train_loss = 3.436\n",
      "Epoch  82 Batch   39/269   train_loss = 3.826\n",
      "Epoch  82 Batch   40/269   train_loss = 3.617\n",
      "Epoch  82 Batch   41/269   train_loss = 3.580\n",
      "Epoch  82 Batch   42/269   train_loss = 3.984\n",
      "Epoch  82 Batch   43/269   train_loss = 3.969\n",
      "Epoch  82 Batch   44/269   train_loss = 3.716\n",
      "Epoch  82 Batch   45/269   train_loss = 3.452\n",
      "Epoch  82 Batch   46/269   train_loss = 3.733\n",
      "Epoch  82 Batch   47/269   train_loss = 3.407\n",
      "Epoch  82 Batch   48/269   train_loss = 3.887\n",
      "Epoch  82 Batch   49/269   train_loss = 3.838\n",
      "Epoch  82 Batch   50/269   train_loss = 3.814\n",
      "Epoch  82 Batch   51/269   train_loss = 3.745\n",
      "Epoch  82 Batch   52/269   train_loss = 3.737\n",
      "Epoch  82 Batch   53/269   train_loss = 3.960\n",
      "Epoch  82 Batch   54/269   train_loss = 3.588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  82 Batch   55/269   train_loss = 3.785\n",
      "Epoch  82 Batch   56/269   train_loss = 3.527\n",
      "Epoch  82 Batch   57/269   train_loss = 3.909\n",
      "Epoch  82 Batch   58/269   train_loss = 3.576\n",
      "Epoch  82 Batch   59/269   train_loss = 3.669\n",
      "Epoch  82 Batch   60/269   train_loss = 3.755\n",
      "Epoch  82 Batch   61/269   train_loss = 3.884\n",
      "Epoch  82 Batch   62/269   train_loss = 3.550\n",
      "Epoch  82 Batch   63/269   train_loss = 3.875\n",
      "Epoch  82 Batch   64/269   train_loss = 3.612\n",
      "Epoch  82 Batch   65/269   train_loss = 3.754\n",
      "Epoch  82 Batch   66/269   train_loss = 3.627\n",
      "Epoch  82 Batch   67/269   train_loss = 3.622\n",
      "Epoch  82 Batch   68/269   train_loss = 3.738\n",
      "Epoch  82 Batch   69/269   train_loss = 3.670\n",
      "Epoch  82 Batch   70/269   train_loss = 3.482\n",
      "Epoch  82 Batch   71/269   train_loss = 3.785\n",
      "Epoch  82 Batch   72/269   train_loss = 3.481\n",
      "Epoch  82 Batch   73/269   train_loss = 3.697\n",
      "Epoch  82 Batch   74/269   train_loss = 3.667\n",
      "Epoch  82 Batch   75/269   train_loss = 3.882\n",
      "Epoch  82 Batch   76/269   train_loss = 3.833\n",
      "Epoch  82 Batch   77/269   train_loss = 3.429\n",
      "Epoch  82 Batch   78/269   train_loss = 3.884\n",
      "Epoch  82 Batch   79/269   train_loss = 3.861\n",
      "Epoch  82 Batch   80/269   train_loss = 3.817\n",
      "Epoch  82 Batch   81/269   train_loss = 3.621\n",
      "Epoch  82 Batch   82/269   train_loss = 3.824\n",
      "Epoch  82 Batch   83/269   train_loss = 3.590\n",
      "Epoch  82 Batch   84/269   train_loss = 3.954\n",
      "Epoch  82 Batch   85/269   train_loss = 4.036\n",
      "Epoch  82 Batch   86/269   train_loss = 4.072\n",
      "Epoch  82 Batch   87/269   train_loss = 3.727\n",
      "Epoch  82 Batch   88/269   train_loss = 3.842\n",
      "Epoch  82 Batch   89/269   train_loss = 3.703\n",
      "Epoch  82 Batch   90/269   train_loss = 3.654\n",
      "Epoch  82 Batch   91/269   train_loss = 3.775\n",
      "Epoch  82 Batch   92/269   train_loss = 3.870\n",
      "Epoch  82 Batch   93/269   train_loss = 3.602\n",
      "Epoch  82 Batch   94/269   train_loss = 4.085\n",
      "Epoch  82 Batch   95/269   train_loss = 3.487\n",
      "Epoch  82 Batch   96/269   train_loss = 3.630\n",
      "Epoch  82 Batch   97/269   train_loss = 3.661\n",
      "Epoch  82 Batch   98/269   train_loss = 3.890\n",
      "Epoch  82 Batch   99/269   train_loss = 3.704\n",
      "Epoch  82 Batch  100/269   train_loss = 3.718\n",
      "Epoch  82 Batch  101/269   train_loss = 3.794\n",
      "Epoch  82 Batch  102/269   train_loss = 3.589\n",
      "Epoch  82 Batch  103/269   train_loss = 3.660\n",
      "Epoch  82 Batch  104/269   train_loss = 3.686\n",
      "Epoch  82 Batch  105/269   train_loss = 3.863\n",
      "Epoch  82 Batch  106/269   train_loss = 3.994\n",
      "Epoch  82 Batch  107/269   train_loss = 3.938\n",
      "Epoch  82 Batch  108/269   train_loss = 3.749\n",
      "Epoch  82 Batch  109/269   train_loss = 3.790\n",
      "Epoch  82 Batch  110/269   train_loss = 3.780\n",
      "Epoch  82 Batch  111/269   train_loss = 3.910\n",
      "Epoch  82 Batch  112/269   train_loss = 3.873\n",
      "Epoch  82 Batch  113/269   train_loss = 3.728\n",
      "Epoch  82 Batch  114/269   train_loss = 3.671\n",
      "Epoch  82 Batch  115/269   train_loss = 3.651\n",
      "Epoch  82 Batch  116/269   train_loss = 3.561\n",
      "Epoch  82 Batch  117/269   train_loss = 3.594\n",
      "Epoch  82 Batch  118/269   train_loss = 3.833\n",
      "Epoch  82 Batch  119/269   train_loss = 3.575\n",
      "Epoch  82 Batch  120/269   train_loss = 3.668\n",
      "Epoch  82 Batch  121/269   train_loss = 3.739\n",
      "Epoch  82 Batch  122/269   train_loss = 3.708\n",
      "Epoch  82 Batch  123/269   train_loss = 3.524\n",
      "Epoch  82 Batch  124/269   train_loss = 3.827\n",
      "Epoch  82 Batch  125/269   train_loss = 3.474\n",
      "Epoch  82 Batch  126/269   train_loss = 3.382\n",
      "Epoch  82 Batch  127/269   train_loss = 3.763\n",
      "Epoch  82 Batch  128/269   train_loss = 3.631\n",
      "Epoch  82 Batch  129/269   train_loss = 3.571\n",
      "Epoch  82 Batch  130/269   train_loss = 3.714\n",
      "Epoch  82 Batch  131/269   train_loss = 3.394\n",
      "Epoch  82 Batch  132/269   train_loss = 3.714\n",
      "Epoch  82 Batch  133/269   train_loss = 3.426\n",
      "Epoch  82 Batch  134/269   train_loss = 3.911\n",
      "Epoch  82 Batch  135/269   train_loss = 3.556\n",
      "Epoch  82 Batch  136/269   train_loss = 3.430\n",
      "Epoch  82 Batch  137/269   train_loss = 3.834\n",
      "Epoch  82 Batch  138/269   train_loss = 3.555\n",
      "Epoch  82 Batch  139/269   train_loss = 3.625\n",
      "Epoch  82 Batch  140/269   train_loss = 3.756\n",
      "Epoch  82 Batch  141/269   train_loss = 3.999\n",
      "Epoch  82 Batch  142/269   train_loss = 3.532\n",
      "Epoch  82 Batch  143/269   train_loss = 3.653\n",
      "Epoch  82 Batch  144/269   train_loss = 3.891\n",
      "Epoch  82 Batch  145/269   train_loss = 3.720\n",
      "Epoch  82 Batch  146/269   train_loss = 3.858\n",
      "Epoch  82 Batch  147/269   train_loss = 3.499\n",
      "Epoch  82 Batch  148/269   train_loss = 3.812\n",
      "Epoch  82 Batch  149/269   train_loss = 3.582\n",
      "Epoch  82 Batch  150/269   train_loss = 3.788\n",
      "Epoch  82 Batch  151/269   train_loss = 3.756\n",
      "Epoch  82 Batch  152/269   train_loss = 3.737\n",
      "Epoch  82 Batch  153/269   train_loss = 3.778\n",
      "Epoch  82 Batch  154/269   train_loss = 3.672\n",
      "Epoch  82 Batch  155/269   train_loss = 3.648\n",
      "Epoch  82 Batch  156/269   train_loss = 4.101\n",
      "Epoch  82 Batch  157/269   train_loss = 3.770\n",
      "Epoch  82 Batch  158/269   train_loss = 3.755\n",
      "Epoch  82 Batch  159/269   train_loss = 3.791\n",
      "Epoch  82 Batch  160/269   train_loss = 3.635\n",
      "Epoch  82 Batch  161/269   train_loss = 3.804\n",
      "Epoch  82 Batch  162/269   train_loss = 3.857\n",
      "Epoch  82 Batch  163/269   train_loss = 3.625\n",
      "Epoch  82 Batch  164/269   train_loss = 3.908\n",
      "Epoch  82 Batch  165/269   train_loss = 3.995\n",
      "Epoch  82 Batch  166/269   train_loss = 3.813\n",
      "Epoch  82 Batch  167/269   train_loss = 3.532\n",
      "Epoch  82 Batch  168/269   train_loss = 3.731\n",
      "Epoch  82 Batch  169/269   train_loss = 3.962\n",
      "Epoch  82 Batch  170/269   train_loss = 3.551\n",
      "Epoch  82 Batch  171/269   train_loss = 3.714\n",
      "Epoch  82 Batch  172/269   train_loss = 3.560\n",
      "Epoch  82 Batch  173/269   train_loss = 3.684\n",
      "Epoch  82 Batch  174/269   train_loss = 3.898\n",
      "Epoch  82 Batch  175/269   train_loss = 3.785\n",
      "Epoch  82 Batch  176/269   train_loss = 3.699\n",
      "Epoch  82 Batch  177/269   train_loss = 3.826\n",
      "Epoch  82 Batch  178/269   train_loss = 3.977\n",
      "Epoch  82 Batch  179/269   train_loss = 3.739\n",
      "Epoch  82 Batch  180/269   train_loss = 3.594\n",
      "Epoch  82 Batch  181/269   train_loss = 3.859\n",
      "Epoch  82 Batch  182/269   train_loss = 3.669\n",
      "Epoch  82 Batch  183/269   train_loss = 3.664\n",
      "Epoch  82 Batch  184/269   train_loss = 3.718\n",
      "Epoch  82 Batch  185/269   train_loss = 3.754\n",
      "Epoch  82 Batch  186/269   train_loss = 3.713\n",
      "Epoch  82 Batch  187/269   train_loss = 3.780\n",
      "Epoch  82 Batch  188/269   train_loss = 3.615\n",
      "Epoch  82 Batch  189/269   train_loss = 3.668\n",
      "Epoch  82 Batch  190/269   train_loss = 4.050\n",
      "Epoch  82 Batch  191/269   train_loss = 3.787\n",
      "Epoch  82 Batch  192/269   train_loss = 3.758\n",
      "Epoch  82 Batch  193/269   train_loss = 3.719\n",
      "Epoch  82 Batch  194/269   train_loss = 3.804\n",
      "Epoch  82 Batch  195/269   train_loss = 3.737\n",
      "Epoch  82 Batch  196/269   train_loss = 3.713\n",
      "Epoch  82 Batch  197/269   train_loss = 3.936\n",
      "Epoch  82 Batch  198/269   train_loss = 3.742\n",
      "Epoch  82 Batch  199/269   train_loss = 3.862\n",
      "Epoch  82 Batch  200/269   train_loss = 3.584\n",
      "Epoch  82 Batch  201/269   train_loss = 3.686\n",
      "Epoch  82 Batch  202/269   train_loss = 3.485\n",
      "Epoch  82 Batch  203/269   train_loss = 3.594\n",
      "Epoch  82 Batch  204/269   train_loss = 3.846\n",
      "Epoch  82 Batch  205/269   train_loss = 3.744\n",
      "Epoch  82 Batch  206/269   train_loss = 3.684\n",
      "Epoch  82 Batch  207/269   train_loss = 3.668\n",
      "Epoch  82 Batch  208/269   train_loss = 3.794\n",
      "Epoch  82 Batch  209/269   train_loss = 3.898\n",
      "Epoch  82 Batch  210/269   train_loss = 3.655\n",
      "Epoch  82 Batch  211/269   train_loss = 3.654\n",
      "Epoch  82 Batch  212/269   train_loss = 4.028\n",
      "Epoch  82 Batch  213/269   train_loss = 3.592\n",
      "Epoch  82 Batch  214/269   train_loss = 3.669\n",
      "Epoch  82 Batch  215/269   train_loss = 3.948\n",
      "Epoch  82 Batch  216/269   train_loss = 3.928\n",
      "Epoch  82 Batch  217/269   train_loss = 3.594\n",
      "Epoch  82 Batch  218/269   train_loss = 3.816\n",
      "Epoch  82 Batch  219/269   train_loss = 3.488\n",
      "Epoch  82 Batch  220/269   train_loss = 3.904\n",
      "Epoch  82 Batch  221/269   train_loss = 3.590\n",
      "Epoch  82 Batch  222/269   train_loss = 3.786\n",
      "Epoch  82 Batch  223/269   train_loss = 3.562\n",
      "Epoch  82 Batch  224/269   train_loss = 3.866\n",
      "Epoch  82 Batch  225/269   train_loss = 3.920\n",
      "Epoch  82 Batch  226/269   train_loss = 3.687\n",
      "Epoch  82 Batch  227/269   train_loss = 3.448\n",
      "Epoch  82 Batch  228/269   train_loss = 3.750\n",
      "Epoch  82 Batch  229/269   train_loss = 3.885\n",
      "Epoch  82 Batch  230/269   train_loss = 3.784\n",
      "Epoch  82 Batch  231/269   train_loss = 3.607\n",
      "Epoch  82 Batch  232/269   train_loss = 3.762\n",
      "Epoch  82 Batch  233/269   train_loss = 3.809\n",
      "Epoch  82 Batch  234/269   train_loss = 3.753\n",
      "Epoch  82 Batch  235/269   train_loss = 3.965\n",
      "Epoch  82 Batch  236/269   train_loss = 3.598\n",
      "Epoch  82 Batch  237/269   train_loss = 3.275\n",
      "Epoch  82 Batch  238/269   train_loss = 3.476\n",
      "Epoch  82 Batch  239/269   train_loss = 3.974\n",
      "Epoch  82 Batch  240/269   train_loss = 3.548\n",
      "Epoch  82 Batch  241/269   train_loss = 3.924\n",
      "Epoch  82 Batch  242/269   train_loss = 3.531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  82 Batch  243/269   train_loss = 3.855\n",
      "Epoch  82 Batch  244/269   train_loss = 3.674\n",
      "Epoch  82 Batch  245/269   train_loss = 3.703\n",
      "Epoch  82 Batch  246/269   train_loss = 3.353\n",
      "Epoch  82 Batch  247/269   train_loss = 3.714\n",
      "Epoch  82 Batch  248/269   train_loss = 3.781\n",
      "Epoch  82 Batch  249/269   train_loss = 3.674\n",
      "Epoch  82 Batch  250/269   train_loss = 3.518\n",
      "Epoch  82 Batch  251/269   train_loss = 3.893\n",
      "Epoch  82 Batch  252/269   train_loss = 3.854\n",
      "Epoch  82 Batch  253/269   train_loss = 3.605\n",
      "Epoch  82 Batch  254/269   train_loss = 3.644\n",
      "Epoch  82 Batch  255/269   train_loss = 3.742\n",
      "Epoch  82 Batch  256/269   train_loss = 3.651\n",
      "Epoch  82 Batch  257/269   train_loss = 3.591\n",
      "Epoch  82 Batch  258/269   train_loss = 3.499\n",
      "Epoch  82 Batch  259/269   train_loss = 3.532\n",
      "Epoch  82 Batch  260/269   train_loss = 3.826\n",
      "Epoch  82 Batch  261/269   train_loss = 3.817\n",
      "Epoch  82 Batch  262/269   train_loss = 3.652\n",
      "Epoch  82 Batch  263/269   train_loss = 3.555\n",
      "Epoch  82 Batch  264/269   train_loss = 3.884\n",
      "Epoch  82 Batch  265/269   train_loss = 3.739\n",
      "Epoch  82 Batch  266/269   train_loss = 3.584\n",
      "Epoch  82 Batch  267/269   train_loss = 3.740\n",
      "Epoch  82 Batch  268/269   train_loss = 3.870\n",
      "Epoch  83 Batch    0/269   train_loss = 3.625\n",
      "Epoch  83 Batch    1/269   train_loss = 3.409\n",
      "Epoch  83 Batch    2/269   train_loss = 3.589\n",
      "Epoch  83 Batch    3/269   train_loss = 3.630\n",
      "Epoch  83 Batch    4/269   train_loss = 4.051\n",
      "Epoch  83 Batch    5/269   train_loss = 3.720\n",
      "Epoch  83 Batch    6/269   train_loss = 3.628\n",
      "Epoch  83 Batch    7/269   train_loss = 3.464\n",
      "Epoch  83 Batch    8/269   train_loss = 3.767\n",
      "Epoch  83 Batch    9/269   train_loss = 3.418\n",
      "Epoch  83 Batch   10/269   train_loss = 3.567\n",
      "Epoch  83 Batch   11/269   train_loss = 3.654\n",
      "Epoch  83 Batch   12/269   train_loss = 3.403\n",
      "Epoch  83 Batch   13/269   train_loss = 3.742\n",
      "Epoch  83 Batch   14/269   train_loss = 3.663\n",
      "Epoch  83 Batch   15/269   train_loss = 3.943\n",
      "Epoch  83 Batch   16/269   train_loss = 3.672\n",
      "Epoch  83 Batch   17/269   train_loss = 3.751\n",
      "Epoch  83 Batch   18/269   train_loss = 3.647\n",
      "Epoch  83 Batch   19/269   train_loss = 3.727\n",
      "Epoch  83 Batch   20/269   train_loss = 4.052\n",
      "Epoch  83 Batch   21/269   train_loss = 3.892\n",
      "Epoch  83 Batch   22/269   train_loss = 3.643\n",
      "Epoch  83 Batch   23/269   train_loss = 3.742\n",
      "Epoch  83 Batch   24/269   train_loss = 3.483\n",
      "Epoch  83 Batch   25/269   train_loss = 3.894\n",
      "Epoch  83 Batch   26/269   train_loss = 3.735\n",
      "Epoch  83 Batch   27/269   train_loss = 3.633\n",
      "Epoch  83 Batch   28/269   train_loss = 3.755\n",
      "Epoch  83 Batch   29/269   train_loss = 3.773\n",
      "Epoch  83 Batch   30/269   train_loss = 4.067\n",
      "Epoch  83 Batch   31/269   train_loss = 3.731\n",
      "Epoch  83 Batch   32/269   train_loss = 3.598\n",
      "Epoch  83 Batch   33/269   train_loss = 3.379\n",
      "Epoch  83 Batch   34/269   train_loss = 3.639\n",
      "Epoch  83 Batch   35/269   train_loss = 3.456\n",
      "Epoch  83 Batch   36/269   train_loss = 3.516\n",
      "Epoch  83 Batch   37/269   train_loss = 3.721\n",
      "Epoch  83 Batch   38/269   train_loss = 3.463\n",
      "Epoch  83 Batch   39/269   train_loss = 3.820\n",
      "Epoch  83 Batch   40/269   train_loss = 3.623\n",
      "Epoch  83 Batch   41/269   train_loss = 3.635\n",
      "Epoch  83 Batch   42/269   train_loss = 4.007\n",
      "Epoch  83 Batch   43/269   train_loss = 3.934\n",
      "Epoch  83 Batch   44/269   train_loss = 3.735\n",
      "Epoch  83 Batch   45/269   train_loss = 3.424\n",
      "Epoch  83 Batch   46/269   train_loss = 3.727\n",
      "Epoch  83 Batch   47/269   train_loss = 3.387\n",
      "Epoch  83 Batch   48/269   train_loss = 3.959\n",
      "Epoch  83 Batch   49/269   train_loss = 3.873\n",
      "Epoch  83 Batch   50/269   train_loss = 3.831\n",
      "Epoch  83 Batch   51/269   train_loss = 3.767\n",
      "Epoch  83 Batch   52/269   train_loss = 3.747\n",
      "Epoch  83 Batch   53/269   train_loss = 3.968\n",
      "Epoch  83 Batch   54/269   train_loss = 3.588\n",
      "Epoch  83 Batch   55/269   train_loss = 3.752\n",
      "Epoch  83 Batch   56/269   train_loss = 3.511\n",
      "Epoch  83 Batch   57/269   train_loss = 3.894\n",
      "Epoch  83 Batch   58/269   train_loss = 3.552\n",
      "Epoch  83 Batch   59/269   train_loss = 3.652\n",
      "Epoch  83 Batch   60/269   train_loss = 3.753\n",
      "Epoch  83 Batch   61/269   train_loss = 3.905\n",
      "Epoch  83 Batch   62/269   train_loss = 3.555\n",
      "Epoch  83 Batch   63/269   train_loss = 3.852\n",
      "Epoch  83 Batch   64/269   train_loss = 3.658\n",
      "Epoch  83 Batch   65/269   train_loss = 3.751\n",
      "Epoch  83 Batch   66/269   train_loss = 3.608\n",
      "Epoch  83 Batch   67/269   train_loss = 3.611\n",
      "Epoch  83 Batch   68/269   train_loss = 3.738\n",
      "Epoch  83 Batch   69/269   train_loss = 3.666\n",
      "Epoch  83 Batch   70/269   train_loss = 3.483\n",
      "Epoch  83 Batch   71/269   train_loss = 3.753\n",
      "Epoch  83 Batch   72/269   train_loss = 3.464\n",
      "Epoch  83 Batch   73/269   train_loss = 3.683\n",
      "Epoch  83 Batch   74/269   train_loss = 3.690\n",
      "Epoch  83 Batch   75/269   train_loss = 3.885\n",
      "Epoch  83 Batch   76/269   train_loss = 3.847\n",
      "Epoch  83 Batch   77/269   train_loss = 3.446\n",
      "Epoch  83 Batch   78/269   train_loss = 3.829\n",
      "Epoch  83 Batch   79/269   train_loss = 3.852\n",
      "Epoch  83 Batch   80/269   train_loss = 3.801\n",
      "Epoch  83 Batch   81/269   train_loss = 3.623\n",
      "Epoch  83 Batch   82/269   train_loss = 3.791\n",
      "Epoch  83 Batch   83/269   train_loss = 3.627\n",
      "Epoch  83 Batch   84/269   train_loss = 3.934\n",
      "Epoch  83 Batch   85/269   train_loss = 4.022\n",
      "Epoch  83 Batch   86/269   train_loss = 4.109\n",
      "Epoch  83 Batch   87/269   train_loss = 3.710\n",
      "Epoch  83 Batch   88/269   train_loss = 3.857\n",
      "Epoch  83 Batch   89/269   train_loss = 3.739\n",
      "Epoch  83 Batch   90/269   train_loss = 3.672\n",
      "Epoch  83 Batch   91/269   train_loss = 3.772\n",
      "Epoch  83 Batch   92/269   train_loss = 3.865\n",
      "Epoch  83 Batch   93/269   train_loss = 3.583\n",
      "Epoch  83 Batch   94/269   train_loss = 4.113\n",
      "Epoch  83 Batch   95/269   train_loss = 3.471\n",
      "Epoch  83 Batch   96/269   train_loss = 3.608\n",
      "Epoch  83 Batch   97/269   train_loss = 3.664\n",
      "Epoch  83 Batch   98/269   train_loss = 3.901\n",
      "Epoch  83 Batch   99/269   train_loss = 3.723\n",
      "Epoch  83 Batch  100/269   train_loss = 3.725\n",
      "Epoch  83 Batch  101/269   train_loss = 3.787\n",
      "Epoch  83 Batch  102/269   train_loss = 3.585\n",
      "Epoch  83 Batch  103/269   train_loss = 3.656\n",
      "Epoch  83 Batch  104/269   train_loss = 3.739\n",
      "Epoch  83 Batch  105/269   train_loss = 3.866\n",
      "Epoch  83 Batch  106/269   train_loss = 4.015\n",
      "Epoch  83 Batch  107/269   train_loss = 3.945\n",
      "Epoch  83 Batch  108/269   train_loss = 3.766\n",
      "Epoch  83 Batch  109/269   train_loss = 3.795\n",
      "Epoch  83 Batch  110/269   train_loss = 3.776\n",
      "Epoch  83 Batch  111/269   train_loss = 3.910\n",
      "Epoch  83 Batch  112/269   train_loss = 3.866\n",
      "Epoch  83 Batch  113/269   train_loss = 3.719\n",
      "Epoch  83 Batch  114/269   train_loss = 3.672\n",
      "Epoch  83 Batch  115/269   train_loss = 3.689\n",
      "Epoch  83 Batch  116/269   train_loss = 3.552\n",
      "Epoch  83 Batch  117/269   train_loss = 3.627\n",
      "Epoch  83 Batch  118/269   train_loss = 3.844\n",
      "Epoch  83 Batch  119/269   train_loss = 3.592\n",
      "Epoch  83 Batch  120/269   train_loss = 3.646\n",
      "Epoch  83 Batch  121/269   train_loss = 3.705\n",
      "Epoch  83 Batch  122/269   train_loss = 3.684\n",
      "Epoch  83 Batch  123/269   train_loss = 3.530\n",
      "Epoch  83 Batch  124/269   train_loss = 3.851\n",
      "Epoch  83 Batch  125/269   train_loss = 3.488\n",
      "Epoch  83 Batch  126/269   train_loss = 3.400\n",
      "Epoch  83 Batch  127/269   train_loss = 3.719\n",
      "Epoch  83 Batch  128/269   train_loss = 3.628\n",
      "Epoch  83 Batch  129/269   train_loss = 3.564\n",
      "Epoch  83 Batch  130/269   train_loss = 3.714\n",
      "Epoch  83 Batch  131/269   train_loss = 3.392\n",
      "Epoch  83 Batch  132/269   train_loss = 3.710\n",
      "Epoch  83 Batch  133/269   train_loss = 3.455\n",
      "Epoch  83 Batch  134/269   train_loss = 3.905\n",
      "Epoch  83 Batch  135/269   train_loss = 3.565\n",
      "Epoch  83 Batch  136/269   train_loss = 3.447\n",
      "Epoch  83 Batch  137/269   train_loss = 3.812\n",
      "Epoch  83 Batch  138/269   train_loss = 3.552\n",
      "Epoch  83 Batch  139/269   train_loss = 3.625\n",
      "Epoch  83 Batch  140/269   train_loss = 3.734\n",
      "Epoch  83 Batch  141/269   train_loss = 3.984\n",
      "Epoch  83 Batch  142/269   train_loss = 3.533\n",
      "Epoch  83 Batch  143/269   train_loss = 3.652\n",
      "Epoch  83 Batch  144/269   train_loss = 3.905\n",
      "Epoch  83 Batch  145/269   train_loss = 3.730\n",
      "Epoch  83 Batch  146/269   train_loss = 3.859\n",
      "Epoch  83 Batch  147/269   train_loss = 3.500\n",
      "Epoch  83 Batch  148/269   train_loss = 3.853\n",
      "Epoch  83 Batch  149/269   train_loss = 3.590\n",
      "Epoch  83 Batch  150/269   train_loss = 3.810\n",
      "Epoch  83 Batch  151/269   train_loss = 3.775\n",
      "Epoch  83 Batch  152/269   train_loss = 3.732\n",
      "Epoch  83 Batch  153/269   train_loss = 3.798\n",
      "Epoch  83 Batch  154/269   train_loss = 3.639\n",
      "Epoch  83 Batch  155/269   train_loss = 3.612\n",
      "Epoch  83 Batch  156/269   train_loss = 4.080\n",
      "Epoch  83 Batch  157/269   train_loss = 3.800\n",
      "Epoch  83 Batch  158/269   train_loss = 3.788\n",
      "Epoch  83 Batch  159/269   train_loss = 3.810\n",
      "Epoch  83 Batch  160/269   train_loss = 3.624\n",
      "Epoch  83 Batch  161/269   train_loss = 3.821\n",
      "Epoch  83 Batch  162/269   train_loss = 3.896\n",
      "Epoch  83 Batch  163/269   train_loss = 3.589\n",
      "Epoch  83 Batch  164/269   train_loss = 3.872\n",
      "Epoch  83 Batch  165/269   train_loss = 3.915\n",
      "Epoch  83 Batch  166/269   train_loss = 3.846\n",
      "Epoch  83 Batch  167/269   train_loss = 3.571\n",
      "Epoch  83 Batch  168/269   train_loss = 3.737\n",
      "Epoch  83 Batch  169/269   train_loss = 3.981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  83 Batch  170/269   train_loss = 3.545\n",
      "Epoch  83 Batch  171/269   train_loss = 3.718\n",
      "Epoch  83 Batch  172/269   train_loss = 3.539\n",
      "Epoch  83 Batch  173/269   train_loss = 3.730\n",
      "Epoch  83 Batch  174/269   train_loss = 3.890\n",
      "Epoch  83 Batch  175/269   train_loss = 3.758\n",
      "Epoch  83 Batch  176/269   train_loss = 3.705\n",
      "Epoch  83 Batch  177/269   train_loss = 3.813\n",
      "Epoch  83 Batch  178/269   train_loss = 3.992\n",
      "Epoch  83 Batch  179/269   train_loss = 3.742\n",
      "Epoch  83 Batch  180/269   train_loss = 3.606\n",
      "Epoch  83 Batch  181/269   train_loss = 3.852\n",
      "Epoch  83 Batch  182/269   train_loss = 3.701\n",
      "Epoch  83 Batch  183/269   train_loss = 3.651\n",
      "Epoch  83 Batch  184/269   train_loss = 3.700\n",
      "Epoch  83 Batch  185/269   train_loss = 3.754\n",
      "Epoch  83 Batch  186/269   train_loss = 3.755\n",
      "Epoch  83 Batch  187/269   train_loss = 3.794\n",
      "Epoch  83 Batch  188/269   train_loss = 3.592\n",
      "Epoch  83 Batch  189/269   train_loss = 3.678\n",
      "Epoch  83 Batch  190/269   train_loss = 4.051\n",
      "Epoch  83 Batch  191/269   train_loss = 3.793\n",
      "Epoch  83 Batch  192/269   train_loss = 3.760\n",
      "Epoch  83 Batch  193/269   train_loss = 3.695\n",
      "Epoch  83 Batch  194/269   train_loss = 3.803\n",
      "Epoch  83 Batch  195/269   train_loss = 3.748\n",
      "Epoch  83 Batch  196/269   train_loss = 3.751\n",
      "Epoch  83 Batch  197/269   train_loss = 3.952\n",
      "Epoch  83 Batch  198/269   train_loss = 3.783\n",
      "Epoch  83 Batch  199/269   train_loss = 3.890\n",
      "Epoch  83 Batch  200/269   train_loss = 3.603\n",
      "Epoch  83 Batch  201/269   train_loss = 3.717\n",
      "Epoch  83 Batch  202/269   train_loss = 3.493\n",
      "Epoch  83 Batch  203/269   train_loss = 3.630\n",
      "Epoch  83 Batch  204/269   train_loss = 3.834\n",
      "Epoch  83 Batch  205/269   train_loss = 3.726\n",
      "Epoch  83 Batch  206/269   train_loss = 3.687\n",
      "Epoch  83 Batch  207/269   train_loss = 3.631\n",
      "Epoch  83 Batch  208/269   train_loss = 3.833\n",
      "Epoch  83 Batch  209/269   train_loss = 3.864\n",
      "Epoch  83 Batch  210/269   train_loss = 3.662\n",
      "Epoch  83 Batch  211/269   train_loss = 3.644\n",
      "Epoch  83 Batch  212/269   train_loss = 4.080\n",
      "Epoch  83 Batch  213/269   train_loss = 3.654\n",
      "Epoch  83 Batch  214/269   train_loss = 3.652\n",
      "Epoch  83 Batch  215/269   train_loss = 3.987\n",
      "Epoch  83 Batch  216/269   train_loss = 3.897\n",
      "Epoch  83 Batch  217/269   train_loss = 3.619\n",
      "Epoch  83 Batch  218/269   train_loss = 3.810\n",
      "Epoch  83 Batch  219/269   train_loss = 3.473\n",
      "Epoch  83 Batch  220/269   train_loss = 3.912\n",
      "Epoch  83 Batch  221/269   train_loss = 3.576\n",
      "Epoch  83 Batch  222/269   train_loss = 3.783\n",
      "Epoch  83 Batch  223/269   train_loss = 3.568\n",
      "Epoch  83 Batch  224/269   train_loss = 3.897\n",
      "Epoch  83 Batch  225/269   train_loss = 3.916\n",
      "Epoch  83 Batch  226/269   train_loss = 3.702\n",
      "Epoch  83 Batch  227/269   train_loss = 3.477\n",
      "Epoch  83 Batch  228/269   train_loss = 3.741\n",
      "Epoch  83 Batch  229/269   train_loss = 3.923\n",
      "Epoch  83 Batch  230/269   train_loss = 3.812\n",
      "Epoch  83 Batch  231/269   train_loss = 3.604\n",
      "Epoch  83 Batch  232/269   train_loss = 3.737\n",
      "Epoch  83 Batch  233/269   train_loss = 3.826\n",
      "Epoch  83 Batch  234/269   train_loss = 3.774\n",
      "Epoch  83 Batch  235/269   train_loss = 3.978\n",
      "Epoch  83 Batch  236/269   train_loss = 3.583\n",
      "Epoch  83 Batch  237/269   train_loss = 3.304\n",
      "Epoch  83 Batch  238/269   train_loss = 3.505\n",
      "Epoch  83 Batch  239/269   train_loss = 3.975\n",
      "Epoch  83 Batch  240/269   train_loss = 3.546\n",
      "Epoch  83 Batch  241/269   train_loss = 3.930\n",
      "Epoch  83 Batch  242/269   train_loss = 3.520\n",
      "Epoch  83 Batch  243/269   train_loss = 3.870\n",
      "Epoch  83 Batch  244/269   train_loss = 3.655\n",
      "Epoch  83 Batch  245/269   train_loss = 3.680\n",
      "Epoch  83 Batch  246/269   train_loss = 3.371\n",
      "Epoch  83 Batch  247/269   train_loss = 3.721\n",
      "Epoch  83 Batch  248/269   train_loss = 3.792\n",
      "Epoch  83 Batch  249/269   train_loss = 3.663\n",
      "Epoch  83 Batch  250/269   train_loss = 3.509\n",
      "Epoch  83 Batch  251/269   train_loss = 3.894\n",
      "Epoch  83 Batch  252/269   train_loss = 3.860\n",
      "Epoch  83 Batch  253/269   train_loss = 3.586\n",
      "Epoch  83 Batch  254/269   train_loss = 3.634\n",
      "Epoch  83 Batch  255/269   train_loss = 3.765\n",
      "Epoch  83 Batch  256/269   train_loss = 3.677\n",
      "Epoch  83 Batch  257/269   train_loss = 3.576\n",
      "Epoch  83 Batch  258/269   train_loss = 3.503\n",
      "Epoch  83 Batch  259/269   train_loss = 3.504\n",
      "Epoch  83 Batch  260/269   train_loss = 3.839\n",
      "Epoch  83 Batch  261/269   train_loss = 3.793\n",
      "Epoch  83 Batch  262/269   train_loss = 3.611\n",
      "Epoch  83 Batch  263/269   train_loss = 3.578\n",
      "Epoch  83 Batch  264/269   train_loss = 3.881\n",
      "Epoch  83 Batch  265/269   train_loss = 3.750\n",
      "Epoch  83 Batch  266/269   train_loss = 3.565\n",
      "Epoch  83 Batch  267/269   train_loss = 3.738\n",
      "Epoch  83 Batch  268/269   train_loss = 3.866\n",
      "Epoch  84 Batch    0/269   train_loss = 3.624\n",
      "Epoch  84 Batch    1/269   train_loss = 3.407\n",
      "Epoch  84 Batch    2/269   train_loss = 3.615\n",
      "Epoch  84 Batch    3/269   train_loss = 3.616\n",
      "Epoch  84 Batch    4/269   train_loss = 4.154\n",
      "Epoch  84 Batch    5/269   train_loss = 3.767\n",
      "Epoch  84 Batch    6/269   train_loss = 3.598\n",
      "Epoch  84 Batch    7/269   train_loss = 3.443\n",
      "Epoch  84 Batch    8/269   train_loss = 3.798\n",
      "Epoch  84 Batch    9/269   train_loss = 3.425\n",
      "Epoch  84 Batch   10/269   train_loss = 3.589\n",
      "Epoch  84 Batch   11/269   train_loss = 3.714\n",
      "Epoch  84 Batch   12/269   train_loss = 3.413\n",
      "Epoch  84 Batch   13/269   train_loss = 3.742\n",
      "Epoch  84 Batch   14/269   train_loss = 3.682\n",
      "Epoch  84 Batch   15/269   train_loss = 3.988\n",
      "Epoch  84 Batch   16/269   train_loss = 3.672\n",
      "Epoch  84 Batch   17/269   train_loss = 3.783\n",
      "Epoch  84 Batch   18/269   train_loss = 3.659\n",
      "Epoch  84 Batch   19/269   train_loss = 3.721\n",
      "Epoch  84 Batch   20/269   train_loss = 4.040\n",
      "Epoch  84 Batch   21/269   train_loss = 3.921\n",
      "Epoch  84 Batch   22/269   train_loss = 3.653\n",
      "Epoch  84 Batch   23/269   train_loss = 3.747\n",
      "Epoch  84 Batch   24/269   train_loss = 3.513\n",
      "Epoch  84 Batch   25/269   train_loss = 3.881\n",
      "Epoch  84 Batch   26/269   train_loss = 3.714\n",
      "Epoch  84 Batch   27/269   train_loss = 3.642\n",
      "Epoch  84 Batch   28/269   train_loss = 3.727\n",
      "Epoch  84 Batch   29/269   train_loss = 3.771\n",
      "Epoch  84 Batch   30/269   train_loss = 4.059\n",
      "Epoch  84 Batch   31/269   train_loss = 3.766\n",
      "Epoch  84 Batch   32/269   train_loss = 3.587\n",
      "Epoch  84 Batch   33/269   train_loss = 3.374\n",
      "Epoch  84 Batch   34/269   train_loss = 3.661\n",
      "Epoch  84 Batch   35/269   train_loss = 3.492\n",
      "Epoch  84 Batch   36/269   train_loss = 3.538\n",
      "Epoch  84 Batch   37/269   train_loss = 3.717\n",
      "Epoch  84 Batch   38/269   train_loss = 3.470\n",
      "Epoch  84 Batch   39/269   train_loss = 3.843\n",
      "Epoch  84 Batch   40/269   train_loss = 3.627\n",
      "Epoch  84 Batch   41/269   train_loss = 3.602\n",
      "Epoch  84 Batch   42/269   train_loss = 4.012\n",
      "Epoch  84 Batch   43/269   train_loss = 3.972\n",
      "Epoch  84 Batch   44/269   train_loss = 3.722\n",
      "Epoch  84 Batch   45/269   train_loss = 3.456\n",
      "Epoch  84 Batch   46/269   train_loss = 3.737\n",
      "Epoch  84 Batch   47/269   train_loss = 3.414\n",
      "Epoch  84 Batch   48/269   train_loss = 3.928\n",
      "Epoch  84 Batch   49/269   train_loss = 3.822\n",
      "Epoch  84 Batch   50/269   train_loss = 3.810\n",
      "Epoch  84 Batch   51/269   train_loss = 3.789\n",
      "Epoch  84 Batch   52/269   train_loss = 3.737\n",
      "Epoch  84 Batch   53/269   train_loss = 3.975\n",
      "Epoch  84 Batch   54/269   train_loss = 3.602\n",
      "Epoch  84 Batch   55/269   train_loss = 3.752\n",
      "Epoch  84 Batch   56/269   train_loss = 3.526\n",
      "Epoch  84 Batch   57/269   train_loss = 3.920\n",
      "Epoch  84 Batch   58/269   train_loss = 3.621\n",
      "Epoch  84 Batch   59/269   train_loss = 3.655\n",
      "Epoch  84 Batch   60/269   train_loss = 3.767\n",
      "Epoch  84 Batch   61/269   train_loss = 3.914\n",
      "Epoch  84 Batch   62/269   train_loss = 3.547\n",
      "Epoch  84 Batch   63/269   train_loss = 3.842\n",
      "Epoch  84 Batch   64/269   train_loss = 3.695\n",
      "Epoch  84 Batch   65/269   train_loss = 3.766\n",
      "Epoch  84 Batch   66/269   train_loss = 3.622\n",
      "Epoch  84 Batch   67/269   train_loss = 3.632\n",
      "Epoch  84 Batch   68/269   train_loss = 3.742\n",
      "Epoch  84 Batch   69/269   train_loss = 3.721\n",
      "Epoch  84 Batch   70/269   train_loss = 3.502\n",
      "Epoch  84 Batch   71/269   train_loss = 3.747\n",
      "Epoch  84 Batch   72/269   train_loss = 3.466\n",
      "Epoch  84 Batch   73/269   train_loss = 3.693\n",
      "Epoch  84 Batch   74/269   train_loss = 3.687\n",
      "Epoch  84 Batch   75/269   train_loss = 3.927\n",
      "Epoch  84 Batch   76/269   train_loss = 3.849\n",
      "Epoch  84 Batch   77/269   train_loss = 3.425\n",
      "Epoch  84 Batch   78/269   train_loss = 3.848\n",
      "Epoch  84 Batch   79/269   train_loss = 3.859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  84 Batch   80/269   train_loss = 3.817\n",
      "Epoch  84 Batch   81/269   train_loss = 3.657\n",
      "Epoch  84 Batch   82/269   train_loss = 3.833\n",
      "Epoch  84 Batch   83/269   train_loss = 3.615\n",
      "Epoch  84 Batch   84/269   train_loss = 3.937\n",
      "Epoch  84 Batch   85/269   train_loss = 4.043\n",
      "Epoch  84 Batch   86/269   train_loss = 4.124\n",
      "Epoch  84 Batch   87/269   train_loss = 3.754\n",
      "Epoch  84 Batch   88/269   train_loss = 3.856\n",
      "Epoch  84 Batch   89/269   train_loss = 3.741\n",
      "Epoch  84 Batch   90/269   train_loss = 3.616\n",
      "Epoch  84 Batch   91/269   train_loss = 3.768\n",
      "Epoch  84 Batch   92/269   train_loss = 3.888\n",
      "Epoch  84 Batch   93/269   train_loss = 3.597\n",
      "Epoch  84 Batch   94/269   train_loss = 4.118\n",
      "Epoch  84 Batch   95/269   train_loss = 3.508\n",
      "Epoch  84 Batch   96/269   train_loss = 3.610\n",
      "Epoch  84 Batch   97/269   train_loss = 3.650\n",
      "Epoch  84 Batch   98/269   train_loss = 3.896\n",
      "Epoch  84 Batch   99/269   train_loss = 3.692\n",
      "Epoch  84 Batch  100/269   train_loss = 3.790\n",
      "Epoch  84 Batch  101/269   train_loss = 3.822\n",
      "Epoch  84 Batch  102/269   train_loss = 3.625\n",
      "Epoch  84 Batch  103/269   train_loss = 3.668\n",
      "Epoch  84 Batch  104/269   train_loss = 3.719\n",
      "Epoch  84 Batch  105/269   train_loss = 3.851\n",
      "Epoch  84 Batch  106/269   train_loss = 4.047\n",
      "Epoch  84 Batch  107/269   train_loss = 3.958\n",
      "Epoch  84 Batch  108/269   train_loss = 3.784\n",
      "Epoch  84 Batch  109/269   train_loss = 3.793\n",
      "Epoch  84 Batch  110/269   train_loss = 3.794\n",
      "Epoch  84 Batch  111/269   train_loss = 3.907\n",
      "Epoch  84 Batch  112/269   train_loss = 3.866\n",
      "Epoch  84 Batch  113/269   train_loss = 3.772\n",
      "Epoch  84 Batch  114/269   train_loss = 3.690\n",
      "Epoch  84 Batch  115/269   train_loss = 3.650\n",
      "Epoch  84 Batch  116/269   train_loss = 3.552\n",
      "Epoch  84 Batch  117/269   train_loss = 3.642\n",
      "Epoch  84 Batch  118/269   train_loss = 3.865\n",
      "Epoch  84 Batch  119/269   train_loss = 3.608\n",
      "Epoch  84 Batch  120/269   train_loss = 3.649\n",
      "Epoch  84 Batch  121/269   train_loss = 3.693\n",
      "Epoch  84 Batch  122/269   train_loss = 3.666\n",
      "Epoch  84 Batch  123/269   train_loss = 3.536\n",
      "Epoch  84 Batch  124/269   train_loss = 3.864\n",
      "Epoch  84 Batch  125/269   train_loss = 3.526\n",
      "Epoch  84 Batch  126/269   train_loss = 3.410\n",
      "Epoch  84 Batch  127/269   train_loss = 3.743\n",
      "Epoch  84 Batch  128/269   train_loss = 3.656\n",
      "Epoch  84 Batch  129/269   train_loss = 3.560\n",
      "Epoch  84 Batch  130/269   train_loss = 3.717\n",
      "Epoch  84 Batch  131/269   train_loss = 3.415\n",
      "Epoch  84 Batch  132/269   train_loss = 3.712\n",
      "Epoch  84 Batch  133/269   train_loss = 3.459\n",
      "Epoch  84 Batch  134/269   train_loss = 3.913\n",
      "Epoch  84 Batch  135/269   train_loss = 3.576\n",
      "Epoch  84 Batch  136/269   train_loss = 3.435\n",
      "Epoch  84 Batch  137/269   train_loss = 3.840\n",
      "Epoch  84 Batch  138/269   train_loss = 3.569\n",
      "Epoch  84 Batch  139/269   train_loss = 3.636\n",
      "Epoch  84 Batch  140/269   train_loss = 3.761\n",
      "Epoch  84 Batch  141/269   train_loss = 3.977\n",
      "Epoch  84 Batch  142/269   train_loss = 3.542\n",
      "Epoch  84 Batch  143/269   train_loss = 3.652\n",
      "Epoch  84 Batch  144/269   train_loss = 3.876\n",
      "Epoch  84 Batch  145/269   train_loss = 3.737\n",
      "Epoch  84 Batch  146/269   train_loss = 3.920\n",
      "Epoch  84 Batch  147/269   train_loss = 3.526\n",
      "Epoch  84 Batch  148/269   train_loss = 3.843\n",
      "Epoch  84 Batch  149/269   train_loss = 3.584\n",
      "Epoch  84 Batch  150/269   train_loss = 3.847\n",
      "Epoch  84 Batch  151/269   train_loss = 3.770\n",
      "Epoch  84 Batch  152/269   train_loss = 3.729\n",
      "Epoch  84 Batch  153/269   train_loss = 3.789\n",
      "Epoch  84 Batch  154/269   train_loss = 3.655\n",
      "Epoch  84 Batch  155/269   train_loss = 3.583\n",
      "Epoch  84 Batch  156/269   train_loss = 4.096\n",
      "Epoch  84 Batch  157/269   train_loss = 3.777\n",
      "Epoch  84 Batch  158/269   train_loss = 3.797\n",
      "Epoch  84 Batch  159/269   train_loss = 3.832\n",
      "Epoch  84 Batch  160/269   train_loss = 3.634\n",
      "Epoch  84 Batch  161/269   train_loss = 3.821\n",
      "Epoch  84 Batch  162/269   train_loss = 3.868\n",
      "Epoch  84 Batch  163/269   train_loss = 3.596\n",
      "Epoch  84 Batch  164/269   train_loss = 3.878\n",
      "Epoch  84 Batch  165/269   train_loss = 3.973\n",
      "Epoch  84 Batch  166/269   train_loss = 3.841\n",
      "Epoch  84 Batch  167/269   train_loss = 3.516\n",
      "Epoch  84 Batch  168/269   train_loss = 3.720\n",
      "Epoch  84 Batch  169/269   train_loss = 3.988\n",
      "Epoch  84 Batch  170/269   train_loss = 3.532\n",
      "Epoch  84 Batch  171/269   train_loss = 3.726\n",
      "Epoch  84 Batch  172/269   train_loss = 3.536\n",
      "Epoch  84 Batch  173/269   train_loss = 3.752\n",
      "Epoch  84 Batch  174/269   train_loss = 3.880\n",
      "Epoch  84 Batch  175/269   train_loss = 3.759\n",
      "Epoch  84 Batch  176/269   train_loss = 3.673\n",
      "Epoch  84 Batch  177/269   train_loss = 3.865\n",
      "Epoch  84 Batch  178/269   train_loss = 3.977\n",
      "Epoch  84 Batch  179/269   train_loss = 3.743\n",
      "Epoch  84 Batch  180/269   train_loss = 3.568\n",
      "Epoch  84 Batch  181/269   train_loss = 3.879\n",
      "Epoch  84 Batch  182/269   train_loss = 3.677\n",
      "Epoch  84 Batch  183/269   train_loss = 3.655\n",
      "Epoch  84 Batch  184/269   train_loss = 3.691\n",
      "Epoch  84 Batch  185/269   train_loss = 3.721\n",
      "Epoch  84 Batch  186/269   train_loss = 3.736\n",
      "Epoch  84 Batch  187/269   train_loss = 3.820\n",
      "Epoch  84 Batch  188/269   train_loss = 3.555\n",
      "Epoch  84 Batch  189/269   train_loss = 3.663\n",
      "Epoch  84 Batch  190/269   train_loss = 4.047\n",
      "Epoch  84 Batch  191/269   train_loss = 3.762\n",
      "Epoch  84 Batch  192/269   train_loss = 3.762\n",
      "Epoch  84 Batch  193/269   train_loss = 3.711\n",
      "Epoch  84 Batch  194/269   train_loss = 3.807\n",
      "Epoch  84 Batch  195/269   train_loss = 3.737\n",
      "Epoch  84 Batch  196/269   train_loss = 3.759\n",
      "Epoch  84 Batch  197/269   train_loss = 3.900\n",
      "Epoch  84 Batch  198/269   train_loss = 3.767\n",
      "Epoch  84 Batch  199/269   train_loss = 3.910\n",
      "Epoch  84 Batch  200/269   train_loss = 3.586\n",
      "Epoch  84 Batch  201/269   train_loss = 3.686\n",
      "Epoch  84 Batch  202/269   train_loss = 3.487\n",
      "Epoch  84 Batch  203/269   train_loss = 3.608\n",
      "Epoch  84 Batch  204/269   train_loss = 3.852\n",
      "Epoch  84 Batch  205/269   train_loss = 3.730\n",
      "Epoch  84 Batch  206/269   train_loss = 3.660\n",
      "Epoch  84 Batch  207/269   train_loss = 3.638\n",
      "Epoch  84 Batch  208/269   train_loss = 3.819\n",
      "Epoch  84 Batch  209/269   train_loss = 3.829\n",
      "Epoch  84 Batch  210/269   train_loss = 3.659\n",
      "Epoch  84 Batch  211/269   train_loss = 3.671\n",
      "Epoch  84 Batch  212/269   train_loss = 4.066\n",
      "Epoch  84 Batch  213/269   train_loss = 3.662\n",
      "Epoch  84 Batch  214/269   train_loss = 3.705\n",
      "Epoch  84 Batch  215/269   train_loss = 4.004\n",
      "Epoch  84 Batch  216/269   train_loss = 3.889\n",
      "Epoch  84 Batch  217/269   train_loss = 3.601\n",
      "Epoch  84 Batch  218/269   train_loss = 3.808\n",
      "Epoch  84 Batch  219/269   train_loss = 3.533\n",
      "Epoch  84 Batch  220/269   train_loss = 3.890\n",
      "Epoch  84 Batch  221/269   train_loss = 3.594\n",
      "Epoch  84 Batch  222/269   train_loss = 3.789\n",
      "Epoch  84 Batch  223/269   train_loss = 3.549\n",
      "Epoch  84 Batch  224/269   train_loss = 3.851\n",
      "Epoch  84 Batch  225/269   train_loss = 3.929\n",
      "Epoch  84 Batch  226/269   train_loss = 3.698\n",
      "Epoch  84 Batch  227/269   train_loss = 3.465\n",
      "Epoch  84 Batch  228/269   train_loss = 3.756\n",
      "Epoch  84 Batch  229/269   train_loss = 3.931\n",
      "Epoch  84 Batch  230/269   train_loss = 3.806\n",
      "Epoch  84 Batch  231/269   train_loss = 3.594\n",
      "Epoch  84 Batch  232/269   train_loss = 3.760\n",
      "Epoch  84 Batch  233/269   train_loss = 3.811\n",
      "Epoch  84 Batch  234/269   train_loss = 3.744\n",
      "Epoch  84 Batch  235/269   train_loss = 3.969\n",
      "Epoch  84 Batch  236/269   train_loss = 3.620\n",
      "Epoch  84 Batch  237/269   train_loss = 3.292\n",
      "Epoch  84 Batch  238/269   train_loss = 3.503\n",
      "Epoch  84 Batch  239/269   train_loss = 3.961\n",
      "Epoch  84 Batch  240/269   train_loss = 3.542\n",
      "Epoch  84 Batch  241/269   train_loss = 3.929\n",
      "Epoch  84 Batch  242/269   train_loss = 3.522\n",
      "Epoch  84 Batch  243/269   train_loss = 3.856\n",
      "Epoch  84 Batch  244/269   train_loss = 3.674\n",
      "Epoch  84 Batch  245/269   train_loss = 3.664\n",
      "Epoch  84 Batch  246/269   train_loss = 3.348\n",
      "Epoch  84 Batch  247/269   train_loss = 3.702\n",
      "Epoch  84 Batch  248/269   train_loss = 3.772\n",
      "Epoch  84 Batch  249/269   train_loss = 3.649\n",
      "Epoch  84 Batch  250/269   train_loss = 3.511\n",
      "Epoch  84 Batch  251/269   train_loss = 3.901\n",
      "Epoch  84 Batch  252/269   train_loss = 3.872\n",
      "Epoch  84 Batch  253/269   train_loss = 3.568\n",
      "Epoch  84 Batch  254/269   train_loss = 3.591\n",
      "Epoch  84 Batch  255/269   train_loss = 3.767\n",
      "Epoch  84 Batch  256/269   train_loss = 3.660\n",
      "Epoch  84 Batch  257/269   train_loss = 3.554\n",
      "Epoch  84 Batch  258/269   train_loss = 3.490\n",
      "Epoch  84 Batch  259/269   train_loss = 3.507\n",
      "Epoch  84 Batch  260/269   train_loss = 3.830\n",
      "Epoch  84 Batch  261/269   train_loss = 3.785\n",
      "Epoch  84 Batch  262/269   train_loss = 3.621\n",
      "Epoch  84 Batch  263/269   train_loss = 3.541\n",
      "Epoch  84 Batch  264/269   train_loss = 3.857\n",
      "Epoch  84 Batch  265/269   train_loss = 3.749\n",
      "Epoch  84 Batch  266/269   train_loss = 3.593\n",
      "Epoch  84 Batch  267/269   train_loss = 3.729\n",
      "Epoch  84 Batch  268/269   train_loss = 3.864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  85 Batch    0/269   train_loss = 3.643\n",
      "Epoch  85 Batch    1/269   train_loss = 3.407\n",
      "Epoch  85 Batch    2/269   train_loss = 3.618\n",
      "Epoch  85 Batch    3/269   train_loss = 3.649\n",
      "Epoch  85 Batch    4/269   train_loss = 4.072\n",
      "Epoch  85 Batch    5/269   train_loss = 3.749\n",
      "Epoch  85 Batch    6/269   train_loss = 3.588\n",
      "Epoch  85 Batch    7/269   train_loss = 3.463\n",
      "Epoch  85 Batch    8/269   train_loss = 3.792\n",
      "Epoch  85 Batch    9/269   train_loss = 3.421\n",
      "Epoch  85 Batch   10/269   train_loss = 3.556\n",
      "Epoch  85 Batch   11/269   train_loss = 3.709\n",
      "Epoch  85 Batch   12/269   train_loss = 3.367\n",
      "Epoch  85 Batch   13/269   train_loss = 3.738\n",
      "Epoch  85 Batch   14/269   train_loss = 3.670\n",
      "Epoch  85 Batch   15/269   train_loss = 3.981\n",
      "Epoch  85 Batch   16/269   train_loss = 3.654\n",
      "Epoch  85 Batch   17/269   train_loss = 3.758\n",
      "Epoch  85 Batch   18/269   train_loss = 3.670\n",
      "Epoch  85 Batch   19/269   train_loss = 3.699\n",
      "Epoch  85 Batch   20/269   train_loss = 4.062\n",
      "Epoch  85 Batch   21/269   train_loss = 3.874\n",
      "Epoch  85 Batch   22/269   train_loss = 3.629\n",
      "Epoch  85 Batch   23/269   train_loss = 3.782\n",
      "Epoch  85 Batch   24/269   train_loss = 3.484\n",
      "Epoch  85 Batch   25/269   train_loss = 3.913\n",
      "Epoch  85 Batch   26/269   train_loss = 3.719\n",
      "Epoch  85 Batch   27/269   train_loss = 3.646\n",
      "Epoch  85 Batch   28/269   train_loss = 3.743\n",
      "Epoch  85 Batch   29/269   train_loss = 3.767\n",
      "Epoch  85 Batch   30/269   train_loss = 4.031\n",
      "Epoch  85 Batch   31/269   train_loss = 3.718\n",
      "Epoch  85 Batch   32/269   train_loss = 3.601\n",
      "Epoch  85 Batch   33/269   train_loss = 3.375\n",
      "Epoch  85 Batch   34/269   train_loss = 3.650\n",
      "Epoch  85 Batch   35/269   train_loss = 3.479\n",
      "Epoch  85 Batch   36/269   train_loss = 3.522\n",
      "Epoch  85 Batch   37/269   train_loss = 3.726\n",
      "Epoch  85 Batch   38/269   train_loss = 3.467\n",
      "Epoch  85 Batch   39/269   train_loss = 3.861\n",
      "Epoch  85 Batch   40/269   train_loss = 3.607\n",
      "Epoch  85 Batch   41/269   train_loss = 3.587\n",
      "Epoch  85 Batch   42/269   train_loss = 4.023\n",
      "Epoch  85 Batch   43/269   train_loss = 3.915\n",
      "Epoch  85 Batch   44/269   train_loss = 3.715\n",
      "Epoch  85 Batch   45/269   train_loss = 3.498\n",
      "Epoch  85 Batch   46/269   train_loss = 3.745\n",
      "Epoch  85 Batch   47/269   train_loss = 3.407\n",
      "Epoch  85 Batch   48/269   train_loss = 3.980\n",
      "Epoch  85 Batch   49/269   train_loss = 3.866\n",
      "Epoch  85 Batch   50/269   train_loss = 3.804\n",
      "Epoch  85 Batch   51/269   train_loss = 3.787\n",
      "Epoch  85 Batch   52/269   train_loss = 3.747\n",
      "Epoch  85 Batch   53/269   train_loss = 3.957\n",
      "Epoch  85 Batch   54/269   train_loss = 3.608\n",
      "Epoch  85 Batch   55/269   train_loss = 3.775\n",
      "Epoch  85 Batch   56/269   train_loss = 3.555\n",
      "Epoch  85 Batch   57/269   train_loss = 3.881\n",
      "Epoch  85 Batch   58/269   train_loss = 3.598\n",
      "Epoch  85 Batch   59/269   train_loss = 3.675\n",
      "Epoch  85 Batch   60/269   train_loss = 3.761\n",
      "Epoch  85 Batch   61/269   train_loss = 3.879\n",
      "Epoch  85 Batch   62/269   train_loss = 3.519\n",
      "Epoch  85 Batch   63/269   train_loss = 3.852\n",
      "Epoch  85 Batch   64/269   train_loss = 3.692\n",
      "Epoch  85 Batch   65/269   train_loss = 3.783\n",
      "Epoch  85 Batch   66/269   train_loss = 3.652\n",
      "Epoch  85 Batch   67/269   train_loss = 3.614\n",
      "Epoch  85 Batch   68/269   train_loss = 3.788\n",
      "Epoch  85 Batch   69/269   train_loss = 3.722\n",
      "Epoch  85 Batch   70/269   train_loss = 3.532\n",
      "Epoch  85 Batch   71/269   train_loss = 3.755\n",
      "Epoch  85 Batch   72/269   train_loss = 3.459\n",
      "Epoch  85 Batch   73/269   train_loss = 3.679\n",
      "Epoch  85 Batch   74/269   train_loss = 3.701\n",
      "Epoch  85 Batch   75/269   train_loss = 3.875\n",
      "Epoch  85 Batch   76/269   train_loss = 3.856\n",
      "Epoch  85 Batch   77/269   train_loss = 3.449\n",
      "Epoch  85 Batch   78/269   train_loss = 3.853\n",
      "Epoch  85 Batch   79/269   train_loss = 3.866\n",
      "Epoch  85 Batch   80/269   train_loss = 3.822\n",
      "Epoch  85 Batch   81/269   train_loss = 3.636\n",
      "Epoch  85 Batch   82/269   train_loss = 3.808\n",
      "Epoch  85 Batch   83/269   train_loss = 3.608\n",
      "Epoch  85 Batch   84/269   train_loss = 3.952\n",
      "Epoch  85 Batch   85/269   train_loss = 4.070\n",
      "Epoch  85 Batch   86/269   train_loss = 4.066\n",
      "Epoch  85 Batch   87/269   train_loss = 3.744\n",
      "Epoch  85 Batch   88/269   train_loss = 3.860\n",
      "Epoch  85 Batch   89/269   train_loss = 3.699\n",
      "Epoch  85 Batch   90/269   train_loss = 3.604\n",
      "Epoch  85 Batch   91/269   train_loss = 3.810\n",
      "Epoch  85 Batch   92/269   train_loss = 3.846\n",
      "Epoch  85 Batch   93/269   train_loss = 3.611\n",
      "Epoch  85 Batch   94/269   train_loss = 4.061\n",
      "Epoch  85 Batch   95/269   train_loss = 3.501\n",
      "Epoch  85 Batch   96/269   train_loss = 3.623\n",
      "Epoch  85 Batch   97/269   train_loss = 3.642\n",
      "Epoch  85 Batch   98/269   train_loss = 3.909\n",
      "Epoch  85 Batch   99/269   train_loss = 3.710\n",
      "Epoch  85 Batch  100/269   train_loss = 3.765\n",
      "Epoch  85 Batch  101/269   train_loss = 3.785\n",
      "Epoch  85 Batch  102/269   train_loss = 3.609\n",
      "Epoch  85 Batch  103/269   train_loss = 3.661\n",
      "Epoch  85 Batch  104/269   train_loss = 3.688\n",
      "Epoch  85 Batch  105/269   train_loss = 3.859\n",
      "Epoch  85 Batch  106/269   train_loss = 4.039\n",
      "Epoch  85 Batch  107/269   train_loss = 3.944\n",
      "Epoch  85 Batch  108/269   train_loss = 3.746\n",
      "Epoch  85 Batch  109/269   train_loss = 3.814\n",
      "Epoch  85 Batch  110/269   train_loss = 3.812\n",
      "Epoch  85 Batch  111/269   train_loss = 3.881\n",
      "Epoch  85 Batch  112/269   train_loss = 3.880\n",
      "Epoch  85 Batch  113/269   train_loss = 3.777\n",
      "Epoch  85 Batch  114/269   train_loss = 3.671\n",
      "Epoch  85 Batch  115/269   train_loss = 3.624\n",
      "Epoch  85 Batch  116/269   train_loss = 3.558\n",
      "Epoch  85 Batch  117/269   train_loss = 3.631\n",
      "Epoch  85 Batch  118/269   train_loss = 3.874\n",
      "Epoch  85 Batch  119/269   train_loss = 3.614\n",
      "Epoch  85 Batch  120/269   train_loss = 3.671\n",
      "Epoch  85 Batch  121/269   train_loss = 3.726\n",
      "Epoch  85 Batch  122/269   train_loss = 3.663\n",
      "Epoch  85 Batch  123/269   train_loss = 3.525\n",
      "Epoch  85 Batch  124/269   train_loss = 3.879\n",
      "Epoch  85 Batch  125/269   train_loss = 3.512\n",
      "Epoch  85 Batch  126/269   train_loss = 3.391\n",
      "Epoch  85 Batch  127/269   train_loss = 3.718\n",
      "Epoch  85 Batch  128/269   train_loss = 3.677\n",
      "Epoch  85 Batch  129/269   train_loss = 3.610\n",
      "Epoch  85 Batch  130/269   train_loss = 3.736\n",
      "Epoch  85 Batch  131/269   train_loss = 3.410\n",
      "Epoch  85 Batch  132/269   train_loss = 3.749\n",
      "Epoch  85 Batch  133/269   train_loss = 3.451\n",
      "Epoch  85 Batch  134/269   train_loss = 3.895\n",
      "Epoch  85 Batch  135/269   train_loss = 3.563\n",
      "Epoch  85 Batch  136/269   train_loss = 3.430\n",
      "Epoch  85 Batch  137/269   train_loss = 3.821\n",
      "Epoch  85 Batch  138/269   train_loss = 3.572\n",
      "Epoch  85 Batch  139/269   train_loss = 3.680\n",
      "Epoch  85 Batch  140/269   train_loss = 3.756\n",
      "Epoch  85 Batch  141/269   train_loss = 4.000\n",
      "Epoch  85 Batch  142/269   train_loss = 3.549\n",
      "Epoch  85 Batch  143/269   train_loss = 3.653\n",
      "Epoch  85 Batch  144/269   train_loss = 3.879\n",
      "Epoch  85 Batch  145/269   train_loss = 3.734\n",
      "Epoch  85 Batch  146/269   train_loss = 3.900\n",
      "Epoch  85 Batch  147/269   train_loss = 3.566\n",
      "Epoch  85 Batch  148/269   train_loss = 3.826\n",
      "Epoch  85 Batch  149/269   train_loss = 3.582\n",
      "Epoch  85 Batch  150/269   train_loss = 3.813\n",
      "Epoch  85 Batch  151/269   train_loss = 3.863\n",
      "Epoch  85 Batch  152/269   train_loss = 3.762\n",
      "Epoch  85 Batch  153/269   train_loss = 3.776\n",
      "Epoch  85 Batch  154/269   train_loss = 3.651\n",
      "Epoch  85 Batch  155/269   train_loss = 3.613\n",
      "Epoch  85 Batch  156/269   train_loss = 4.092\n",
      "Epoch  85 Batch  157/269   train_loss = 3.811\n",
      "Epoch  85 Batch  158/269   train_loss = 3.796\n",
      "Epoch  85 Batch  159/269   train_loss = 3.831\n",
      "Epoch  85 Batch  160/269   train_loss = 3.605\n",
      "Epoch  85 Batch  161/269   train_loss = 3.789\n",
      "Epoch  85 Batch  162/269   train_loss = 3.867\n",
      "Epoch  85 Batch  163/269   train_loss = 3.610\n",
      "Epoch  85 Batch  164/269   train_loss = 3.877\n",
      "Epoch  85 Batch  165/269   train_loss = 3.986\n",
      "Epoch  85 Batch  166/269   train_loss = 3.820\n",
      "Epoch  85 Batch  167/269   train_loss = 3.516\n",
      "Epoch  85 Batch  168/269   train_loss = 3.751\n",
      "Epoch  85 Batch  169/269   train_loss = 3.962\n",
      "Epoch  85 Batch  170/269   train_loss = 3.525\n",
      "Epoch  85 Batch  171/269   train_loss = 3.757\n",
      "Epoch  85 Batch  172/269   train_loss = 3.532\n",
      "Epoch  85 Batch  173/269   train_loss = 3.757\n",
      "Epoch  85 Batch  174/269   train_loss = 3.868\n",
      "Epoch  85 Batch  175/269   train_loss = 3.746\n",
      "Epoch  85 Batch  176/269   train_loss = 3.658\n",
      "Epoch  85 Batch  177/269   train_loss = 3.866\n",
      "Epoch  85 Batch  178/269   train_loss = 3.970\n",
      "Epoch  85 Batch  179/269   train_loss = 3.738\n",
      "Epoch  85 Batch  180/269   train_loss = 3.572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  85 Batch  181/269   train_loss = 3.847\n",
      "Epoch  85 Batch  182/269   train_loss = 3.688\n",
      "Epoch  85 Batch  183/269   train_loss = 3.623\n",
      "Epoch  85 Batch  184/269   train_loss = 3.687\n",
      "Epoch  85 Batch  185/269   train_loss = 3.754\n",
      "Epoch  85 Batch  186/269   train_loss = 3.711\n",
      "Epoch  85 Batch  187/269   train_loss = 3.822\n",
      "Epoch  85 Batch  188/269   train_loss = 3.570\n",
      "Epoch  85 Batch  189/269   train_loss = 3.678\n",
      "Epoch  85 Batch  190/269   train_loss = 4.053\n",
      "Epoch  85 Batch  191/269   train_loss = 3.763\n",
      "Epoch  85 Batch  192/269   train_loss = 3.774\n",
      "Epoch  85 Batch  193/269   train_loss = 3.703\n",
      "Epoch  85 Batch  194/269   train_loss = 3.800\n",
      "Epoch  85 Batch  195/269   train_loss = 3.748\n",
      "Epoch  85 Batch  196/269   train_loss = 3.751\n",
      "Epoch  85 Batch  197/269   train_loss = 3.903\n",
      "Epoch  85 Batch  198/269   train_loss = 3.770\n",
      "Epoch  85 Batch  199/269   train_loss = 3.875\n",
      "Epoch  85 Batch  200/269   train_loss = 3.601\n",
      "Epoch  85 Batch  201/269   train_loss = 3.728\n",
      "Epoch  85 Batch  202/269   train_loss = 3.493\n",
      "Epoch  85 Batch  203/269   train_loss = 3.636\n",
      "Epoch  85 Batch  204/269   train_loss = 3.843\n",
      "Epoch  85 Batch  205/269   train_loss = 3.730\n",
      "Epoch  85 Batch  206/269   train_loss = 3.682\n",
      "Epoch  85 Batch  207/269   train_loss = 3.647\n",
      "Epoch  85 Batch  208/269   train_loss = 3.813\n",
      "Epoch  85 Batch  209/269   train_loss = 3.836\n",
      "Epoch  85 Batch  210/269   train_loss = 3.621\n",
      "Epoch  85 Batch  211/269   train_loss = 3.660\n",
      "Epoch  85 Batch  212/269   train_loss = 4.057\n",
      "Epoch  85 Batch  213/269   train_loss = 3.613\n",
      "Epoch  85 Batch  214/269   train_loss = 3.664\n",
      "Epoch  85 Batch  215/269   train_loss = 3.993\n",
      "Epoch  85 Batch  216/269   train_loss = 3.905\n",
      "Epoch  85 Batch  217/269   train_loss = 3.603\n",
      "Epoch  85 Batch  218/269   train_loss = 3.806\n",
      "Epoch  85 Batch  219/269   train_loss = 3.536\n",
      "Epoch  85 Batch  220/269   train_loss = 3.900\n",
      "Epoch  85 Batch  221/269   train_loss = 3.597\n",
      "Epoch  85 Batch  222/269   train_loss = 3.756\n",
      "Epoch  85 Batch  223/269   train_loss = 3.526\n",
      "Epoch  85 Batch  224/269   train_loss = 3.882\n",
      "Epoch  85 Batch  225/269   train_loss = 3.920\n",
      "Epoch  85 Batch  226/269   train_loss = 3.714\n",
      "Epoch  85 Batch  227/269   train_loss = 3.430\n",
      "Epoch  85 Batch  228/269   train_loss = 3.784\n",
      "Epoch  85 Batch  229/269   train_loss = 3.968\n",
      "Epoch  85 Batch  230/269   train_loss = 3.835\n",
      "Epoch  85 Batch  231/269   train_loss = 3.633\n",
      "Epoch  85 Batch  232/269   train_loss = 3.739\n",
      "Epoch  85 Batch  233/269   train_loss = 3.816\n",
      "Epoch  85 Batch  234/269   train_loss = 3.776\n",
      "Epoch  85 Batch  235/269   train_loss = 3.976\n",
      "Epoch  85 Batch  236/269   train_loss = 3.587\n",
      "Epoch  85 Batch  237/269   train_loss = 3.289\n",
      "Epoch  85 Batch  238/269   train_loss = 3.471\n",
      "Epoch  85 Batch  239/269   train_loss = 3.983\n",
      "Epoch  85 Batch  240/269   train_loss = 3.554\n",
      "Epoch  85 Batch  241/269   train_loss = 3.927\n",
      "Epoch  85 Batch  242/269   train_loss = 3.535\n",
      "Epoch  85 Batch  243/269   train_loss = 3.852\n",
      "Epoch  85 Batch  244/269   train_loss = 3.663\n",
      "Epoch  85 Batch  245/269   train_loss = 3.682\n",
      "Epoch  85 Batch  246/269   train_loss = 3.344\n",
      "Epoch  85 Batch  247/269   train_loss = 3.702\n",
      "Epoch  85 Batch  248/269   train_loss = 3.824\n",
      "Epoch  85 Batch  249/269   train_loss = 3.649\n",
      "Epoch  85 Batch  250/269   train_loss = 3.510\n",
      "Epoch  85 Batch  251/269   train_loss = 3.947\n",
      "Epoch  85 Batch  252/269   train_loss = 3.848\n",
      "Epoch  85 Batch  253/269   train_loss = 3.604\n",
      "Epoch  85 Batch  254/269   train_loss = 3.615\n",
      "Epoch  85 Batch  255/269   train_loss = 3.782\n",
      "Epoch  85 Batch  256/269   train_loss = 3.645\n",
      "Epoch  85 Batch  257/269   train_loss = 3.559\n",
      "Epoch  85 Batch  258/269   train_loss = 3.510\n",
      "Epoch  85 Batch  259/269   train_loss = 3.514\n",
      "Epoch  85 Batch  260/269   train_loss = 3.827\n",
      "Epoch  85 Batch  261/269   train_loss = 3.781\n",
      "Epoch  85 Batch  262/269   train_loss = 3.590\n",
      "Epoch  85 Batch  263/269   train_loss = 3.521\n",
      "Epoch  85 Batch  264/269   train_loss = 3.864\n",
      "Epoch  85 Batch  265/269   train_loss = 3.751\n",
      "Epoch  85 Batch  266/269   train_loss = 3.620\n",
      "Epoch  85 Batch  267/269   train_loss = 3.738\n",
      "Epoch  85 Batch  268/269   train_loss = 3.882\n",
      "Epoch  86 Batch    0/269   train_loss = 3.651\n",
      "Epoch  86 Batch    1/269   train_loss = 3.439\n",
      "Epoch  86 Batch    2/269   train_loss = 3.608\n",
      "Epoch  86 Batch    3/269   train_loss = 3.665\n",
      "Epoch  86 Batch    4/269   train_loss = 4.111\n",
      "Epoch  86 Batch    5/269   train_loss = 3.743\n",
      "Epoch  86 Batch    6/269   train_loss = 3.559\n",
      "Epoch  86 Batch    7/269   train_loss = 3.516\n",
      "Epoch  86 Batch    8/269   train_loss = 3.791\n",
      "Epoch  86 Batch    9/269   train_loss = 3.433\n",
      "Epoch  86 Batch   10/269   train_loss = 3.565\n",
      "Epoch  86 Batch   11/269   train_loss = 3.707\n",
      "Epoch  86 Batch   12/269   train_loss = 3.372\n",
      "Epoch  86 Batch   13/269   train_loss = 3.733\n",
      "Epoch  86 Batch   14/269   train_loss = 3.639\n",
      "Epoch  86 Batch   15/269   train_loss = 3.988\n",
      "Epoch  86 Batch   16/269   train_loss = 3.675\n",
      "Epoch  86 Batch   17/269   train_loss = 3.734\n",
      "Epoch  86 Batch   18/269   train_loss = 3.660\n",
      "Epoch  86 Batch   19/269   train_loss = 3.704\n",
      "Epoch  86 Batch   20/269   train_loss = 4.085\n",
      "Epoch  86 Batch   21/269   train_loss = 3.874\n",
      "Epoch  86 Batch   22/269   train_loss = 3.644\n",
      "Epoch  86 Batch   23/269   train_loss = 3.778\n",
      "Epoch  86 Batch   24/269   train_loss = 3.590\n",
      "Epoch  86 Batch   25/269   train_loss = 3.906\n",
      "Epoch  86 Batch   26/269   train_loss = 3.723\n",
      "Epoch  86 Batch   27/269   train_loss = 3.629\n",
      "Epoch  86 Batch   28/269   train_loss = 3.750\n",
      "Epoch  86 Batch   29/269   train_loss = 3.777\n",
      "Epoch  86 Batch   30/269   train_loss = 4.035\n",
      "Epoch  86 Batch   31/269   train_loss = 3.721\n",
      "Epoch  86 Batch   32/269   train_loss = 3.647\n",
      "Epoch  86 Batch   33/269   train_loss = 3.383\n",
      "Epoch  86 Batch   34/269   train_loss = 3.689\n",
      "Epoch  86 Batch   35/269   train_loss = 3.464\n",
      "Epoch  86 Batch   36/269   train_loss = 3.532\n",
      "Epoch  86 Batch   37/269   train_loss = 3.734\n",
      "Epoch  86 Batch   38/269   train_loss = 3.468\n",
      "Epoch  86 Batch   39/269   train_loss = 3.858\n",
      "Epoch  86 Batch   40/269   train_loss = 3.599\n",
      "Epoch  86 Batch   41/269   train_loss = 3.596\n",
      "Epoch  86 Batch   42/269   train_loss = 4.029\n",
      "Epoch  86 Batch   43/269   train_loss = 3.974\n",
      "Epoch  86 Batch   44/269   train_loss = 3.721\n",
      "Epoch  86 Batch   45/269   train_loss = 3.488\n",
      "Epoch  86 Batch   46/269   train_loss = 3.744\n",
      "Epoch  86 Batch   47/269   train_loss = 3.392\n",
      "Epoch  86 Batch   48/269   train_loss = 3.970\n",
      "Epoch  86 Batch   49/269   train_loss = 3.847\n",
      "Epoch  86 Batch   50/269   train_loss = 3.818\n",
      "Epoch  86 Batch   51/269   train_loss = 3.788\n",
      "Epoch  86 Batch   52/269   train_loss = 3.781\n",
      "Epoch  86 Batch   53/269   train_loss = 4.000\n",
      "Epoch  86 Batch   54/269   train_loss = 3.633\n",
      "Epoch  86 Batch   55/269   train_loss = 3.765\n",
      "Epoch  86 Batch   56/269   train_loss = 3.537\n",
      "Epoch  86 Batch   57/269   train_loss = 3.905\n",
      "Epoch  86 Batch   58/269   train_loss = 3.596\n",
      "Epoch  86 Batch   59/269   train_loss = 3.664\n",
      "Epoch  86 Batch   60/269   train_loss = 3.773\n",
      "Epoch  86 Batch   61/269   train_loss = 3.899\n",
      "Epoch  86 Batch   62/269   train_loss = 3.581\n",
      "Epoch  86 Batch   63/269   train_loss = 3.873\n",
      "Epoch  86 Batch   64/269   train_loss = 3.697\n",
      "Epoch  86 Batch   65/269   train_loss = 3.789\n",
      "Epoch  86 Batch   66/269   train_loss = 3.616\n",
      "Epoch  86 Batch   67/269   train_loss = 3.627\n",
      "Epoch  86 Batch   68/269   train_loss = 3.798\n",
      "Epoch  86 Batch   69/269   train_loss = 3.793\n",
      "Epoch  86 Batch   70/269   train_loss = 3.523\n",
      "Epoch  86 Batch   71/269   train_loss = 3.754\n",
      "Epoch  86 Batch   72/269   train_loss = 3.484\n",
      "Epoch  86 Batch   73/269   train_loss = 3.696\n",
      "Epoch  86 Batch   74/269   train_loss = 3.707\n",
      "Epoch  86 Batch   75/269   train_loss = 3.881\n",
      "Epoch  86 Batch   76/269   train_loss = 3.842\n",
      "Epoch  86 Batch   77/269   train_loss = 3.478\n",
      "Epoch  86 Batch   78/269   train_loss = 3.822\n",
      "Epoch  86 Batch   79/269   train_loss = 3.883\n",
      "Epoch  86 Batch   80/269   train_loss = 3.814\n",
      "Epoch  86 Batch   81/269   train_loss = 3.647\n",
      "Epoch  86 Batch   82/269   train_loss = 3.839\n",
      "Epoch  86 Batch   83/269   train_loss = 3.632\n",
      "Epoch  86 Batch   84/269   train_loss = 3.968\n",
      "Epoch  86 Batch   85/269   train_loss = 4.058\n",
      "Epoch  86 Batch   86/269   train_loss = 4.061\n",
      "Epoch  86 Batch   87/269   train_loss = 3.764\n",
      "Epoch  86 Batch   88/269   train_loss = 3.864\n",
      "Epoch  86 Batch   89/269   train_loss = 3.706\n",
      "Epoch  86 Batch   90/269   train_loss = 3.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  86 Batch   91/269   train_loss = 3.805\n",
      "Epoch  86 Batch   92/269   train_loss = 3.849\n",
      "Epoch  86 Batch   93/269   train_loss = 3.600\n",
      "Epoch  86 Batch   94/269   train_loss = 4.069\n",
      "Epoch  86 Batch   95/269   train_loss = 3.528\n",
      "Epoch  86 Batch   96/269   train_loss = 3.649\n",
      "Epoch  86 Batch   97/269   train_loss = 3.622\n",
      "Epoch  86 Batch   98/269   train_loss = 3.931\n",
      "Epoch  86 Batch   99/269   train_loss = 3.712\n",
      "Epoch  86 Batch  100/269   train_loss = 3.747\n",
      "Epoch  86 Batch  101/269   train_loss = 3.790\n",
      "Epoch  86 Batch  102/269   train_loss = 3.585\n",
      "Epoch  86 Batch  103/269   train_loss = 3.698\n",
      "Epoch  86 Batch  104/269   train_loss = 3.675\n",
      "Epoch  86 Batch  105/269   train_loss = 3.851\n",
      "Epoch  86 Batch  106/269   train_loss = 4.042\n",
      "Epoch  86 Batch  107/269   train_loss = 3.941\n",
      "Epoch  86 Batch  108/269   train_loss = 3.786\n",
      "Epoch  86 Batch  109/269   train_loss = 3.822\n",
      "Epoch  86 Batch  110/269   train_loss = 3.799\n",
      "Epoch  86 Batch  111/269   train_loss = 3.902\n",
      "Epoch  86 Batch  112/269   train_loss = 3.883\n",
      "Epoch  86 Batch  113/269   train_loss = 3.779\n",
      "Epoch  86 Batch  114/269   train_loss = 3.684\n",
      "Epoch  86 Batch  115/269   train_loss = 3.620\n",
      "Epoch  86 Batch  116/269   train_loss = 3.550\n",
      "Epoch  86 Batch  117/269   train_loss = 3.619\n",
      "Epoch  86 Batch  118/269   train_loss = 3.871\n",
      "Epoch  86 Batch  119/269   train_loss = 3.578\n",
      "Epoch  86 Batch  120/269   train_loss = 3.655\n",
      "Epoch  86 Batch  121/269   train_loss = 3.710\n",
      "Epoch  86 Batch  122/269   train_loss = 3.721\n",
      "Epoch  86 Batch  123/269   train_loss = 3.505\n",
      "Epoch  86 Batch  124/269   train_loss = 3.880\n",
      "Epoch  86 Batch  125/269   train_loss = 3.509\n",
      "Epoch  86 Batch  126/269   train_loss = 3.412\n",
      "Epoch  86 Batch  127/269   train_loss = 3.717\n",
      "Epoch  86 Batch  128/269   train_loss = 3.703\n",
      "Epoch  86 Batch  129/269   train_loss = 3.622\n",
      "Epoch  86 Batch  130/269   train_loss = 3.777\n",
      "Epoch  86 Batch  131/269   train_loss = 3.415\n",
      "Epoch  86 Batch  132/269   train_loss = 3.750\n",
      "Epoch  86 Batch  133/269   train_loss = 3.460\n",
      "Epoch  86 Batch  134/269   train_loss = 3.882\n",
      "Epoch  86 Batch  135/269   train_loss = 3.583\n",
      "Epoch  86 Batch  136/269   train_loss = 3.400\n",
      "Epoch  86 Batch  137/269   train_loss = 3.885\n",
      "Epoch  86 Batch  138/269   train_loss = 3.552\n",
      "Epoch  86 Batch  139/269   train_loss = 3.676\n",
      "Epoch  86 Batch  140/269   train_loss = 3.786\n",
      "Epoch  86 Batch  141/269   train_loss = 3.998\n",
      "Epoch  86 Batch  142/269   train_loss = 3.575\n",
      "Epoch  86 Batch  143/269   train_loss = 3.661\n",
      "Epoch  86 Batch  144/269   train_loss = 3.870\n",
      "Epoch  86 Batch  145/269   train_loss = 3.730\n",
      "Epoch  86 Batch  146/269   train_loss = 3.905\n",
      "Epoch  86 Batch  147/269   train_loss = 3.537\n",
      "Epoch  86 Batch  148/269   train_loss = 3.815\n",
      "Epoch  86 Batch  149/269   train_loss = 3.615\n",
      "Epoch  86 Batch  150/269   train_loss = 3.792\n",
      "Epoch  86 Batch  151/269   train_loss = 3.797\n",
      "Epoch  86 Batch  152/269   train_loss = 3.730\n",
      "Epoch  86 Batch  153/269   train_loss = 3.791\n",
      "Epoch  86 Batch  154/269   train_loss = 3.660\n",
      "Epoch  86 Batch  155/269   train_loss = 3.620\n",
      "Epoch  86 Batch  156/269   train_loss = 4.089\n",
      "Epoch  86 Batch  157/269   train_loss = 3.826\n",
      "Epoch  86 Batch  158/269   train_loss = 3.766\n",
      "Epoch  86 Batch  159/269   train_loss = 3.835\n",
      "Epoch  86 Batch  160/269   train_loss = 3.644\n",
      "Epoch  86 Batch  161/269   train_loss = 3.819\n",
      "Epoch  86 Batch  162/269   train_loss = 3.834\n",
      "Epoch  86 Batch  163/269   train_loss = 3.582\n",
      "Epoch  86 Batch  164/269   train_loss = 3.882\n",
      "Epoch  86 Batch  165/269   train_loss = 3.968\n",
      "Epoch  86 Batch  166/269   train_loss = 3.805\n",
      "Epoch  86 Batch  167/269   train_loss = 3.522\n",
      "Epoch  86 Batch  168/269   train_loss = 3.768\n",
      "Epoch  86 Batch  169/269   train_loss = 3.971\n",
      "Epoch  86 Batch  170/269   train_loss = 3.547\n",
      "Epoch  86 Batch  171/269   train_loss = 3.732\n",
      "Epoch  86 Batch  172/269   train_loss = 3.524\n",
      "Epoch  86 Batch  173/269   train_loss = 3.771\n",
      "Epoch  86 Batch  174/269   train_loss = 3.870\n",
      "Epoch  86 Batch  175/269   train_loss = 3.744\n",
      "Epoch  86 Batch  176/269   train_loss = 3.678\n",
      "Epoch  86 Batch  177/269   train_loss = 3.869\n",
      "Epoch  86 Batch  178/269   train_loss = 3.974\n",
      "Epoch  86 Batch  179/269   train_loss = 3.738\n",
      "Epoch  86 Batch  180/269   train_loss = 3.588\n",
      "Epoch  86 Batch  181/269   train_loss = 3.833\n",
      "Epoch  86 Batch  182/269   train_loss = 3.693\n",
      "Epoch  86 Batch  183/269   train_loss = 3.663\n",
      "Epoch  86 Batch  184/269   train_loss = 3.696\n",
      "Epoch  86 Batch  185/269   train_loss = 3.774\n",
      "Epoch  86 Batch  186/269   train_loss = 3.702\n",
      "Epoch  86 Batch  187/269   train_loss = 3.824\n",
      "Epoch  86 Batch  188/269   train_loss = 3.539\n",
      "Epoch  86 Batch  189/269   train_loss = 3.673\n",
      "Epoch  86 Batch  190/269   train_loss = 4.068\n",
      "Epoch  86 Batch  191/269   train_loss = 3.773\n",
      "Epoch  86 Batch  192/269   train_loss = 3.754\n",
      "Epoch  86 Batch  193/269   train_loss = 3.715\n",
      "Epoch  86 Batch  194/269   train_loss = 3.817\n",
      "Epoch  86 Batch  195/269   train_loss = 3.750\n",
      "Epoch  86 Batch  196/269   train_loss = 3.781\n",
      "Epoch  86 Batch  197/269   train_loss = 3.907\n",
      "Epoch  86 Batch  198/269   train_loss = 3.724\n",
      "Epoch  86 Batch  199/269   train_loss = 3.844\n",
      "Epoch  86 Batch  200/269   train_loss = 3.600\n",
      "Epoch  86 Batch  201/269   train_loss = 3.715\n",
      "Epoch  86 Batch  202/269   train_loss = 3.506\n",
      "Epoch  86 Batch  203/269   train_loss = 3.643\n",
      "Epoch  86 Batch  204/269   train_loss = 3.849\n",
      "Epoch  86 Batch  205/269   train_loss = 3.733\n",
      "Epoch  86 Batch  206/269   train_loss = 3.657\n",
      "Epoch  86 Batch  207/269   train_loss = 3.641\n",
      "Epoch  86 Batch  208/269   train_loss = 3.810\n",
      "Epoch  86 Batch  209/269   train_loss = 3.825\n",
      "Epoch  86 Batch  210/269   train_loss = 3.652\n",
      "Epoch  86 Batch  211/269   train_loss = 3.647\n",
      "Epoch  86 Batch  212/269   train_loss = 4.046\n",
      "Epoch  86 Batch  213/269   train_loss = 3.639\n",
      "Epoch  86 Batch  214/269   train_loss = 3.662\n",
      "Epoch  86 Batch  215/269   train_loss = 3.965\n",
      "Epoch  86 Batch  216/269   train_loss = 3.909\n",
      "Epoch  86 Batch  217/269   train_loss = 3.643\n",
      "Epoch  86 Batch  218/269   train_loss = 3.779\n",
      "Epoch  86 Batch  219/269   train_loss = 3.559\n",
      "Epoch  86 Batch  220/269   train_loss = 3.884\n",
      "Epoch  86 Batch  221/269   train_loss = 3.624\n",
      "Epoch  86 Batch  222/269   train_loss = 3.744\n",
      "Epoch  86 Batch  223/269   train_loss = 3.536\n",
      "Epoch  86 Batch  224/269   train_loss = 3.881\n",
      "Epoch  86 Batch  225/269   train_loss = 3.915\n",
      "Epoch  86 Batch  226/269   train_loss = 3.726\n",
      "Epoch  86 Batch  227/269   train_loss = 3.435\n",
      "Epoch  86 Batch  228/269   train_loss = 3.743\n",
      "Epoch  86 Batch  229/269   train_loss = 3.927\n",
      "Epoch  86 Batch  230/269   train_loss = 3.849\n",
      "Epoch  86 Batch  231/269   train_loss = 3.656\n",
      "Epoch  86 Batch  232/269   train_loss = 3.752\n",
      "Epoch  86 Batch  233/269   train_loss = 3.809\n",
      "Epoch  86 Batch  234/269   train_loss = 3.761\n",
      "Epoch  86 Batch  235/269   train_loss = 3.989\n",
      "Epoch  86 Batch  236/269   train_loss = 3.581\n",
      "Epoch  86 Batch  237/269   train_loss = 3.277\n",
      "Epoch  86 Batch  238/269   train_loss = 3.469\n",
      "Epoch  86 Batch  239/269   train_loss = 3.993\n",
      "Epoch  86 Batch  240/269   train_loss = 3.536\n",
      "Epoch  86 Batch  241/269   train_loss = 3.939\n",
      "Epoch  86 Batch  242/269   train_loss = 3.530\n",
      "Epoch  86 Batch  243/269   train_loss = 3.845\n",
      "Epoch  86 Batch  244/269   train_loss = 3.724\n",
      "Epoch  86 Batch  245/269   train_loss = 3.688\n",
      "Epoch  86 Batch  246/269   train_loss = 3.344\n",
      "Epoch  86 Batch  247/269   train_loss = 3.712\n",
      "Epoch  86 Batch  248/269   train_loss = 3.839\n",
      "Epoch  86 Batch  249/269   train_loss = 3.648\n",
      "Epoch  86 Batch  250/269   train_loss = 3.516\n",
      "Epoch  86 Batch  251/269   train_loss = 3.940\n",
      "Epoch  86 Batch  252/269   train_loss = 3.833\n",
      "Epoch  86 Batch  253/269   train_loss = 3.612\n",
      "Epoch  86 Batch  254/269   train_loss = 3.624\n",
      "Epoch  86 Batch  255/269   train_loss = 3.746\n",
      "Epoch  86 Batch  256/269   train_loss = 3.649\n",
      "Epoch  86 Batch  257/269   train_loss = 3.563\n",
      "Epoch  86 Batch  258/269   train_loss = 3.507\n",
      "Epoch  86 Batch  259/269   train_loss = 3.538\n",
      "Epoch  86 Batch  260/269   train_loss = 3.814\n",
      "Epoch  86 Batch  261/269   train_loss = 3.785\n",
      "Epoch  86 Batch  262/269   train_loss = 3.592\n",
      "Epoch  86 Batch  263/269   train_loss = 3.505\n",
      "Epoch  86 Batch  264/269   train_loss = 3.863\n",
      "Epoch  86 Batch  265/269   train_loss = 3.747\n",
      "Epoch  86 Batch  266/269   train_loss = 3.606\n",
      "Epoch  86 Batch  267/269   train_loss = 3.722\n",
      "Epoch  86 Batch  268/269   train_loss = 3.873\n",
      "Epoch  87 Batch    0/269   train_loss = 3.657\n",
      "Epoch  87 Batch    1/269   train_loss = 3.447\n",
      "Epoch  87 Batch    2/269   train_loss = 3.590\n",
      "Epoch  87 Batch    3/269   train_loss = 3.699\n",
      "Epoch  87 Batch    4/269   train_loss = 4.119\n",
      "Epoch  87 Batch    5/269   train_loss = 3.723\n",
      "Epoch  87 Batch    6/269   train_loss = 3.537\n",
      "Epoch  87 Batch    7/269   train_loss = 3.495\n",
      "Epoch  87 Batch    8/269   train_loss = 3.768\n",
      "Epoch  87 Batch    9/269   train_loss = 3.408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  87 Batch   10/269   train_loss = 3.584\n",
      "Epoch  87 Batch   11/269   train_loss = 3.688\n",
      "Epoch  87 Batch   12/269   train_loss = 3.366\n",
      "Epoch  87 Batch   13/269   train_loss = 3.708\n",
      "Epoch  87 Batch   14/269   train_loss = 3.619\n",
      "Epoch  87 Batch   15/269   train_loss = 3.979\n",
      "Epoch  87 Batch   16/269   train_loss = 3.623\n",
      "Epoch  87 Batch   17/269   train_loss = 3.711\n",
      "Epoch  87 Batch   18/269   train_loss = 3.686\n",
      "Epoch  87 Batch   19/269   train_loss = 3.727\n",
      "Epoch  87 Batch   20/269   train_loss = 4.019\n",
      "Epoch  87 Batch   21/269   train_loss = 3.893\n",
      "Epoch  87 Batch   22/269   train_loss = 3.640\n",
      "Epoch  87 Batch   23/269   train_loss = 3.760\n",
      "Epoch  87 Batch   24/269   train_loss = 3.491\n",
      "Epoch  87 Batch   25/269   train_loss = 3.890\n",
      "Epoch  87 Batch   26/269   train_loss = 3.707\n",
      "Epoch  87 Batch   27/269   train_loss = 3.619\n",
      "Epoch  87 Batch   28/269   train_loss = 3.763\n",
      "Epoch  87 Batch   29/269   train_loss = 3.768\n",
      "Epoch  87 Batch   30/269   train_loss = 4.027\n",
      "Epoch  87 Batch   31/269   train_loss = 3.769\n",
      "Epoch  87 Batch   32/269   train_loss = 3.618\n",
      "Epoch  87 Batch   33/269   train_loss = 3.392\n",
      "Epoch  87 Batch   34/269   train_loss = 3.684\n",
      "Epoch  87 Batch   35/269   train_loss = 3.494\n",
      "Epoch  87 Batch   36/269   train_loss = 3.513\n",
      "Epoch  87 Batch   37/269   train_loss = 3.730\n",
      "Epoch  87 Batch   38/269   train_loss = 3.437\n",
      "Epoch  87 Batch   39/269   train_loss = 3.807\n",
      "Epoch  87 Batch   40/269   train_loss = 3.574\n",
      "Epoch  87 Batch   41/269   train_loss = 3.640\n",
      "Epoch  87 Batch   42/269   train_loss = 4.044\n",
      "Epoch  87 Batch   43/269   train_loss = 3.971\n",
      "Epoch  87 Batch   44/269   train_loss = 3.737\n",
      "Epoch  87 Batch   45/269   train_loss = 3.454\n",
      "Epoch  87 Batch   46/269   train_loss = 3.730\n",
      "Epoch  87 Batch   47/269   train_loss = 3.378\n",
      "Epoch  87 Batch   48/269   train_loss = 3.965\n",
      "Epoch  87 Batch   49/269   train_loss = 3.838\n",
      "Epoch  87 Batch   50/269   train_loss = 3.800\n",
      "Epoch  87 Batch   51/269   train_loss = 3.781\n",
      "Epoch  87 Batch   52/269   train_loss = 3.765\n",
      "Epoch  87 Batch   53/269   train_loss = 3.990\n",
      "Epoch  87 Batch   54/269   train_loss = 3.621\n",
      "Epoch  87 Batch   55/269   train_loss = 3.759\n",
      "Epoch  87 Batch   56/269   train_loss = 3.518\n",
      "Epoch  87 Batch   57/269   train_loss = 3.894\n",
      "Epoch  87 Batch   58/269   train_loss = 3.606\n",
      "Epoch  87 Batch   59/269   train_loss = 3.651\n",
      "Epoch  87 Batch   60/269   train_loss = 3.780\n",
      "Epoch  87 Batch   61/269   train_loss = 3.942\n",
      "Epoch  87 Batch   62/269   train_loss = 3.505\n",
      "Epoch  87 Batch   63/269   train_loss = 3.939\n",
      "Epoch  87 Batch   64/269   train_loss = 3.715\n",
      "Epoch  87 Batch   65/269   train_loss = 3.792\n",
      "Epoch  87 Batch   66/269   train_loss = 3.621\n",
      "Epoch  87 Batch   67/269   train_loss = 3.651\n",
      "Epoch  87 Batch   68/269   train_loss = 3.765\n",
      "Epoch  87 Batch   69/269   train_loss = 3.748\n",
      "Epoch  87 Batch   70/269   train_loss = 3.544\n",
      "Epoch  87 Batch   71/269   train_loss = 3.748\n",
      "Epoch  87 Batch   72/269   train_loss = 3.480\n",
      "Epoch  87 Batch   73/269   train_loss = 3.661\n",
      "Epoch  87 Batch   74/269   train_loss = 3.702\n",
      "Epoch  87 Batch   75/269   train_loss = 3.912\n",
      "Epoch  87 Batch   76/269   train_loss = 3.842\n",
      "Epoch  87 Batch   77/269   train_loss = 3.446\n",
      "Epoch  87 Batch   78/269   train_loss = 3.858\n",
      "Epoch  87 Batch   79/269   train_loss = 3.877\n",
      "Epoch  87 Batch   80/269   train_loss = 3.814\n",
      "Epoch  87 Batch   81/269   train_loss = 3.631\n",
      "Epoch  87 Batch   82/269   train_loss = 3.826\n",
      "Epoch  87 Batch   83/269   train_loss = 3.657\n",
      "Epoch  87 Batch   84/269   train_loss = 3.967\n",
      "Epoch  87 Batch   85/269   train_loss = 4.032\n",
      "Epoch  87 Batch   86/269   train_loss = 4.090\n",
      "Epoch  87 Batch   87/269   train_loss = 3.749\n",
      "Epoch  87 Batch   88/269   train_loss = 3.851\n",
      "Epoch  87 Batch   89/269   train_loss = 3.756\n",
      "Epoch  87 Batch   90/269   train_loss = 3.615\n",
      "Epoch  87 Batch   91/269   train_loss = 3.828\n",
      "Epoch  87 Batch   92/269   train_loss = 3.847\n",
      "Epoch  87 Batch   93/269   train_loss = 3.609\n",
      "Epoch  87 Batch   94/269   train_loss = 4.065\n",
      "Epoch  87 Batch   95/269   train_loss = 3.499\n",
      "Epoch  87 Batch   96/269   train_loss = 3.635\n",
      "Epoch  87 Batch   97/269   train_loss = 3.617\n",
      "Epoch  87 Batch   98/269   train_loss = 3.932\n",
      "Epoch  87 Batch   99/269   train_loss = 3.766\n",
      "Epoch  87 Batch  100/269   train_loss = 3.726\n",
      "Epoch  87 Batch  101/269   train_loss = 3.787\n",
      "Epoch  87 Batch  102/269   train_loss = 3.595\n",
      "Epoch  87 Batch  103/269   train_loss = 3.713\n",
      "Epoch  87 Batch  104/269   train_loss = 3.647\n",
      "Epoch  87 Batch  105/269   train_loss = 3.850\n",
      "Epoch  87 Batch  106/269   train_loss = 4.009\n",
      "Epoch  87 Batch  107/269   train_loss = 3.938\n",
      "Epoch  87 Batch  108/269   train_loss = 3.739\n",
      "Epoch  87 Batch  109/269   train_loss = 3.809\n",
      "Epoch  87 Batch  110/269   train_loss = 3.816\n",
      "Epoch  87 Batch  111/269   train_loss = 3.903\n",
      "Epoch  87 Batch  112/269   train_loss = 3.873\n",
      "Epoch  87 Batch  113/269   train_loss = 3.782\n",
      "Epoch  87 Batch  114/269   train_loss = 3.688\n",
      "Epoch  87 Batch  115/269   train_loss = 3.608\n",
      "Epoch  87 Batch  116/269   train_loss = 3.509\n",
      "Epoch  87 Batch  117/269   train_loss = 3.638\n",
      "Epoch  87 Batch  118/269   train_loss = 3.865\n",
      "Epoch  87 Batch  119/269   train_loss = 3.582\n",
      "Epoch  87 Batch  120/269   train_loss = 3.620\n",
      "Epoch  87 Batch  121/269   train_loss = 3.722\n",
      "Epoch  87 Batch  122/269   train_loss = 3.671\n",
      "Epoch  87 Batch  123/269   train_loss = 3.505\n",
      "Epoch  87 Batch  124/269   train_loss = 3.866\n",
      "Epoch  87 Batch  125/269   train_loss = 3.516\n",
      "Epoch  87 Batch  126/269   train_loss = 3.420\n",
      "Epoch  87 Batch  127/269   train_loss = 3.738\n",
      "Epoch  87 Batch  128/269   train_loss = 3.699\n",
      "Epoch  87 Batch  129/269   train_loss = 3.617\n",
      "Epoch  87 Batch  130/269   train_loss = 3.792\n",
      "Epoch  87 Batch  131/269   train_loss = 3.401\n",
      "Epoch  87 Batch  132/269   train_loss = 3.750\n",
      "Epoch  87 Batch  133/269   train_loss = 3.452\n",
      "Epoch  87 Batch  134/269   train_loss = 3.875\n",
      "Epoch  87 Batch  135/269   train_loss = 3.588\n",
      "Epoch  87 Batch  136/269   train_loss = 3.453\n",
      "Epoch  87 Batch  137/269   train_loss = 3.848\n",
      "Epoch  87 Batch  138/269   train_loss = 3.535\n",
      "Epoch  87 Batch  139/269   train_loss = 3.666\n",
      "Epoch  87 Batch  140/269   train_loss = 3.794\n",
      "Epoch  87 Batch  141/269   train_loss = 3.981\n",
      "Epoch  87 Batch  142/269   train_loss = 3.544\n",
      "Epoch  87 Batch  143/269   train_loss = 3.653\n",
      "Epoch  87 Batch  144/269   train_loss = 3.883\n",
      "Epoch  87 Batch  145/269   train_loss = 3.711\n",
      "Epoch  87 Batch  146/269   train_loss = 3.900\n",
      "Epoch  87 Batch  147/269   train_loss = 3.513\n",
      "Epoch  87 Batch  148/269   train_loss = 3.797\n",
      "Epoch  87 Batch  149/269   train_loss = 3.614\n",
      "Epoch  87 Batch  150/269   train_loss = 3.826\n",
      "Epoch  87 Batch  151/269   train_loss = 3.807\n",
      "Epoch  87 Batch  152/269   train_loss = 3.712\n",
      "Epoch  87 Batch  153/269   train_loss = 3.802\n",
      "Epoch  87 Batch  154/269   train_loss = 3.665\n",
      "Epoch  87 Batch  155/269   train_loss = 3.637\n",
      "Epoch  87 Batch  156/269   train_loss = 4.097\n",
      "Epoch  87 Batch  157/269   train_loss = 3.862\n",
      "Epoch  87 Batch  158/269   train_loss = 3.780\n",
      "Epoch  87 Batch  159/269   train_loss = 3.859\n",
      "Epoch  87 Batch  160/269   train_loss = 3.612\n",
      "Epoch  87 Batch  161/269   train_loss = 3.849\n",
      "Epoch  87 Batch  162/269   train_loss = 3.842\n",
      "Epoch  87 Batch  163/269   train_loss = 3.629\n",
      "Epoch  87 Batch  164/269   train_loss = 3.886\n",
      "Epoch  87 Batch  165/269   train_loss = 3.944\n",
      "Epoch  87 Batch  166/269   train_loss = 3.802\n",
      "Epoch  87 Batch  167/269   train_loss = 3.545\n",
      "Epoch  87 Batch  168/269   train_loss = 3.723\n",
      "Epoch  87 Batch  169/269   train_loss = 3.969\n",
      "Epoch  87 Batch  170/269   train_loss = 3.552\n",
      "Epoch  87 Batch  171/269   train_loss = 3.742\n",
      "Epoch  87 Batch  172/269   train_loss = 3.520\n",
      "Epoch  87 Batch  173/269   train_loss = 3.791\n",
      "Epoch  87 Batch  174/269   train_loss = 3.912\n",
      "Epoch  87 Batch  175/269   train_loss = 3.771\n",
      "Epoch  87 Batch  176/269   train_loss = 3.682\n",
      "Epoch  87 Batch  177/269   train_loss = 3.859\n",
      "Epoch  87 Batch  178/269   train_loss = 3.935\n",
      "Epoch  87 Batch  179/269   train_loss = 3.728\n",
      "Epoch  87 Batch  180/269   train_loss = 3.602\n",
      "Epoch  87 Batch  181/269   train_loss = 3.838\n",
      "Epoch  87 Batch  182/269   train_loss = 3.664\n",
      "Epoch  87 Batch  183/269   train_loss = 3.685\n",
      "Epoch  87 Batch  184/269   train_loss = 3.710\n",
      "Epoch  87 Batch  185/269   train_loss = 3.800\n",
      "Epoch  87 Batch  186/269   train_loss = 3.725\n",
      "Epoch  87 Batch  187/269   train_loss = 3.857\n",
      "Epoch  87 Batch  188/269   train_loss = 3.539\n",
      "Epoch  87 Batch  189/269   train_loss = 3.648\n",
      "Epoch  87 Batch  190/269   train_loss = 4.088\n",
      "Epoch  87 Batch  191/269   train_loss = 3.725\n",
      "Epoch  87 Batch  192/269   train_loss = 3.785\n",
      "Epoch  87 Batch  193/269   train_loss = 3.739\n",
      "Epoch  87 Batch  194/269   train_loss = 3.801\n",
      "Epoch  87 Batch  195/269   train_loss = 3.753\n",
      "Epoch  87 Batch  196/269   train_loss = 3.778\n",
      "Epoch  87 Batch  197/269   train_loss = 3.926\n",
      "Epoch  87 Batch  198/269   train_loss = 3.739\n",
      "Epoch  87 Batch  199/269   train_loss = 3.860\n",
      "Epoch  87 Batch  200/269   train_loss = 3.636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  87 Batch  201/269   train_loss = 3.689\n",
      "Epoch  87 Batch  202/269   train_loss = 3.505\n",
      "Epoch  87 Batch  203/269   train_loss = 3.618\n",
      "Epoch  87 Batch  204/269   train_loss = 3.834\n",
      "Epoch  87 Batch  205/269   train_loss = 3.730\n",
      "Epoch  87 Batch  206/269   train_loss = 3.667\n",
      "Epoch  87 Batch  207/269   train_loss = 3.663\n",
      "Epoch  87 Batch  208/269   train_loss = 3.840\n",
      "Epoch  87 Batch  209/269   train_loss = 3.815\n",
      "Epoch  87 Batch  210/269   train_loss = 3.709\n",
      "Epoch  87 Batch  211/269   train_loss = 3.641\n",
      "Epoch  87 Batch  212/269   train_loss = 4.058\n",
      "Epoch  87 Batch  213/269   train_loss = 3.618\n",
      "Epoch  87 Batch  214/269   train_loss = 3.644\n",
      "Epoch  87 Batch  215/269   train_loss = 3.977\n",
      "Epoch  87 Batch  216/269   train_loss = 3.950\n",
      "Epoch  87 Batch  217/269   train_loss = 3.603\n",
      "Epoch  87 Batch  218/269   train_loss = 3.779\n",
      "Epoch  87 Batch  219/269   train_loss = 3.549\n",
      "Epoch  87 Batch  220/269   train_loss = 3.870\n",
      "Epoch  87 Batch  221/269   train_loss = 3.579\n",
      "Epoch  87 Batch  222/269   train_loss = 3.751\n",
      "Epoch  87 Batch  223/269   train_loss = 3.567\n",
      "Epoch  87 Batch  224/269   train_loss = 3.882\n",
      "Epoch  87 Batch  225/269   train_loss = 3.887\n",
      "Epoch  87 Batch  226/269   train_loss = 3.692\n",
      "Epoch  87 Batch  227/269   train_loss = 3.452\n",
      "Epoch  87 Batch  228/269   train_loss = 3.750\n",
      "Epoch  87 Batch  229/269   train_loss = 3.927\n",
      "Epoch  87 Batch  230/269   train_loss = 3.832\n",
      "Epoch  87 Batch  231/269   train_loss = 3.702\n",
      "Epoch  87 Batch  232/269   train_loss = 3.762\n",
      "Epoch  87 Batch  233/269   train_loss = 3.801\n",
      "Epoch  87 Batch  234/269   train_loss = 3.756\n",
      "Epoch  87 Batch  235/269   train_loss = 3.960\n",
      "Epoch  87 Batch  236/269   train_loss = 3.572\n",
      "Epoch  87 Batch  237/269   train_loss = 3.280\n",
      "Epoch  87 Batch  238/269   train_loss = 3.479\n",
      "Epoch  87 Batch  239/269   train_loss = 4.000\n",
      "Epoch  87 Batch  240/269   train_loss = 3.538\n",
      "Epoch  87 Batch  241/269   train_loss = 3.947\n",
      "Epoch  87 Batch  242/269   train_loss = 3.539\n",
      "Epoch  87 Batch  243/269   train_loss = 3.840\n",
      "Epoch  87 Batch  244/269   train_loss = 3.665\n",
      "Epoch  87 Batch  245/269   train_loss = 3.749\n",
      "Epoch  87 Batch  246/269   train_loss = 3.348\n",
      "Epoch  87 Batch  247/269   train_loss = 3.710\n",
      "Epoch  87 Batch  248/269   train_loss = 3.930\n",
      "Epoch  87 Batch  249/269   train_loss = 3.651\n",
      "Epoch  87 Batch  250/269   train_loss = 3.535\n",
      "Epoch  87 Batch  251/269   train_loss = 3.913\n",
      "Epoch  87 Batch  252/269   train_loss = 3.840\n",
      "Epoch  87 Batch  253/269   train_loss = 3.612\n",
      "Epoch  87 Batch  254/269   train_loss = 3.678\n",
      "Epoch  87 Batch  255/269   train_loss = 3.772\n",
      "Epoch  87 Batch  256/269   train_loss = 3.648\n",
      "Epoch  87 Batch  257/269   train_loss = 3.606\n",
      "Epoch  87 Batch  258/269   train_loss = 3.516\n",
      "Epoch  87 Batch  259/269   train_loss = 3.597\n",
      "Epoch  87 Batch  260/269   train_loss = 3.823\n",
      "Epoch  87 Batch  261/269   train_loss = 3.817\n",
      "Epoch  87 Batch  262/269   train_loss = 3.603\n",
      "Epoch  87 Batch  263/269   train_loss = 3.556\n",
      "Epoch  87 Batch  264/269   train_loss = 3.862\n",
      "Epoch  87 Batch  265/269   train_loss = 3.733\n",
      "Epoch  87 Batch  266/269   train_loss = 3.665\n",
      "Epoch  87 Batch  267/269   train_loss = 3.744\n",
      "Epoch  87 Batch  268/269   train_loss = 3.899\n",
      "Epoch  88 Batch    0/269   train_loss = 3.738\n",
      "Epoch  88 Batch    1/269   train_loss = 3.458\n",
      "Epoch  88 Batch    2/269   train_loss = 3.614\n",
      "Epoch  88 Batch    3/269   train_loss = 3.640\n",
      "Epoch  88 Batch    4/269   train_loss = 4.164\n",
      "Epoch  88 Batch    5/269   train_loss = 3.774\n",
      "Epoch  88 Batch    6/269   train_loss = 3.570\n",
      "Epoch  88 Batch    7/269   train_loss = 3.502\n",
      "Epoch  88 Batch    8/269   train_loss = 3.806\n",
      "Epoch  88 Batch    9/269   train_loss = 3.444\n",
      "Epoch  88 Batch   10/269   train_loss = 3.647\n",
      "Epoch  88 Batch   11/269   train_loss = 3.736\n",
      "Epoch  88 Batch   12/269   train_loss = 3.411\n",
      "Epoch  88 Batch   13/269   train_loss = 3.723\n",
      "Epoch  88 Batch   14/269   train_loss = 3.602\n",
      "Epoch  88 Batch   15/269   train_loss = 3.965\n",
      "Epoch  88 Batch   16/269   train_loss = 3.617\n",
      "Epoch  88 Batch   17/269   train_loss = 3.723\n",
      "Epoch  88 Batch   18/269   train_loss = 3.686\n",
      "Epoch  88 Batch   19/269   train_loss = 3.724\n",
      "Epoch  88 Batch   20/269   train_loss = 4.022\n",
      "Epoch  88 Batch   21/269   train_loss = 3.889\n",
      "Epoch  88 Batch   22/269   train_loss = 3.647\n",
      "Epoch  88 Batch   23/269   train_loss = 3.770\n",
      "Epoch  88 Batch   24/269   train_loss = 3.577\n",
      "Epoch  88 Batch   25/269   train_loss = 3.926\n",
      "Epoch  88 Batch   26/269   train_loss = 3.689\n",
      "Epoch  88 Batch   27/269   train_loss = 3.610\n",
      "Epoch  88 Batch   28/269   train_loss = 3.716\n",
      "Epoch  88 Batch   29/269   train_loss = 3.737\n",
      "Epoch  88 Batch   30/269   train_loss = 4.038\n",
      "Epoch  88 Batch   31/269   train_loss = 3.728\n",
      "Epoch  88 Batch   32/269   train_loss = 3.601\n",
      "Epoch  88 Batch   33/269   train_loss = 3.406\n",
      "Epoch  88 Batch   34/269   train_loss = 3.677\n",
      "Epoch  88 Batch   35/269   train_loss = 3.465\n",
      "Epoch  88 Batch   36/269   train_loss = 3.543\n",
      "Epoch  88 Batch   37/269   train_loss = 3.729\n",
      "Epoch  88 Batch   38/269   train_loss = 3.433\n",
      "Epoch  88 Batch   39/269   train_loss = 3.844\n",
      "Epoch  88 Batch   40/269   train_loss = 3.577\n",
      "Epoch  88 Batch   41/269   train_loss = 3.635\n",
      "Epoch  88 Batch   42/269   train_loss = 4.021\n",
      "Epoch  88 Batch   43/269   train_loss = 3.981\n",
      "Epoch  88 Batch   44/269   train_loss = 3.733\n",
      "Epoch  88 Batch   45/269   train_loss = 3.437\n",
      "Epoch  88 Batch   46/269   train_loss = 3.734\n",
      "Epoch  88 Batch   47/269   train_loss = 3.409\n",
      "Epoch  88 Batch   48/269   train_loss = 3.941\n",
      "Epoch  88 Batch   49/269   train_loss = 3.846\n",
      "Epoch  88 Batch   50/269   train_loss = 3.804\n",
      "Epoch  88 Batch   51/269   train_loss = 3.773\n",
      "Epoch  88 Batch   52/269   train_loss = 3.762\n",
      "Epoch  88 Batch   53/269   train_loss = 3.977\n",
      "Epoch  88 Batch   54/269   train_loss = 3.620\n",
      "Epoch  88 Batch   55/269   train_loss = 3.743\n",
      "Epoch  88 Batch   56/269   train_loss = 3.516\n",
      "Epoch  88 Batch   57/269   train_loss = 3.887\n",
      "Epoch  88 Batch   58/269   train_loss = 3.662\n",
      "Epoch  88 Batch   59/269   train_loss = 3.646\n",
      "Epoch  88 Batch   60/269   train_loss = 3.772\n",
      "Epoch  88 Batch   61/269   train_loss = 3.947\n",
      "Epoch  88 Batch   62/269   train_loss = 3.533\n",
      "Epoch  88 Batch   63/269   train_loss = 3.898\n",
      "Epoch  88 Batch   64/269   train_loss = 3.728\n",
      "Epoch  88 Batch   65/269   train_loss = 3.807\n",
      "Epoch  88 Batch   66/269   train_loss = 3.609\n",
      "Epoch  88 Batch   67/269   train_loss = 3.630\n",
      "Epoch  88 Batch   68/269   train_loss = 3.758\n",
      "Epoch  88 Batch   69/269   train_loss = 3.732\n",
      "Epoch  88 Batch   70/269   train_loss = 3.538\n",
      "Epoch  88 Batch   71/269   train_loss = 3.737\n",
      "Epoch  88 Batch   72/269   train_loss = 3.488\n",
      "Epoch  88 Batch   73/269   train_loss = 3.659\n",
      "Epoch  88 Batch   74/269   train_loss = 3.692\n",
      "Epoch  88 Batch   75/269   train_loss = 3.904\n",
      "Epoch  88 Batch   76/269   train_loss = 3.850\n",
      "Epoch  88 Batch   77/269   train_loss = 3.469\n",
      "Epoch  88 Batch   78/269   train_loss = 3.826\n",
      "Epoch  88 Batch   79/269   train_loss = 3.854\n",
      "Epoch  88 Batch   80/269   train_loss = 3.824\n",
      "Epoch  88 Batch   81/269   train_loss = 3.618\n",
      "Epoch  88 Batch   82/269   train_loss = 3.825\n",
      "Epoch  88 Batch   83/269   train_loss = 3.658\n",
      "Epoch  88 Batch   84/269   train_loss = 3.946\n",
      "Epoch  88 Batch   85/269   train_loss = 4.054\n",
      "Epoch  88 Batch   86/269   train_loss = 4.063\n",
      "Epoch  88 Batch   87/269   train_loss = 3.755\n",
      "Epoch  88 Batch   88/269   train_loss = 3.849\n",
      "Epoch  88 Batch   89/269   train_loss = 3.727\n",
      "Epoch  88 Batch   90/269   train_loss = 3.622\n",
      "Epoch  88 Batch   91/269   train_loss = 3.830\n",
      "Epoch  88 Batch   92/269   train_loss = 3.846\n",
      "Epoch  88 Batch   93/269   train_loss = 3.633\n",
      "Epoch  88 Batch   94/269   train_loss = 4.092\n",
      "Epoch  88 Batch   95/269   train_loss = 3.513\n",
      "Epoch  88 Batch   96/269   train_loss = 3.651\n",
      "Epoch  88 Batch   97/269   train_loss = 3.625\n",
      "Epoch  88 Batch   98/269   train_loss = 3.918\n",
      "Epoch  88 Batch   99/269   train_loss = 3.756\n",
      "Epoch  88 Batch  100/269   train_loss = 3.709\n",
      "Epoch  88 Batch  101/269   train_loss = 3.749\n",
      "Epoch  88 Batch  102/269   train_loss = 3.564\n",
      "Epoch  88 Batch  103/269   train_loss = 3.699\n",
      "Epoch  88 Batch  104/269   train_loss = 3.681\n",
      "Epoch  88 Batch  105/269   train_loss = 3.850\n",
      "Epoch  88 Batch  106/269   train_loss = 3.994\n",
      "Epoch  88 Batch  107/269   train_loss = 3.931\n",
      "Epoch  88 Batch  108/269   train_loss = 3.760\n",
      "Epoch  88 Batch  109/269   train_loss = 3.808\n",
      "Epoch  88 Batch  110/269   train_loss = 3.794\n",
      "Epoch  88 Batch  111/269   train_loss = 3.889\n",
      "Epoch  88 Batch  112/269   train_loss = 3.878\n",
      "Epoch  88 Batch  113/269   train_loss = 3.802\n",
      "Epoch  88 Batch  114/269   train_loss = 3.693\n",
      "Epoch  88 Batch  115/269   train_loss = 3.612\n",
      "Epoch  88 Batch  116/269   train_loss = 3.525\n",
      "Epoch  88 Batch  117/269   train_loss = 3.632\n",
      "Epoch  88 Batch  118/269   train_loss = 3.868\n",
      "Epoch  88 Batch  119/269   train_loss = 3.577\n",
      "Epoch  88 Batch  120/269   train_loss = 3.628\n",
      "Epoch  88 Batch  121/269   train_loss = 3.707\n",
      "Epoch  88 Batch  122/269   train_loss = 3.684\n",
      "Epoch  88 Batch  123/269   train_loss = 3.491\n",
      "Epoch  88 Batch  124/269   train_loss = 3.859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  88 Batch  125/269   train_loss = 3.541\n",
      "Epoch  88 Batch  126/269   train_loss = 3.402\n",
      "Epoch  88 Batch  127/269   train_loss = 3.722\n",
      "Epoch  88 Batch  128/269   train_loss = 3.696\n",
      "Epoch  88 Batch  129/269   train_loss = 3.615\n",
      "Epoch  88 Batch  130/269   train_loss = 3.779\n",
      "Epoch  88 Batch  131/269   train_loss = 3.415\n",
      "Epoch  88 Batch  132/269   train_loss = 3.749\n",
      "Epoch  88 Batch  133/269   train_loss = 3.454\n",
      "Epoch  88 Batch  134/269   train_loss = 3.875\n",
      "Epoch  88 Batch  135/269   train_loss = 3.600\n",
      "Epoch  88 Batch  136/269   train_loss = 3.437\n",
      "Epoch  88 Batch  137/269   train_loss = 3.835\n",
      "Epoch  88 Batch  138/269   train_loss = 3.521\n",
      "Epoch  88 Batch  139/269   train_loss = 3.642\n",
      "Epoch  88 Batch  140/269   train_loss = 3.778\n",
      "Epoch  88 Batch  141/269   train_loss = 3.986\n",
      "Epoch  88 Batch  142/269   train_loss = 3.563\n",
      "Epoch  88 Batch  143/269   train_loss = 3.663\n",
      "Epoch  88 Batch  144/269   train_loss = 3.882\n",
      "Epoch  88 Batch  145/269   train_loss = 3.697\n",
      "Epoch  88 Batch  146/269   train_loss = 3.914\n",
      "Epoch  88 Batch  147/269   train_loss = 3.504\n",
      "Epoch  88 Batch  148/269   train_loss = 3.796\n",
      "Epoch  88 Batch  149/269   train_loss = 3.602\n",
      "Epoch  88 Batch  150/269   train_loss = 3.808\n",
      "Epoch  88 Batch  151/269   train_loss = 3.802\n",
      "Epoch  88 Batch  152/269   train_loss = 3.706\n",
      "Epoch  88 Batch  153/269   train_loss = 3.815\n",
      "Epoch  88 Batch  154/269   train_loss = 3.656\n",
      "Epoch  88 Batch  155/269   train_loss = 3.585\n",
      "Epoch  88 Batch  156/269   train_loss = 4.105\n",
      "Epoch  88 Batch  157/269   train_loss = 3.834\n",
      "Epoch  88 Batch  158/269   train_loss = 3.761\n",
      "Epoch  88 Batch  159/269   train_loss = 3.839\n",
      "Epoch  88 Batch  160/269   train_loss = 3.613\n",
      "Epoch  88 Batch  161/269   train_loss = 3.849\n",
      "Epoch  88 Batch  162/269   train_loss = 3.819\n",
      "Epoch  88 Batch  163/269   train_loss = 3.646\n",
      "Epoch  88 Batch  164/269   train_loss = 3.857\n",
      "Epoch  88 Batch  165/269   train_loss = 3.973\n",
      "Epoch  88 Batch  166/269   train_loss = 3.771\n",
      "Epoch  88 Batch  167/269   train_loss = 3.531\n",
      "Epoch  88 Batch  168/269   train_loss = 3.718\n",
      "Epoch  88 Batch  169/269   train_loss = 3.966\n",
      "Epoch  88 Batch  170/269   train_loss = 3.547\n",
      "Epoch  88 Batch  171/269   train_loss = 3.778\n",
      "Epoch  88 Batch  172/269   train_loss = 3.516\n",
      "Epoch  88 Batch  173/269   train_loss = 3.758\n",
      "Epoch  88 Batch  174/269   train_loss = 3.920\n",
      "Epoch  88 Batch  175/269   train_loss = 3.783\n",
      "Epoch  88 Batch  176/269   train_loss = 3.675\n",
      "Epoch  88 Batch  177/269   train_loss = 3.857\n",
      "Epoch  88 Batch  178/269   train_loss = 3.953\n",
      "Epoch  88 Batch  179/269   train_loss = 3.745\n",
      "Epoch  88 Batch  180/269   train_loss = 3.592\n",
      "Epoch  88 Batch  181/269   train_loss = 3.837\n",
      "Epoch  88 Batch  182/269   train_loss = 3.634\n",
      "Epoch  88 Batch  183/269   train_loss = 3.659\n",
      "Epoch  88 Batch  184/269   train_loss = 3.677\n",
      "Epoch  88 Batch  185/269   train_loss = 3.756\n",
      "Epoch  88 Batch  186/269   train_loss = 3.698\n",
      "Epoch  88 Batch  187/269   train_loss = 3.852\n",
      "Epoch  88 Batch  188/269   train_loss = 3.576\n",
      "Epoch  88 Batch  189/269   train_loss = 3.648\n",
      "Epoch  88 Batch  190/269   train_loss = 4.077\n",
      "Epoch  88 Batch  191/269   train_loss = 3.747\n",
      "Epoch  88 Batch  192/269   train_loss = 3.771\n",
      "Epoch  88 Batch  193/269   train_loss = 3.717\n",
      "Epoch  88 Batch  194/269   train_loss = 3.793\n",
      "Epoch  88 Batch  195/269   train_loss = 3.750\n",
      "Epoch  88 Batch  196/269   train_loss = 3.760\n",
      "Epoch  88 Batch  197/269   train_loss = 3.908\n",
      "Epoch  88 Batch  198/269   train_loss = 3.723\n",
      "Epoch  88 Batch  199/269   train_loss = 3.834\n",
      "Epoch  88 Batch  200/269   train_loss = 3.549\n",
      "Epoch  88 Batch  201/269   train_loss = 3.680\n",
      "Epoch  88 Batch  202/269   train_loss = 3.531\n",
      "Epoch  88 Batch  203/269   train_loss = 3.621\n",
      "Epoch  88 Batch  204/269   train_loss = 3.825\n",
      "Epoch  88 Batch  205/269   train_loss = 3.724\n",
      "Epoch  88 Batch  206/269   train_loss = 3.640\n",
      "Epoch  88 Batch  207/269   train_loss = 3.669\n",
      "Epoch  88 Batch  208/269   train_loss = 3.823\n",
      "Epoch  88 Batch  209/269   train_loss = 3.810\n",
      "Epoch  88 Batch  210/269   train_loss = 3.690\n",
      "Epoch  88 Batch  211/269   train_loss = 3.640\n",
      "Epoch  88 Batch  212/269   train_loss = 4.098\n",
      "Epoch  88 Batch  213/269   train_loss = 3.590\n",
      "Epoch  88 Batch  214/269   train_loss = 3.639\n",
      "Epoch  88 Batch  215/269   train_loss = 3.977\n",
      "Epoch  88 Batch  216/269   train_loss = 3.959\n",
      "Epoch  88 Batch  217/269   train_loss = 3.576\n",
      "Epoch  88 Batch  218/269   train_loss = 3.780\n",
      "Epoch  88 Batch  219/269   train_loss = 3.555\n",
      "Epoch  88 Batch  220/269   train_loss = 3.875\n",
      "Epoch  88 Batch  221/269   train_loss = 3.564\n",
      "Epoch  88 Batch  222/269   train_loss = 3.758\n",
      "Epoch  88 Batch  223/269   train_loss = 3.546\n",
      "Epoch  88 Batch  224/269   train_loss = 3.859\n",
      "Epoch  88 Batch  225/269   train_loss = 3.897\n",
      "Epoch  88 Batch  226/269   train_loss = 3.690\n",
      "Epoch  88 Batch  227/269   train_loss = 3.474\n",
      "Epoch  88 Batch  228/269   train_loss = 3.746\n",
      "Epoch  88 Batch  229/269   train_loss = 3.959\n",
      "Epoch  88 Batch  230/269   train_loss = 3.799\n",
      "Epoch  88 Batch  231/269   train_loss = 3.652\n",
      "Epoch  88 Batch  232/269   train_loss = 3.767\n",
      "Epoch  88 Batch  233/269   train_loss = 3.779\n",
      "Epoch  88 Batch  234/269   train_loss = 3.757\n",
      "Epoch  88 Batch  235/269   train_loss = 3.959\n",
      "Epoch  88 Batch  236/269   train_loss = 3.542\n",
      "Epoch  88 Batch  237/269   train_loss = 3.239\n",
      "Epoch  88 Batch  238/269   train_loss = 3.499\n",
      "Epoch  88 Batch  239/269   train_loss = 3.988\n",
      "Epoch  88 Batch  240/269   train_loss = 3.548\n",
      "Epoch  88 Batch  241/269   train_loss = 3.936\n",
      "Epoch  88 Batch  242/269   train_loss = 3.571\n",
      "Epoch  88 Batch  243/269   train_loss = 3.832\n",
      "Epoch  88 Batch  244/269   train_loss = 3.666\n",
      "Epoch  88 Batch  245/269   train_loss = 3.727\n",
      "Epoch  88 Batch  246/269   train_loss = 3.316\n",
      "Epoch  88 Batch  247/269   train_loss = 3.747\n",
      "Epoch  88 Batch  248/269   train_loss = 3.857\n",
      "Epoch  88 Batch  249/269   train_loss = 3.641\n",
      "Epoch  88 Batch  250/269   train_loss = 3.531\n",
      "Epoch  88 Batch  251/269   train_loss = 3.911\n",
      "Epoch  88 Batch  252/269   train_loss = 3.831\n",
      "Epoch  88 Batch  253/269   train_loss = 3.606\n",
      "Epoch  88 Batch  254/269   train_loss = 3.618\n",
      "Epoch  88 Batch  255/269   train_loss = 3.737\n",
      "Epoch  88 Batch  256/269   train_loss = 3.628\n",
      "Epoch  88 Batch  257/269   train_loss = 3.579\n",
      "Epoch  88 Batch  258/269   train_loss = 3.504\n",
      "Epoch  88 Batch  259/269   train_loss = 3.547\n",
      "Epoch  88 Batch  260/269   train_loss = 3.795\n",
      "Epoch  88 Batch  261/269   train_loss = 3.815\n",
      "Epoch  88 Batch  262/269   train_loss = 3.593\n",
      "Epoch  88 Batch  263/269   train_loss = 3.506\n",
      "Epoch  88 Batch  264/269   train_loss = 3.840\n",
      "Epoch  88 Batch  265/269   train_loss = 3.733\n",
      "Epoch  88 Batch  266/269   train_loss = 3.640\n",
      "Epoch  88 Batch  267/269   train_loss = 3.732\n",
      "Epoch  88 Batch  268/269   train_loss = 3.872\n",
      "Epoch  89 Batch    0/269   train_loss = 3.688\n",
      "Epoch  89 Batch    1/269   train_loss = 3.457\n",
      "Epoch  89 Batch    2/269   train_loss = 3.593\n",
      "Epoch  89 Batch    3/269   train_loss = 3.634\n",
      "Epoch  89 Batch    4/269   train_loss = 4.146\n",
      "Epoch  89 Batch    5/269   train_loss = 3.755\n",
      "Epoch  89 Batch    6/269   train_loss = 3.569\n",
      "Epoch  89 Batch    7/269   train_loss = 3.492\n",
      "Epoch  89 Batch    8/269   train_loss = 3.793\n",
      "Epoch  89 Batch    9/269   train_loss = 3.434\n",
      "Epoch  89 Batch   10/269   train_loss = 3.631\n",
      "Epoch  89 Batch   11/269   train_loss = 3.678\n",
      "Epoch  89 Batch   12/269   train_loss = 3.393\n",
      "Epoch  89 Batch   13/269   train_loss = 3.703\n",
      "Epoch  89 Batch   14/269   train_loss = 3.575\n",
      "Epoch  89 Batch   15/269   train_loss = 3.906\n",
      "Epoch  89 Batch   16/269   train_loss = 3.620\n",
      "Epoch  89 Batch   17/269   train_loss = 3.697\n",
      "Epoch  89 Batch   18/269   train_loss = 3.677\n",
      "Epoch  89 Batch   19/269   train_loss = 3.710\n",
      "Epoch  89 Batch   20/269   train_loss = 4.052\n",
      "Epoch  89 Batch   21/269   train_loss = 3.890\n",
      "Epoch  89 Batch   22/269   train_loss = 3.649\n",
      "Epoch  89 Batch   23/269   train_loss = 3.772\n",
      "Epoch  89 Batch   24/269   train_loss = 3.608\n",
      "Epoch  89 Batch   25/269   train_loss = 3.897\n",
      "Epoch  89 Batch   26/269   train_loss = 3.687\n",
      "Epoch  89 Batch   27/269   train_loss = 3.593\n",
      "Epoch  89 Batch   28/269   train_loss = 3.741\n",
      "Epoch  89 Batch   29/269   train_loss = 3.726\n",
      "Epoch  89 Batch   30/269   train_loss = 4.055\n",
      "Epoch  89 Batch   31/269   train_loss = 3.699\n",
      "Epoch  89 Batch   32/269   train_loss = 3.567\n",
      "Epoch  89 Batch   33/269   train_loss = 3.414\n",
      "Epoch  89 Batch   34/269   train_loss = 3.665\n",
      "Epoch  89 Batch   35/269   train_loss = 3.483\n",
      "Epoch  89 Batch   36/269   train_loss = 3.524\n",
      "Epoch  89 Batch   37/269   train_loss = 3.746\n",
      "Epoch  89 Batch   38/269   train_loss = 3.420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  89 Batch   39/269   train_loss = 3.812\n",
      "Epoch  89 Batch   40/269   train_loss = 3.588\n",
      "Epoch  89 Batch   41/269   train_loss = 3.610\n",
      "Epoch  89 Batch   42/269   train_loss = 3.998\n",
      "Epoch  89 Batch   43/269   train_loss = 3.966\n",
      "Epoch  89 Batch   44/269   train_loss = 3.729\n",
      "Epoch  89 Batch   45/269   train_loss = 3.421\n",
      "Epoch  89 Batch   46/269   train_loss = 3.709\n",
      "Epoch  89 Batch   47/269   train_loss = 3.391\n",
      "Epoch  89 Batch   48/269   train_loss = 3.953\n",
      "Epoch  89 Batch   49/269   train_loss = 3.870\n",
      "Epoch  89 Batch   50/269   train_loss = 3.810\n",
      "Epoch  89 Batch   51/269   train_loss = 3.759\n",
      "Epoch  89 Batch   52/269   train_loss = 3.754\n",
      "Epoch  89 Batch   53/269   train_loss = 3.976\n",
      "Epoch  89 Batch   54/269   train_loss = 3.624\n",
      "Epoch  89 Batch   55/269   train_loss = 3.740\n",
      "Epoch  89 Batch   56/269   train_loss = 3.508\n",
      "Epoch  89 Batch   57/269   train_loss = 3.879\n",
      "Epoch  89 Batch   58/269   train_loss = 3.566\n",
      "Epoch  89 Batch   59/269   train_loss = 3.629\n",
      "Epoch  89 Batch   60/269   train_loss = 3.769\n",
      "Epoch  89 Batch   61/269   train_loss = 3.935\n",
      "Epoch  89 Batch   62/269   train_loss = 3.572\n",
      "Epoch  89 Batch   63/269   train_loss = 3.904\n",
      "Epoch  89 Batch   64/269   train_loss = 3.677\n",
      "Epoch  89 Batch   65/269   train_loss = 3.784\n",
      "Epoch  89 Batch   66/269   train_loss = 3.600\n",
      "Epoch  89 Batch   67/269   train_loss = 3.625\n",
      "Epoch  89 Batch   68/269   train_loss = 3.757\n",
      "Epoch  89 Batch   69/269   train_loss = 3.761\n",
      "Epoch  89 Batch   70/269   train_loss = 3.542\n",
      "Epoch  89 Batch   71/269   train_loss = 3.722\n",
      "Epoch  89 Batch   72/269   train_loss = 3.491\n",
      "Epoch  89 Batch   73/269   train_loss = 3.679\n",
      "Epoch  89 Batch   74/269   train_loss = 3.746\n",
      "Epoch  89 Batch   75/269   train_loss = 3.883\n",
      "Epoch  89 Batch   76/269   train_loss = 3.840\n",
      "Epoch  89 Batch   77/269   train_loss = 3.454\n",
      "Epoch  89 Batch   78/269   train_loss = 3.822\n",
      "Epoch  89 Batch   79/269   train_loss = 3.855\n",
      "Epoch  89 Batch   80/269   train_loss = 3.792\n",
      "Epoch  89 Batch   81/269   train_loss = 3.611\n",
      "Epoch  89 Batch   82/269   train_loss = 3.778\n",
      "Epoch  89 Batch   83/269   train_loss = 3.652\n",
      "Epoch  89 Batch   84/269   train_loss = 3.927\n",
      "Epoch  89 Batch   85/269   train_loss = 4.025\n",
      "Epoch  89 Batch   86/269   train_loss = 4.052\n",
      "Epoch  89 Batch   87/269   train_loss = 3.750\n",
      "Epoch  89 Batch   88/269   train_loss = 3.811\n",
      "Epoch  89 Batch   89/269   train_loss = 3.724\n",
      "Epoch  89 Batch   90/269   train_loss = 3.627\n",
      "Epoch  89 Batch   91/269   train_loss = 3.825\n",
      "Epoch  89 Batch   92/269   train_loss = 3.880\n",
      "Epoch  89 Batch   93/269   train_loss = 3.604\n",
      "Epoch  89 Batch   94/269   train_loss = 4.056\n",
      "Epoch  89 Batch   95/269   train_loss = 3.491\n",
      "Epoch  89 Batch   96/269   train_loss = 3.641\n",
      "Epoch  89 Batch   97/269   train_loss = 3.640\n",
      "Epoch  89 Batch   98/269   train_loss = 3.965\n",
      "Epoch  89 Batch   99/269   train_loss = 3.777\n",
      "Epoch  89 Batch  100/269   train_loss = 3.755\n",
      "Epoch  89 Batch  101/269   train_loss = 3.771\n",
      "Epoch  89 Batch  102/269   train_loss = 3.585\n",
      "Epoch  89 Batch  103/269   train_loss = 3.743\n",
      "Epoch  89 Batch  104/269   train_loss = 3.644\n",
      "Epoch  89 Batch  105/269   train_loss = 3.851\n",
      "Epoch  89 Batch  106/269   train_loss = 4.007\n",
      "Epoch  89 Batch  107/269   train_loss = 3.932\n",
      "Epoch  89 Batch  108/269   train_loss = 3.745\n",
      "Epoch  89 Batch  109/269   train_loss = 3.809\n",
      "Epoch  89 Batch  110/269   train_loss = 3.830\n",
      "Epoch  89 Batch  111/269   train_loss = 3.878\n",
      "Epoch  89 Batch  112/269   train_loss = 3.875\n",
      "Epoch  89 Batch  113/269   train_loss = 3.784\n",
      "Epoch  89 Batch  114/269   train_loss = 3.703\n",
      "Epoch  89 Batch  115/269   train_loss = 3.610\n",
      "Epoch  89 Batch  116/269   train_loss = 3.518\n",
      "Epoch  89 Batch  117/269   train_loss = 3.623\n",
      "Epoch  89 Batch  118/269   train_loss = 3.859\n",
      "Epoch  89 Batch  119/269   train_loss = 3.572\n",
      "Epoch  89 Batch  120/269   train_loss = 3.633\n",
      "Epoch  89 Batch  121/269   train_loss = 3.709\n",
      "Epoch  89 Batch  122/269   train_loss = 3.705\n",
      "Epoch  89 Batch  123/269   train_loss = 3.483\n",
      "Epoch  89 Batch  124/269   train_loss = 3.883\n",
      "Epoch  89 Batch  125/269   train_loss = 3.542\n",
      "Epoch  89 Batch  126/269   train_loss = 3.379\n",
      "Epoch  89 Batch  127/269   train_loss = 3.770\n",
      "Epoch  89 Batch  128/269   train_loss = 3.689\n",
      "Epoch  89 Batch  129/269   train_loss = 3.611\n",
      "Epoch  89 Batch  130/269   train_loss = 3.786\n",
      "Epoch  89 Batch  131/269   train_loss = 3.403\n",
      "Epoch  89 Batch  132/269   train_loss = 3.753\n",
      "Epoch  89 Batch  133/269   train_loss = 3.444\n",
      "Epoch  89 Batch  134/269   train_loss = 3.840\n",
      "Epoch  89 Batch  135/269   train_loss = 3.627\n",
      "Epoch  89 Batch  136/269   train_loss = 3.449\n",
      "Epoch  89 Batch  137/269   train_loss = 3.815\n",
      "Epoch  89 Batch  138/269   train_loss = 3.532\n",
      "Epoch  89 Batch  139/269   train_loss = 3.635\n",
      "Epoch  89 Batch  140/269   train_loss = 3.797\n",
      "Epoch  89 Batch  141/269   train_loss = 4.027\n",
      "Epoch  89 Batch  142/269   train_loss = 3.561\n",
      "Epoch  89 Batch  143/269   train_loss = 3.671\n",
      "Epoch  89 Batch  144/269   train_loss = 3.871\n",
      "Epoch  89 Batch  145/269   train_loss = 3.691\n",
      "Epoch  89 Batch  146/269   train_loss = 3.936\n",
      "Epoch  89 Batch  147/269   train_loss = 3.528\n",
      "Epoch  89 Batch  148/269   train_loss = 3.779\n",
      "Epoch  89 Batch  149/269   train_loss = 3.615\n",
      "Epoch  89 Batch  150/269   train_loss = 3.810\n",
      "Epoch  89 Batch  151/269   train_loss = 3.820\n",
      "Epoch  89 Batch  152/269   train_loss = 3.692\n",
      "Epoch  89 Batch  153/269   train_loss = 3.787\n",
      "Epoch  89 Batch  154/269   train_loss = 3.640\n",
      "Epoch  89 Batch  155/269   train_loss = 3.552\n",
      "Epoch  89 Batch  156/269   train_loss = 4.089\n",
      "Epoch  89 Batch  157/269   train_loss = 3.818\n",
      "Epoch  89 Batch  158/269   train_loss = 3.746\n",
      "Epoch  89 Batch  159/269   train_loss = 3.827\n",
      "Epoch  89 Batch  160/269   train_loss = 3.613\n",
      "Epoch  89 Batch  161/269   train_loss = 3.824\n",
      "Epoch  89 Batch  162/269   train_loss = 3.831\n",
      "Epoch  89 Batch  163/269   train_loss = 3.610\n",
      "Epoch  89 Batch  164/269   train_loss = 3.891\n",
      "Epoch  89 Batch  165/269   train_loss = 4.002\n",
      "Epoch  89 Batch  166/269   train_loss = 3.767\n",
      "Epoch  89 Batch  167/269   train_loss = 3.528\n",
      "Epoch  89 Batch  168/269   train_loss = 3.713\n",
      "Epoch  89 Batch  169/269   train_loss = 3.986\n",
      "Epoch  89 Batch  170/269   train_loss = 3.535\n",
      "Epoch  89 Batch  171/269   train_loss = 3.779\n",
      "Epoch  89 Batch  172/269   train_loss = 3.525\n",
      "Epoch  89 Batch  173/269   train_loss = 3.745\n",
      "Epoch  89 Batch  174/269   train_loss = 3.893\n",
      "Epoch  89 Batch  175/269   train_loss = 3.774\n",
      "Epoch  89 Batch  176/269   train_loss = 3.667\n",
      "Epoch  89 Batch  177/269   train_loss = 3.844\n",
      "Epoch  89 Batch  178/269   train_loss = 3.952\n",
      "Epoch  89 Batch  179/269   train_loss = 3.792\n",
      "Epoch  89 Batch  180/269   train_loss = 3.607\n",
      "Epoch  89 Batch  181/269   train_loss = 3.830\n",
      "Epoch  89 Batch  182/269   train_loss = 3.656\n",
      "Epoch  89 Batch  183/269   train_loss = 3.679\n",
      "Epoch  89 Batch  184/269   train_loss = 3.681\n",
      "Epoch  89 Batch  185/269   train_loss = 3.785\n",
      "Epoch  89 Batch  186/269   train_loss = 3.733\n",
      "Epoch  89 Batch  187/269   train_loss = 3.831\n",
      "Epoch  89 Batch  188/269   train_loss = 3.583\n",
      "Epoch  89 Batch  189/269   train_loss = 3.650\n",
      "Epoch  89 Batch  190/269   train_loss = 4.059\n",
      "Epoch  89 Batch  191/269   train_loss = 3.728\n",
      "Epoch  89 Batch  192/269   train_loss = 3.789\n",
      "Epoch  89 Batch  193/269   train_loss = 3.722\n",
      "Epoch  89 Batch  194/269   train_loss = 3.819\n",
      "Epoch  89 Batch  195/269   train_loss = 3.766\n",
      "Epoch  89 Batch  196/269   train_loss = 3.740\n",
      "Epoch  89 Batch  197/269   train_loss = 3.920\n",
      "Epoch  89 Batch  198/269   train_loss = 3.779\n",
      "Epoch  89 Batch  199/269   train_loss = 3.871\n",
      "Epoch  89 Batch  200/269   train_loss = 3.555\n",
      "Epoch  89 Batch  201/269   train_loss = 3.715\n",
      "Epoch  89 Batch  202/269   train_loss = 3.530\n",
      "Epoch  89 Batch  203/269   train_loss = 3.633\n",
      "Epoch  89 Batch  204/269   train_loss = 3.823\n",
      "Epoch  89 Batch  205/269   train_loss = 3.773\n",
      "Epoch  89 Batch  206/269   train_loss = 3.656\n",
      "Epoch  89 Batch  207/269   train_loss = 3.644\n",
      "Epoch  89 Batch  208/269   train_loss = 3.797\n",
      "Epoch  89 Batch  209/269   train_loss = 3.819\n",
      "Epoch  89 Batch  210/269   train_loss = 3.647\n",
      "Epoch  89 Batch  211/269   train_loss = 3.645\n",
      "Epoch  89 Batch  212/269   train_loss = 4.100\n",
      "Epoch  89 Batch  213/269   train_loss = 3.580\n",
      "Epoch  89 Batch  214/269   train_loss = 3.667\n",
      "Epoch  89 Batch  215/269   train_loss = 3.968\n",
      "Epoch  89 Batch  216/269   train_loss = 3.955\n",
      "Epoch  89 Batch  217/269   train_loss = 3.583\n",
      "Epoch  89 Batch  218/269   train_loss = 3.775\n",
      "Epoch  89 Batch  219/269   train_loss = 3.555\n",
      "Epoch  89 Batch  220/269   train_loss = 3.911\n",
      "Epoch  89 Batch  221/269   train_loss = 3.579\n",
      "Epoch  89 Batch  222/269   train_loss = 3.765\n",
      "Epoch  89 Batch  223/269   train_loss = 3.546\n",
      "Epoch  89 Batch  224/269   train_loss = 3.859\n",
      "Epoch  89 Batch  225/269   train_loss = 3.897\n",
      "Epoch  89 Batch  226/269   train_loss = 3.720\n",
      "Epoch  89 Batch  227/269   train_loss = 3.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  89 Batch  228/269   train_loss = 3.730\n",
      "Epoch  89 Batch  229/269   train_loss = 3.996\n",
      "Epoch  89 Batch  230/269   train_loss = 3.831\n",
      "Epoch  89 Batch  231/269   train_loss = 3.671\n",
      "Epoch  89 Batch  232/269   train_loss = 3.743\n",
      "Epoch  89 Batch  233/269   train_loss = 3.811\n",
      "Epoch  89 Batch  234/269   train_loss = 3.770\n",
      "Epoch  89 Batch  235/269   train_loss = 3.930\n",
      "Epoch  89 Batch  236/269   train_loss = 3.537\n",
      "Epoch  89 Batch  237/269   train_loss = 3.264\n",
      "Epoch  89 Batch  238/269   train_loss = 3.559\n",
      "Epoch  89 Batch  239/269   train_loss = 3.997\n",
      "Epoch  89 Batch  240/269   train_loss = 3.608\n",
      "Epoch  89 Batch  241/269   train_loss = 3.941\n",
      "Epoch  89 Batch  242/269   train_loss = 3.578\n",
      "Epoch  89 Batch  243/269   train_loss = 3.838\n",
      "Epoch  89 Batch  244/269   train_loss = 3.697\n",
      "Epoch  89 Batch  245/269   train_loss = 3.719\n",
      "Epoch  89 Batch  246/269   train_loss = 3.342\n",
      "Epoch  89 Batch  247/269   train_loss = 3.765\n",
      "Epoch  89 Batch  248/269   train_loss = 3.855\n",
      "Epoch  89 Batch  249/269   train_loss = 3.627\n",
      "Epoch  89 Batch  250/269   train_loss = 3.568\n",
      "Epoch  89 Batch  251/269   train_loss = 3.956\n",
      "Epoch  89 Batch  252/269   train_loss = 3.858\n",
      "Epoch  89 Batch  253/269   train_loss = 3.660\n",
      "Epoch  89 Batch  254/269   train_loss = 3.648\n",
      "Epoch  89 Batch  255/269   train_loss = 3.755\n",
      "Epoch  89 Batch  256/269   train_loss = 3.665\n",
      "Epoch  89 Batch  257/269   train_loss = 3.551\n",
      "Epoch  89 Batch  258/269   train_loss = 3.514\n",
      "Epoch  89 Batch  259/269   train_loss = 3.528\n",
      "Epoch  89 Batch  260/269   train_loss = 3.798\n",
      "Epoch  89 Batch  261/269   train_loss = 3.800\n",
      "Epoch  89 Batch  262/269   train_loss = 3.624\n",
      "Epoch  89 Batch  263/269   train_loss = 3.524\n",
      "Epoch  89 Batch  264/269   train_loss = 3.888\n",
      "Epoch  89 Batch  265/269   train_loss = 3.719\n",
      "Epoch  89 Batch  266/269   train_loss = 3.657\n",
      "Epoch  89 Batch  267/269   train_loss = 3.733\n",
      "Epoch  89 Batch  268/269   train_loss = 3.921\n",
      "Epoch  90 Batch    0/269   train_loss = 3.683\n",
      "Epoch  90 Batch    1/269   train_loss = 3.494\n",
      "Epoch  90 Batch    2/269   train_loss = 3.584\n",
      "Epoch  90 Batch    3/269   train_loss = 3.659\n",
      "Epoch  90 Batch    4/269   train_loss = 4.121\n",
      "Epoch  90 Batch    5/269   train_loss = 3.745\n",
      "Epoch  90 Batch    6/269   train_loss = 3.595\n",
      "Epoch  90 Batch    7/269   train_loss = 3.478\n",
      "Epoch  90 Batch    8/269   train_loss = 3.865\n",
      "Epoch  90 Batch    9/269   train_loss = 3.466\n",
      "Epoch  90 Batch   10/269   train_loss = 3.701\n",
      "Epoch  90 Batch   11/269   train_loss = 3.673\n",
      "Epoch  90 Batch   12/269   train_loss = 3.402\n",
      "Epoch  90 Batch   13/269   train_loss = 3.754\n",
      "Epoch  90 Batch   14/269   train_loss = 3.595\n",
      "Epoch  90 Batch   15/269   train_loss = 3.981\n",
      "Epoch  90 Batch   16/269   train_loss = 3.657\n",
      "Epoch  90 Batch   17/269   train_loss = 3.715\n",
      "Epoch  90 Batch   18/269   train_loss = 3.669\n",
      "Epoch  90 Batch   19/269   train_loss = 3.701\n",
      "Epoch  90 Batch   20/269   train_loss = 4.047\n",
      "Epoch  90 Batch   21/269   train_loss = 3.891\n",
      "Epoch  90 Batch   22/269   train_loss = 3.631\n",
      "Epoch  90 Batch   23/269   train_loss = 3.801\n",
      "Epoch  90 Batch   24/269   train_loss = 3.623\n",
      "Epoch  90 Batch   25/269   train_loss = 3.926\n",
      "Epoch  90 Batch   26/269   train_loss = 3.726\n",
      "Epoch  90 Batch   27/269   train_loss = 3.600\n",
      "Epoch  90 Batch   28/269   train_loss = 3.741\n",
      "Epoch  90 Batch   29/269   train_loss = 3.742\n",
      "Epoch  90 Batch   30/269   train_loss = 4.114\n",
      "Epoch  90 Batch   31/269   train_loss = 3.715\n",
      "Epoch  90 Batch   32/269   train_loss = 3.569\n",
      "Epoch  90 Batch   33/269   train_loss = 3.424\n",
      "Epoch  90 Batch   34/269   train_loss = 3.666\n",
      "Epoch  90 Batch   35/269   train_loss = 3.547\n",
      "Epoch  90 Batch   36/269   train_loss = 3.523\n",
      "Epoch  90 Batch   37/269   train_loss = 3.769\n",
      "Epoch  90 Batch   38/269   train_loss = 3.437\n",
      "Epoch  90 Batch   39/269   train_loss = 3.839\n",
      "Epoch  90 Batch   40/269   train_loss = 3.626\n",
      "Epoch  90 Batch   41/269   train_loss = 3.623\n",
      "Epoch  90 Batch   42/269   train_loss = 4.015\n",
      "Epoch  90 Batch   43/269   train_loss = 3.978\n",
      "Epoch  90 Batch   44/269   train_loss = 3.741\n",
      "Epoch  90 Batch   45/269   train_loss = 3.473\n",
      "Epoch  90 Batch   46/269   train_loss = 3.747\n",
      "Epoch  90 Batch   47/269   train_loss = 3.395\n",
      "Epoch  90 Batch   48/269   train_loss = 3.957\n",
      "Epoch  90 Batch   49/269   train_loss = 3.871\n",
      "Epoch  90 Batch   50/269   train_loss = 3.875\n",
      "Epoch  90 Batch   51/269   train_loss = 3.751\n",
      "Epoch  90 Batch   52/269   train_loss = 3.747\n",
      "Epoch  90 Batch   53/269   train_loss = 3.991\n",
      "Epoch  90 Batch   54/269   train_loss = 3.669\n",
      "Epoch  90 Batch   55/269   train_loss = 3.748\n",
      "Epoch  90 Batch   56/269   train_loss = 3.549\n",
      "Epoch  90 Batch   57/269   train_loss = 3.924\n",
      "Epoch  90 Batch   58/269   train_loss = 3.591\n",
      "Epoch  90 Batch   59/269   train_loss = 3.649\n",
      "Epoch  90 Batch   60/269   train_loss = 3.829\n",
      "Epoch  90 Batch   61/269   train_loss = 3.938\n",
      "Epoch  90 Batch   62/269   train_loss = 3.565\n",
      "Epoch  90 Batch   63/269   train_loss = 3.910\n",
      "Epoch  90 Batch   64/269   train_loss = 3.703\n",
      "Epoch  90 Batch   65/269   train_loss = 3.811\n",
      "Epoch  90 Batch   66/269   train_loss = 3.625\n",
      "Epoch  90 Batch   67/269   train_loss = 3.600\n",
      "Epoch  90 Batch   68/269   train_loss = 3.771\n",
      "Epoch  90 Batch   69/269   train_loss = 3.739\n",
      "Epoch  90 Batch   70/269   train_loss = 3.580\n",
      "Epoch  90 Batch   71/269   train_loss = 3.783\n",
      "Epoch  90 Batch   72/269   train_loss = 3.528\n",
      "Epoch  90 Batch   73/269   train_loss = 3.666\n",
      "Epoch  90 Batch   74/269   train_loss = 3.722\n",
      "Epoch  90 Batch   75/269   train_loss = 3.887\n",
      "Epoch  90 Batch   76/269   train_loss = 3.867\n",
      "Epoch  90 Batch   77/269   train_loss = 3.448\n",
      "Epoch  90 Batch   78/269   train_loss = 3.801\n",
      "Epoch  90 Batch   79/269   train_loss = 3.843\n",
      "Epoch  90 Batch   80/269   train_loss = 3.814\n",
      "Epoch  90 Batch   81/269   train_loss = 3.660\n",
      "Epoch  90 Batch   82/269   train_loss = 3.807\n",
      "Epoch  90 Batch   83/269   train_loss = 3.644\n",
      "Epoch  90 Batch   84/269   train_loss = 3.931\n",
      "Epoch  90 Batch   85/269   train_loss = 4.032\n",
      "Epoch  90 Batch   86/269   train_loss = 4.063\n",
      "Epoch  90 Batch   87/269   train_loss = 3.762\n",
      "Epoch  90 Batch   88/269   train_loss = 3.843\n",
      "Epoch  90 Batch   89/269   train_loss = 3.728\n",
      "Epoch  90 Batch   90/269   train_loss = 3.680\n",
      "Epoch  90 Batch   91/269   train_loss = 3.853\n",
      "Epoch  90 Batch   92/269   train_loss = 3.916\n",
      "Epoch  90 Batch   93/269   train_loss = 3.651\n",
      "Epoch  90 Batch   94/269   train_loss = 4.043\n",
      "Epoch  90 Batch   95/269   train_loss = 3.513\n",
      "Epoch  90 Batch   96/269   train_loss = 3.660\n",
      "Epoch  90 Batch   97/269   train_loss = 3.685\n",
      "Epoch  90 Batch   98/269   train_loss = 3.930\n",
      "Epoch  90 Batch   99/269   train_loss = 3.804\n",
      "Epoch  90 Batch  100/269   train_loss = 3.728\n",
      "Epoch  90 Batch  101/269   train_loss = 3.799\n",
      "Epoch  90 Batch  102/269   train_loss = 3.649\n",
      "Epoch  90 Batch  103/269   train_loss = 3.750\n",
      "Epoch  90 Batch  104/269   train_loss = 3.652\n",
      "Epoch  90 Batch  105/269   train_loss = 3.891\n",
      "Epoch  90 Batch  106/269   train_loss = 4.057\n",
      "Epoch  90 Batch  107/269   train_loss = 4.011\n",
      "Epoch  90 Batch  108/269   train_loss = 3.770\n",
      "Epoch  90 Batch  109/269   train_loss = 3.871\n",
      "Epoch  90 Batch  110/269   train_loss = 3.845\n",
      "Epoch  90 Batch  111/269   train_loss = 3.879\n",
      "Epoch  90 Batch  112/269   train_loss = 3.926\n",
      "Epoch  90 Batch  113/269   train_loss = 3.787\n",
      "Epoch  90 Batch  114/269   train_loss = 3.686\n",
      "Epoch  90 Batch  115/269   train_loss = 3.600\n",
      "Epoch  90 Batch  116/269   train_loss = 3.524\n",
      "Epoch  90 Batch  117/269   train_loss = 3.619\n",
      "Epoch  90 Batch  118/269   train_loss = 3.866\n",
      "Epoch  90 Batch  119/269   train_loss = 3.635\n",
      "Epoch  90 Batch  120/269   train_loss = 3.649\n",
      "Epoch  90 Batch  121/269   train_loss = 3.722\n",
      "Epoch  90 Batch  122/269   train_loss = 3.764\n",
      "Epoch  90 Batch  123/269   train_loss = 3.543\n",
      "Epoch  90 Batch  124/269   train_loss = 3.910\n",
      "Epoch  90 Batch  125/269   train_loss = 3.515\n",
      "Epoch  90 Batch  126/269   train_loss = 3.389\n",
      "Epoch  90 Batch  127/269   train_loss = 3.776\n",
      "Epoch  90 Batch  128/269   train_loss = 3.693\n",
      "Epoch  90 Batch  129/269   train_loss = 3.648\n",
      "Epoch  90 Batch  130/269   train_loss = 3.782\n",
      "Epoch  90 Batch  131/269   train_loss = 3.445\n",
      "Epoch  90 Batch  132/269   train_loss = 3.746\n",
      "Epoch  90 Batch  133/269   train_loss = 3.439\n",
      "Epoch  90 Batch  134/269   train_loss = 3.866\n",
      "Epoch  90 Batch  135/269   train_loss = 3.617\n",
      "Epoch  90 Batch  136/269   train_loss = 3.467\n",
      "Epoch  90 Batch  137/269   train_loss = 3.839\n",
      "Epoch  90 Batch  138/269   train_loss = 3.587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  90 Batch  139/269   train_loss = 3.694\n",
      "Epoch  90 Batch  140/269   train_loss = 3.849\n",
      "Epoch  90 Batch  141/269   train_loss = 4.084\n",
      "Epoch  90 Batch  142/269   train_loss = 3.554\n",
      "Epoch  90 Batch  143/269   train_loss = 3.659\n",
      "Epoch  90 Batch  144/269   train_loss = 3.859\n",
      "Epoch  90 Batch  145/269   train_loss = 3.679\n",
      "Epoch  90 Batch  146/269   train_loss = 3.937\n",
      "Epoch  90 Batch  147/269   train_loss = 3.548\n",
      "Epoch  90 Batch  148/269   train_loss = 3.819\n",
      "Epoch  90 Batch  149/269   train_loss = 3.621\n",
      "Epoch  90 Batch  150/269   train_loss = 3.824\n",
      "Epoch  90 Batch  151/269   train_loss = 3.824\n",
      "Epoch  90 Batch  152/269   train_loss = 3.732\n",
      "Epoch  90 Batch  153/269   train_loss = 3.804\n",
      "Epoch  90 Batch  154/269   train_loss = 3.652\n",
      "Epoch  90 Batch  155/269   train_loss = 3.578\n",
      "Epoch  90 Batch  156/269   train_loss = 4.102\n",
      "Epoch  90 Batch  157/269   train_loss = 3.918\n",
      "Epoch  90 Batch  158/269   train_loss = 3.790\n",
      "Epoch  90 Batch  159/269   train_loss = 3.832\n",
      "Epoch  90 Batch  160/269   train_loss = 3.619\n",
      "Epoch  90 Batch  161/269   train_loss = 3.903\n",
      "Epoch  90 Batch  162/269   train_loss = 3.895\n",
      "Epoch  90 Batch  163/269   train_loss = 3.611\n",
      "Epoch  90 Batch  164/269   train_loss = 3.897\n",
      "Epoch  90 Batch  165/269   train_loss = 4.008\n",
      "Epoch  90 Batch  166/269   train_loss = 3.750\n",
      "Epoch  90 Batch  167/269   train_loss = 3.509\n",
      "Epoch  90 Batch  168/269   train_loss = 3.696\n",
      "Epoch  90 Batch  169/269   train_loss = 3.993\n",
      "Epoch  90 Batch  170/269   train_loss = 3.533\n",
      "Epoch  90 Batch  171/269   train_loss = 3.840\n",
      "Epoch  90 Batch  172/269   train_loss = 3.519\n",
      "Epoch  90 Batch  173/269   train_loss = 3.811\n",
      "Epoch  90 Batch  174/269   train_loss = 3.869\n",
      "Epoch  90 Batch  175/269   train_loss = 3.780\n",
      "Epoch  90 Batch  176/269   train_loss = 3.683\n",
      "Epoch  90 Batch  177/269   train_loss = 3.861\n",
      "Epoch  90 Batch  178/269   train_loss = 3.938\n",
      "Epoch  90 Batch  179/269   train_loss = 3.787\n",
      "Epoch  90 Batch  180/269   train_loss = 3.589\n",
      "Epoch  90 Batch  181/269   train_loss = 3.821\n",
      "Epoch  90 Batch  182/269   train_loss = 3.652\n",
      "Epoch  90 Batch  183/269   train_loss = 3.671\n",
      "Epoch  90 Batch  184/269   train_loss = 3.690\n",
      "Epoch  90 Batch  185/269   train_loss = 3.794\n",
      "Epoch  90 Batch  186/269   train_loss = 3.781\n",
      "Epoch  90 Batch  187/269   train_loss = 3.853\n",
      "Epoch  90 Batch  188/269   train_loss = 3.583\n",
      "Epoch  90 Batch  189/269   train_loss = 3.665\n",
      "Epoch  90 Batch  190/269   train_loss = 4.101\n",
      "Epoch  90 Batch  191/269   train_loss = 3.706\n",
      "Epoch  90 Batch  192/269   train_loss = 3.812\n",
      "Epoch  90 Batch  193/269   train_loss = 3.714\n",
      "Epoch  90 Batch  194/269   train_loss = 3.814\n",
      "Epoch  90 Batch  195/269   train_loss = 3.754\n",
      "Epoch  90 Batch  196/269   train_loss = 3.740\n",
      "Epoch  90 Batch  197/269   train_loss = 3.903\n",
      "Epoch  90 Batch  198/269   train_loss = 3.816\n",
      "Epoch  90 Batch  199/269   train_loss = 3.860\n",
      "Epoch  90 Batch  200/269   train_loss = 3.563\n",
      "Epoch  90 Batch  201/269   train_loss = 3.733\n",
      "Epoch  90 Batch  202/269   train_loss = 3.550\n",
      "Epoch  90 Batch  203/269   train_loss = 3.635\n",
      "Epoch  90 Batch  204/269   train_loss = 3.824\n",
      "Epoch  90 Batch  205/269   train_loss = 3.753\n",
      "Epoch  90 Batch  206/269   train_loss = 3.647\n",
      "Epoch  90 Batch  207/269   train_loss = 3.646\n",
      "Epoch  90 Batch  208/269   train_loss = 3.796\n",
      "Epoch  90 Batch  209/269   train_loss = 3.863\n",
      "Epoch  90 Batch  210/269   train_loss = 3.622\n",
      "Epoch  90 Batch  211/269   train_loss = 3.704\n",
      "Epoch  90 Batch  212/269   train_loss = 4.079\n",
      "Epoch  90 Batch  213/269   train_loss = 3.614\n",
      "Epoch  90 Batch  214/269   train_loss = 3.650\n",
      "Epoch  90 Batch  215/269   train_loss = 3.965\n",
      "Epoch  90 Batch  216/269   train_loss = 3.958\n",
      "Epoch  90 Batch  217/269   train_loss = 3.623\n",
      "Epoch  90 Batch  218/269   train_loss = 3.768\n",
      "Epoch  90 Batch  219/269   train_loss = 3.552\n",
      "Epoch  90 Batch  220/269   train_loss = 3.882\n",
      "Epoch  90 Batch  221/269   train_loss = 3.564\n",
      "Epoch  90 Batch  222/269   train_loss = 3.762\n",
      "Epoch  90 Batch  223/269   train_loss = 3.541\n",
      "Epoch  90 Batch  224/269   train_loss = 3.865\n",
      "Epoch  90 Batch  225/269   train_loss = 3.892\n",
      "Epoch  90 Batch  226/269   train_loss = 3.697\n",
      "Epoch  90 Batch  227/269   train_loss = 3.455\n",
      "Epoch  90 Batch  228/269   train_loss = 3.719\n",
      "Epoch  90 Batch  229/269   train_loss = 3.977\n",
      "Epoch  90 Batch  230/269   train_loss = 3.857\n",
      "Epoch  90 Batch  231/269   train_loss = 3.662\n",
      "Epoch  90 Batch  232/269   train_loss = 3.737\n",
      "Epoch  90 Batch  233/269   train_loss = 3.833\n",
      "Epoch  90 Batch  234/269   train_loss = 3.780\n",
      "Epoch  90 Batch  235/269   train_loss = 3.951\n",
      "Epoch  90 Batch  236/269   train_loss = 3.550\n",
      "Epoch  90 Batch  237/269   train_loss = 3.285\n",
      "Epoch  90 Batch  238/269   train_loss = 3.562\n",
      "Epoch  90 Batch  239/269   train_loss = 3.981\n",
      "Epoch  90 Batch  240/269   train_loss = 3.567\n",
      "Epoch  90 Batch  241/269   train_loss = 3.955\n",
      "Epoch  90 Batch  242/269   train_loss = 3.576\n",
      "Epoch  90 Batch  243/269   train_loss = 3.829\n",
      "Epoch  90 Batch  244/269   train_loss = 3.667\n",
      "Epoch  90 Batch  245/269   train_loss = 3.740\n",
      "Epoch  90 Batch  246/269   train_loss = 3.339\n",
      "Epoch  90 Batch  247/269   train_loss = 3.723\n",
      "Epoch  90 Batch  248/269   train_loss = 3.864\n",
      "Epoch  90 Batch  249/269   train_loss = 3.657\n",
      "Epoch  90 Batch  250/269   train_loss = 3.590\n",
      "Epoch  90 Batch  251/269   train_loss = 3.909\n",
      "Epoch  90 Batch  252/269   train_loss = 3.828\n",
      "Epoch  90 Batch  253/269   train_loss = 3.636\n",
      "Epoch  90 Batch  254/269   train_loss = 3.625\n",
      "Epoch  90 Batch  255/269   train_loss = 3.757\n",
      "Epoch  90 Batch  256/269   train_loss = 3.634\n",
      "Epoch  90 Batch  257/269   train_loss = 3.573\n",
      "Epoch  90 Batch  258/269   train_loss = 3.516\n",
      "Epoch  90 Batch  259/269   train_loss = 3.568\n",
      "Epoch  90 Batch  260/269   train_loss = 3.823\n",
      "Epoch  90 Batch  261/269   train_loss = 3.797\n",
      "Epoch  90 Batch  262/269   train_loss = 3.608\n",
      "Epoch  90 Batch  263/269   train_loss = 3.530\n",
      "Epoch  90 Batch  264/269   train_loss = 3.901\n",
      "Epoch  90 Batch  265/269   train_loss = 3.767\n",
      "Epoch  90 Batch  266/269   train_loss = 3.636\n",
      "Epoch  90 Batch  267/269   train_loss = 3.745\n",
      "Epoch  90 Batch  268/269   train_loss = 3.882\n",
      "Epoch  91 Batch    0/269   train_loss = 3.672\n",
      "Epoch  91 Batch    1/269   train_loss = 3.496\n",
      "Epoch  91 Batch    2/269   train_loss = 3.601\n",
      "Epoch  91 Batch    3/269   train_loss = 3.675\n",
      "Epoch  91 Batch    4/269   train_loss = 4.084\n",
      "Epoch  91 Batch    5/269   train_loss = 3.722\n",
      "Epoch  91 Batch    6/269   train_loss = 3.597\n",
      "Epoch  91 Batch    7/269   train_loss = 3.492\n",
      "Epoch  91 Batch    8/269   train_loss = 3.808\n",
      "Epoch  91 Batch    9/269   train_loss = 3.454\n",
      "Epoch  91 Batch   10/269   train_loss = 3.631\n",
      "Epoch  91 Batch   11/269   train_loss = 3.667\n",
      "Epoch  91 Batch   12/269   train_loss = 3.394\n",
      "Epoch  91 Batch   13/269   train_loss = 3.754\n",
      "Epoch  91 Batch   14/269   train_loss = 3.595\n",
      "Epoch  91 Batch   15/269   train_loss = 3.958\n",
      "Epoch  91 Batch   16/269   train_loss = 3.596\n",
      "Epoch  91 Batch   17/269   train_loss = 3.717\n",
      "Epoch  91 Batch   18/269   train_loss = 3.658\n",
      "Epoch  91 Batch   19/269   train_loss = 3.700\n",
      "Epoch  91 Batch   20/269   train_loss = 4.036\n",
      "Epoch  91 Batch   21/269   train_loss = 3.896\n",
      "Epoch  91 Batch   22/269   train_loss = 3.650\n",
      "Epoch  91 Batch   23/269   train_loss = 3.788\n",
      "Epoch  91 Batch   24/269   train_loss = 3.519\n",
      "Epoch  91 Batch   25/269   train_loss = 3.931\n",
      "Epoch  91 Batch   26/269   train_loss = 3.729\n",
      "Epoch  91 Batch   27/269   train_loss = 3.586\n",
      "Epoch  91 Batch   28/269   train_loss = 3.728\n",
      "Epoch  91 Batch   29/269   train_loss = 3.724\n",
      "Epoch  91 Batch   30/269   train_loss = 4.101\n",
      "Epoch  91 Batch   31/269   train_loss = 3.713\n",
      "Epoch  91 Batch   32/269   train_loss = 3.561\n",
      "Epoch  91 Batch   33/269   train_loss = 3.393\n",
      "Epoch  91 Batch   34/269   train_loss = 3.691\n",
      "Epoch  91 Batch   35/269   train_loss = 3.485\n",
      "Epoch  91 Batch   36/269   train_loss = 3.538\n",
      "Epoch  91 Batch   37/269   train_loss = 3.776\n",
      "Epoch  91 Batch   38/269   train_loss = 3.449\n",
      "Epoch  91 Batch   39/269   train_loss = 3.845\n",
      "Epoch  91 Batch   40/269   train_loss = 3.605\n",
      "Epoch  91 Batch   41/269   train_loss = 3.616\n",
      "Epoch  91 Batch   42/269   train_loss = 4.007\n",
      "Epoch  91 Batch   43/269   train_loss = 3.979\n",
      "Epoch  91 Batch   44/269   train_loss = 3.731\n",
      "Epoch  91 Batch   45/269   train_loss = 3.475\n",
      "Epoch  91 Batch   46/269   train_loss = 3.759\n",
      "Epoch  91 Batch   47/269   train_loss = 3.397\n",
      "Epoch  91 Batch   48/269   train_loss = 3.970\n",
      "Epoch  91 Batch   49/269   train_loss = 3.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  91 Batch   50/269   train_loss = 3.868\n",
      "Epoch  91 Batch   51/269   train_loss = 3.731\n",
      "Epoch  91 Batch   52/269   train_loss = 3.769\n",
      "Epoch  91 Batch   53/269   train_loss = 3.989\n",
      "Epoch  91 Batch   54/269   train_loss = 3.675\n",
      "Epoch  91 Batch   55/269   train_loss = 3.731\n",
      "Epoch  91 Batch   56/269   train_loss = 3.479\n",
      "Epoch  91 Batch   57/269   train_loss = 3.887\n",
      "Epoch  91 Batch   58/269   train_loss = 3.580\n",
      "Epoch  91 Batch   59/269   train_loss = 3.657\n",
      "Epoch  91 Batch   60/269   train_loss = 3.792\n",
      "Epoch  91 Batch   61/269   train_loss = 3.902\n",
      "Epoch  91 Batch   62/269   train_loss = 3.562\n",
      "Epoch  91 Batch   63/269   train_loss = 3.886\n",
      "Epoch  91 Batch   64/269   train_loss = 3.677\n",
      "Epoch  91 Batch   65/269   train_loss = 3.761\n",
      "Epoch  91 Batch   66/269   train_loss = 3.644\n",
      "Epoch  91 Batch   67/269   train_loss = 3.597\n",
      "Epoch  91 Batch   68/269   train_loss = 3.774\n",
      "Epoch  91 Batch   69/269   train_loss = 3.706\n",
      "Epoch  91 Batch   70/269   train_loss = 3.557\n",
      "Epoch  91 Batch   71/269   train_loss = 3.753\n",
      "Epoch  91 Batch   72/269   train_loss = 3.499\n",
      "Epoch  91 Batch   73/269   train_loss = 3.704\n",
      "Epoch  91 Batch   74/269   train_loss = 3.700\n",
      "Epoch  91 Batch   75/269   train_loss = 3.878\n",
      "Epoch  91 Batch   76/269   train_loss = 3.855\n",
      "Epoch  91 Batch   77/269   train_loss = 3.447\n",
      "Epoch  91 Batch   78/269   train_loss = 3.797\n",
      "Epoch  91 Batch   79/269   train_loss = 3.884\n",
      "Epoch  91 Batch   80/269   train_loss = 3.837\n",
      "Epoch  91 Batch   81/269   train_loss = 3.640\n",
      "Epoch  91 Batch   82/269   train_loss = 3.825\n",
      "Epoch  91 Batch   83/269   train_loss = 3.632\n",
      "Epoch  91 Batch   84/269   train_loss = 3.910\n",
      "Epoch  91 Batch   85/269   train_loss = 4.062\n",
      "Epoch  91 Batch   86/269   train_loss = 4.023\n",
      "Epoch  91 Batch   87/269   train_loss = 3.713\n",
      "Epoch  91 Batch   88/269   train_loss = 3.805\n",
      "Epoch  91 Batch   89/269   train_loss = 3.735\n",
      "Epoch  91 Batch   90/269   train_loss = 3.613\n",
      "Epoch  91 Batch   91/269   train_loss = 3.798\n",
      "Epoch  91 Batch   92/269   train_loss = 3.881\n",
      "Epoch  91 Batch   93/269   train_loss = 3.638\n",
      "Epoch  91 Batch   94/269   train_loss = 4.033\n",
      "Epoch  91 Batch   95/269   train_loss = 3.504\n",
      "Epoch  91 Batch   96/269   train_loss = 3.628\n",
      "Epoch  91 Batch   97/269   train_loss = 3.658\n",
      "Epoch  91 Batch   98/269   train_loss = 3.921\n",
      "Epoch  91 Batch   99/269   train_loss = 3.764\n",
      "Epoch  91 Batch  100/269   train_loss = 3.726\n",
      "Epoch  91 Batch  101/269   train_loss = 3.785\n",
      "Epoch  91 Batch  102/269   train_loss = 3.665\n",
      "Epoch  91 Batch  103/269   train_loss = 3.795\n",
      "Epoch  91 Batch  104/269   train_loss = 3.676\n",
      "Epoch  91 Batch  105/269   train_loss = 3.881\n",
      "Epoch  91 Batch  106/269   train_loss = 4.018\n",
      "Epoch  91 Batch  107/269   train_loss = 3.976\n",
      "Epoch  91 Batch  108/269   train_loss = 3.779\n",
      "Epoch  91 Batch  109/269   train_loss = 3.882\n",
      "Epoch  91 Batch  110/269   train_loss = 3.829\n",
      "Epoch  91 Batch  111/269   train_loss = 3.892\n",
      "Epoch  91 Batch  112/269   train_loss = 3.887\n",
      "Epoch  91 Batch  113/269   train_loss = 3.781\n",
      "Epoch  91 Batch  114/269   train_loss = 3.687\n",
      "Epoch  91 Batch  115/269   train_loss = 3.602\n",
      "Epoch  91 Batch  116/269   train_loss = 3.510\n",
      "Epoch  91 Batch  117/269   train_loss = 3.603\n",
      "Epoch  91 Batch  118/269   train_loss = 3.836\n",
      "Epoch  91 Batch  119/269   train_loss = 3.627\n",
      "Epoch  91 Batch  120/269   train_loss = 3.670\n",
      "Epoch  91 Batch  121/269   train_loss = 3.713\n",
      "Epoch  91 Batch  122/269   train_loss = 3.717\n",
      "Epoch  91 Batch  123/269   train_loss = 3.530\n",
      "Epoch  91 Batch  124/269   train_loss = 3.908\n",
      "Epoch  91 Batch  125/269   train_loss = 3.510\n",
      "Epoch  91 Batch  126/269   train_loss = 3.371\n",
      "Epoch  91 Batch  127/269   train_loss = 3.769\n",
      "Epoch  91 Batch  128/269   train_loss = 3.672\n",
      "Epoch  91 Batch  129/269   train_loss = 3.634\n",
      "Epoch  91 Batch  130/269   train_loss = 3.804\n",
      "Epoch  91 Batch  131/269   train_loss = 3.408\n",
      "Epoch  91 Batch  132/269   train_loss = 3.738\n",
      "Epoch  91 Batch  133/269   train_loss = 3.439\n",
      "Epoch  91 Batch  134/269   train_loss = 3.875\n",
      "Epoch  91 Batch  135/269   train_loss = 3.612\n",
      "Epoch  91 Batch  136/269   train_loss = 3.464\n",
      "Epoch  91 Batch  137/269   train_loss = 3.818\n",
      "Epoch  91 Batch  138/269   train_loss = 3.537\n",
      "Epoch  91 Batch  139/269   train_loss = 3.688\n",
      "Epoch  91 Batch  140/269   train_loss = 3.842\n",
      "Epoch  91 Batch  141/269   train_loss = 4.084\n",
      "Epoch  91 Batch  142/269   train_loss = 3.548\n",
      "Epoch  91 Batch  143/269   train_loss = 3.667\n",
      "Epoch  91 Batch  144/269   train_loss = 3.850\n",
      "Epoch  91 Batch  145/269   train_loss = 3.692\n",
      "Epoch  91 Batch  146/269   train_loss = 3.925\n",
      "Epoch  91 Batch  147/269   train_loss = 3.516\n",
      "Epoch  91 Batch  148/269   train_loss = 3.842\n",
      "Epoch  91 Batch  149/269   train_loss = 3.647\n",
      "Epoch  91 Batch  150/269   train_loss = 3.832\n",
      "Epoch  91 Batch  151/269   train_loss = 3.809\n",
      "Epoch  91 Batch  152/269   train_loss = 3.729\n",
      "Epoch  91 Batch  153/269   train_loss = 3.777\n",
      "Epoch  91 Batch  154/269   train_loss = 3.641\n",
      "Epoch  91 Batch  155/269   train_loss = 3.557\n",
      "Epoch  91 Batch  156/269   train_loss = 4.099\n",
      "Epoch  91 Batch  157/269   train_loss = 3.885\n",
      "Epoch  91 Batch  158/269   train_loss = 3.752\n",
      "Epoch  91 Batch  159/269   train_loss = 3.801\n",
      "Epoch  91 Batch  160/269   train_loss = 3.605\n",
      "Epoch  91 Batch  161/269   train_loss = 3.885\n",
      "Epoch  91 Batch  162/269   train_loss = 3.869\n",
      "Epoch  91 Batch  163/269   train_loss = 3.598\n",
      "Epoch  91 Batch  164/269   train_loss = 3.891\n",
      "Epoch  91 Batch  165/269   train_loss = 3.995\n",
      "Epoch  91 Batch  166/269   train_loss = 3.759\n",
      "Epoch  91 Batch  167/269   train_loss = 3.519\n",
      "Epoch  91 Batch  168/269   train_loss = 3.708\n",
      "Epoch  91 Batch  169/269   train_loss = 4.063\n",
      "Epoch  91 Batch  170/269   train_loss = 3.520\n",
      "Epoch  91 Batch  171/269   train_loss = 3.839\n",
      "Epoch  91 Batch  172/269   train_loss = 3.520\n",
      "Epoch  91 Batch  173/269   train_loss = 3.770\n",
      "Epoch  91 Batch  174/269   train_loss = 3.861\n",
      "Epoch  91 Batch  175/269   train_loss = 3.774\n",
      "Epoch  91 Batch  176/269   train_loss = 3.682\n",
      "Epoch  91 Batch  177/269   train_loss = 3.856\n",
      "Epoch  91 Batch  178/269   train_loss = 3.941\n",
      "Epoch  91 Batch  179/269   train_loss = 3.792\n",
      "Epoch  91 Batch  180/269   train_loss = 3.620\n",
      "Epoch  91 Batch  181/269   train_loss = 3.819\n",
      "Epoch  91 Batch  182/269   train_loss = 3.658\n",
      "Epoch  91 Batch  183/269   train_loss = 3.667\n",
      "Epoch  91 Batch  184/269   train_loss = 3.708\n",
      "Epoch  91 Batch  185/269   train_loss = 3.783\n",
      "Epoch  91 Batch  186/269   train_loss = 3.775\n",
      "Epoch  91 Batch  187/269   train_loss = 3.840\n",
      "Epoch  91 Batch  188/269   train_loss = 3.542\n",
      "Epoch  91 Batch  189/269   train_loss = 3.689\n",
      "Epoch  91 Batch  190/269   train_loss = 4.113\n",
      "Epoch  91 Batch  191/269   train_loss = 3.763\n",
      "Epoch  91 Batch  192/269   train_loss = 3.771\n",
      "Epoch  91 Batch  193/269   train_loss = 3.682\n",
      "Epoch  91 Batch  194/269   train_loss = 3.796\n",
      "Epoch  91 Batch  195/269   train_loss = 3.729\n",
      "Epoch  91 Batch  196/269   train_loss = 3.758\n",
      "Epoch  91 Batch  197/269   train_loss = 3.900\n",
      "Epoch  91 Batch  198/269   train_loss = 3.780\n",
      "Epoch  91 Batch  199/269   train_loss = 3.854\n",
      "Epoch  91 Batch  200/269   train_loss = 3.554\n",
      "Epoch  91 Batch  201/269   train_loss = 3.744\n",
      "Epoch  91 Batch  202/269   train_loss = 3.540\n",
      "Epoch  91 Batch  203/269   train_loss = 3.617\n",
      "Epoch  91 Batch  204/269   train_loss = 3.795\n",
      "Epoch  91 Batch  205/269   train_loss = 3.729\n",
      "Epoch  91 Batch  206/269   train_loss = 3.655\n",
      "Epoch  91 Batch  207/269   train_loss = 3.651\n",
      "Epoch  91 Batch  208/269   train_loss = 3.791\n",
      "Epoch  91 Batch  209/269   train_loss = 3.833\n",
      "Epoch  91 Batch  210/269   train_loss = 3.654\n",
      "Epoch  91 Batch  211/269   train_loss = 3.689\n",
      "Epoch  91 Batch  212/269   train_loss = 4.055\n",
      "Epoch  91 Batch  213/269   train_loss = 3.684\n",
      "Epoch  91 Batch  214/269   train_loss = 3.657\n",
      "Epoch  91 Batch  215/269   train_loss = 3.948\n",
      "Epoch  91 Batch  216/269   train_loss = 3.931\n",
      "Epoch  91 Batch  217/269   train_loss = 3.602\n",
      "Epoch  91 Batch  218/269   train_loss = 3.753\n",
      "Epoch  91 Batch  219/269   train_loss = 3.545\n",
      "Epoch  91 Batch  220/269   train_loss = 3.875\n",
      "Epoch  91 Batch  221/269   train_loss = 3.603\n",
      "Epoch  91 Batch  222/269   train_loss = 3.750\n",
      "Epoch  91 Batch  223/269   train_loss = 3.570\n",
      "Epoch  91 Batch  224/269   train_loss = 3.858\n",
      "Epoch  91 Batch  225/269   train_loss = 3.899\n",
      "Epoch  91 Batch  226/269   train_loss = 3.672\n",
      "Epoch  91 Batch  227/269   train_loss = 3.451\n",
      "Epoch  91 Batch  228/269   train_loss = 3.740\n",
      "Epoch  91 Batch  229/269   train_loss = 3.973\n",
      "Epoch  91 Batch  230/269   train_loss = 3.859\n",
      "Epoch  91 Batch  231/269   train_loss = 3.668\n",
      "Epoch  91 Batch  232/269   train_loss = 3.724\n",
      "Epoch  91 Batch  233/269   train_loss = 3.808\n",
      "Epoch  91 Batch  234/269   train_loss = 3.783\n",
      "Epoch  91 Batch  235/269   train_loss = 3.954\n",
      "Epoch  91 Batch  236/269   train_loss = 3.555\n",
      "Epoch  91 Batch  237/269   train_loss = 3.253\n",
      "Epoch  91 Batch  238/269   train_loss = 3.576\n",
      "Epoch  91 Batch  239/269   train_loss = 4.004\n",
      "Epoch  91 Batch  240/269   train_loss = 3.573\n",
      "Epoch  91 Batch  241/269   train_loss = 3.945\n",
      "Epoch  91 Batch  242/269   train_loss = 3.586\n",
      "Epoch  91 Batch  243/269   train_loss = 3.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  91 Batch  244/269   train_loss = 3.651\n",
      "Epoch  91 Batch  245/269   train_loss = 3.722\n",
      "Epoch  91 Batch  246/269   train_loss = 3.339\n",
      "Epoch  91 Batch  247/269   train_loss = 3.731\n",
      "Epoch  91 Batch  248/269   train_loss = 3.952\n",
      "Epoch  91 Batch  249/269   train_loss = 3.636\n",
      "Epoch  91 Batch  250/269   train_loss = 3.572\n",
      "Epoch  91 Batch  251/269   train_loss = 3.900\n",
      "Epoch  91 Batch  252/269   train_loss = 3.835\n",
      "Epoch  91 Batch  253/269   train_loss = 3.633\n",
      "Epoch  91 Batch  254/269   train_loss = 3.614\n",
      "Epoch  91 Batch  255/269   train_loss = 3.766\n",
      "Epoch  91 Batch  256/269   train_loss = 3.632\n",
      "Epoch  91 Batch  257/269   train_loss = 3.578\n",
      "Epoch  91 Batch  258/269   train_loss = 3.509\n",
      "Epoch  91 Batch  259/269   train_loss = 3.527\n",
      "Epoch  91 Batch  260/269   train_loss = 3.806\n",
      "Epoch  91 Batch  261/269   train_loss = 3.804\n",
      "Epoch  91 Batch  262/269   train_loss = 3.601\n",
      "Epoch  91 Batch  263/269   train_loss = 3.584\n",
      "Epoch  91 Batch  264/269   train_loss = 3.896\n",
      "Epoch  91 Batch  265/269   train_loss = 3.747\n",
      "Epoch  91 Batch  266/269   train_loss = 3.641\n",
      "Epoch  91 Batch  267/269   train_loss = 3.724\n",
      "Epoch  91 Batch  268/269   train_loss = 3.893\n",
      "Epoch  92 Batch    0/269   train_loss = 3.658\n",
      "Epoch  92 Batch    1/269   train_loss = 3.500\n",
      "Epoch  92 Batch    2/269   train_loss = 3.579\n",
      "Epoch  92 Batch    3/269   train_loss = 3.687\n",
      "Epoch  92 Batch    4/269   train_loss = 4.088\n",
      "Epoch  92 Batch    5/269   train_loss = 3.741\n",
      "Epoch  92 Batch    6/269   train_loss = 3.577\n",
      "Epoch  92 Batch    7/269   train_loss = 3.499\n",
      "Epoch  92 Batch    8/269   train_loss = 3.808\n",
      "Epoch  92 Batch    9/269   train_loss = 3.447\n",
      "Epoch  92 Batch   10/269   train_loss = 3.639\n",
      "Epoch  92 Batch   11/269   train_loss = 3.661\n",
      "Epoch  92 Batch   12/269   train_loss = 3.397\n",
      "Epoch  92 Batch   13/269   train_loss = 3.727\n",
      "Epoch  92 Batch   14/269   train_loss = 3.613\n",
      "Epoch  92 Batch   15/269   train_loss = 3.962\n",
      "Epoch  92 Batch   16/269   train_loss = 3.607\n",
      "Epoch  92 Batch   17/269   train_loss = 3.712\n",
      "Epoch  92 Batch   18/269   train_loss = 3.684\n",
      "Epoch  92 Batch   19/269   train_loss = 3.692\n",
      "Epoch  92 Batch   20/269   train_loss = 4.026\n",
      "Epoch  92 Batch   21/269   train_loss = 3.871\n",
      "Epoch  92 Batch   22/269   train_loss = 3.649\n",
      "Epoch  92 Batch   23/269   train_loss = 3.788\n",
      "Epoch  92 Batch   24/269   train_loss = 3.486\n",
      "Epoch  92 Batch   25/269   train_loss = 3.974\n",
      "Epoch  92 Batch   26/269   train_loss = 3.710\n",
      "Epoch  92 Batch   27/269   train_loss = 3.601\n",
      "Epoch  92 Batch   28/269   train_loss = 3.730\n",
      "Epoch  92 Batch   29/269   train_loss = 3.741\n",
      "Epoch  92 Batch   30/269   train_loss = 4.099\n",
      "Epoch  92 Batch   31/269   train_loss = 3.708\n",
      "Epoch  92 Batch   32/269   train_loss = 3.544\n",
      "Epoch  92 Batch   33/269   train_loss = 3.375\n",
      "Epoch  92 Batch   34/269   train_loss = 3.696\n",
      "Epoch  92 Batch   35/269   train_loss = 3.501\n",
      "Epoch  92 Batch   36/269   train_loss = 3.535\n",
      "Epoch  92 Batch   37/269   train_loss = 3.765\n",
      "Epoch  92 Batch   38/269   train_loss = 3.451\n",
      "Epoch  92 Batch   39/269   train_loss = 3.846\n",
      "Epoch  92 Batch   40/269   train_loss = 3.625\n",
      "Epoch  92 Batch   41/269   train_loss = 3.651\n",
      "Epoch  92 Batch   42/269   train_loss = 4.037\n",
      "Epoch  92 Batch   43/269   train_loss = 3.951\n",
      "Epoch  92 Batch   44/269   train_loss = 3.726\n",
      "Epoch  92 Batch   45/269   train_loss = 3.479\n",
      "Epoch  92 Batch   46/269   train_loss = 3.739\n",
      "Epoch  92 Batch   47/269   train_loss = 3.396\n",
      "Epoch  92 Batch   48/269   train_loss = 3.949\n",
      "Epoch  92 Batch   49/269   train_loss = 3.884\n",
      "Epoch  92 Batch   50/269   train_loss = 3.871\n",
      "Epoch  92 Batch   51/269   train_loss = 3.726\n",
      "Epoch  92 Batch   52/269   train_loss = 3.753\n",
      "Epoch  92 Batch   53/269   train_loss = 3.978\n",
      "Epoch  92 Batch   54/269   train_loss = 3.659\n",
      "Epoch  92 Batch   55/269   train_loss = 3.716\n",
      "Epoch  92 Batch   56/269   train_loss = 3.481\n",
      "Epoch  92 Batch   57/269   train_loss = 3.904\n",
      "Epoch  92 Batch   58/269   train_loss = 3.590\n",
      "Epoch  92 Batch   59/269   train_loss = 3.665\n",
      "Epoch  92 Batch   60/269   train_loss = 3.805\n",
      "Epoch  92 Batch   61/269   train_loss = 3.897\n",
      "Epoch  92 Batch   62/269   train_loss = 3.620\n",
      "Epoch  92 Batch   63/269   train_loss = 3.889\n",
      "Epoch  92 Batch   64/269   train_loss = 3.680\n",
      "Epoch  92 Batch   65/269   train_loss = 3.777\n",
      "Epoch  92 Batch   66/269   train_loss = 3.654\n",
      "Epoch  92 Batch   67/269   train_loss = 3.590\n",
      "Epoch  92 Batch   68/269   train_loss = 3.760\n",
      "Epoch  92 Batch   69/269   train_loss = 3.705\n",
      "Epoch  92 Batch   70/269   train_loss = 3.550\n",
      "Epoch  92 Batch   71/269   train_loss = 3.760\n",
      "Epoch  92 Batch   72/269   train_loss = 3.510\n",
      "Epoch  92 Batch   73/269   train_loss = 3.677\n",
      "Epoch  92 Batch   74/269   train_loss = 3.689\n",
      "Epoch  92 Batch   75/269   train_loss = 3.874\n",
      "Epoch  92 Batch   76/269   train_loss = 3.841\n",
      "Epoch  92 Batch   77/269   train_loss = 3.432\n",
      "Epoch  92 Batch   78/269   train_loss = 3.807\n",
      "Epoch  92 Batch   79/269   train_loss = 3.875\n",
      "Epoch  92 Batch   80/269   train_loss = 3.864\n",
      "Epoch  92 Batch   81/269   train_loss = 3.699\n",
      "Epoch  92 Batch   82/269   train_loss = 3.822\n",
      "Epoch  92 Batch   83/269   train_loss = 3.630\n",
      "Epoch  92 Batch   84/269   train_loss = 3.926\n",
      "Epoch  92 Batch   85/269   train_loss = 4.039\n",
      "Epoch  92 Batch   86/269   train_loss = 4.038\n",
      "Epoch  92 Batch   87/269   train_loss = 3.705\n",
      "Epoch  92 Batch   88/269   train_loss = 3.815\n",
      "Epoch  92 Batch   89/269   train_loss = 3.723\n",
      "Epoch  92 Batch   90/269   train_loss = 3.584\n",
      "Epoch  92 Batch   91/269   train_loss = 3.932\n",
      "Epoch  92 Batch   92/269   train_loss = 3.919\n",
      "Epoch  92 Batch   93/269   train_loss = 3.634\n",
      "Epoch  92 Batch   94/269   train_loss = 4.043\n",
      "Epoch  92 Batch   95/269   train_loss = 3.525\n",
      "Epoch  92 Batch   96/269   train_loss = 3.622\n",
      "Epoch  92 Batch   97/269   train_loss = 3.652\n",
      "Epoch  92 Batch   98/269   train_loss = 3.966\n",
      "Epoch  92 Batch   99/269   train_loss = 3.759\n",
      "Epoch  92 Batch  100/269   train_loss = 3.756\n",
      "Epoch  92 Batch  101/269   train_loss = 3.801\n",
      "Epoch  92 Batch  102/269   train_loss = 3.652\n",
      "Epoch  92 Batch  103/269   train_loss = 3.738\n",
      "Epoch  92 Batch  104/269   train_loss = 3.668\n",
      "Epoch  92 Batch  105/269   train_loss = 3.870\n",
      "Epoch  92 Batch  106/269   train_loss = 4.030\n",
      "Epoch  92 Batch  107/269   train_loss = 3.971\n",
      "Epoch  92 Batch  108/269   train_loss = 3.773\n",
      "Epoch  92 Batch  109/269   train_loss = 3.865\n",
      "Epoch  92 Batch  110/269   train_loss = 3.814\n",
      "Epoch  92 Batch  111/269   train_loss = 3.909\n",
      "Epoch  92 Batch  112/269   train_loss = 3.894\n",
      "Epoch  92 Batch  113/269   train_loss = 3.769\n",
      "Epoch  92 Batch  114/269   train_loss = 3.688\n",
      "Epoch  92 Batch  115/269   train_loss = 3.600\n",
      "Epoch  92 Batch  116/269   train_loss = 3.533\n",
      "Epoch  92 Batch  117/269   train_loss = 3.613\n",
      "Epoch  92 Batch  118/269   train_loss = 3.855\n",
      "Epoch  92 Batch  119/269   train_loss = 3.611\n",
      "Epoch  92 Batch  120/269   train_loss = 3.659\n",
      "Epoch  92 Batch  121/269   train_loss = 3.711\n",
      "Epoch  92 Batch  122/269   train_loss = 3.711\n",
      "Epoch  92 Batch  123/269   train_loss = 3.541\n",
      "Epoch  92 Batch  124/269   train_loss = 3.892\n",
      "Epoch  92 Batch  125/269   train_loss = 3.507\n",
      "Epoch  92 Batch  126/269   train_loss = 3.372\n",
      "Epoch  92 Batch  127/269   train_loss = 3.777\n",
      "Epoch  92 Batch  128/269   train_loss = 3.675\n",
      "Epoch  92 Batch  129/269   train_loss = 3.646\n",
      "Epoch  92 Batch  130/269   train_loss = 3.792\n",
      "Epoch  92 Batch  131/269   train_loss = 3.428\n",
      "Epoch  92 Batch  132/269   train_loss = 3.746\n",
      "Epoch  92 Batch  133/269   train_loss = 3.446\n",
      "Epoch  92 Batch  134/269   train_loss = 3.875\n",
      "Epoch  92 Batch  135/269   train_loss = 3.601\n",
      "Epoch  92 Batch  136/269   train_loss = 3.446\n",
      "Epoch  92 Batch  137/269   train_loss = 3.807\n",
      "Epoch  92 Batch  138/269   train_loss = 3.563\n",
      "Epoch  92 Batch  139/269   train_loss = 3.724\n",
      "Epoch  92 Batch  140/269   train_loss = 3.859\n",
      "Epoch  92 Batch  141/269   train_loss = 4.102\n",
      "Epoch  92 Batch  142/269   train_loss = 3.546\n",
      "Epoch  92 Batch  143/269   train_loss = 3.663\n",
      "Epoch  92 Batch  144/269   train_loss = 3.864\n",
      "Epoch  92 Batch  145/269   train_loss = 3.684\n",
      "Epoch  92 Batch  146/269   train_loss = 3.855\n",
      "Epoch  92 Batch  147/269   train_loss = 3.519\n",
      "Epoch  92 Batch  148/269   train_loss = 3.860\n",
      "Epoch  92 Batch  149/269   train_loss = 3.605\n",
      "Epoch  92 Batch  150/269   train_loss = 3.825\n",
      "Epoch  92 Batch  151/269   train_loss = 3.791\n",
      "Epoch  92 Batch  152/269   train_loss = 3.729\n",
      "Epoch  92 Batch  153/269   train_loss = 3.765\n",
      "Epoch  92 Batch  154/269   train_loss = 3.624\n",
      "Epoch  92 Batch  155/269   train_loss = 3.639\n",
      "Epoch  92 Batch  156/269   train_loss = 4.097\n",
      "Epoch  92 Batch  157/269   train_loss = 3.880\n",
      "Epoch  92 Batch  158/269   train_loss = 3.763\n",
      "Epoch  92 Batch  159/269   train_loss = 3.828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  92 Batch  160/269   train_loss = 3.614\n",
      "Epoch  92 Batch  161/269   train_loss = 3.882\n",
      "Epoch  92 Batch  162/269   train_loss = 3.864\n",
      "Epoch  92 Batch  163/269   train_loss = 3.644\n",
      "Epoch  92 Batch  164/269   train_loss = 3.880\n",
      "Epoch  92 Batch  165/269   train_loss = 3.941\n",
      "Epoch  92 Batch  166/269   train_loss = 3.767\n",
      "Epoch  92 Batch  167/269   train_loss = 3.555\n",
      "Epoch  92 Batch  168/269   train_loss = 3.712\n",
      "Epoch  92 Batch  169/269   train_loss = 4.052\n",
      "Epoch  92 Batch  170/269   train_loss = 3.557\n",
      "Epoch  92 Batch  171/269   train_loss = 3.820\n",
      "Epoch  92 Batch  172/269   train_loss = 3.524\n",
      "Epoch  92 Batch  173/269   train_loss = 3.823\n",
      "Epoch  92 Batch  174/269   train_loss = 3.861\n",
      "Epoch  92 Batch  175/269   train_loss = 3.771\n",
      "Epoch  92 Batch  176/269   train_loss = 3.685\n",
      "Epoch  92 Batch  177/269   train_loss = 3.835\n",
      "Epoch  92 Batch  178/269   train_loss = 3.977\n",
      "Epoch  92 Batch  179/269   train_loss = 3.839\n",
      "Epoch  92 Batch  180/269   train_loss = 3.649\n",
      "Epoch  92 Batch  181/269   train_loss = 3.855\n",
      "Epoch  92 Batch  182/269   train_loss = 3.701\n",
      "Epoch  92 Batch  183/269   train_loss = 3.628\n",
      "Epoch  92 Batch  184/269   train_loss = 3.694\n",
      "Epoch  92 Batch  185/269   train_loss = 3.822\n",
      "Epoch  92 Batch  186/269   train_loss = 3.778\n",
      "Epoch  92 Batch  187/269   train_loss = 3.891\n",
      "Epoch  92 Batch  188/269   train_loss = 3.555\n",
      "Epoch  92 Batch  189/269   train_loss = 3.702\n",
      "Epoch  92 Batch  190/269   train_loss = 4.168\n",
      "Epoch  92 Batch  191/269   train_loss = 3.774\n",
      "Epoch  92 Batch  192/269   train_loss = 3.778\n",
      "Epoch  92 Batch  193/269   train_loss = 3.708\n",
      "Epoch  92 Batch  194/269   train_loss = 3.804\n",
      "Epoch  92 Batch  195/269   train_loss = 3.726\n",
      "Epoch  92 Batch  196/269   train_loss = 3.742\n",
      "Epoch  92 Batch  197/269   train_loss = 3.896\n",
      "Epoch  92 Batch  198/269   train_loss = 3.760\n",
      "Epoch  92 Batch  199/269   train_loss = 3.861\n",
      "Epoch  92 Batch  200/269   train_loss = 3.599\n",
      "Epoch  92 Batch  201/269   train_loss = 3.700\n",
      "Epoch  92 Batch  202/269   train_loss = 3.552\n",
      "Epoch  92 Batch  203/269   train_loss = 3.676\n",
      "Epoch  92 Batch  204/269   train_loss = 3.803\n",
      "Epoch  92 Batch  205/269   train_loss = 3.750\n",
      "Epoch  92 Batch  206/269   train_loss = 3.678\n",
      "Epoch  92 Batch  207/269   train_loss = 3.665\n",
      "Epoch  92 Batch  208/269   train_loss = 3.804\n",
      "Epoch  92 Batch  209/269   train_loss = 3.839\n",
      "Epoch  92 Batch  210/269   train_loss = 3.603\n",
      "Epoch  92 Batch  211/269   train_loss = 3.708\n",
      "Epoch  92 Batch  212/269   train_loss = 4.054\n",
      "Epoch  92 Batch  213/269   train_loss = 3.624\n",
      "Epoch  92 Batch  214/269   train_loss = 3.635\n",
      "Epoch  92 Batch  215/269   train_loss = 3.954\n",
      "Epoch  92 Batch  216/269   train_loss = 3.942\n",
      "Epoch  92 Batch  217/269   train_loss = 3.690\n",
      "Epoch  92 Batch  218/269   train_loss = 3.752\n",
      "Epoch  92 Batch  219/269   train_loss = 3.542\n",
      "Epoch  92 Batch  220/269   train_loss = 3.868\n",
      "Epoch  92 Batch  221/269   train_loss = 3.579\n",
      "Epoch  92 Batch  222/269   train_loss = 3.752\n",
      "Epoch  92 Batch  223/269   train_loss = 3.589\n",
      "Epoch  92 Batch  224/269   train_loss = 3.929\n",
      "Epoch  92 Batch  225/269   train_loss = 3.899\n",
      "Epoch  92 Batch  226/269   train_loss = 3.693\n",
      "Epoch  92 Batch  227/269   train_loss = 3.450\n",
      "Epoch  92 Batch  228/269   train_loss = 3.736\n",
      "Epoch  92 Batch  229/269   train_loss = 3.963\n",
      "Epoch  92 Batch  230/269   train_loss = 3.835\n",
      "Epoch  92 Batch  231/269   train_loss = 3.681\n",
      "Epoch  92 Batch  232/269   train_loss = 3.758\n",
      "Epoch  92 Batch  233/269   train_loss = 3.797\n",
      "Epoch  92 Batch  234/269   train_loss = 3.778\n",
      "Epoch  92 Batch  235/269   train_loss = 3.966\n",
      "Epoch  92 Batch  236/269   train_loss = 3.582\n",
      "Epoch  92 Batch  237/269   train_loss = 3.283\n",
      "Epoch  92 Batch  238/269   train_loss = 3.552\n",
      "Epoch  92 Batch  239/269   train_loss = 3.999\n",
      "Epoch  92 Batch  240/269   train_loss = 3.581\n",
      "Epoch  92 Batch  241/269   train_loss = 3.942\n",
      "Epoch  92 Batch  242/269   train_loss = 3.583\n",
      "Epoch  92 Batch  243/269   train_loss = 3.824\n",
      "Epoch  92 Batch  244/269   train_loss = 3.661\n",
      "Epoch  92 Batch  245/269   train_loss = 3.709\n",
      "Epoch  92 Batch  246/269   train_loss = 3.362\n",
      "Epoch  92 Batch  247/269   train_loss = 3.718\n",
      "Epoch  92 Batch  248/269   train_loss = 3.929\n",
      "Epoch  92 Batch  249/269   train_loss = 3.639\n",
      "Epoch  92 Batch  250/269   train_loss = 3.550\n",
      "Epoch  92 Batch  251/269   train_loss = 3.893\n",
      "Epoch  92 Batch  252/269   train_loss = 3.807\n",
      "Epoch  92 Batch  253/269   train_loss = 3.690\n",
      "Epoch  92 Batch  254/269   train_loss = 3.644\n",
      "Epoch  92 Batch  255/269   train_loss = 3.776\n",
      "Epoch  92 Batch  256/269   train_loss = 3.650\n",
      "Epoch  92 Batch  257/269   train_loss = 3.577\n",
      "Epoch  92 Batch  258/269   train_loss = 3.516\n",
      "Epoch  92 Batch  259/269   train_loss = 3.521\n",
      "Epoch  92 Batch  260/269   train_loss = 3.821\n",
      "Epoch  92 Batch  261/269   train_loss = 3.826\n",
      "Epoch  92 Batch  262/269   train_loss = 3.590\n",
      "Epoch  92 Batch  263/269   train_loss = 3.538\n",
      "Epoch  92 Batch  264/269   train_loss = 3.899\n",
      "Epoch  92 Batch  265/269   train_loss = 3.739\n",
      "Epoch  92 Batch  266/269   train_loss = 3.643\n",
      "Epoch  92 Batch  267/269   train_loss = 3.736\n",
      "Epoch  92 Batch  268/269   train_loss = 3.889\n",
      "Epoch  93 Batch    0/269   train_loss = 3.659\n",
      "Epoch  93 Batch    1/269   train_loss = 3.541\n",
      "Epoch  93 Batch    2/269   train_loss = 3.592\n",
      "Epoch  93 Batch    3/269   train_loss = 3.663\n",
      "Epoch  93 Batch    4/269   train_loss = 4.098\n",
      "Epoch  93 Batch    5/269   train_loss = 3.737\n",
      "Epoch  93 Batch    6/269   train_loss = 3.567\n",
      "Epoch  93 Batch    7/269   train_loss = 3.488\n",
      "Epoch  93 Batch    8/269   train_loss = 3.785\n",
      "Epoch  93 Batch    9/269   train_loss = 3.426\n",
      "Epoch  93 Batch   10/269   train_loss = 3.630\n",
      "Epoch  93 Batch   11/269   train_loss = 3.657\n",
      "Epoch  93 Batch   12/269   train_loss = 3.417\n",
      "Epoch  93 Batch   13/269   train_loss = 3.732\n",
      "Epoch  93 Batch   14/269   train_loss = 3.625\n",
      "Epoch  93 Batch   15/269   train_loss = 3.953\n",
      "Epoch  93 Batch   16/269   train_loss = 3.611\n",
      "Epoch  93 Batch   17/269   train_loss = 3.719\n",
      "Epoch  93 Batch   18/269   train_loss = 3.667\n",
      "Epoch  93 Batch   19/269   train_loss = 3.692\n",
      "Epoch  93 Batch   20/269   train_loss = 4.032\n",
      "Epoch  93 Batch   21/269   train_loss = 3.897\n",
      "Epoch  93 Batch   22/269   train_loss = 3.640\n",
      "Epoch  93 Batch   23/269   train_loss = 3.828\n",
      "Epoch  93 Batch   24/269   train_loss = 3.499\n",
      "Epoch  93 Batch   25/269   train_loss = 3.885\n",
      "Epoch  93 Batch   26/269   train_loss = 3.713\n",
      "Epoch  93 Batch   27/269   train_loss = 3.600\n",
      "Epoch  93 Batch   28/269   train_loss = 3.711\n",
      "Epoch  93 Batch   29/269   train_loss = 3.706\n",
      "Epoch  93 Batch   30/269   train_loss = 4.071\n",
      "Epoch  93 Batch   31/269   train_loss = 3.715\n",
      "Epoch  93 Batch   32/269   train_loss = 3.577\n",
      "Epoch  93 Batch   33/269   train_loss = 3.381\n",
      "Epoch  93 Batch   34/269   train_loss = 3.660\n",
      "Epoch  93 Batch   35/269   train_loss = 3.461\n",
      "Epoch  93 Batch   36/269   train_loss = 3.536\n",
      "Epoch  93 Batch   37/269   train_loss = 3.772\n",
      "Epoch  93 Batch   38/269   train_loss = 3.477\n",
      "Epoch  93 Batch   39/269   train_loss = 3.843\n",
      "Epoch  93 Batch   40/269   train_loss = 3.622\n",
      "Epoch  93 Batch   41/269   train_loss = 3.638\n",
      "Epoch  93 Batch   42/269   train_loss = 4.073\n",
      "Epoch  93 Batch   43/269   train_loss = 3.955\n",
      "Epoch  93 Batch   44/269   train_loss = 3.730\n",
      "Epoch  93 Batch   45/269   train_loss = 3.433\n",
      "Epoch  93 Batch   46/269   train_loss = 3.743\n",
      "Epoch  93 Batch   47/269   train_loss = 3.431\n",
      "Epoch  93 Batch   48/269   train_loss = 3.940\n",
      "Epoch  93 Batch   49/269   train_loss = 3.869\n",
      "Epoch  93 Batch   50/269   train_loss = 3.906\n",
      "Epoch  93 Batch   51/269   train_loss = 3.719\n",
      "Epoch  93 Batch   52/269   train_loss = 3.738\n",
      "Epoch  93 Batch   53/269   train_loss = 3.965\n",
      "Epoch  93 Batch   54/269   train_loss = 3.684\n",
      "Epoch  93 Batch   55/269   train_loss = 3.736\n",
      "Epoch  93 Batch   56/269   train_loss = 3.519\n",
      "Epoch  93 Batch   57/269   train_loss = 3.898\n",
      "Epoch  93 Batch   58/269   train_loss = 3.571\n",
      "Epoch  93 Batch   59/269   train_loss = 3.687\n",
      "Epoch  93 Batch   60/269   train_loss = 3.790\n",
      "Epoch  93 Batch   61/269   train_loss = 3.893\n",
      "Epoch  93 Batch   62/269   train_loss = 3.562\n",
      "Epoch  93 Batch   63/269   train_loss = 3.888\n",
      "Epoch  93 Batch   64/269   train_loss = 3.669\n",
      "Epoch  93 Batch   65/269   train_loss = 3.761\n",
      "Epoch  93 Batch   66/269   train_loss = 3.623\n",
      "Epoch  93 Batch   67/269   train_loss = 3.616\n",
      "Epoch  93 Batch   68/269   train_loss = 3.778\n",
      "Epoch  93 Batch   69/269   train_loss = 3.717\n",
      "Epoch  93 Batch   70/269   train_loss = 3.576\n",
      "Epoch  93 Batch   71/269   train_loss = 3.792\n",
      "Epoch  93 Batch   72/269   train_loss = 3.512\n",
      "Epoch  93 Batch   73/269   train_loss = 3.657\n",
      "Epoch  93 Batch   74/269   train_loss = 3.720\n",
      "Epoch  93 Batch   75/269   train_loss = 3.873\n",
      "Epoch  93 Batch   76/269   train_loss = 3.856\n",
      "Epoch  93 Batch   77/269   train_loss = 3.442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  93 Batch   78/269   train_loss = 3.868\n",
      "Epoch  93 Batch   79/269   train_loss = 3.886\n",
      "Epoch  93 Batch   80/269   train_loss = 3.829\n",
      "Epoch  93 Batch   81/269   train_loss = 3.681\n",
      "Epoch  93 Batch   82/269   train_loss = 3.793\n",
      "Epoch  93 Batch   83/269   train_loss = 3.591\n",
      "Epoch  93 Batch   84/269   train_loss = 3.928\n",
      "Epoch  93 Batch   85/269   train_loss = 4.022\n",
      "Epoch  93 Batch   86/269   train_loss = 4.054\n",
      "Epoch  93 Batch   87/269   train_loss = 3.755\n",
      "Epoch  93 Batch   88/269   train_loss = 3.831\n",
      "Epoch  93 Batch   89/269   train_loss = 3.711\n",
      "Epoch  93 Batch   90/269   train_loss = 3.592\n",
      "Epoch  93 Batch   91/269   train_loss = 3.841\n",
      "Epoch  93 Batch   92/269   train_loss = 3.912\n",
      "Epoch  93 Batch   93/269   train_loss = 3.635\n",
      "Epoch  93 Batch   94/269   train_loss = 4.023\n",
      "Epoch  93 Batch   95/269   train_loss = 3.537\n",
      "Epoch  93 Batch   96/269   train_loss = 3.611\n",
      "Epoch  93 Batch   97/269   train_loss = 3.681\n",
      "Epoch  93 Batch   98/269   train_loss = 3.969\n",
      "Epoch  93 Batch   99/269   train_loss = 3.783\n",
      "Epoch  93 Batch  100/269   train_loss = 3.738\n",
      "Epoch  93 Batch  101/269   train_loss = 3.801\n",
      "Epoch  93 Batch  102/269   train_loss = 3.646\n",
      "Epoch  93 Batch  103/269   train_loss = 3.718\n",
      "Epoch  93 Batch  104/269   train_loss = 3.659\n",
      "Epoch  93 Batch  105/269   train_loss = 3.894\n",
      "Epoch  93 Batch  106/269   train_loss = 4.056\n",
      "Epoch  93 Batch  107/269   train_loss = 3.997\n",
      "Epoch  93 Batch  108/269   train_loss = 3.786\n",
      "Epoch  93 Batch  109/269   train_loss = 3.925\n",
      "Epoch  93 Batch  110/269   train_loss = 3.893\n",
      "Epoch  93 Batch  111/269   train_loss = 3.900\n",
      "Epoch  93 Batch  112/269   train_loss = 3.887\n",
      "Epoch  93 Batch  113/269   train_loss = 3.778\n",
      "Epoch  93 Batch  114/269   train_loss = 3.678\n",
      "Epoch  93 Batch  115/269   train_loss = 3.589\n",
      "Epoch  93 Batch  116/269   train_loss = 3.556\n",
      "Epoch  93 Batch  117/269   train_loss = 3.633\n",
      "Epoch  93 Batch  118/269   train_loss = 3.877\n",
      "Epoch  93 Batch  119/269   train_loss = 3.628\n",
      "Epoch  93 Batch  120/269   train_loss = 3.669\n",
      "Epoch  93 Batch  121/269   train_loss = 3.731\n",
      "Epoch  93 Batch  122/269   train_loss = 3.751\n",
      "Epoch  93 Batch  123/269   train_loss = 3.517\n",
      "Epoch  93 Batch  124/269   train_loss = 3.912\n",
      "Epoch  93 Batch  125/269   train_loss = 3.534\n",
      "Epoch  93 Batch  126/269   train_loss = 3.431\n",
      "Epoch  93 Batch  127/269   train_loss = 3.780\n",
      "Epoch  93 Batch  128/269   train_loss = 3.646\n",
      "Epoch  93 Batch  129/269   train_loss = 3.642\n",
      "Epoch  93 Batch  130/269   train_loss = 3.821\n",
      "Epoch  93 Batch  131/269   train_loss = 3.427\n",
      "Epoch  93 Batch  132/269   train_loss = 3.737\n",
      "Epoch  93 Batch  133/269   train_loss = 3.478\n",
      "Epoch  93 Batch  134/269   train_loss = 3.879\n",
      "Epoch  93 Batch  135/269   train_loss = 3.580\n",
      "Epoch  93 Batch  136/269   train_loss = 3.430\n",
      "Epoch  93 Batch  137/269   train_loss = 3.863\n",
      "Epoch  93 Batch  138/269   train_loss = 3.608\n",
      "Epoch  93 Batch  139/269   train_loss = 3.686\n",
      "Epoch  93 Batch  140/269   train_loss = 3.897\n",
      "Epoch  93 Batch  141/269   train_loss = 4.134\n",
      "Epoch  93 Batch  142/269   train_loss = 3.563\n",
      "Epoch  93 Batch  143/269   train_loss = 3.681\n",
      "Epoch  93 Batch  144/269   train_loss = 3.873\n",
      "Epoch  93 Batch  145/269   train_loss = 3.698\n",
      "Epoch  93 Batch  146/269   train_loss = 3.864\n",
      "Epoch  93 Batch  147/269   train_loss = 3.525\n",
      "Epoch  93 Batch  148/269   train_loss = 3.833\n",
      "Epoch  93 Batch  149/269   train_loss = 3.619\n",
      "Epoch  93 Batch  150/269   train_loss = 3.839\n",
      "Epoch  93 Batch  151/269   train_loss = 3.783\n",
      "Epoch  93 Batch  152/269   train_loss = 3.761\n",
      "Epoch  93 Batch  153/269   train_loss = 3.808\n",
      "Epoch  93 Batch  154/269   train_loss = 3.635\n",
      "Epoch  93 Batch  155/269   train_loss = 3.616\n",
      "Epoch  93 Batch  156/269   train_loss = 4.093\n",
      "Epoch  93 Batch  157/269   train_loss = 3.874\n",
      "Epoch  93 Batch  158/269   train_loss = 3.795\n",
      "Epoch  93 Batch  159/269   train_loss = 3.804\n",
      "Epoch  93 Batch  160/269   train_loss = 3.657\n",
      "Epoch  93 Batch  161/269   train_loss = 3.893\n",
      "Epoch  93 Batch  162/269   train_loss = 3.904\n",
      "Epoch  93 Batch  163/269   train_loss = 3.593\n",
      "Epoch  93 Batch  164/269   train_loss = 3.852\n",
      "Epoch  93 Batch  165/269   train_loss = 4.018\n",
      "Epoch  93 Batch  166/269   train_loss = 3.765\n",
      "Epoch  93 Batch  167/269   train_loss = 3.581\n",
      "Epoch  93 Batch  168/269   train_loss = 3.719\n",
      "Epoch  93 Batch  169/269   train_loss = 3.992\n",
      "Epoch  93 Batch  170/269   train_loss = 3.560\n",
      "Epoch  93 Batch  171/269   train_loss = 3.835\n",
      "Epoch  93 Batch  172/269   train_loss = 3.573\n",
      "Epoch  93 Batch  173/269   train_loss = 3.769\n",
      "Epoch  93 Batch  174/269   train_loss = 3.874\n",
      "Epoch  93 Batch  175/269   train_loss = 3.744\n",
      "Epoch  93 Batch  176/269   train_loss = 3.700\n",
      "Epoch  93 Batch  177/269   train_loss = 3.836\n",
      "Epoch  93 Batch  178/269   train_loss = 4.002\n",
      "Epoch  93 Batch  179/269   train_loss = 3.787\n",
      "Epoch  93 Batch  180/269   train_loss = 3.595\n",
      "Epoch  93 Batch  181/269   train_loss = 3.842\n",
      "Epoch  93 Batch  182/269   train_loss = 3.643\n",
      "Epoch  93 Batch  183/269   train_loss = 3.631\n",
      "Epoch  93 Batch  184/269   train_loss = 3.699\n",
      "Epoch  93 Batch  185/269   train_loss = 3.777\n",
      "Epoch  93 Batch  186/269   train_loss = 3.770\n",
      "Epoch  93 Batch  187/269   train_loss = 3.844\n",
      "Epoch  93 Batch  188/269   train_loss = 3.573\n",
      "Epoch  93 Batch  189/269   train_loss = 3.685\n",
      "Epoch  93 Batch  190/269   train_loss = 4.151\n",
      "Epoch  93 Batch  191/269   train_loss = 3.782\n",
      "Epoch  93 Batch  192/269   train_loss = 3.824\n",
      "Epoch  93 Batch  193/269   train_loss = 3.677\n",
      "Epoch  93 Batch  194/269   train_loss = 3.803\n",
      "Epoch  93 Batch  195/269   train_loss = 3.740\n",
      "Epoch  93 Batch  196/269   train_loss = 3.718\n",
      "Epoch  93 Batch  197/269   train_loss = 3.904\n",
      "Epoch  93 Batch  198/269   train_loss = 3.768\n",
      "Epoch  93 Batch  199/269   train_loss = 3.852\n",
      "Epoch  93 Batch  200/269   train_loss = 3.539\n",
      "Epoch  93 Batch  201/269   train_loss = 3.704\n",
      "Epoch  93 Batch  202/269   train_loss = 3.586\n",
      "Epoch  93 Batch  203/269   train_loss = 3.665\n",
      "Epoch  93 Batch  204/269   train_loss = 3.819\n",
      "Epoch  93 Batch  205/269   train_loss = 3.760\n",
      "Epoch  93 Batch  206/269   train_loss = 3.652\n",
      "Epoch  93 Batch  207/269   train_loss = 3.673\n",
      "Epoch  93 Batch  208/269   train_loss = 3.815\n",
      "Epoch  93 Batch  209/269   train_loss = 3.824\n",
      "Epoch  93 Batch  210/269   train_loss = 3.622\n",
      "Epoch  93 Batch  211/269   train_loss = 3.712\n",
      "Epoch  93 Batch  212/269   train_loss = 4.038\n",
      "Epoch  93 Batch  213/269   train_loss = 3.601\n",
      "Epoch  93 Batch  214/269   train_loss = 3.633\n",
      "Epoch  93 Batch  215/269   train_loss = 3.928\n",
      "Epoch  93 Batch  216/269   train_loss = 3.962\n",
      "Epoch  93 Batch  217/269   train_loss = 3.610\n",
      "Epoch  93 Batch  218/269   train_loss = 3.749\n",
      "Epoch  93 Batch  219/269   train_loss = 3.525\n",
      "Epoch  93 Batch  220/269   train_loss = 3.897\n",
      "Epoch  93 Batch  221/269   train_loss = 3.601\n",
      "Epoch  93 Batch  222/269   train_loss = 3.771\n",
      "Epoch  93 Batch  223/269   train_loss = 3.593\n",
      "Epoch  93 Batch  224/269   train_loss = 3.904\n",
      "Epoch  93 Batch  225/269   train_loss = 3.915\n",
      "Epoch  93 Batch  226/269   train_loss = 3.667\n",
      "Epoch  93 Batch  227/269   train_loss = 3.451\n",
      "Epoch  93 Batch  228/269   train_loss = 3.757\n",
      "Epoch  93 Batch  229/269   train_loss = 3.961\n",
      "Epoch  93 Batch  230/269   train_loss = 3.811\n",
      "Epoch  93 Batch  231/269   train_loss = 3.665\n",
      "Epoch  93 Batch  232/269   train_loss = 3.758\n",
      "Epoch  93 Batch  233/269   train_loss = 3.880\n",
      "Epoch  93 Batch  234/269   train_loss = 3.755\n",
      "Epoch  93 Batch  235/269   train_loss = 4.025\n",
      "Epoch  93 Batch  236/269   train_loss = 3.591\n",
      "Epoch  93 Batch  237/269   train_loss = 3.250\n",
      "Epoch  93 Batch  238/269   train_loss = 3.586\n",
      "Epoch  93 Batch  239/269   train_loss = 4.013\n",
      "Epoch  93 Batch  240/269   train_loss = 3.599\n",
      "Epoch  93 Batch  241/269   train_loss = 3.929\n",
      "Epoch  93 Batch  242/269   train_loss = 3.595\n",
      "Epoch  93 Batch  243/269   train_loss = 3.829\n",
      "Epoch  93 Batch  244/269   train_loss = 3.684\n",
      "Epoch  93 Batch  245/269   train_loss = 3.695\n",
      "Epoch  93 Batch  246/269   train_loss = 3.369\n",
      "Epoch  93 Batch  247/269   train_loss = 3.708\n",
      "Epoch  93 Batch  248/269   train_loss = 3.874\n",
      "Epoch  93 Batch  249/269   train_loss = 3.653\n",
      "Epoch  93 Batch  250/269   train_loss = 3.579\n",
      "Epoch  93 Batch  251/269   train_loss = 3.888\n",
      "Epoch  93 Batch  252/269   train_loss = 3.811\n",
      "Epoch  93 Batch  253/269   train_loss = 3.661\n",
      "Epoch  93 Batch  254/269   train_loss = 3.711\n",
      "Epoch  93 Batch  255/269   train_loss = 3.748\n",
      "Epoch  93 Batch  256/269   train_loss = 3.664\n",
      "Epoch  93 Batch  257/269   train_loss = 3.615\n",
      "Epoch  93 Batch  258/269   train_loss = 3.528\n",
      "Epoch  93 Batch  259/269   train_loss = 3.577\n",
      "Epoch  93 Batch  260/269   train_loss = 3.781\n",
      "Epoch  93 Batch  261/269   train_loss = 3.808\n",
      "Epoch  93 Batch  262/269   train_loss = 3.582\n",
      "Epoch  93 Batch  263/269   train_loss = 3.529\n",
      "Epoch  93 Batch  264/269   train_loss = 3.887\n",
      "Epoch  93 Batch  265/269   train_loss = 3.764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  93 Batch  266/269   train_loss = 3.668\n",
      "Epoch  93 Batch  267/269   train_loss = 3.758\n",
      "Epoch  93 Batch  268/269   train_loss = 3.885\n",
      "Epoch  94 Batch    0/269   train_loss = 3.683\n",
      "Epoch  94 Batch    1/269   train_loss = 3.536\n",
      "Epoch  94 Batch    2/269   train_loss = 3.575\n",
      "Epoch  94 Batch    3/269   train_loss = 3.638\n",
      "Epoch  94 Batch    4/269   train_loss = 4.067\n",
      "Epoch  94 Batch    5/269   train_loss = 3.733\n",
      "Epoch  94 Batch    6/269   train_loss = 3.532\n",
      "Epoch  94 Batch    7/269   train_loss = 3.505\n",
      "Epoch  94 Batch    8/269   train_loss = 3.808\n",
      "Epoch  94 Batch    9/269   train_loss = 3.439\n",
      "Epoch  94 Batch   10/269   train_loss = 3.604\n",
      "Epoch  94 Batch   11/269   train_loss = 3.687\n",
      "Epoch  94 Batch   12/269   train_loss = 3.393\n",
      "Epoch  94 Batch   13/269   train_loss = 3.717\n",
      "Epoch  94 Batch   14/269   train_loss = 3.587\n",
      "Epoch  94 Batch   15/269   train_loss = 3.960\n",
      "Epoch  94 Batch   16/269   train_loss = 3.606\n",
      "Epoch  94 Batch   17/269   train_loss = 3.736\n",
      "Epoch  94 Batch   18/269   train_loss = 3.696\n",
      "Epoch  94 Batch   19/269   train_loss = 3.708\n",
      "Epoch  94 Batch   20/269   train_loss = 4.019\n",
      "Epoch  94 Batch   21/269   train_loss = 3.903\n",
      "Epoch  94 Batch   22/269   train_loss = 3.687\n",
      "Epoch  94 Batch   23/269   train_loss = 3.820\n",
      "Epoch  94 Batch   24/269   train_loss = 3.518\n",
      "Epoch  94 Batch   25/269   train_loss = 3.894\n",
      "Epoch  94 Batch   26/269   train_loss = 3.705\n",
      "Epoch  94 Batch   27/269   train_loss = 3.640\n",
      "Epoch  94 Batch   28/269   train_loss = 3.730\n",
      "Epoch  94 Batch   29/269   train_loss = 3.714\n",
      "Epoch  94 Batch   30/269   train_loss = 4.055\n",
      "Epoch  94 Batch   31/269   train_loss = 3.718\n",
      "Epoch  94 Batch   32/269   train_loss = 3.558\n",
      "Epoch  94 Batch   33/269   train_loss = 3.365\n",
      "Epoch  94 Batch   34/269   train_loss = 3.655\n",
      "Epoch  94 Batch   35/269   train_loss = 3.442\n",
      "Epoch  94 Batch   36/269   train_loss = 3.526\n",
      "Epoch  94 Batch   37/269   train_loss = 3.776\n",
      "Epoch  94 Batch   38/269   train_loss = 3.440\n",
      "Epoch  94 Batch   39/269   train_loss = 3.876\n",
      "Epoch  94 Batch   40/269   train_loss = 3.631\n",
      "Epoch  94 Batch   41/269   train_loss = 3.659\n",
      "Epoch  94 Batch   42/269   train_loss = 4.073\n",
      "Epoch  94 Batch   43/269   train_loss = 3.976\n",
      "Epoch  94 Batch   44/269   train_loss = 3.750\n",
      "Epoch  94 Batch   45/269   train_loss = 3.418\n",
      "Epoch  94 Batch   46/269   train_loss = 3.752\n",
      "Epoch  94 Batch   47/269   train_loss = 3.398\n",
      "Epoch  94 Batch   48/269   train_loss = 3.929\n",
      "Epoch  94 Batch   49/269   train_loss = 3.885\n",
      "Epoch  94 Batch   50/269   train_loss = 3.911\n",
      "Epoch  94 Batch   51/269   train_loss = 3.739\n",
      "Epoch  94 Batch   52/269   train_loss = 3.732\n",
      "Epoch  94 Batch   53/269   train_loss = 3.987\n",
      "Epoch  94 Batch   54/269   train_loss = 3.650\n",
      "Epoch  94 Batch   55/269   train_loss = 3.732\n",
      "Epoch  94 Batch   56/269   train_loss = 3.502\n",
      "Epoch  94 Batch   57/269   train_loss = 3.957\n",
      "Epoch  94 Batch   58/269   train_loss = 3.595\n",
      "Epoch  94 Batch   59/269   train_loss = 3.675\n",
      "Epoch  94 Batch   60/269   train_loss = 3.775\n",
      "Epoch  94 Batch   61/269   train_loss = 3.923\n",
      "Epoch  94 Batch   62/269   train_loss = 3.556\n",
      "Epoch  94 Batch   63/269   train_loss = 3.875\n",
      "Epoch  94 Batch   64/269   train_loss = 3.682\n",
      "Epoch  94 Batch   65/269   train_loss = 3.755\n",
      "Epoch  94 Batch   66/269   train_loss = 3.601\n",
      "Epoch  94 Batch   67/269   train_loss = 3.606\n",
      "Epoch  94 Batch   68/269   train_loss = 3.744\n",
      "Epoch  94 Batch   69/269   train_loss = 3.702\n",
      "Epoch  94 Batch   70/269   train_loss = 3.573\n",
      "Epoch  94 Batch   71/269   train_loss = 3.774\n",
      "Epoch  94 Batch   72/269   train_loss = 3.506\n",
      "Epoch  94 Batch   73/269   train_loss = 3.637\n",
      "Epoch  94 Batch   74/269   train_loss = 3.682\n",
      "Epoch  94 Batch   75/269   train_loss = 3.864\n",
      "Epoch  94 Batch   76/269   train_loss = 3.831\n",
      "Epoch  94 Batch   77/269   train_loss = 3.447\n",
      "Epoch  94 Batch   78/269   train_loss = 3.851\n",
      "Epoch  94 Batch   79/269   train_loss = 3.880\n",
      "Epoch  94 Batch   80/269   train_loss = 3.825\n",
      "Epoch  94 Batch   81/269   train_loss = 3.689\n",
      "Epoch  94 Batch   82/269   train_loss = 3.795\n",
      "Epoch  94 Batch   83/269   train_loss = 3.572\n",
      "Epoch  94 Batch   84/269   train_loss = 3.910\n",
      "Epoch  94 Batch   85/269   train_loss = 4.063\n",
      "Epoch  94 Batch   86/269   train_loss = 4.005\n",
      "Epoch  94 Batch   87/269   train_loss = 3.726\n",
      "Epoch  94 Batch   88/269   train_loss = 3.805\n",
      "Epoch  94 Batch   89/269   train_loss = 3.716\n",
      "Epoch  94 Batch   90/269   train_loss = 3.581\n",
      "Epoch  94 Batch   91/269   train_loss = 3.810\n",
      "Epoch  94 Batch   92/269   train_loss = 3.925\n",
      "Epoch  94 Batch   93/269   train_loss = 3.638\n",
      "Epoch  94 Batch   94/269   train_loss = 4.016\n",
      "Epoch  94 Batch   95/269   train_loss = 3.525\n",
      "Epoch  94 Batch   96/269   train_loss = 3.619\n",
      "Epoch  94 Batch   97/269   train_loss = 3.736\n",
      "Epoch  94 Batch   98/269   train_loss = 3.979\n",
      "Epoch  94 Batch   99/269   train_loss = 3.809\n",
      "Epoch  94 Batch  100/269   train_loss = 3.754\n",
      "Epoch  94 Batch  101/269   train_loss = 3.855\n",
      "Epoch  94 Batch  102/269   train_loss = 3.596\n",
      "Epoch  94 Batch  103/269   train_loss = 3.739\n",
      "Epoch  94 Batch  104/269   train_loss = 3.662\n",
      "Epoch  94 Batch  105/269   train_loss = 3.861\n",
      "Epoch  94 Batch  106/269   train_loss = 4.025\n",
      "Epoch  94 Batch  107/269   train_loss = 4.043\n",
      "Epoch  94 Batch  108/269   train_loss = 3.826\n",
      "Epoch  94 Batch  109/269   train_loss = 3.858\n",
      "Epoch  94 Batch  110/269   train_loss = 3.859\n",
      "Epoch  94 Batch  111/269   train_loss = 3.876\n",
      "Epoch  94 Batch  112/269   train_loss = 3.900\n",
      "Epoch  94 Batch  113/269   train_loss = 3.803\n",
      "Epoch  94 Batch  114/269   train_loss = 3.681\n",
      "Epoch  94 Batch  115/269   train_loss = 3.604\n",
      "Epoch  94 Batch  116/269   train_loss = 3.516\n",
      "Epoch  94 Batch  117/269   train_loss = 3.641\n",
      "Epoch  94 Batch  118/269   train_loss = 3.876\n",
      "Epoch  94 Batch  119/269   train_loss = 3.625\n",
      "Epoch  94 Batch  120/269   train_loss = 3.664\n",
      "Epoch  94 Batch  121/269   train_loss = 3.703\n",
      "Epoch  94 Batch  122/269   train_loss = 3.715\n",
      "Epoch  94 Batch  123/269   train_loss = 3.519\n",
      "Epoch  94 Batch  124/269   train_loss = 3.920\n",
      "Epoch  94 Batch  125/269   train_loss = 3.525\n",
      "Epoch  94 Batch  126/269   train_loss = 3.426\n",
      "Epoch  94 Batch  127/269   train_loss = 3.796\n",
      "Epoch  94 Batch  128/269   train_loss = 3.640\n",
      "Epoch  94 Batch  129/269   train_loss = 3.645\n",
      "Epoch  94 Batch  130/269   train_loss = 3.817\n",
      "Epoch  94 Batch  131/269   train_loss = 3.408\n",
      "Epoch  94 Batch  132/269   train_loss = 3.748\n",
      "Epoch  94 Batch  133/269   train_loss = 3.451\n",
      "Epoch  94 Batch  134/269   train_loss = 3.881\n",
      "Epoch  94 Batch  135/269   train_loss = 3.581\n",
      "Epoch  94 Batch  136/269   train_loss = 3.445\n",
      "Epoch  94 Batch  137/269   train_loss = 3.836\n",
      "Epoch  94 Batch  138/269   train_loss = 3.567\n",
      "Epoch  94 Batch  139/269   train_loss = 3.640\n",
      "Epoch  94 Batch  140/269   train_loss = 3.903\n",
      "Epoch  94 Batch  141/269   train_loss = 4.130\n",
      "Epoch  94 Batch  142/269   train_loss = 3.574\n",
      "Epoch  94 Batch  143/269   train_loss = 3.703\n",
      "Epoch  94 Batch  144/269   train_loss = 3.875\n",
      "Epoch  94 Batch  145/269   train_loss = 3.708\n",
      "Epoch  94 Batch  146/269   train_loss = 3.903\n",
      "Epoch  94 Batch  147/269   train_loss = 3.532\n",
      "Epoch  94 Batch  148/269   train_loss = 3.851\n",
      "Epoch  94 Batch  149/269   train_loss = 3.622\n",
      "Epoch  94 Batch  150/269   train_loss = 3.872\n",
      "Epoch  94 Batch  151/269   train_loss = 3.796\n",
      "Epoch  94 Batch  152/269   train_loss = 3.755\n",
      "Epoch  94 Batch  153/269   train_loss = 3.801\n",
      "Epoch  94 Batch  154/269   train_loss = 3.662\n",
      "Epoch  94 Batch  155/269   train_loss = 3.625\n",
      "Epoch  94 Batch  156/269   train_loss = 4.075\n",
      "Epoch  94 Batch  157/269   train_loss = 3.896\n",
      "Epoch  94 Batch  158/269   train_loss = 3.773\n",
      "Epoch  94 Batch  159/269   train_loss = 3.768\n",
      "Epoch  94 Batch  160/269   train_loss = 3.650\n",
      "Epoch  94 Batch  161/269   train_loss = 3.906\n",
      "Epoch  94 Batch  162/269   train_loss = 3.829\n",
      "Epoch  94 Batch  163/269   train_loss = 3.595\n",
      "Epoch  94 Batch  164/269   train_loss = 3.858\n",
      "Epoch  94 Batch  165/269   train_loss = 4.001\n",
      "Epoch  94 Batch  166/269   train_loss = 3.751\n",
      "Epoch  94 Batch  167/269   train_loss = 3.581\n",
      "Epoch  94 Batch  168/269   train_loss = 3.712\n",
      "Epoch  94 Batch  169/269   train_loss = 3.983\n",
      "Epoch  94 Batch  170/269   train_loss = 3.571\n",
      "Epoch  94 Batch  171/269   train_loss = 3.834\n",
      "Epoch  94 Batch  172/269   train_loss = 3.544\n",
      "Epoch  94 Batch  173/269   train_loss = 3.757\n",
      "Epoch  94 Batch  174/269   train_loss = 3.890\n",
      "Epoch  94 Batch  175/269   train_loss = 3.768\n",
      "Epoch  94 Batch  176/269   train_loss = 3.699\n",
      "Epoch  94 Batch  177/269   train_loss = 3.840\n",
      "Epoch  94 Batch  178/269   train_loss = 3.969\n",
      "Epoch  94 Batch  179/269   train_loss = 3.782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  94 Batch  180/269   train_loss = 3.602\n",
      "Epoch  94 Batch  181/269   train_loss = 3.836\n",
      "Epoch  94 Batch  182/269   train_loss = 3.666\n",
      "Epoch  94 Batch  183/269   train_loss = 3.641\n",
      "Epoch  94 Batch  184/269   train_loss = 3.774\n",
      "Epoch  94 Batch  185/269   train_loss = 3.749\n",
      "Epoch  94 Batch  186/269   train_loss = 3.795\n",
      "Epoch  94 Batch  187/269   train_loss = 3.828\n",
      "Epoch  94 Batch  188/269   train_loss = 3.548\n",
      "Epoch  94 Batch  189/269   train_loss = 3.676\n",
      "Epoch  94 Batch  190/269   train_loss = 4.116\n",
      "Epoch  94 Batch  191/269   train_loss = 3.754\n",
      "Epoch  94 Batch  192/269   train_loss = 3.820\n",
      "Epoch  94 Batch  193/269   train_loss = 3.703\n",
      "Epoch  94 Batch  194/269   train_loss = 3.792\n",
      "Epoch  94 Batch  195/269   train_loss = 3.735\n",
      "Epoch  94 Batch  196/269   train_loss = 3.713\n",
      "Epoch  94 Batch  197/269   train_loss = 3.890\n",
      "Epoch  94 Batch  198/269   train_loss = 3.750\n",
      "Epoch  94 Batch  199/269   train_loss = 3.840\n",
      "Epoch  94 Batch  200/269   train_loss = 3.573\n",
      "Epoch  94 Batch  201/269   train_loss = 3.702\n",
      "Epoch  94 Batch  202/269   train_loss = 3.554\n",
      "Epoch  94 Batch  203/269   train_loss = 3.680\n",
      "Epoch  94 Batch  204/269   train_loss = 3.821\n",
      "Epoch  94 Batch  205/269   train_loss = 3.761\n",
      "Epoch  94 Batch  206/269   train_loss = 3.696\n",
      "Epoch  94 Batch  207/269   train_loss = 3.692\n",
      "Epoch  94 Batch  208/269   train_loss = 3.831\n",
      "Epoch  94 Batch  209/269   train_loss = 3.822\n",
      "Epoch  94 Batch  210/269   train_loss = 3.616\n",
      "Epoch  94 Batch  211/269   train_loss = 3.737\n",
      "Epoch  94 Batch  212/269   train_loss = 4.077\n",
      "Epoch  94 Batch  213/269   train_loss = 3.601\n",
      "Epoch  94 Batch  214/269   train_loss = 3.678\n",
      "Epoch  94 Batch  215/269   train_loss = 3.922\n",
      "Epoch  94 Batch  216/269   train_loss = 3.972\n",
      "Epoch  94 Batch  217/269   train_loss = 3.605\n",
      "Epoch  94 Batch  218/269   train_loss = 3.733\n",
      "Epoch  94 Batch  219/269   train_loss = 3.534\n",
      "Epoch  94 Batch  220/269   train_loss = 3.947\n",
      "Epoch  94 Batch  221/269   train_loss = 3.580\n",
      "Epoch  94 Batch  222/269   train_loss = 3.772\n",
      "Epoch  94 Batch  223/269   train_loss = 3.587\n",
      "Epoch  94 Batch  224/269   train_loss = 3.842\n",
      "Epoch  94 Batch  225/269   train_loss = 3.927\n",
      "Epoch  94 Batch  226/269   train_loss = 3.708\n",
      "Epoch  94 Batch  227/269   train_loss = 3.440\n",
      "Epoch  94 Batch  228/269   train_loss = 3.745\n",
      "Epoch  94 Batch  229/269   train_loss = 3.960\n",
      "Epoch  94 Batch  230/269   train_loss = 3.825\n",
      "Epoch  94 Batch  231/269   train_loss = 3.688\n",
      "Epoch  94 Batch  232/269   train_loss = 3.739\n",
      "Epoch  94 Batch  233/269   train_loss = 3.904\n",
      "Epoch  94 Batch  234/269   train_loss = 3.772\n",
      "Epoch  94 Batch  235/269   train_loss = 4.040\n",
      "Epoch  94 Batch  236/269   train_loss = 3.604\n",
      "Epoch  94 Batch  237/269   train_loss = 3.273\n",
      "Epoch  94 Batch  238/269   train_loss = 3.559\n",
      "Epoch  94 Batch  239/269   train_loss = 4.031\n",
      "Epoch  94 Batch  240/269   train_loss = 3.605\n",
      "Epoch  94 Batch  241/269   train_loss = 3.908\n",
      "Epoch  94 Batch  242/269   train_loss = 3.567\n",
      "Epoch  94 Batch  243/269   train_loss = 3.856\n",
      "Epoch  94 Batch  244/269   train_loss = 3.651\n",
      "Epoch  94 Batch  245/269   train_loss = 3.711\n",
      "Epoch  94 Batch  246/269   train_loss = 3.387\n",
      "Epoch  94 Batch  247/269   train_loss = 3.746\n",
      "Epoch  94 Batch  248/269   train_loss = 3.893\n",
      "Epoch  94 Batch  249/269   train_loss = 3.681\n",
      "Epoch  94 Batch  250/269   train_loss = 3.563\n",
      "Epoch  94 Batch  251/269   train_loss = 3.917\n",
      "Epoch  94 Batch  252/269   train_loss = 3.825\n",
      "Epoch  94 Batch  253/269   train_loss = 3.610\n",
      "Epoch  94 Batch  254/269   train_loss = 3.662\n",
      "Epoch  94 Batch  255/269   train_loss = 3.737\n",
      "Epoch  94 Batch  256/269   train_loss = 3.648\n",
      "Epoch  94 Batch  257/269   train_loss = 3.558\n",
      "Epoch  94 Batch  258/269   train_loss = 3.515\n",
      "Epoch  94 Batch  259/269   train_loss = 3.568\n",
      "Epoch  94 Batch  260/269   train_loss = 3.789\n",
      "Epoch  94 Batch  261/269   train_loss = 3.794\n",
      "Epoch  94 Batch  262/269   train_loss = 3.601\n",
      "Epoch  94 Batch  263/269   train_loss = 3.490\n",
      "Epoch  94 Batch  264/269   train_loss = 3.889\n",
      "Epoch  94 Batch  265/269   train_loss = 3.746\n",
      "Epoch  94 Batch  266/269   train_loss = 3.684\n",
      "Epoch  94 Batch  267/269   train_loss = 3.735\n",
      "Epoch  94 Batch  268/269   train_loss = 3.871\n",
      "Epoch  95 Batch    0/269   train_loss = 3.669\n",
      "Epoch  95 Batch    1/269   train_loss = 3.524\n",
      "Epoch  95 Batch    2/269   train_loss = 3.573\n",
      "Epoch  95 Batch    3/269   train_loss = 3.640\n",
      "Epoch  95 Batch    4/269   train_loss = 4.052\n",
      "Epoch  95 Batch    5/269   train_loss = 3.734\n",
      "Epoch  95 Batch    6/269   train_loss = 3.506\n",
      "Epoch  95 Batch    7/269   train_loss = 3.532\n",
      "Epoch  95 Batch    8/269   train_loss = 3.798\n",
      "Epoch  95 Batch    9/269   train_loss = 3.424\n",
      "Epoch  95 Batch   10/269   train_loss = 3.588\n",
      "Epoch  95 Batch   11/269   train_loss = 3.689\n",
      "Epoch  95 Batch   12/269   train_loss = 3.400\n",
      "Epoch  95 Batch   13/269   train_loss = 3.726\n",
      "Epoch  95 Batch   14/269   train_loss = 3.587\n",
      "Epoch  95 Batch   15/269   train_loss = 3.967\n",
      "Epoch  95 Batch   16/269   train_loss = 3.632\n",
      "Epoch  95 Batch   17/269   train_loss = 3.703\n",
      "Epoch  95 Batch   18/269   train_loss = 3.683\n",
      "Epoch  95 Batch   19/269   train_loss = 3.719\n",
      "Epoch  95 Batch   20/269   train_loss = 4.080\n",
      "Epoch  95 Batch   21/269   train_loss = 3.880\n",
      "Epoch  95 Batch   22/269   train_loss = 3.663\n",
      "Epoch  95 Batch   23/269   train_loss = 3.811\n",
      "Epoch  95 Batch   24/269   train_loss = 3.509\n",
      "Epoch  95 Batch   25/269   train_loss = 3.911\n",
      "Epoch  95 Batch   26/269   train_loss = 3.698\n",
      "Epoch  95 Batch   27/269   train_loss = 3.635\n",
      "Epoch  95 Batch   28/269   train_loss = 3.707\n",
      "Epoch  95 Batch   29/269   train_loss = 3.749\n",
      "Epoch  95 Batch   30/269   train_loss = 4.090\n",
      "Epoch  95 Batch   31/269   train_loss = 3.740\n",
      "Epoch  95 Batch   32/269   train_loss = 3.590\n",
      "Epoch  95 Batch   33/269   train_loss = 3.372\n",
      "Epoch  95 Batch   34/269   train_loss = 3.648\n",
      "Epoch  95 Batch   35/269   train_loss = 3.453\n",
      "Epoch  95 Batch   36/269   train_loss = 3.538\n",
      "Epoch  95 Batch   37/269   train_loss = 3.791\n",
      "Epoch  95 Batch   38/269   train_loss = 3.441\n",
      "Epoch  95 Batch   39/269   train_loss = 3.855\n",
      "Epoch  95 Batch   40/269   train_loss = 3.642\n",
      "Epoch  95 Batch   41/269   train_loss = 3.655\n",
      "Epoch  95 Batch   42/269   train_loss = 4.081\n",
      "Epoch  95 Batch   43/269   train_loss = 3.986\n",
      "Epoch  95 Batch   44/269   train_loss = 3.771\n",
      "Epoch  95 Batch   45/269   train_loss = 3.420\n",
      "Epoch  95 Batch   46/269   train_loss = 3.757\n",
      "Epoch  95 Batch   47/269   train_loss = 3.401\n",
      "Epoch  95 Batch   48/269   train_loss = 3.932\n",
      "Epoch  95 Batch   49/269   train_loss = 3.941\n",
      "Epoch  95 Batch   50/269   train_loss = 3.920\n",
      "Epoch  95 Batch   51/269   train_loss = 3.752\n",
      "Epoch  95 Batch   52/269   train_loss = 3.740\n",
      "Epoch  95 Batch   53/269   train_loss = 3.983\n",
      "Epoch  95 Batch   54/269   train_loss = 3.654\n",
      "Epoch  95 Batch   55/269   train_loss = 3.716\n",
      "Epoch  95 Batch   56/269   train_loss = 3.505\n",
      "Epoch  95 Batch   57/269   train_loss = 3.932\n",
      "Epoch  95 Batch   58/269   train_loss = 3.581\n",
      "Epoch  95 Batch   59/269   train_loss = 3.678\n",
      "Epoch  95 Batch   60/269   train_loss = 3.806\n",
      "Epoch  95 Batch   61/269   train_loss = 3.946\n",
      "Epoch  95 Batch   62/269   train_loss = 3.579\n",
      "Epoch  95 Batch   63/269   train_loss = 3.883\n",
      "Epoch  95 Batch   64/269   train_loss = 3.656\n",
      "Epoch  95 Batch   65/269   train_loss = 3.777\n",
      "Epoch  95 Batch   66/269   train_loss = 3.592\n",
      "Epoch  95 Batch   67/269   train_loss = 3.625\n",
      "Epoch  95 Batch   68/269   train_loss = 3.732\n",
      "Epoch  95 Batch   69/269   train_loss = 3.706\n",
      "Epoch  95 Batch   70/269   train_loss = 3.573\n",
      "Epoch  95 Batch   71/269   train_loss = 3.791\n",
      "Epoch  95 Batch   72/269   train_loss = 3.524\n",
      "Epoch  95 Batch   73/269   train_loss = 3.684\n",
      "Epoch  95 Batch   74/269   train_loss = 3.720\n",
      "Epoch  95 Batch   75/269   train_loss = 3.853\n",
      "Epoch  95 Batch   76/269   train_loss = 3.837\n",
      "Epoch  95 Batch   77/269   train_loss = 3.449\n",
      "Epoch  95 Batch   78/269   train_loss = 3.867\n",
      "Epoch  95 Batch   79/269   train_loss = 3.883\n",
      "Epoch  95 Batch   80/269   train_loss = 3.800\n",
      "Epoch  95 Batch   81/269   train_loss = 3.703\n",
      "Epoch  95 Batch   82/269   train_loss = 3.795\n",
      "Epoch  95 Batch   83/269   train_loss = 3.596\n",
      "Epoch  95 Batch   84/269   train_loss = 3.939\n",
      "Epoch  95 Batch   85/269   train_loss = 4.073\n",
      "Epoch  95 Batch   86/269   train_loss = 4.042\n",
      "Epoch  95 Batch   87/269   train_loss = 3.714\n",
      "Epoch  95 Batch   88/269   train_loss = 3.819\n",
      "Epoch  95 Batch   89/269   train_loss = 3.699\n",
      "Epoch  95 Batch   90/269   train_loss = 3.570\n",
      "Epoch  95 Batch   91/269   train_loss = 3.813\n",
      "Epoch  95 Batch   92/269   train_loss = 3.898\n",
      "Epoch  95 Batch   93/269   train_loss = 3.624\n",
      "Epoch  95 Batch   94/269   train_loss = 3.983\n",
      "Epoch  95 Batch   95/269   train_loss = 3.518\n",
      "Epoch  95 Batch   96/269   train_loss = 3.602\n",
      "Epoch  95 Batch   97/269   train_loss = 3.706\n",
      "Epoch  95 Batch   98/269   train_loss = 3.987\n",
      "Epoch  95 Batch   99/269   train_loss = 3.805\n",
      "Epoch  95 Batch  100/269   train_loss = 3.740\n",
      "Epoch  95 Batch  101/269   train_loss = 3.817\n",
      "Epoch  95 Batch  102/269   train_loss = 3.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  95 Batch  103/269   train_loss = 3.691\n",
      "Epoch  95 Batch  104/269   train_loss = 3.647\n",
      "Epoch  95 Batch  105/269   train_loss = 3.856\n",
      "Epoch  95 Batch  106/269   train_loss = 4.031\n",
      "Epoch  95 Batch  107/269   train_loss = 4.002\n",
      "Epoch  95 Batch  108/269   train_loss = 3.777\n",
      "Epoch  95 Batch  109/269   train_loss = 3.855\n",
      "Epoch  95 Batch  110/269   train_loss = 3.818\n",
      "Epoch  95 Batch  111/269   train_loss = 3.877\n",
      "Epoch  95 Batch  112/269   train_loss = 3.920\n",
      "Epoch  95 Batch  113/269   train_loss = 3.782\n",
      "Epoch  95 Batch  114/269   train_loss = 3.667\n",
      "Epoch  95 Batch  115/269   train_loss = 3.640\n",
      "Epoch  95 Batch  116/269   train_loss = 3.513\n",
      "Epoch  95 Batch  117/269   train_loss = 3.630\n",
      "Epoch  95 Batch  118/269   train_loss = 3.866\n",
      "Epoch  95 Batch  119/269   train_loss = 3.617\n",
      "Epoch  95 Batch  120/269   train_loss = 3.663\n",
      "Epoch  95 Batch  121/269   train_loss = 3.693\n",
      "Epoch  95 Batch  122/269   train_loss = 3.729\n",
      "Epoch  95 Batch  123/269   train_loss = 3.500\n",
      "Epoch  95 Batch  124/269   train_loss = 3.902\n",
      "Epoch  95 Batch  125/269   train_loss = 3.517\n",
      "Epoch  95 Batch  126/269   train_loss = 3.443\n",
      "Epoch  95 Batch  127/269   train_loss = 3.788\n",
      "Epoch  95 Batch  128/269   train_loss = 3.666\n",
      "Epoch  95 Batch  129/269   train_loss = 3.647\n",
      "Epoch  95 Batch  130/269   train_loss = 3.806\n",
      "Epoch  95 Batch  131/269   train_loss = 3.384\n",
      "Epoch  95 Batch  132/269   train_loss = 3.816\n",
      "Epoch  95 Batch  133/269   train_loss = 3.463\n",
      "Epoch  95 Batch  134/269   train_loss = 3.883\n",
      "Epoch  95 Batch  135/269   train_loss = 3.595\n",
      "Epoch  95 Batch  136/269   train_loss = 3.420\n",
      "Epoch  95 Batch  137/269   train_loss = 3.784\n",
      "Epoch  95 Batch  138/269   train_loss = 3.574\n",
      "Epoch  95 Batch  139/269   train_loss = 3.693\n",
      "Epoch  95 Batch  140/269   train_loss = 3.872\n",
      "Epoch  95 Batch  141/269   train_loss = 4.134\n",
      "Epoch  95 Batch  142/269   train_loss = 3.541\n",
      "Epoch  95 Batch  143/269   train_loss = 3.691\n",
      "Epoch  95 Batch  144/269   train_loss = 3.856\n",
      "Epoch  95 Batch  145/269   train_loss = 3.708\n",
      "Epoch  95 Batch  146/269   train_loss = 3.889\n",
      "Epoch  95 Batch  147/269   train_loss = 3.515\n",
      "Epoch  95 Batch  148/269   train_loss = 3.859\n",
      "Epoch  95 Batch  149/269   train_loss = 3.606\n",
      "Epoch  95 Batch  150/269   train_loss = 3.885\n",
      "Epoch  95 Batch  151/269   train_loss = 3.788\n",
      "Epoch  95 Batch  152/269   train_loss = 3.738\n",
      "Epoch  95 Batch  153/269   train_loss = 3.793\n",
      "Epoch  95 Batch  154/269   train_loss = 3.692\n",
      "Epoch  95 Batch  155/269   train_loss = 3.621\n",
      "Epoch  95 Batch  156/269   train_loss = 4.085\n",
      "Epoch  95 Batch  157/269   train_loss = 3.848\n",
      "Epoch  95 Batch  158/269   train_loss = 3.749\n",
      "Epoch  95 Batch  159/269   train_loss = 3.748\n",
      "Epoch  95 Batch  160/269   train_loss = 3.630\n",
      "Epoch  95 Batch  161/269   train_loss = 3.903\n",
      "Epoch  95 Batch  162/269   train_loss = 3.868\n",
      "Epoch  95 Batch  163/269   train_loss = 3.604\n",
      "Epoch  95 Batch  164/269   train_loss = 3.859\n",
      "Epoch  95 Batch  165/269   train_loss = 3.978\n",
      "Epoch  95 Batch  166/269   train_loss = 3.755\n",
      "Epoch  95 Batch  167/269   train_loss = 3.558\n",
      "Epoch  95 Batch  168/269   train_loss = 3.713\n",
      "Epoch  95 Batch  169/269   train_loss = 3.988\n",
      "Epoch  95 Batch  170/269   train_loss = 3.548\n",
      "Epoch  95 Batch  171/269   train_loss = 3.843\n",
      "Epoch  95 Batch  172/269   train_loss = 3.504\n",
      "Epoch  95 Batch  173/269   train_loss = 3.756\n",
      "Epoch  95 Batch  174/269   train_loss = 3.900\n",
      "Epoch  95 Batch  175/269   train_loss = 3.752\n",
      "Epoch  95 Batch  176/269   train_loss = 3.703\n",
      "Epoch  95 Batch  177/269   train_loss = 3.835\n",
      "Epoch  95 Batch  178/269   train_loss = 3.931\n",
      "Epoch  95 Batch  179/269   train_loss = 3.785\n",
      "Epoch  95 Batch  180/269   train_loss = 3.592\n",
      "Epoch  95 Batch  181/269   train_loss = 3.830\n",
      "Epoch  95 Batch  182/269   train_loss = 3.628\n",
      "Epoch  95 Batch  183/269   train_loss = 3.629\n",
      "Epoch  95 Batch  184/269   train_loss = 3.762\n",
      "Epoch  95 Batch  185/269   train_loss = 3.734\n",
      "Epoch  95 Batch  186/269   train_loss = 3.783\n",
      "Epoch  95 Batch  187/269   train_loss = 3.851\n",
      "Epoch  95 Batch  188/269   train_loss = 3.567\n",
      "Epoch  95 Batch  189/269   train_loss = 3.667\n",
      "Epoch  95 Batch  190/269   train_loss = 4.095\n",
      "Epoch  95 Batch  191/269   train_loss = 3.742\n",
      "Epoch  95 Batch  192/269   train_loss = 3.789\n",
      "Epoch  95 Batch  193/269   train_loss = 3.703\n",
      "Epoch  95 Batch  194/269   train_loss = 3.839\n",
      "Epoch  95 Batch  195/269   train_loss = 3.750\n",
      "Epoch  95 Batch  196/269   train_loss = 3.715\n",
      "Epoch  95 Batch  197/269   train_loss = 3.912\n",
      "Epoch  95 Batch  198/269   train_loss = 3.805\n",
      "Epoch  95 Batch  199/269   train_loss = 3.821\n",
      "Epoch  95 Batch  200/269   train_loss = 3.589\n",
      "Epoch  95 Batch  201/269   train_loss = 3.700\n",
      "Epoch  95 Batch  202/269   train_loss = 3.546\n",
      "Epoch  95 Batch  203/269   train_loss = 3.687\n",
      "Epoch  95 Batch  204/269   train_loss = 3.818\n",
      "Epoch  95 Batch  205/269   train_loss = 3.740\n",
      "Epoch  95 Batch  206/269   train_loss = 3.682\n",
      "Epoch  95 Batch  207/269   train_loss = 3.699\n",
      "Epoch  95 Batch  208/269   train_loss = 3.828\n",
      "Epoch  95 Batch  209/269   train_loss = 3.839\n",
      "Epoch  95 Batch  210/269   train_loss = 3.623\n",
      "Epoch  95 Batch  211/269   train_loss = 3.718\n",
      "Epoch  95 Batch  212/269   train_loss = 4.070\n",
      "Epoch  95 Batch  213/269   train_loss = 3.605\n",
      "Epoch  95 Batch  214/269   train_loss = 3.643\n",
      "Epoch  95 Batch  215/269   train_loss = 3.900\n",
      "Epoch  95 Batch  216/269   train_loss = 3.952\n",
      "Epoch  95 Batch  217/269   train_loss = 3.606\n",
      "Epoch  95 Batch  218/269   train_loss = 3.745\n",
      "Epoch  95 Batch  219/269   train_loss = 3.526\n",
      "Epoch  95 Batch  220/269   train_loss = 3.929\n",
      "Epoch  95 Batch  221/269   train_loss = 3.585\n",
      "Epoch  95 Batch  222/269   train_loss = 3.751\n",
      "Epoch  95 Batch  223/269   train_loss = 3.597\n",
      "Epoch  95 Batch  224/269   train_loss = 3.850\n",
      "Epoch  95 Batch  225/269   train_loss = 3.920\n",
      "Epoch  95 Batch  226/269   train_loss = 3.674\n",
      "Epoch  95 Batch  227/269   train_loss = 3.440\n",
      "Epoch  95 Batch  228/269   train_loss = 3.772\n",
      "Epoch  95 Batch  229/269   train_loss = 3.929\n",
      "Epoch  95 Batch  230/269   train_loss = 3.777\n",
      "Epoch  95 Batch  231/269   train_loss = 3.679\n",
      "Epoch  95 Batch  232/269   train_loss = 3.758\n",
      "Epoch  95 Batch  233/269   train_loss = 3.896\n",
      "Epoch  95 Batch  234/269   train_loss = 3.748\n",
      "Epoch  95 Batch  235/269   train_loss = 4.055\n",
      "Epoch  95 Batch  236/269   train_loss = 3.604\n",
      "Epoch  95 Batch  237/269   train_loss = 3.265\n",
      "Epoch  95 Batch  238/269   train_loss = 3.559\n",
      "Epoch  95 Batch  239/269   train_loss = 4.036\n",
      "Epoch  95 Batch  240/269   train_loss = 3.593\n",
      "Epoch  95 Batch  241/269   train_loss = 3.913\n",
      "Epoch  95 Batch  242/269   train_loss = 3.588\n",
      "Epoch  95 Batch  243/269   train_loss = 3.872\n",
      "Epoch  95 Batch  244/269   train_loss = 3.665\n",
      "Epoch  95 Batch  245/269   train_loss = 3.686\n",
      "Epoch  95 Batch  246/269   train_loss = 3.388\n",
      "Epoch  95 Batch  247/269   train_loss = 3.741\n",
      "Epoch  95 Batch  248/269   train_loss = 3.869\n",
      "Epoch  95 Batch  249/269   train_loss = 3.709\n",
      "Epoch  95 Batch  250/269   train_loss = 3.560\n",
      "Epoch  95 Batch  251/269   train_loss = 3.940\n",
      "Epoch  95 Batch  252/269   train_loss = 3.850\n",
      "Epoch  95 Batch  253/269   train_loss = 3.614\n",
      "Epoch  95 Batch  254/269   train_loss = 3.641\n",
      "Epoch  95 Batch  255/269   train_loss = 3.737\n",
      "Epoch  95 Batch  256/269   train_loss = 3.642\n",
      "Epoch  95 Batch  257/269   train_loss = 3.593\n",
      "Epoch  95 Batch  258/269   train_loss = 3.545\n",
      "Epoch  95 Batch  259/269   train_loss = 3.530\n",
      "Epoch  95 Batch  260/269   train_loss = 3.818\n",
      "Epoch  95 Batch  261/269   train_loss = 3.813\n",
      "Epoch  95 Batch  262/269   train_loss = 3.621\n",
      "Epoch  95 Batch  263/269   train_loss = 3.511\n",
      "Epoch  95 Batch  264/269   train_loss = 3.881\n",
      "Epoch  95 Batch  265/269   train_loss = 3.765\n",
      "Epoch  95 Batch  266/269   train_loss = 3.705\n",
      "Epoch  95 Batch  267/269   train_loss = 3.723\n",
      "Epoch  95 Batch  268/269   train_loss = 3.890\n",
      "Epoch  96 Batch    0/269   train_loss = 3.749\n",
      "Epoch  96 Batch    1/269   train_loss = 3.539\n",
      "Epoch  96 Batch    2/269   train_loss = 3.580\n",
      "Epoch  96 Batch    3/269   train_loss = 3.660\n",
      "Epoch  96 Batch    4/269   train_loss = 4.048\n",
      "Epoch  96 Batch    5/269   train_loss = 3.747\n",
      "Epoch  96 Batch    6/269   train_loss = 3.499\n",
      "Epoch  96 Batch    7/269   train_loss = 3.504\n",
      "Epoch  96 Batch    8/269   train_loss = 3.798\n",
      "Epoch  96 Batch    9/269   train_loss = 3.429\n",
      "Epoch  96 Batch   10/269   train_loss = 3.594\n",
      "Epoch  96 Batch   11/269   train_loss = 3.706\n",
      "Epoch  96 Batch   12/269   train_loss = 3.375\n",
      "Epoch  96 Batch   13/269   train_loss = 3.726\n",
      "Epoch  96 Batch   14/269   train_loss = 3.603\n",
      "Epoch  96 Batch   15/269   train_loss = 3.960\n",
      "Epoch  96 Batch   16/269   train_loss = 3.627\n",
      "Epoch  96 Batch   17/269   train_loss = 3.702\n",
      "Epoch  96 Batch   18/269   train_loss = 3.677\n",
      "Epoch  96 Batch   19/269   train_loss = 3.761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  96 Batch   20/269   train_loss = 4.072\n",
      "Epoch  96 Batch   21/269   train_loss = 3.891\n",
      "Epoch  96 Batch   22/269   train_loss = 3.671\n",
      "Epoch  96 Batch   23/269   train_loss = 3.770\n",
      "Epoch  96 Batch   24/269   train_loss = 3.518\n",
      "Epoch  96 Batch   25/269   train_loss = 3.937\n",
      "Epoch  96 Batch   26/269   train_loss = 3.749\n",
      "Epoch  96 Batch   27/269   train_loss = 3.641\n",
      "Epoch  96 Batch   28/269   train_loss = 3.731\n",
      "Epoch  96 Batch   29/269   train_loss = 3.734\n",
      "Epoch  96 Batch   30/269   train_loss = 4.101\n",
      "Epoch  96 Batch   31/269   train_loss = 3.752\n",
      "Epoch  96 Batch   32/269   train_loss = 3.596\n",
      "Epoch  96 Batch   33/269   train_loss = 3.388\n",
      "Epoch  96 Batch   34/269   train_loss = 3.621\n",
      "Epoch  96 Batch   35/269   train_loss = 3.465\n",
      "Epoch  96 Batch   36/269   train_loss = 3.555\n",
      "Epoch  96 Batch   37/269   train_loss = 3.804\n",
      "Epoch  96 Batch   38/269   train_loss = 3.486\n",
      "Epoch  96 Batch   39/269   train_loss = 3.888\n",
      "Epoch  96 Batch   40/269   train_loss = 3.633\n",
      "Epoch  96 Batch   41/269   train_loss = 3.636\n",
      "Epoch  96 Batch   42/269   train_loss = 4.096\n",
      "Epoch  96 Batch   43/269   train_loss = 3.975\n",
      "Epoch  96 Batch   44/269   train_loss = 3.811\n",
      "Epoch  96 Batch   45/269   train_loss = 3.415\n",
      "Epoch  96 Batch   46/269   train_loss = 3.791\n",
      "Epoch  96 Batch   47/269   train_loss = 3.387\n",
      "Epoch  96 Batch   48/269   train_loss = 3.900\n",
      "Epoch  96 Batch   49/269   train_loss = 3.890\n",
      "Epoch  96 Batch   50/269   train_loss = 3.890\n",
      "Epoch  96 Batch   51/269   train_loss = 3.741\n",
      "Epoch  96 Batch   52/269   train_loss = 3.752\n",
      "Epoch  96 Batch   53/269   train_loss = 3.971\n",
      "Epoch  96 Batch   54/269   train_loss = 3.669\n",
      "Epoch  96 Batch   55/269   train_loss = 3.732\n",
      "Epoch  96 Batch   56/269   train_loss = 3.524\n",
      "Epoch  96 Batch   57/269   train_loss = 3.942\n",
      "Epoch  96 Batch   58/269   train_loss = 3.550\n",
      "Epoch  96 Batch   59/269   train_loss = 3.668\n",
      "Epoch  96 Batch   60/269   train_loss = 3.863\n",
      "Epoch  96 Batch   61/269   train_loss = 3.962\n",
      "Epoch  96 Batch   62/269   train_loss = 3.573\n",
      "Epoch  96 Batch   63/269   train_loss = 3.902\n",
      "Epoch  96 Batch   64/269   train_loss = 3.637\n",
      "Epoch  96 Batch   65/269   train_loss = 3.767\n",
      "Epoch  96 Batch   66/269   train_loss = 3.608\n",
      "Epoch  96 Batch   67/269   train_loss = 3.644\n",
      "Epoch  96 Batch   68/269   train_loss = 3.739\n",
      "Epoch  96 Batch   69/269   train_loss = 3.702\n",
      "Epoch  96 Batch   70/269   train_loss = 3.559\n",
      "Epoch  96 Batch   71/269   train_loss = 3.865\n",
      "Epoch  96 Batch   72/269   train_loss = 3.539\n",
      "Epoch  96 Batch   73/269   train_loss = 3.718\n",
      "Epoch  96 Batch   74/269   train_loss = 3.690\n",
      "Epoch  96 Batch   75/269   train_loss = 3.860\n",
      "Epoch  96 Batch   76/269   train_loss = 3.871\n",
      "Epoch  96 Batch   77/269   train_loss = 3.470\n",
      "Epoch  96 Batch   78/269   train_loss = 3.886\n",
      "Epoch  96 Batch   79/269   train_loss = 3.862\n",
      "Epoch  96 Batch   80/269   train_loss = 3.830\n",
      "Epoch  96 Batch   81/269   train_loss = 3.721\n",
      "Epoch  96 Batch   82/269   train_loss = 3.815\n",
      "Epoch  96 Batch   83/269   train_loss = 3.622\n",
      "Epoch  96 Batch   84/269   train_loss = 3.981\n",
      "Epoch  96 Batch   85/269   train_loss = 4.114\n",
      "Epoch  96 Batch   86/269   train_loss = 4.073\n",
      "Epoch  96 Batch   87/269   train_loss = 3.728\n",
      "Epoch  96 Batch   88/269   train_loss = 3.822\n",
      "Epoch  96 Batch   89/269   train_loss = 3.758\n",
      "Epoch  96 Batch   90/269   train_loss = 3.600\n",
      "Epoch  96 Batch   91/269   train_loss = 3.843\n",
      "Epoch  96 Batch   92/269   train_loss = 3.914\n",
      "Epoch  96 Batch   93/269   train_loss = 3.621\n",
      "Epoch  96 Batch   94/269   train_loss = 3.998\n",
      "Epoch  96 Batch   95/269   train_loss = 3.538\n",
      "Epoch  96 Batch   96/269   train_loss = 3.623\n",
      "Epoch  96 Batch   97/269   train_loss = 3.711\n",
      "Epoch  96 Batch   98/269   train_loss = 3.978\n",
      "Epoch  96 Batch   99/269   train_loss = 3.829\n",
      "Epoch  96 Batch  100/269   train_loss = 3.758\n",
      "Epoch  96 Batch  101/269   train_loss = 3.814\n",
      "Epoch  96 Batch  102/269   train_loss = 3.597\n",
      "Epoch  96 Batch  103/269   train_loss = 3.676\n",
      "Epoch  96 Batch  104/269   train_loss = 3.640\n",
      "Epoch  96 Batch  105/269   train_loss = 3.843\n",
      "Epoch  96 Batch  106/269   train_loss = 4.036\n",
      "Epoch  96 Batch  107/269   train_loss = 3.973\n",
      "Epoch  96 Batch  108/269   train_loss = 3.771\n",
      "Epoch  96 Batch  109/269   train_loss = 3.870\n",
      "Epoch  96 Batch  110/269   train_loss = 3.844\n",
      "Epoch  96 Batch  111/269   train_loss = 3.927\n",
      "Epoch  96 Batch  112/269   train_loss = 3.908\n",
      "Epoch  96 Batch  113/269   train_loss = 3.792\n",
      "Epoch  96 Batch  114/269   train_loss = 3.673\n",
      "Epoch  96 Batch  115/269   train_loss = 3.665\n",
      "Epoch  96 Batch  116/269   train_loss = 3.515\n",
      "Epoch  96 Batch  117/269   train_loss = 3.648\n",
      "Epoch  96 Batch  118/269   train_loss = 3.851\n",
      "Epoch  96 Batch  119/269   train_loss = 3.675\n",
      "Epoch  96 Batch  120/269   train_loss = 3.670\n",
      "Epoch  96 Batch  121/269   train_loss = 3.711\n",
      "Epoch  96 Batch  122/269   train_loss = 3.712\n",
      "Epoch  96 Batch  123/269   train_loss = 3.508\n",
      "Epoch  96 Batch  124/269   train_loss = 3.929\n",
      "Epoch  96 Batch  125/269   train_loss = 3.518\n",
      "Epoch  96 Batch  126/269   train_loss = 3.435\n",
      "Epoch  96 Batch  127/269   train_loss = 3.809\n",
      "Epoch  96 Batch  128/269   train_loss = 3.683\n",
      "Epoch  96 Batch  129/269   train_loss = 3.648\n",
      "Epoch  96 Batch  130/269   train_loss = 3.850\n",
      "Epoch  96 Batch  131/269   train_loss = 3.426\n",
      "Epoch  96 Batch  132/269   train_loss = 3.788\n",
      "Epoch  96 Batch  133/269   train_loss = 3.471\n",
      "Epoch  96 Batch  134/269   train_loss = 3.871\n",
      "Epoch  96 Batch  135/269   train_loss = 3.584\n",
      "Epoch  96 Batch  136/269   train_loss = 3.426\n",
      "Epoch  96 Batch  137/269   train_loss = 3.790\n",
      "Epoch  96 Batch  138/269   train_loss = 3.597\n",
      "Epoch  96 Batch  139/269   train_loss = 3.690\n",
      "Epoch  96 Batch  140/269   train_loss = 3.868\n",
      "Epoch  96 Batch  141/269   train_loss = 4.136\n",
      "Epoch  96 Batch  142/269   train_loss = 3.534\n",
      "Epoch  96 Batch  143/269   train_loss = 3.713\n",
      "Epoch  96 Batch  144/269   train_loss = 3.887\n",
      "Epoch  96 Batch  145/269   train_loss = 3.715\n",
      "Epoch  96 Batch  146/269   train_loss = 3.904\n",
      "Epoch  96 Batch  147/269   train_loss = 3.514\n",
      "Epoch  96 Batch  148/269   train_loss = 3.872\n",
      "Epoch  96 Batch  149/269   train_loss = 3.613\n",
      "Epoch  96 Batch  150/269   train_loss = 3.878\n",
      "Epoch  96 Batch  151/269   train_loss = 3.810\n",
      "Epoch  96 Batch  152/269   train_loss = 3.742\n",
      "Epoch  96 Batch  153/269   train_loss = 3.829\n",
      "Epoch  96 Batch  154/269   train_loss = 3.719\n",
      "Epoch  96 Batch  155/269   train_loss = 3.598\n",
      "Epoch  96 Batch  156/269   train_loss = 4.103\n",
      "Epoch  96 Batch  157/269   train_loss = 3.855\n",
      "Epoch  96 Batch  158/269   train_loss = 3.759\n",
      "Epoch  96 Batch  159/269   train_loss = 3.818\n",
      "Epoch  96 Batch  160/269   train_loss = 3.614\n",
      "Epoch  96 Batch  161/269   train_loss = 3.885\n",
      "Epoch  96 Batch  162/269   train_loss = 3.849\n",
      "Epoch  96 Batch  163/269   train_loss = 3.612\n",
      "Epoch  96 Batch  164/269   train_loss = 3.885\n",
      "Epoch  96 Batch  165/269   train_loss = 3.945\n",
      "Epoch  96 Batch  166/269   train_loss = 3.725\n",
      "Epoch  96 Batch  167/269   train_loss = 3.572\n",
      "Epoch  96 Batch  168/269   train_loss = 3.717\n",
      "Epoch  96 Batch  169/269   train_loss = 3.975\n",
      "Epoch  96 Batch  170/269   train_loss = 3.548\n",
      "Epoch  96 Batch  171/269   train_loss = 3.839\n",
      "Epoch  96 Batch  172/269   train_loss = 3.527\n",
      "Epoch  96 Batch  173/269   train_loss = 3.731\n",
      "Epoch  96 Batch  174/269   train_loss = 3.935\n",
      "Epoch  96 Batch  175/269   train_loss = 3.766\n",
      "Epoch  96 Batch  176/269   train_loss = 3.695\n",
      "Epoch  96 Batch  177/269   train_loss = 3.816\n",
      "Epoch  96 Batch  178/269   train_loss = 3.974\n",
      "Epoch  96 Batch  179/269   train_loss = 3.791\n",
      "Epoch  96 Batch  180/269   train_loss = 3.611\n",
      "Epoch  96 Batch  181/269   train_loss = 3.827\n",
      "Epoch  96 Batch  182/269   train_loss = 3.630\n",
      "Epoch  96 Batch  183/269   train_loss = 3.608\n",
      "Epoch  96 Batch  184/269   train_loss = 3.771\n",
      "Epoch  96 Batch  185/269   train_loss = 3.762\n",
      "Epoch  96 Batch  186/269   train_loss = 3.788\n",
      "Epoch  96 Batch  187/269   train_loss = 3.866\n",
      "Epoch  96 Batch  188/269   train_loss = 3.560\n",
      "Epoch  96 Batch  189/269   train_loss = 3.690\n",
      "Epoch  96 Batch  190/269   train_loss = 4.147\n",
      "Epoch  96 Batch  191/269   train_loss = 3.743\n",
      "Epoch  96 Batch  192/269   train_loss = 3.814\n",
      "Epoch  96 Batch  193/269   train_loss = 3.732\n",
      "Epoch  96 Batch  194/269   train_loss = 3.840\n",
      "Epoch  96 Batch  195/269   train_loss = 3.749\n",
      "Epoch  96 Batch  196/269   train_loss = 3.731\n",
      "Epoch  96 Batch  197/269   train_loss = 3.900\n",
      "Epoch  96 Batch  198/269   train_loss = 3.782\n",
      "Epoch  96 Batch  199/269   train_loss = 3.833\n",
      "Epoch  96 Batch  200/269   train_loss = 3.544\n",
      "Epoch  96 Batch  201/269   train_loss = 3.711\n",
      "Epoch  96 Batch  202/269   train_loss = 3.552\n",
      "Epoch  96 Batch  203/269   train_loss = 3.704\n",
      "Epoch  96 Batch  204/269   train_loss = 3.808\n",
      "Epoch  96 Batch  205/269   train_loss = 3.750\n",
      "Epoch  96 Batch  206/269   train_loss = 3.700\n",
      "Epoch  96 Batch  207/269   train_loss = 3.690\n",
      "Epoch  96 Batch  208/269   train_loss = 3.836\n",
      "Epoch  96 Batch  209/269   train_loss = 3.856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  96 Batch  210/269   train_loss = 3.598\n",
      "Epoch  96 Batch  211/269   train_loss = 3.698\n",
      "Epoch  96 Batch  212/269   train_loss = 4.093\n",
      "Epoch  96 Batch  213/269   train_loss = 3.641\n",
      "Epoch  96 Batch  214/269   train_loss = 3.668\n",
      "Epoch  96 Batch  215/269   train_loss = 3.905\n",
      "Epoch  96 Batch  216/269   train_loss = 3.950\n",
      "Epoch  96 Batch  217/269   train_loss = 3.602\n",
      "Epoch  96 Batch  218/269   train_loss = 3.760\n",
      "Epoch  96 Batch  219/269   train_loss = 3.576\n",
      "Epoch  96 Batch  220/269   train_loss = 3.959\n",
      "Epoch  96 Batch  221/269   train_loss = 3.587\n",
      "Epoch  96 Batch  222/269   train_loss = 3.765\n",
      "Epoch  96 Batch  223/269   train_loss = 3.576\n",
      "Epoch  96 Batch  224/269   train_loss = 3.853\n",
      "Epoch  96 Batch  225/269   train_loss = 3.894\n",
      "Epoch  96 Batch  226/269   train_loss = 3.687\n",
      "Epoch  96 Batch  227/269   train_loss = 3.450\n",
      "Epoch  96 Batch  228/269   train_loss = 3.791\n",
      "Epoch  96 Batch  229/269   train_loss = 3.964\n",
      "Epoch  96 Batch  230/269   train_loss = 3.788\n",
      "Epoch  96 Batch  231/269   train_loss = 3.651\n",
      "Epoch  96 Batch  232/269   train_loss = 3.751\n",
      "Epoch  96 Batch  233/269   train_loss = 3.893\n",
      "Epoch  96 Batch  234/269   train_loss = 3.756\n",
      "Epoch  96 Batch  235/269   train_loss = 4.034\n",
      "Epoch  96 Batch  236/269   train_loss = 3.611\n",
      "Epoch  96 Batch  237/269   train_loss = 3.268\n",
      "Epoch  96 Batch  238/269   train_loss = 3.631\n",
      "Epoch  96 Batch  239/269   train_loss = 4.007\n",
      "Epoch  96 Batch  240/269   train_loss = 3.597\n",
      "Epoch  96 Batch  241/269   train_loss = 3.897\n",
      "Epoch  96 Batch  242/269   train_loss = 3.579\n",
      "Epoch  96 Batch  243/269   train_loss = 3.858\n",
      "Epoch  96 Batch  244/269   train_loss = 3.646\n",
      "Epoch  96 Batch  245/269   train_loss = 3.692\n",
      "Epoch  96 Batch  246/269   train_loss = 3.389\n",
      "Epoch  96 Batch  247/269   train_loss = 3.701\n",
      "Epoch  96 Batch  248/269   train_loss = 3.883\n",
      "Epoch  96 Batch  249/269   train_loss = 3.715\n",
      "Epoch  96 Batch  250/269   train_loss = 3.566\n",
      "Epoch  96 Batch  251/269   train_loss = 3.921\n",
      "Epoch  96 Batch  252/269   train_loss = 3.846\n",
      "Epoch  96 Batch  253/269   train_loss = 3.595\n",
      "Epoch  96 Batch  254/269   train_loss = 3.645\n",
      "Epoch  96 Batch  255/269   train_loss = 3.731\n",
      "Epoch  96 Batch  256/269   train_loss = 3.665\n",
      "Epoch  96 Batch  257/269   train_loss = 3.593\n",
      "Epoch  96 Batch  258/269   train_loss = 3.543\n",
      "Epoch  96 Batch  259/269   train_loss = 3.534\n",
      "Epoch  96 Batch  260/269   train_loss = 3.793\n",
      "Epoch  96 Batch  261/269   train_loss = 3.784\n",
      "Epoch  96 Batch  262/269   train_loss = 3.599\n",
      "Epoch  96 Batch  263/269   train_loss = 3.494\n",
      "Epoch  96 Batch  264/269   train_loss = 3.901\n",
      "Epoch  96 Batch  265/269   train_loss = 3.727\n",
      "Epoch  96 Batch  266/269   train_loss = 3.659\n",
      "Epoch  96 Batch  267/269   train_loss = 3.718\n",
      "Epoch  96 Batch  268/269   train_loss = 3.895\n",
      "Epoch  97 Batch    0/269   train_loss = 3.680\n",
      "Epoch  97 Batch    1/269   train_loss = 3.552\n",
      "Epoch  97 Batch    2/269   train_loss = 3.572\n",
      "Epoch  97 Batch    3/269   train_loss = 3.592\n",
      "Epoch  97 Batch    4/269   train_loss = 4.034\n",
      "Epoch  97 Batch    5/269   train_loss = 3.744\n",
      "Epoch  97 Batch    6/269   train_loss = 3.502\n",
      "Epoch  97 Batch    7/269   train_loss = 3.483\n",
      "Epoch  97 Batch    8/269   train_loss = 3.775\n",
      "Epoch  97 Batch    9/269   train_loss = 3.427\n",
      "Epoch  97 Batch   10/269   train_loss = 3.571\n",
      "Epoch  97 Batch   11/269   train_loss = 3.712\n",
      "Epoch  97 Batch   12/269   train_loss = 3.404\n",
      "Epoch  97 Batch   13/269   train_loss = 3.704\n",
      "Epoch  97 Batch   14/269   train_loss = 3.574\n",
      "Epoch  97 Batch   15/269   train_loss = 3.986\n",
      "Epoch  97 Batch   16/269   train_loss = 3.620\n",
      "Epoch  97 Batch   17/269   train_loss = 3.705\n",
      "Epoch  97 Batch   18/269   train_loss = 3.693\n",
      "Epoch  97 Batch   19/269   train_loss = 3.721\n",
      "Epoch  97 Batch   20/269   train_loss = 4.068\n",
      "Epoch  97 Batch   21/269   train_loss = 3.897\n",
      "Epoch  97 Batch   22/269   train_loss = 3.642\n",
      "Epoch  97 Batch   23/269   train_loss = 3.800\n",
      "Epoch  97 Batch   24/269   train_loss = 3.495\n",
      "Epoch  97 Batch   25/269   train_loss = 3.912\n",
      "Epoch  97 Batch   26/269   train_loss = 3.737\n",
      "Epoch  97 Batch   27/269   train_loss = 3.642\n",
      "Epoch  97 Batch   28/269   train_loss = 3.705\n",
      "Epoch  97 Batch   29/269   train_loss = 3.725\n",
      "Epoch  97 Batch   30/269   train_loss = 4.085\n",
      "Epoch  97 Batch   31/269   train_loss = 3.719\n",
      "Epoch  97 Batch   32/269   train_loss = 3.582\n",
      "Epoch  97 Batch   33/269   train_loss = 3.382\n",
      "Epoch  97 Batch   34/269   train_loss = 3.632\n",
      "Epoch  97 Batch   35/269   train_loss = 3.464\n",
      "Epoch  97 Batch   36/269   train_loss = 3.519\n",
      "Epoch  97 Batch   37/269   train_loss = 3.771\n",
      "Epoch  97 Batch   38/269   train_loss = 3.429\n",
      "Epoch  97 Batch   39/269   train_loss = 3.806\n",
      "Epoch  97 Batch   40/269   train_loss = 3.620\n",
      "Epoch  97 Batch   41/269   train_loss = 3.618\n",
      "Epoch  97 Batch   42/269   train_loss = 4.082\n",
      "Epoch  97 Batch   43/269   train_loss = 3.955\n",
      "Epoch  97 Batch   44/269   train_loss = 3.796\n",
      "Epoch  97 Batch   45/269   train_loss = 3.411\n",
      "Epoch  97 Batch   46/269   train_loss = 3.790\n",
      "Epoch  97 Batch   47/269   train_loss = 3.372\n",
      "Epoch  97 Batch   48/269   train_loss = 3.910\n",
      "Epoch  97 Batch   49/269   train_loss = 3.920\n",
      "Epoch  97 Batch   50/269   train_loss = 3.875\n",
      "Epoch  97 Batch   51/269   train_loss = 3.762\n",
      "Epoch  97 Batch   52/269   train_loss = 3.720\n",
      "Epoch  97 Batch   53/269   train_loss = 3.997\n",
      "Epoch  97 Batch   54/269   train_loss = 3.663\n",
      "Epoch  97 Batch   55/269   train_loss = 3.725\n",
      "Epoch  97 Batch   56/269   train_loss = 3.527\n",
      "Epoch  97 Batch   57/269   train_loss = 3.943\n",
      "Epoch  97 Batch   58/269   train_loss = 3.584\n",
      "Epoch  97 Batch   59/269   train_loss = 3.624\n",
      "Epoch  97 Batch   60/269   train_loss = 3.836\n",
      "Epoch  97 Batch   61/269   train_loss = 3.946\n",
      "Epoch  97 Batch   62/269   train_loss = 3.557\n",
      "Epoch  97 Batch   63/269   train_loss = 3.882\n",
      "Epoch  97 Batch   64/269   train_loss = 3.667\n",
      "Epoch  97 Batch   65/269   train_loss = 3.768\n",
      "Epoch  97 Batch   66/269   train_loss = 3.592\n",
      "Epoch  97 Batch   67/269   train_loss = 3.636\n",
      "Epoch  97 Batch   68/269   train_loss = 3.744\n",
      "Epoch  97 Batch   69/269   train_loss = 3.697\n",
      "Epoch  97 Batch   70/269   train_loss = 3.559\n",
      "Epoch  97 Batch   71/269   train_loss = 3.828\n",
      "Epoch  97 Batch   72/269   train_loss = 3.522\n",
      "Epoch  97 Batch   73/269   train_loss = 3.694\n",
      "Epoch  97 Batch   74/269   train_loss = 3.689\n",
      "Epoch  97 Batch   75/269   train_loss = 3.873\n",
      "Epoch  97 Batch   76/269   train_loss = 3.825\n",
      "Epoch  97 Batch   77/269   train_loss = 3.439\n",
      "Epoch  97 Batch   78/269   train_loss = 3.917\n",
      "Epoch  97 Batch   79/269   train_loss = 3.863\n",
      "Epoch  97 Batch   80/269   train_loss = 3.845\n",
      "Epoch  97 Batch   81/269   train_loss = 3.734\n",
      "Epoch  97 Batch   82/269   train_loss = 3.816\n",
      "Epoch  97 Batch   83/269   train_loss = 3.596\n",
      "Epoch  97 Batch   84/269   train_loss = 3.970\n",
      "Epoch  97 Batch   85/269   train_loss = 4.112\n",
      "Epoch  97 Batch   86/269   train_loss = 4.112\n",
      "Epoch  97 Batch   87/269   train_loss = 3.706\n",
      "Epoch  97 Batch   88/269   train_loss = 3.852\n",
      "Epoch  97 Batch   89/269   train_loss = 3.751\n",
      "Epoch  97 Batch   90/269   train_loss = 3.609\n",
      "Epoch  97 Batch   91/269   train_loss = 3.840\n",
      "Epoch  97 Batch   92/269   train_loss = 3.870\n",
      "Epoch  97 Batch   93/269   train_loss = 3.617\n",
      "Epoch  97 Batch   94/269   train_loss = 4.019\n",
      "Epoch  97 Batch   95/269   train_loss = 3.537\n",
      "Epoch  97 Batch   96/269   train_loss = 3.603\n",
      "Epoch  97 Batch   97/269   train_loss = 3.735\n",
      "Epoch  97 Batch   98/269   train_loss = 3.973\n",
      "Epoch  97 Batch   99/269   train_loss = 3.803\n",
      "Epoch  97 Batch  100/269   train_loss = 3.802\n",
      "Epoch  97 Batch  101/269   train_loss = 3.837\n",
      "Epoch  97 Batch  102/269   train_loss = 3.588\n",
      "Epoch  97 Batch  103/269   train_loss = 3.671\n",
      "Epoch  97 Batch  104/269   train_loss = 3.627\n",
      "Epoch  97 Batch  105/269   train_loss = 3.875\n",
      "Epoch  97 Batch  106/269   train_loss = 4.028\n",
      "Epoch  97 Batch  107/269   train_loss = 4.005\n",
      "Epoch  97 Batch  108/269   train_loss = 3.747\n",
      "Epoch  97 Batch  109/269   train_loss = 3.875\n",
      "Epoch  97 Batch  110/269   train_loss = 3.797\n",
      "Epoch  97 Batch  111/269   train_loss = 3.853\n",
      "Epoch  97 Batch  112/269   train_loss = 3.907\n",
      "Epoch  97 Batch  113/269   train_loss = 3.792\n",
      "Epoch  97 Batch  114/269   train_loss = 3.665\n",
      "Epoch  97 Batch  115/269   train_loss = 3.633\n",
      "Epoch  97 Batch  116/269   train_loss = 3.523\n",
      "Epoch  97 Batch  117/269   train_loss = 3.612\n",
      "Epoch  97 Batch  118/269   train_loss = 3.853\n",
      "Epoch  97 Batch  119/269   train_loss = 3.690\n",
      "Epoch  97 Batch  120/269   train_loss = 3.713\n",
      "Epoch  97 Batch  121/269   train_loss = 3.689\n",
      "Epoch  97 Batch  122/269   train_loss = 3.700\n",
      "Epoch  97 Batch  123/269   train_loss = 3.504\n",
      "Epoch  97 Batch  124/269   train_loss = 3.963\n",
      "Epoch  97 Batch  125/269   train_loss = 3.497\n",
      "Epoch  97 Batch  126/269   train_loss = 3.481\n",
      "Epoch  97 Batch  127/269   train_loss = 3.808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  97 Batch  128/269   train_loss = 3.663\n",
      "Epoch  97 Batch  129/269   train_loss = 3.653\n",
      "Epoch  97 Batch  130/269   train_loss = 3.805\n",
      "Epoch  97 Batch  131/269   train_loss = 3.410\n",
      "Epoch  97 Batch  132/269   train_loss = 3.808\n",
      "Epoch  97 Batch  133/269   train_loss = 3.448\n",
      "Epoch  97 Batch  134/269   train_loss = 3.889\n",
      "Epoch  97 Batch  135/269   train_loss = 3.595\n",
      "Epoch  97 Batch  136/269   train_loss = 3.416\n",
      "Epoch  97 Batch  137/269   train_loss = 3.778\n",
      "Epoch  97 Batch  138/269   train_loss = 3.556\n",
      "Epoch  97 Batch  139/269   train_loss = 3.767\n",
      "Epoch  97 Batch  140/269   train_loss = 3.838\n",
      "Epoch  97 Batch  141/269   train_loss = 4.138\n",
      "Epoch  97 Batch  142/269   train_loss = 3.522\n",
      "Epoch  97 Batch  143/269   train_loss = 3.706\n",
      "Epoch  97 Batch  144/269   train_loss = 3.902\n",
      "Epoch  97 Batch  145/269   train_loss = 3.704\n",
      "Epoch  97 Batch  146/269   train_loss = 3.925\n",
      "Epoch  97 Batch  147/269   train_loss = 3.505\n",
      "Epoch  97 Batch  148/269   train_loss = 3.930\n",
      "Epoch  97 Batch  149/269   train_loss = 3.623\n",
      "Epoch  97 Batch  150/269   train_loss = 3.948\n",
      "Epoch  97 Batch  151/269   train_loss = 3.777\n",
      "Epoch  97 Batch  152/269   train_loss = 3.744\n",
      "Epoch  97 Batch  153/269   train_loss = 3.830\n",
      "Epoch  97 Batch  154/269   train_loss = 3.705\n",
      "Epoch  97 Batch  155/269   train_loss = 3.602\n",
      "Epoch  97 Batch  156/269   train_loss = 4.116\n",
      "Epoch  97 Batch  157/269   train_loss = 3.884\n",
      "Epoch  97 Batch  158/269   train_loss = 3.757\n",
      "Epoch  97 Batch  159/269   train_loss = 3.849\n",
      "Epoch  97 Batch  160/269   train_loss = 3.615\n",
      "Epoch  97 Batch  161/269   train_loss = 3.889\n",
      "Epoch  97 Batch  162/269   train_loss = 3.908\n",
      "Epoch  97 Batch  163/269   train_loss = 3.613\n",
      "Epoch  97 Batch  164/269   train_loss = 3.936\n",
      "Epoch  97 Batch  165/269   train_loss = 3.938\n",
      "Epoch  97 Batch  166/269   train_loss = 3.743\n",
      "Epoch  97 Batch  167/269   train_loss = 3.526\n",
      "Epoch  97 Batch  168/269   train_loss = 3.726\n",
      "Epoch  97 Batch  169/269   train_loss = 3.947\n",
      "Epoch  97 Batch  170/269   train_loss = 3.534\n",
      "Epoch  97 Batch  171/269   train_loss = 3.859\n",
      "Epoch  97 Batch  172/269   train_loss = 3.519\n",
      "Epoch  97 Batch  173/269   train_loss = 3.770\n",
      "Epoch  97 Batch  174/269   train_loss = 3.999\n",
      "Epoch  97 Batch  175/269   train_loss = 3.750\n",
      "Epoch  97 Batch  176/269   train_loss = 3.682\n",
      "Epoch  97 Batch  177/269   train_loss = 3.837\n",
      "Epoch  97 Batch  178/269   train_loss = 3.981\n",
      "Epoch  97 Batch  179/269   train_loss = 3.802\n",
      "Epoch  97 Batch  180/269   train_loss = 3.679\n",
      "Epoch  97 Batch  181/269   train_loss = 3.815\n",
      "Epoch  97 Batch  182/269   train_loss = 3.658\n",
      "Epoch  97 Batch  183/269   train_loss = 3.638\n",
      "Epoch  97 Batch  184/269   train_loss = 3.737\n",
      "Epoch  97 Batch  185/269   train_loss = 3.790\n",
      "Epoch  97 Batch  186/269   train_loss = 3.744\n",
      "Epoch  97 Batch  187/269   train_loss = 3.859\n",
      "Epoch  97 Batch  188/269   train_loss = 3.599\n",
      "Epoch  97 Batch  189/269   train_loss = 3.702\n",
      "Epoch  97 Batch  190/269   train_loss = 4.108\n",
      "Epoch  97 Batch  191/269   train_loss = 3.738\n",
      "Epoch  97 Batch  192/269   train_loss = 3.809\n",
      "Epoch  97 Batch  193/269   train_loss = 3.768\n",
      "Epoch  97 Batch  194/269   train_loss = 3.857\n",
      "Epoch  97 Batch  195/269   train_loss = 3.756\n",
      "Epoch  97 Batch  196/269   train_loss = 3.713\n",
      "Epoch  97 Batch  197/269   train_loss = 3.931\n",
      "Epoch  97 Batch  198/269   train_loss = 3.749\n",
      "Epoch  97 Batch  199/269   train_loss = 3.885\n",
      "Epoch  97 Batch  200/269   train_loss = 3.618\n",
      "Epoch  97 Batch  201/269   train_loss = 3.727\n",
      "Epoch  97 Batch  202/269   train_loss = 3.562\n",
      "Epoch  97 Batch  203/269   train_loss = 3.667\n",
      "Epoch  97 Batch  204/269   train_loss = 3.803\n",
      "Epoch  97 Batch  205/269   train_loss = 3.830\n",
      "Epoch  97 Batch  206/269   train_loss = 3.673\n",
      "Epoch  97 Batch  207/269   train_loss = 3.674\n",
      "Epoch  97 Batch  208/269   train_loss = 3.839\n",
      "Epoch  97 Batch  209/269   train_loss = 3.909\n",
      "Epoch  97 Batch  210/269   train_loss = 3.606\n",
      "Epoch  97 Batch  211/269   train_loss = 3.750\n",
      "Epoch  97 Batch  212/269   train_loss = 4.048\n",
      "Epoch  97 Batch  213/269   train_loss = 3.608\n",
      "Epoch  97 Batch  214/269   train_loss = 3.662\n",
      "Epoch  97 Batch  215/269   train_loss = 3.899\n",
      "Epoch  97 Batch  216/269   train_loss = 3.924\n",
      "Epoch  97 Batch  217/269   train_loss = 3.591\n",
      "Epoch  97 Batch  218/269   train_loss = 3.765\n",
      "Epoch  97 Batch  219/269   train_loss = 3.532\n",
      "Epoch  97 Batch  220/269   train_loss = 3.920\n",
      "Epoch  97 Batch  221/269   train_loss = 3.560\n",
      "Epoch  97 Batch  222/269   train_loss = 3.751\n",
      "Epoch  97 Batch  223/269   train_loss = 3.610\n",
      "Epoch  97 Batch  224/269   train_loss = 3.883\n",
      "Epoch  97 Batch  225/269   train_loss = 3.878\n",
      "Epoch  97 Batch  226/269   train_loss = 3.715\n",
      "Epoch  97 Batch  227/269   train_loss = 3.443\n",
      "Epoch  97 Batch  228/269   train_loss = 3.768\n",
      "Epoch  97 Batch  229/269   train_loss = 3.989\n",
      "Epoch  97 Batch  230/269   train_loss = 3.831\n",
      "Epoch  97 Batch  231/269   train_loss = 3.660\n",
      "Epoch  97 Batch  232/269   train_loss = 3.737\n",
      "Epoch  97 Batch  233/269   train_loss = 3.854\n",
      "Epoch  97 Batch  234/269   train_loss = 3.767\n",
      "Epoch  97 Batch  235/269   train_loss = 4.039\n",
      "Epoch  97 Batch  236/269   train_loss = 3.598\n",
      "Epoch  97 Batch  237/269   train_loss = 3.302\n",
      "Epoch  97 Batch  238/269   train_loss = 3.566\n",
      "Epoch  97 Batch  239/269   train_loss = 4.014\n",
      "Epoch  97 Batch  240/269   train_loss = 3.563\n",
      "Epoch  97 Batch  241/269   train_loss = 3.953\n",
      "Epoch  97 Batch  242/269   train_loss = 3.614\n",
      "Epoch  97 Batch  243/269   train_loss = 3.854\n",
      "Epoch  97 Batch  244/269   train_loss = 3.644\n",
      "Epoch  97 Batch  245/269   train_loss = 3.684\n",
      "Epoch  97 Batch  246/269   train_loss = 3.369\n",
      "Epoch  97 Batch  247/269   train_loss = 3.748\n",
      "Epoch  97 Batch  248/269   train_loss = 3.904\n",
      "Epoch  97 Batch  249/269   train_loss = 3.679\n",
      "Epoch  97 Batch  250/269   train_loss = 3.532\n",
      "Epoch  97 Batch  251/269   train_loss = 3.931\n",
      "Epoch  97 Batch  252/269   train_loss = 3.851\n",
      "Epoch  97 Batch  253/269   train_loss = 3.653\n",
      "Epoch  97 Batch  254/269   train_loss = 3.637\n",
      "Epoch  97 Batch  255/269   train_loss = 3.735\n",
      "Epoch  97 Batch  256/269   train_loss = 3.683\n",
      "Epoch  97 Batch  257/269   train_loss = 3.577\n",
      "Epoch  97 Batch  258/269   train_loss = 3.520\n",
      "Epoch  97 Batch  259/269   train_loss = 3.545\n",
      "Epoch  97 Batch  260/269   train_loss = 3.843\n",
      "Epoch  97 Batch  261/269   train_loss = 3.788\n",
      "Epoch  97 Batch  262/269   train_loss = 3.653\n",
      "Epoch  97 Batch  263/269   train_loss = 3.477\n",
      "Epoch  97 Batch  264/269   train_loss = 3.918\n",
      "Epoch  97 Batch  265/269   train_loss = 3.768\n",
      "Epoch  97 Batch  266/269   train_loss = 3.671\n",
      "Epoch  97 Batch  267/269   train_loss = 3.722\n",
      "Epoch  97 Batch  268/269   train_loss = 3.870\n",
      "Epoch  98 Batch    0/269   train_loss = 3.683\n",
      "Epoch  98 Batch    1/269   train_loss = 3.514\n",
      "Epoch  98 Batch    2/269   train_loss = 3.606\n",
      "Epoch  98 Batch    3/269   train_loss = 3.596\n",
      "Epoch  98 Batch    4/269   train_loss = 4.068\n",
      "Epoch  98 Batch    5/269   train_loss = 3.736\n",
      "Epoch  98 Batch    6/269   train_loss = 3.538\n",
      "Epoch  98 Batch    7/269   train_loss = 3.493\n",
      "Epoch  98 Batch    8/269   train_loss = 3.841\n",
      "Epoch  98 Batch    9/269   train_loss = 3.483\n",
      "Epoch  98 Batch   10/269   train_loss = 3.597\n",
      "Epoch  98 Batch   11/269   train_loss = 3.703\n",
      "Epoch  98 Batch   12/269   train_loss = 3.425\n",
      "Epoch  98 Batch   13/269   train_loss = 3.707\n",
      "Epoch  98 Batch   14/269   train_loss = 3.582\n",
      "Epoch  98 Batch   15/269   train_loss = 4.039\n",
      "Epoch  98 Batch   16/269   train_loss = 3.652\n",
      "Epoch  98 Batch   17/269   train_loss = 3.734\n",
      "Epoch  98 Batch   18/269   train_loss = 3.711\n",
      "Epoch  98 Batch   19/269   train_loss = 3.744\n",
      "Epoch  98 Batch   20/269   train_loss = 4.064\n",
      "Epoch  98 Batch   21/269   train_loss = 3.867\n",
      "Epoch  98 Batch   22/269   train_loss = 3.671\n",
      "Epoch  98 Batch   23/269   train_loss = 3.796\n",
      "Epoch  98 Batch   24/269   train_loss = 3.506\n",
      "Epoch  98 Batch   25/269   train_loss = 3.982\n",
      "Epoch  98 Batch   26/269   train_loss = 3.732\n",
      "Epoch  98 Batch   27/269   train_loss = 3.632\n",
      "Epoch  98 Batch   28/269   train_loss = 3.713\n",
      "Epoch  98 Batch   29/269   train_loss = 3.752\n",
      "Epoch  98 Batch   30/269   train_loss = 4.095\n",
      "Epoch  98 Batch   31/269   train_loss = 3.767\n",
      "Epoch  98 Batch   32/269   train_loss = 3.627\n",
      "Epoch  98 Batch   33/269   train_loss = 3.384\n",
      "Epoch  98 Batch   34/269   train_loss = 3.750\n",
      "Epoch  98 Batch   35/269   train_loss = 3.476\n",
      "Epoch  98 Batch   36/269   train_loss = 3.525\n",
      "Epoch  98 Batch   37/269   train_loss = 3.819\n",
      "Epoch  98 Batch   38/269   train_loss = 3.446\n",
      "Epoch  98 Batch   39/269   train_loss = 3.800\n",
      "Epoch  98 Batch   40/269   train_loss = 3.650\n",
      "Epoch  98 Batch   41/269   train_loss = 3.590\n",
      "Epoch  98 Batch   42/269   train_loss = 4.085\n",
      "Epoch  98 Batch   43/269   train_loss = 3.971\n",
      "Epoch  98 Batch   44/269   train_loss = 3.773\n",
      "Epoch  98 Batch   45/269   train_loss = 3.443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  98 Batch   46/269   train_loss = 3.753\n",
      "Epoch  98 Batch   47/269   train_loss = 3.363\n",
      "Epoch  98 Batch   48/269   train_loss = 3.943\n",
      "Epoch  98 Batch   49/269   train_loss = 3.887\n",
      "Epoch  98 Batch   50/269   train_loss = 3.870\n",
      "Epoch  98 Batch   51/269   train_loss = 3.791\n",
      "Epoch  98 Batch   52/269   train_loss = 3.733\n",
      "Epoch  98 Batch   53/269   train_loss = 3.996\n",
      "Epoch  98 Batch   54/269   train_loss = 3.681\n",
      "Epoch  98 Batch   55/269   train_loss = 3.730\n",
      "Epoch  98 Batch   56/269   train_loss = 3.518\n",
      "Epoch  98 Batch   57/269   train_loss = 3.969\n",
      "Epoch  98 Batch   58/269   train_loss = 3.584\n",
      "Epoch  98 Batch   59/269   train_loss = 3.662\n",
      "Epoch  98 Batch   60/269   train_loss = 3.843\n",
      "Epoch  98 Batch   61/269   train_loss = 3.924\n",
      "Epoch  98 Batch   62/269   train_loss = 3.609\n",
      "Epoch  98 Batch   63/269   train_loss = 3.900\n",
      "Epoch  98 Batch   64/269   train_loss = 3.620\n",
      "Epoch  98 Batch   65/269   train_loss = 3.770\n",
      "Epoch  98 Batch   66/269   train_loss = 3.635\n",
      "Epoch  98 Batch   67/269   train_loss = 3.608\n",
      "Epoch  98 Batch   68/269   train_loss = 3.751\n",
      "Epoch  98 Batch   69/269   train_loss = 3.694\n",
      "Epoch  98 Batch   70/269   train_loss = 3.572\n",
      "Epoch  98 Batch   71/269   train_loss = 3.838\n",
      "Epoch  98 Batch   72/269   train_loss = 3.538\n",
      "Epoch  98 Batch   73/269   train_loss = 3.717\n",
      "Epoch  98 Batch   74/269   train_loss = 3.671\n",
      "Epoch  98 Batch   75/269   train_loss = 3.883\n",
      "Epoch  98 Batch   76/269   train_loss = 3.832\n",
      "Epoch  98 Batch   77/269   train_loss = 3.411\n",
      "Epoch  98 Batch   78/269   train_loss = 3.925\n",
      "Epoch  98 Batch   79/269   train_loss = 3.942\n",
      "Epoch  98 Batch   80/269   train_loss = 3.842\n",
      "Epoch  98 Batch   81/269   train_loss = 3.733\n",
      "Epoch  98 Batch   82/269   train_loss = 3.865\n",
      "Epoch  98 Batch   83/269   train_loss = 3.604\n",
      "Epoch  98 Batch   84/269   train_loss = 3.944\n",
      "Epoch  98 Batch   85/269   train_loss = 4.108\n",
      "Epoch  98 Batch   86/269   train_loss = 4.112\n",
      "Epoch  98 Batch   87/269   train_loss = 3.695\n",
      "Epoch  98 Batch   88/269   train_loss = 3.867\n",
      "Epoch  98 Batch   89/269   train_loss = 3.741\n",
      "Epoch  98 Batch   90/269   train_loss = 3.575\n",
      "Epoch  98 Batch   91/269   train_loss = 3.817\n",
      "Epoch  98 Batch   92/269   train_loss = 3.876\n",
      "Epoch  98 Batch   93/269   train_loss = 3.632\n",
      "Epoch  98 Batch   94/269   train_loss = 4.012\n",
      "Epoch  98 Batch   95/269   train_loss = 3.622\n",
      "Epoch  98 Batch   96/269   train_loss = 3.594\n",
      "Epoch  98 Batch   97/269   train_loss = 3.717\n",
      "Epoch  98 Batch   98/269   train_loss = 3.971\n",
      "Epoch  98 Batch   99/269   train_loss = 3.811\n",
      "Epoch  98 Batch  100/269   train_loss = 3.764\n",
      "Epoch  98 Batch  101/269   train_loss = 3.818\n",
      "Epoch  98 Batch  102/269   train_loss = 3.575\n",
      "Epoch  98 Batch  103/269   train_loss = 3.681\n",
      "Epoch  98 Batch  104/269   train_loss = 3.619\n",
      "Epoch  98 Batch  105/269   train_loss = 3.849\n",
      "Epoch  98 Batch  106/269   train_loss = 4.021\n",
      "Epoch  98 Batch  107/269   train_loss = 4.020\n",
      "Epoch  98 Batch  108/269   train_loss = 3.743\n",
      "Epoch  98 Batch  109/269   train_loss = 3.864\n",
      "Epoch  98 Batch  110/269   train_loss = 3.825\n",
      "Epoch  98 Batch  111/269   train_loss = 3.840\n",
      "Epoch  98 Batch  112/269   train_loss = 3.900\n",
      "Epoch  98 Batch  113/269   train_loss = 3.737\n",
      "Epoch  98 Batch  114/269   train_loss = 3.668\n",
      "Epoch  98 Batch  115/269   train_loss = 3.613\n",
      "Epoch  98 Batch  116/269   train_loss = 3.510\n",
      "Epoch  98 Batch  117/269   train_loss = 3.613\n",
      "Epoch  98 Batch  118/269   train_loss = 3.875\n",
      "Epoch  98 Batch  119/269   train_loss = 3.627\n",
      "Epoch  98 Batch  120/269   train_loss = 3.733\n",
      "Epoch  98 Batch  121/269   train_loss = 3.706\n",
      "Epoch  98 Batch  122/269   train_loss = 3.688\n",
      "Epoch  98 Batch  123/269   train_loss = 3.564\n",
      "Epoch  98 Batch  124/269   train_loss = 3.971\n",
      "Epoch  98 Batch  125/269   train_loss = 3.515\n",
      "Epoch  98 Batch  126/269   train_loss = 3.474\n",
      "Epoch  98 Batch  127/269   train_loss = 3.796\n",
      "Epoch  98 Batch  128/269   train_loss = 3.648\n",
      "Epoch  98 Batch  129/269   train_loss = 3.647\n",
      "Epoch  98 Batch  130/269   train_loss = 3.808\n",
      "Epoch  98 Batch  131/269   train_loss = 3.404\n",
      "Epoch  98 Batch  132/269   train_loss = 3.803\n",
      "Epoch  98 Batch  133/269   train_loss = 3.513\n",
      "Epoch  98 Batch  134/269   train_loss = 3.907\n",
      "Epoch  98 Batch  135/269   train_loss = 3.593\n",
      "Epoch  98 Batch  136/269   train_loss = 3.402\n",
      "Epoch  98 Batch  137/269   train_loss = 3.794\n",
      "Epoch  98 Batch  138/269   train_loss = 3.546\n",
      "Epoch  98 Batch  139/269   train_loss = 3.724\n",
      "Epoch  98 Batch  140/269   train_loss = 3.845\n",
      "Epoch  98 Batch  141/269   train_loss = 4.141\n",
      "Epoch  98 Batch  142/269   train_loss = 3.524\n",
      "Epoch  98 Batch  143/269   train_loss = 3.685\n",
      "Epoch  98 Batch  144/269   train_loss = 3.902\n",
      "Epoch  98 Batch  145/269   train_loss = 3.690\n",
      "Epoch  98 Batch  146/269   train_loss = 3.938\n",
      "Epoch  98 Batch  147/269   train_loss = 3.501\n",
      "Epoch  98 Batch  148/269   train_loss = 3.893\n",
      "Epoch  98 Batch  149/269   train_loss = 3.621\n",
      "Epoch  98 Batch  150/269   train_loss = 3.902\n",
      "Epoch  98 Batch  151/269   train_loss = 3.771\n",
      "Epoch  98 Batch  152/269   train_loss = 3.729\n",
      "Epoch  98 Batch  153/269   train_loss = 3.901\n",
      "Epoch  98 Batch  154/269   train_loss = 3.683\n",
      "Epoch  98 Batch  155/269   train_loss = 3.613\n",
      "Epoch  98 Batch  156/269   train_loss = 4.121\n",
      "Epoch  98 Batch  157/269   train_loss = 3.860\n",
      "Epoch  98 Batch  158/269   train_loss = 3.767\n",
      "Epoch  98 Batch  159/269   train_loss = 3.834\n",
      "Epoch  98 Batch  160/269   train_loss = 3.617\n",
      "Epoch  98 Batch  161/269   train_loss = 3.900\n",
      "Epoch  98 Batch  162/269   train_loss = 3.872\n",
      "Epoch  98 Batch  163/269   train_loss = 3.625\n",
      "Epoch  98 Batch  164/269   train_loss = 3.901\n",
      "Epoch  98 Batch  165/269   train_loss = 3.910\n",
      "Epoch  98 Batch  166/269   train_loss = 3.733\n",
      "Epoch  98 Batch  167/269   train_loss = 3.512\n",
      "Epoch  98 Batch  168/269   train_loss = 3.699\n",
      "Epoch  98 Batch  169/269   train_loss = 3.979\n",
      "Epoch  98 Batch  170/269   train_loss = 3.516\n",
      "Epoch  98 Batch  171/269   train_loss = 3.845\n",
      "Epoch  98 Batch  172/269   train_loss = 3.526\n",
      "Epoch  98 Batch  173/269   train_loss = 3.785\n",
      "Epoch  98 Batch  174/269   train_loss = 4.033\n",
      "Epoch  98 Batch  175/269   train_loss = 3.737\n",
      "Epoch  98 Batch  176/269   train_loss = 3.671\n",
      "Epoch  98 Batch  177/269   train_loss = 3.818\n",
      "Epoch  98 Batch  178/269   train_loss = 3.960\n",
      "Epoch  98 Batch  179/269   train_loss = 3.787\n",
      "Epoch  98 Batch  180/269   train_loss = 3.622\n",
      "Epoch  98 Batch  181/269   train_loss = 3.816\n",
      "Epoch  98 Batch  182/269   train_loss = 3.661\n",
      "Epoch  98 Batch  183/269   train_loss = 3.606\n",
      "Epoch  98 Batch  184/269   train_loss = 3.756\n",
      "Epoch  98 Batch  185/269   train_loss = 3.800\n",
      "Epoch  98 Batch  186/269   train_loss = 3.758\n",
      "Epoch  98 Batch  187/269   train_loss = 3.850\n",
      "Epoch  98 Batch  188/269   train_loss = 3.588\n",
      "Epoch  98 Batch  189/269   train_loss = 3.694\n",
      "Epoch  98 Batch  190/269   train_loss = 4.081\n",
      "Epoch  98 Batch  191/269   train_loss = 3.753\n",
      "Epoch  98 Batch  192/269   train_loss = 3.803\n",
      "Epoch  98 Batch  193/269   train_loss = 3.740\n",
      "Epoch  98 Batch  194/269   train_loss = 3.864\n",
      "Epoch  98 Batch  195/269   train_loss = 3.753\n",
      "Epoch  98 Batch  196/269   train_loss = 3.722\n",
      "Epoch  98 Batch  197/269   train_loss = 3.972\n",
      "Epoch  98 Batch  198/269   train_loss = 3.720\n",
      "Epoch  98 Batch  199/269   train_loss = 3.881\n",
      "Epoch  98 Batch  200/269   train_loss = 3.597\n",
      "Epoch  98 Batch  201/269   train_loss = 3.743\n",
      "Epoch  98 Batch  202/269   train_loss = 3.568\n",
      "Epoch  98 Batch  203/269   train_loss = 3.700\n",
      "Epoch  98 Batch  204/269   train_loss = 3.813\n",
      "Epoch  98 Batch  205/269   train_loss = 3.742\n",
      "Epoch  98 Batch  206/269   train_loss = 3.673\n",
      "Epoch  98 Batch  207/269   train_loss = 3.687\n",
      "Epoch  98 Batch  208/269   train_loss = 3.822\n",
      "Epoch  98 Batch  209/269   train_loss = 3.880\n",
      "Epoch  98 Batch  210/269   train_loss = 3.586\n",
      "Epoch  98 Batch  211/269   train_loss = 3.711\n",
      "Epoch  98 Batch  212/269   train_loss = 4.045\n",
      "Epoch  98 Batch  213/269   train_loss = 3.619\n",
      "Epoch  98 Batch  214/269   train_loss = 3.677\n",
      "Epoch  98 Batch  215/269   train_loss = 3.913\n",
      "Epoch  98 Batch  216/269   train_loss = 3.926\n",
      "Epoch  98 Batch  217/269   train_loss = 3.610\n",
      "Epoch  98 Batch  218/269   train_loss = 3.776\n",
      "Epoch  98 Batch  219/269   train_loss = 3.483\n",
      "Epoch  98 Batch  220/269   train_loss = 3.925\n",
      "Epoch  98 Batch  221/269   train_loss = 3.560\n",
      "Epoch  98 Batch  222/269   train_loss = 3.761\n",
      "Epoch  98 Batch  223/269   train_loss = 3.639\n",
      "Epoch  98 Batch  224/269   train_loss = 3.838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  98 Batch  225/269   train_loss = 3.876\n",
      "Epoch  98 Batch  226/269   train_loss = 3.700\n",
      "Epoch  98 Batch  227/269   train_loss = 3.444\n",
      "Epoch  98 Batch  228/269   train_loss = 3.786\n",
      "Epoch  98 Batch  229/269   train_loss = 3.959\n",
      "Epoch  98 Batch  230/269   train_loss = 3.830\n",
      "Epoch  98 Batch  231/269   train_loss = 3.698\n",
      "Epoch  98 Batch  232/269   train_loss = 3.726\n",
      "Epoch  98 Batch  233/269   train_loss = 3.852\n",
      "Epoch  98 Batch  234/269   train_loss = 3.745\n",
      "Epoch  98 Batch  235/269   train_loss = 4.060\n",
      "Epoch  98 Batch  236/269   train_loss = 3.628\n",
      "Epoch  98 Batch  237/269   train_loss = 3.276\n",
      "Epoch  98 Batch  238/269   train_loss = 3.607\n",
      "Epoch  98 Batch  239/269   train_loss = 4.020\n",
      "Epoch  98 Batch  240/269   train_loss = 3.571\n",
      "Epoch  98 Batch  241/269   train_loss = 3.936\n",
      "Epoch  98 Batch  242/269   train_loss = 3.612\n",
      "Epoch  98 Batch  243/269   train_loss = 3.903\n",
      "Epoch  98 Batch  244/269   train_loss = 3.685\n",
      "Epoch  98 Batch  245/269   train_loss = 3.743\n",
      "Epoch  98 Batch  246/269   train_loss = 3.390\n",
      "Epoch  98 Batch  247/269   train_loss = 3.759\n",
      "Epoch  98 Batch  248/269   train_loss = 3.900\n",
      "Epoch  98 Batch  249/269   train_loss = 3.681\n",
      "Epoch  98 Batch  250/269   train_loss = 3.528\n",
      "Epoch  98 Batch  251/269   train_loss = 3.925\n",
      "Epoch  98 Batch  252/269   train_loss = 3.875\n",
      "Epoch  98 Batch  253/269   train_loss = 3.643\n",
      "Epoch  98 Batch  254/269   train_loss = 3.635\n",
      "Epoch  98 Batch  255/269   train_loss = 3.731\n",
      "Epoch  98 Batch  256/269   train_loss = 3.676\n",
      "Epoch  98 Batch  257/269   train_loss = 3.576\n",
      "Epoch  98 Batch  258/269   train_loss = 3.539\n",
      "Epoch  98 Batch  259/269   train_loss = 3.543\n",
      "Epoch  98 Batch  260/269   train_loss = 3.832\n",
      "Epoch  98 Batch  261/269   train_loss = 3.815\n",
      "Epoch  98 Batch  262/269   train_loss = 3.641\n",
      "Epoch  98 Batch  263/269   train_loss = 3.490\n",
      "Epoch  98 Batch  264/269   train_loss = 3.910\n",
      "Epoch  98 Batch  265/269   train_loss = 3.752\n",
      "Epoch  98 Batch  266/269   train_loss = 3.667\n",
      "Epoch  98 Batch  267/269   train_loss = 3.737\n",
      "Epoch  98 Batch  268/269   train_loss = 3.908\n",
      "Epoch  99 Batch    0/269   train_loss = 3.675\n",
      "Epoch  99 Batch    1/269   train_loss = 3.517\n",
      "Epoch  99 Batch    2/269   train_loss = 3.620\n",
      "Epoch  99 Batch    3/269   train_loss = 3.602\n",
      "Epoch  99 Batch    4/269   train_loss = 4.057\n",
      "Epoch  99 Batch    5/269   train_loss = 3.754\n",
      "Epoch  99 Batch    6/269   train_loss = 3.546\n",
      "Epoch  99 Batch    7/269   train_loss = 3.440\n",
      "Epoch  99 Batch    8/269   train_loss = 3.860\n",
      "Epoch  99 Batch    9/269   train_loss = 3.447\n",
      "Epoch  99 Batch   10/269   train_loss = 3.577\n",
      "Epoch  99 Batch   11/269   train_loss = 3.701\n",
      "Epoch  99 Batch   12/269   train_loss = 3.425\n",
      "Epoch  99 Batch   13/269   train_loss = 3.707\n",
      "Epoch  99 Batch   14/269   train_loss = 3.567\n",
      "Epoch  99 Batch   15/269   train_loss = 4.011\n",
      "Epoch  99 Batch   16/269   train_loss = 3.645\n",
      "Epoch  99 Batch   17/269   train_loss = 3.736\n",
      "Epoch  99 Batch   18/269   train_loss = 3.693\n",
      "Epoch  99 Batch   19/269   train_loss = 3.704\n",
      "Epoch  99 Batch   20/269   train_loss = 4.050\n",
      "Epoch  99 Batch   21/269   train_loss = 3.878\n",
      "Epoch  99 Batch   22/269   train_loss = 3.667\n",
      "Epoch  99 Batch   23/269   train_loss = 3.818\n",
      "Epoch  99 Batch   24/269   train_loss = 3.491\n",
      "Epoch  99 Batch   25/269   train_loss = 3.957\n",
      "Epoch  99 Batch   26/269   train_loss = 3.749\n",
      "Epoch  99 Batch   27/269   train_loss = 3.638\n",
      "Epoch  99 Batch   28/269   train_loss = 3.718\n",
      "Epoch  99 Batch   29/269   train_loss = 3.786\n",
      "Epoch  99 Batch   30/269   train_loss = 4.073\n",
      "Epoch  99 Batch   31/269   train_loss = 3.740\n",
      "Epoch  99 Batch   32/269   train_loss = 3.662\n",
      "Epoch  99 Batch   33/269   train_loss = 3.372\n",
      "Epoch  99 Batch   34/269   train_loss = 3.689\n",
      "Epoch  99 Batch   35/269   train_loss = 3.473\n",
      "Epoch  99 Batch   36/269   train_loss = 3.519\n",
      "Epoch  99 Batch   37/269   train_loss = 3.809\n",
      "Epoch  99 Batch   38/269   train_loss = 3.441\n",
      "Epoch  99 Batch   39/269   train_loss = 3.775\n",
      "Epoch  99 Batch   40/269   train_loss = 3.674\n",
      "Epoch  99 Batch   41/269   train_loss = 3.614\n",
      "Epoch  99 Batch   42/269   train_loss = 4.062\n",
      "Epoch  99 Batch   43/269   train_loss = 3.967\n",
      "Epoch  99 Batch   44/269   train_loss = 3.778\n",
      "Epoch  99 Batch   45/269   train_loss = 3.477\n",
      "Epoch  99 Batch   46/269   train_loss = 3.760\n",
      "Epoch  99 Batch   47/269   train_loss = 3.367\n",
      "Epoch  99 Batch   48/269   train_loss = 3.921\n",
      "Epoch  99 Batch   49/269   train_loss = 3.871\n",
      "Epoch  99 Batch   50/269   train_loss = 3.849\n",
      "Epoch  99 Batch   51/269   train_loss = 3.805\n",
      "Epoch  99 Batch   52/269   train_loss = 3.743\n",
      "Epoch  99 Batch   53/269   train_loss = 4.006\n",
      "Epoch  99 Batch   54/269   train_loss = 3.681\n",
      "Epoch  99 Batch   55/269   train_loss = 3.715\n",
      "Epoch  99 Batch   56/269   train_loss = 3.505\n",
      "Epoch  99 Batch   57/269   train_loss = 3.953\n",
      "Epoch  99 Batch   58/269   train_loss = 3.579\n",
      "Epoch  99 Batch   59/269   train_loss = 3.636\n",
      "Epoch  99 Batch   60/269   train_loss = 3.875\n",
      "Epoch  99 Batch   61/269   train_loss = 3.938\n",
      "Epoch  99 Batch   62/269   train_loss = 3.579\n",
      "Epoch  99 Batch   63/269   train_loss = 3.888\n",
      "Epoch  99 Batch   64/269   train_loss = 3.625\n",
      "Epoch  99 Batch   65/269   train_loss = 3.733\n",
      "Epoch  99 Batch   66/269   train_loss = 3.626\n",
      "Epoch  99 Batch   67/269   train_loss = 3.630\n",
      "Epoch  99 Batch   68/269   train_loss = 3.748\n",
      "Epoch  99 Batch   69/269   train_loss = 3.643\n",
      "Epoch  99 Batch   70/269   train_loss = 3.564\n",
      "Epoch  99 Batch   71/269   train_loss = 3.832\n",
      "Epoch  99 Batch   72/269   train_loss = 3.525\n",
      "Epoch  99 Batch   73/269   train_loss = 3.700\n",
      "Epoch  99 Batch   74/269   train_loss = 3.646\n",
      "Epoch  99 Batch   75/269   train_loss = 3.880\n",
      "Epoch  99 Batch   76/269   train_loss = 3.841\n",
      "Epoch  99 Batch   77/269   train_loss = 3.395\n",
      "Epoch  99 Batch   78/269   train_loss = 3.900\n",
      "Epoch  99 Batch   79/269   train_loss = 3.921\n",
      "Epoch  99 Batch   80/269   train_loss = 3.847\n",
      "Epoch  99 Batch   81/269   train_loss = 3.723\n",
      "Epoch  99 Batch   82/269   train_loss = 3.820\n",
      "Epoch  99 Batch   83/269   train_loss = 3.594\n",
      "Epoch  99 Batch   84/269   train_loss = 3.918\n",
      "Epoch  99 Batch   85/269   train_loss = 4.095\n",
      "Epoch  99 Batch   86/269   train_loss = 4.084\n",
      "Epoch  99 Batch   87/269   train_loss = 3.684\n",
      "Epoch  99 Batch   88/269   train_loss = 3.840\n",
      "Epoch  99 Batch   89/269   train_loss = 3.719\n",
      "Epoch  99 Batch   90/269   train_loss = 3.574\n",
      "Epoch  99 Batch   91/269   train_loss = 3.828\n",
      "Epoch  99 Batch   92/269   train_loss = 3.866\n",
      "Epoch  99 Batch   93/269   train_loss = 3.617\n",
      "Epoch  99 Batch   94/269   train_loss = 4.008\n",
      "Epoch  99 Batch   95/269   train_loss = 3.598\n",
      "Epoch  99 Batch   96/269   train_loss = 3.588\n",
      "Epoch  99 Batch   97/269   train_loss = 3.703\n",
      "Epoch  99 Batch   98/269   train_loss = 3.988\n",
      "Epoch  99 Batch   99/269   train_loss = 3.804\n",
      "Epoch  99 Batch  100/269   train_loss = 3.764\n",
      "Epoch  99 Batch  101/269   train_loss = 3.810\n",
      "Epoch  99 Batch  102/269   train_loss = 3.583\n",
      "Epoch  99 Batch  103/269   train_loss = 3.714\n",
      "Epoch  99 Batch  104/269   train_loss = 3.644\n",
      "Epoch  99 Batch  105/269   train_loss = 3.831\n",
      "Epoch  99 Batch  106/269   train_loss = 4.027\n",
      "Epoch  99 Batch  107/269   train_loss = 4.022\n",
      "Epoch  99 Batch  108/269   train_loss = 3.755\n",
      "Epoch  99 Batch  109/269   train_loss = 3.873\n",
      "Epoch  99 Batch  110/269   train_loss = 3.794\n",
      "Epoch  99 Batch  111/269   train_loss = 3.860\n",
      "Epoch  99 Batch  112/269   train_loss = 3.900\n",
      "Epoch  99 Batch  113/269   train_loss = 3.730\n",
      "Epoch  99 Batch  114/269   train_loss = 3.662\n",
      "Epoch  99 Batch  115/269   train_loss = 3.609\n",
      "Epoch  99 Batch  116/269   train_loss = 3.510\n",
      "Epoch  99 Batch  117/269   train_loss = 3.596\n",
      "Epoch  99 Batch  118/269   train_loss = 3.845\n",
      "Epoch  99 Batch  119/269   train_loss = 3.613\n",
      "Epoch  99 Batch  120/269   train_loss = 3.727\n",
      "Epoch  99 Batch  121/269   train_loss = 3.714\n",
      "Epoch  99 Batch  122/269   train_loss = 3.737\n",
      "Epoch  99 Batch  123/269   train_loss = 3.495\n",
      "Epoch  99 Batch  124/269   train_loss = 3.980\n",
      "Epoch  99 Batch  125/269   train_loss = 3.496\n",
      "Epoch  99 Batch  126/269   train_loss = 3.429\n",
      "Epoch  99 Batch  127/269   train_loss = 3.794\n",
      "Epoch  99 Batch  128/269   train_loss = 3.655\n",
      "Epoch  99 Batch  129/269   train_loss = 3.658\n",
      "Epoch  99 Batch  130/269   train_loss = 3.786\n",
      "Epoch  99 Batch  131/269   train_loss = 3.392\n",
      "Epoch  99 Batch  132/269   train_loss = 3.806\n",
      "Epoch  99 Batch  133/269   train_loss = 3.534\n",
      "Epoch  99 Batch  134/269   train_loss = 3.896\n",
      "Epoch  99 Batch  135/269   train_loss = 3.605\n",
      "Epoch  99 Batch  136/269   train_loss = 3.409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  99 Batch  137/269   train_loss = 3.817\n",
      "Epoch  99 Batch  138/269   train_loss = 3.595\n",
      "Epoch  99 Batch  139/269   train_loss = 3.739\n",
      "Epoch  99 Batch  140/269   train_loss = 3.853\n",
      "Epoch  99 Batch  141/269   train_loss = 4.118\n",
      "Epoch  99 Batch  142/269   train_loss = 3.541\n",
      "Epoch  99 Batch  143/269   train_loss = 3.676\n",
      "Epoch  99 Batch  144/269   train_loss = 3.896\n",
      "Epoch  99 Batch  145/269   train_loss = 3.695\n",
      "Epoch  99 Batch  146/269   train_loss = 3.931\n",
      "Epoch  99 Batch  147/269   train_loss = 3.483\n",
      "Epoch  99 Batch  148/269   train_loss = 3.873\n",
      "Epoch  99 Batch  149/269   train_loss = 3.636\n",
      "Epoch  99 Batch  150/269   train_loss = 3.902\n",
      "Epoch  99 Batch  151/269   train_loss = 3.777\n",
      "Epoch  99 Batch  152/269   train_loss = 3.739\n",
      "Epoch  99 Batch  153/269   train_loss = 3.818\n",
      "Epoch  99 Batch  154/269   train_loss = 3.703\n",
      "Epoch  99 Batch  155/269   train_loss = 3.601\n",
      "Epoch  99 Batch  156/269   train_loss = 4.122\n",
      "Epoch  99 Batch  157/269   train_loss = 3.870\n",
      "Epoch  99 Batch  158/269   train_loss = 3.748\n",
      "Epoch  99 Batch  159/269   train_loss = 3.848\n",
      "Epoch  99 Batch  160/269   train_loss = 3.635\n",
      "Epoch  99 Batch  161/269   train_loss = 3.895\n",
      "Epoch  99 Batch  162/269   train_loss = 3.842\n",
      "Epoch  99 Batch  163/269   train_loss = 3.635\n",
      "Epoch  99 Batch  164/269   train_loss = 3.910\n",
      "Epoch  99 Batch  165/269   train_loss = 3.928\n",
      "Epoch  99 Batch  166/269   train_loss = 3.745\n",
      "Epoch  99 Batch  167/269   train_loss = 3.512\n",
      "Epoch  99 Batch  168/269   train_loss = 3.699\n",
      "Epoch  99 Batch  169/269   train_loss = 3.982\n",
      "Epoch  99 Batch  170/269   train_loss = 3.554\n",
      "Epoch  99 Batch  171/269   train_loss = 3.816\n",
      "Epoch  99 Batch  172/269   train_loss = 3.520\n",
      "Epoch  99 Batch  173/269   train_loss = 3.786\n",
      "Epoch  99 Batch  174/269   train_loss = 3.920\n",
      "Epoch  99 Batch  175/269   train_loss = 3.760\n",
      "Epoch  99 Batch  176/269   train_loss = 3.673\n",
      "Epoch  99 Batch  177/269   train_loss = 3.801\n",
      "Epoch  99 Batch  178/269   train_loss = 3.978\n",
      "Epoch  99 Batch  179/269   train_loss = 3.755\n",
      "Epoch  99 Batch  180/269   train_loss = 3.605\n",
      "Epoch  99 Batch  181/269   train_loss = 3.897\n",
      "Epoch  99 Batch  182/269   train_loss = 3.667\n",
      "Epoch  99 Batch  183/269   train_loss = 3.596\n",
      "Epoch  99 Batch  184/269   train_loss = 3.735\n",
      "Epoch  99 Batch  185/269   train_loss = 3.811\n",
      "Epoch  99 Batch  186/269   train_loss = 3.725\n",
      "Epoch  99 Batch  187/269   train_loss = 3.828\n",
      "Epoch  99 Batch  188/269   train_loss = 3.568\n",
      "Epoch  99 Batch  189/269   train_loss = 3.712\n",
      "Epoch  99 Batch  190/269   train_loss = 4.086\n",
      "Epoch  99 Batch  191/269   train_loss = 3.753\n",
      "Epoch  99 Batch  192/269   train_loss = 3.788\n",
      "Epoch  99 Batch  193/269   train_loss = 3.756\n",
      "Epoch  99 Batch  194/269   train_loss = 3.844\n",
      "Epoch  99 Batch  195/269   train_loss = 3.760\n",
      "Epoch  99 Batch  196/269   train_loss = 3.687\n",
      "Epoch  99 Batch  197/269   train_loss = 3.939\n",
      "Epoch  99 Batch  198/269   train_loss = 3.731\n",
      "Epoch  99 Batch  199/269   train_loss = 3.879\n",
      "Epoch  99 Batch  200/269   train_loss = 3.583\n",
      "Epoch  99 Batch  201/269   train_loss = 3.740\n",
      "Epoch  99 Batch  202/269   train_loss = 3.573\n",
      "Epoch  99 Batch  203/269   train_loss = 3.682\n",
      "Epoch  99 Batch  204/269   train_loss = 3.793\n",
      "Epoch  99 Batch  205/269   train_loss = 3.733\n",
      "Epoch  99 Batch  206/269   train_loss = 3.665\n",
      "Epoch  99 Batch  207/269   train_loss = 3.646\n",
      "Epoch  99 Batch  208/269   train_loss = 3.809\n",
      "Epoch  99 Batch  209/269   train_loss = 3.890\n",
      "Epoch  99 Batch  210/269   train_loss = 3.601\n",
      "Epoch  99 Batch  211/269   train_loss = 3.726\n",
      "Epoch  99 Batch  212/269   train_loss = 4.021\n",
      "Epoch  99 Batch  213/269   train_loss = 3.602\n",
      "Epoch  99 Batch  214/269   train_loss = 3.657\n",
      "Epoch  99 Batch  215/269   train_loss = 3.942\n",
      "Epoch  99 Batch  216/269   train_loss = 3.913\n",
      "Epoch  99 Batch  217/269   train_loss = 3.656\n",
      "Epoch  99 Batch  218/269   train_loss = 3.795\n",
      "Epoch  99 Batch  219/269   train_loss = 3.475\n",
      "Epoch  99 Batch  220/269   train_loss = 3.919\n",
      "Epoch  99 Batch  221/269   train_loss = 3.557\n",
      "Epoch  99 Batch  222/269   train_loss = 3.814\n",
      "Epoch  99 Batch  223/269   train_loss = 3.596\n",
      "Epoch  99 Batch  224/269   train_loss = 3.821\n",
      "Epoch  99 Batch  225/269   train_loss = 3.885\n",
      "Epoch  99 Batch  226/269   train_loss = 3.691\n",
      "Epoch  99 Batch  227/269   train_loss = 3.408\n",
      "Epoch  99 Batch  228/269   train_loss = 3.791\n",
      "Epoch  99 Batch  229/269   train_loss = 4.004\n",
      "Epoch  99 Batch  230/269   train_loss = 3.807\n",
      "Epoch  99 Batch  231/269   train_loss = 3.686\n",
      "Epoch  99 Batch  232/269   train_loss = 3.735\n",
      "Epoch  99 Batch  233/269   train_loss = 3.838\n",
      "Epoch  99 Batch  234/269   train_loss = 3.759\n",
      "Epoch  99 Batch  235/269   train_loss = 4.018\n",
      "Epoch  99 Batch  236/269   train_loss = 3.625\n",
      "Epoch  99 Batch  237/269   train_loss = 3.292\n",
      "Epoch  99 Batch  238/269   train_loss = 3.618\n",
      "Epoch  99 Batch  239/269   train_loss = 3.990\n",
      "Epoch  99 Batch  240/269   train_loss = 3.562\n",
      "Epoch  99 Batch  241/269   train_loss = 3.920\n",
      "Epoch  99 Batch  242/269   train_loss = 3.584\n",
      "Epoch  99 Batch  243/269   train_loss = 3.859\n",
      "Epoch  99 Batch  244/269   train_loss = 3.696\n",
      "Epoch  99 Batch  245/269   train_loss = 3.684\n",
      "Epoch  99 Batch  246/269   train_loss = 3.373\n",
      "Epoch  99 Batch  247/269   train_loss = 3.780\n",
      "Epoch  99 Batch  248/269   train_loss = 3.867\n",
      "Epoch  99 Batch  249/269   train_loss = 3.664\n",
      "Epoch  99 Batch  250/269   train_loss = 3.537\n",
      "Epoch  99 Batch  251/269   train_loss = 3.929\n",
      "Epoch  99 Batch  252/269   train_loss = 3.880\n",
      "Epoch  99 Batch  253/269   train_loss = 3.637\n",
      "Epoch  99 Batch  254/269   train_loss = 3.630\n",
      "Epoch  99 Batch  255/269   train_loss = 3.731\n",
      "Epoch  99 Batch  256/269   train_loss = 3.662\n",
      "Epoch  99 Batch  257/269   train_loss = 3.579\n",
      "Epoch  99 Batch  258/269   train_loss = 3.517\n",
      "Epoch  99 Batch  259/269   train_loss = 3.542\n",
      "Epoch  99 Batch  260/269   train_loss = 3.877\n",
      "Epoch  99 Batch  261/269   train_loss = 3.800\n",
      "Epoch  99 Batch  262/269   train_loss = 3.642\n",
      "Epoch  99 Batch  263/269   train_loss = 3.502\n",
      "Epoch  99 Batch  264/269   train_loss = 3.887\n",
      "Epoch  99 Batch  265/269   train_loss = 3.746\n",
      "Epoch  99 Batch  266/269   train_loss = 3.658\n",
      "Epoch  99 Batch  267/269   train_loss = 3.711\n",
      "Epoch  99 Batch  268/269   train_loss = 3.879\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return [loaded_graph.get_tensor_by_name(name + \":0\") for name in [\"input\", \"initial_state\", \"final_state\", \"probs\"]]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Ugh. Different values for test vs the generate below.\n",
    "    p = probabilities\n",
    "    if probabilities.ndim > 1:\n",
    "        p = np.reshape(probabilities, probabilities.size)\n",
    "    idx = (np.random.choice(np.arange(p.size), 1, True, p))[0]\n",
    "    return int_to_vocab[idx]\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "moe_szyslak:--\n",
      "homer_simpson:(singing) ayyy! hey(happy noise chuckle) i'll never follow me with it a beer that isotopes!\n",
      "moe_szyslak:(back in me in, my wallet!\n",
      "joey_kramer: miracle tree!\n",
      "carl_carlson: before my cruiser.\n",
      "homer_simpson: no, sweetheart. i had i'll teach you?\n",
      "carl_carlson: oh thanks, that's it, if i get your wife out with nothin'. while!\n",
      "homer_simpson: who'll gee, let me you all two there might be gentle!\n",
      "don't help, always here sure over cars guy in the bow for keeping out, the clientele dress up out back, i had handed 'em for you get him, sisters i never throw wanted, great with here?\n",
      "carl_carlson: come about me!\n",
      "homer_simpson:(victorious chuckle)\n",
      "moe_szyslak: whoa!\n",
      "barney_gumble: i'm selling, the matter, but who be bedridden for an english is a lot time.. i've gotta teach ya!\n",
      "\n",
      "\n",
      "homer_simpson:(leans from tv his favorite) hot blooded / i ain't real register easy, homer here\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        pred_word = pick_word(probabilities[(0, dyn_seq_length-1)], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
